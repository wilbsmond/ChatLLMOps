[
    {
        "title": "Large Language Models in Production Round-table Conversation",
        "video_id": "rpjLTHrl-S4",
        "video_url": "https://www.youtube.com/watch?v=rpjLTHrl-S4",
        "length": "57:21",
        "transcript": "hey everybody super glad that you are listening to this large language model in production Round Table I want to mention before we jump into it that because of the success of this round table we ended up organizing a full-on conference it's a virtual conference it will be held on April 13th and you can register now we're going to have incredible speakers like the founder of human Loop the creator of Lang chain just to name a few and also the CTO of adept AI they just raised 250 million dollars wow all right so if you want to register for that check out the link in the description and we will see you there on April 13th I'm not gonna chat for too long I just want to talk real fast about the survey that we just put out in the mlops community this survey is kind of like playing off of what we're about to talk about and I'm fascinated by like using large language models in production and how people are doing it if they are doing it and if you're not why are you not so please feel free to fill out that survey it'll take like literally five minutes it's the most basic thing ever and that is my like one call to action that I have for everyone before we jump into this great conversation I'm going to hand it over to Diego man I appreciate you running this so we will talk soon I'll be back in a bit excited to have everybody here a great subject great amount of experts so excited about that the next hour we're going to be chatting about large language models so I figured we kind of start really quickly with a quick lightning round of introductions uh Rebecca we'll we'll start with you hi everyone yeah super excited to be here and about this discussion um I'm Rebecca I'm a research engineer at Facebook AI research and so I work on primarily NLP are focusing on the robustness of large language models I also have worked on um fairness and responsible AR um aspective um of MLP as well excellent how about you David hey hey uh good to see everybody I'm a vice president at unusual Ventures VC fund I focus on our investments in ml so I've been thinking a lot about language models and have spent before that like uh six-ish years working on ML Ops across a couple of startups techcon and determined Ai and uh producers at Ford where I did ml platform stuff like like a lot of folks I've talked to in the community so honest hi um my name is honest I'm a machine learning engineer at digits um at the great chance to write machine learning related Publications um one about natural language processing and then about envelopes and we use uh language models in production to assist the company so we're digits are storing is like basically a real-time engine to understand books for business owners and accountants and in basically no time and we use those language models to help uh start the conversation between accountants and the business owners so we can talk about the details later excellent and James everyone uh really started to be here I'm James CEO and co-founder of Bountiful and we're building a monitoring and testing tool for foundation model workflows so one tool for FM app developers that allows them to actually observe and then test the cost quality and latency across their workflows excellent great and I'm Diego Oppenheimer I'll be your opinionated moderator today I uh to give you I'm currently serve as a partner at factory HQ which is a venture fund uh that specializes in AI Investments previous to that I was an executive running on the lops for a company called Data robot and previous to that I was the CEO and founder of algorithmia which was one of the first mlops companies and also is lucky enough to be part of this in my labs Community since I believe there was double digits uh of people in the in the community so uh it's been really really exciting time to see all the hard work that Dimitris and pellen Etc and friends have put into this guy because well what up what size are we at now Dimitrius because it's a it's a cross fifteen thousand right yeah it's around 15K it's wild easy amazing so uh really excited so um a lot a lot of hype around Chachi Pichi and kind of what these large language models look like and so I figured that we could start with grounding on like what do we actually mean by a large language model I'm gonna pick on Rebecca on this one uh to kind of uh you know Define uh what's what's the proper definition of what we would look at as a large language model sure yeah so this is a bit of a contrarian opinion but I think this is actually really poorly defined and this is uh just a new term for you know what we can say is transfer learning which is actually existed for decades but I can give people a bit of an overview um for those who aren't as familiar with the background and history of ml so starting in the 1990s this is when people started talking about machine learning and artificial intelligence and this was a period of smaller statistical models and gradually in 2010 we start seeing deep learning so Indiana coins term so we had larger data sets more computation we'll see higher level features emerge through training on these data sets and so really this is a process of homogenization so rather having bespoke feature engineering type plans for each application like you see that you could have the same neural network well deep neural network architecture use it for a lot of different applications and this process of homogenization has continued and that's really what we see leading up to today where we have a really powerful large language models can be used for a variety of applications and so I'd say another inflection point was 2017 when we had the transfer Wars Paper like the attention is I need paper and then in 2018 we saw the Burt model so then he's um you know the years from 2018 until now we saw so much like exponential growth and NLP and mostly happening on the research side but now on the production side as well so it's been a super exciting time for LLP um you know 2018 we had Bert and then 2019 we had the Roberta paper which showed that a few um train with architecture about like 10x data set you actually can beat Bert on pretty much like all the tasks of the blue benchmark and then we just saw you know this is this is like what showed that a scaling matters and scaling is really powerful and then we just saw this influx of you know money and resources being poured towards training large language models so we had Brit large with 340 million parameters gpd2 with 1.5 billion parameters T5 with 11 billion gpt3 with 175 billion which I imagine a lot of people here are probably using for your apps and and now we we have like really powerful guys from opening eye that we can use and I would say that this is um more of a succession of just us moving from very specific models for specific tasks to large pre-trained models that we can use for a variety of tasks so yeah I would say that this idea that's really Central to Foundation models or large language models is this idea of transfer learning which is that you know you can train a model on a lot of information and you can use it for a variety of Downstream tasks so instead of having like one model for sentiment classification and one model for question answering and one model for maybe like ranking your ads you would just have one large pre-trained model that's very powerful and you can just apply it with very minimal changes to a variety of accounts from tasks okay so I guess to clarify a couple of things we look at it as you know large language models is a subset of foundational models right because I think foundational models in general would actually apply to not just language models but also kind of like image multi-model whatever is coming out there's some characteristics to these large language foundational models I think in general we can call them that they're no code in terms of like we access them via natural language which really expands the ability for people to use them right I mean I think that's really what's one of the biggest things that's inspiring the second kind of what you mentioned Rebecca they're kind of you know there are no tasks right so they're actually general you know pretty General they're not task specific and they generalize quite well and I guess we're still figuring out to what other tasks they generalize on on a daily basis and I think one thing that that I know which is I think also really interesting is that they don't really suffer from a traditional ml cold Stark problem right which is like you can actually start working with them without any sort of like pre-trained data so one thing and I know you said you had a contrarian review but like one of the things that had been like thrown out there is like what do we consider large and it feels like some people say like okay like the billion parameter kind of like uh you know line is the you know we'll dry it it felt kind of of arbitrary I mean it sounds like a nice little round number but uh you know do you have any comment on that yeah yeah so um I I think this is uh also super interesting I'm sure people have seen the the Llama models coming from Facebook um but now we're actually seeing them maybe um you can do more with smaller models but just train it on more data um so for anyone who's curious you can look up the chinchilla paper so there's this idea that models have to be a chinchilla optimal which means that if you have like a one billion parameter model you have to train on x amount of data but then if you increase the number of parameters you have to train on maybe like 10x or a thousand extra data for it to be chinchilla optimal so um really it's not just about the parameter size it's also about the amount of data that it was trained on because a lot of I'd say the first generation of these large models were under trained and that's uh what researchers are realizing right now got it awesome great so so we have these you know access to these really large models uh you know and I think there's a uh I think it's kind of funny because we went from you know very very specific like task models tons of training data hey we're gonna go do this this and this and now to the more generalized models to now like really or say well actually what we really want to do is kind of like the generalized models but also combine them with smaller models and I think a lot of the reasons why combining the smaller models is really around okay these things are actually really hard to put into production they cost a fortune and the kind of like economics are slightly flipped right I think like anybody who's um I've spent almost a decade looking at just production problems and as soon as we kind of looked at these like llms I was like I I don't like how where like how we're gonna make businesses out of it but cool thing is here is that hot is actually spent quite a bit of time doing some of that work of like okay how do we actually make decisions do we use apis we use our own models do we continually pre-train our own model so I just I'd love to give you the floor in time to talk to us about like your use case whatever you're willing to share but also kind of like what the thought process was right and how you broke down getting to uh you know a a specific in production use case here yeah um so there's like we use large language models in a variety of cases like we have this case of like regenerating some text for for some users we use like language models to Define very custom embeddings and I think to me that that is some beauty right now like in fact like five years ago we said like theater is a new oral with the large language models because there's so much work already pre like they're pre-trained it's more important to have clean and good smaller data sets so we can imagine that very custom use cases so it's not like that the quantity of the data anymore where we can bring them into production I think Rebecca made a really good point you need a lot of like data to train it to get to your initial stage but once you start reusing them then you can find this with a lot of uh with smaller data sets so that there's a the number one would be like there's a good motivation to use uh those uh language models because we can get to a production ready model with this smaller sample sets I remember like back in the day like 2015 uh and so on we were working on there on on our similarity model where we didn't have large language models and we spend a lot of time like curating the data just to get to initial state the other thing is like when we talk about like large language models in terms of generating texts we need to think about like how do we integrate this into the product so for example we need to protect the users against hallucinations or against like weird generated texts and so what we do on the digit side when we generate a text it goes through a very stringent not safer work filter if it's like the slightest rating there we store this several away and then we have we check constantly against patterns of hallucinations and if it follows into one of those patterns we also throw the sample and then um we make sure in our app whatever we show is like reviewed by an accounting before they actually send it to their client and then that is another layer of like there's a human in the loop who's like who could Satan this makes sense that doesn't make sense but at the same time um those large language models allow to integrate amazing applications so for example we're helping business owners to understand their their books in real time but accounting is highly subjective so we use large language ones to understand it so to understand the subjectivity with large language models and then we use like we train like similarity-based in machine learning models or we use those generated from AI some of uh the decisions you made previous to starting to work with these models so I think like it really really what I'd love to understand a little bit better is like you have the option of using potentially an API you add the option of grabbing some of these open source models that exist today maybe actually is it worth like for the panel actually like talking about what exists in the open source Community because I think a lot of people are unaware that there's a huge movement that is uh you know it's not just open AI building you know these models or the actually like big cloud providers but there's actually quite a big community of researchers who built open source large language models like Luther um and a couple others so um I'll actually let any of the panelists anything else want to kind of like take that one and uh you know because it sounds like you've all looked at it so uh I don't know if uh I got any volunteers here no pipe up right yeah yeah a lot of time kind of in the open source communities I mean number one it's unbelievable uh some of the capabilities that are kind of communities of people are are turning out I think I'm gonna actually message in the chat a few key Discord servers that I think literally just jumping into summarize some of the best work if you want to go and understand the open source pushes and so yeah you've got to lose the REI you've got copper AI who are championing some of the the attempts to kind of build open source alternatives to some of the largest models uh lion as well doing really interesting work I think look I'm I'm not a developer so I'd love kind of alongside me Hannah's or anyone else to kind of give a developer's viewpoint on this for sure when talking to companies who are building at the application layer right I think they start at least in in our experience they start building on top of apis and then they start to consider okay there's this sea of Open Source equivalent models that I could be using that might give me some Advantage based on what I really care about for my users let me start investigating this right and a great place to start is literally jumping into those three Discord servers to understand what's being done and obviously hugging face as well is kind of a lineup of what models exist and what could be used great so one of the things to talk about and then I'll I'll pass this on to harness right so these models are available for download which are great I mean they're not as easy as accessing them as like you know hitting an API in a lot of cases you're gonna have to do some Mainline pre-training or some fine-tuning yourself to actually make them usable but they're portable right and they're not relying on just being behind an API so when you have a lot of use cases that are potentially not good for just using somebody API and sending data these are definitely options so you're not kind of boxed out of using llms because of it so with that Preamble honest love to like you know walk us through the thought process here of and you don't have to give up obviously any secrets but would love to understand like considerations how you thought about the problem like what were those qualifications and then you know kind of like we can walk into like how you did it yeah there's no there's no secrets I would say like we actually in fact we published a blog post about like how we implemented it so somebody wants to Repro this go ahead and Conquer um yeah but um so when we saw this business problem with generated questions for example um obviously people go to the open AI playground to try it out and you see like okay is it actually visible with a few prompts and and then the person that was like can we actually Repro those things with an off-the-shelf a model which we could host in a house because the thinking there is like if that is possible then we have the opportunity to quantize and prune the model to reduce the latency and we have this major benefit of that not handing over the data to the to an external API which we're not in control over anymore and so for us with DNS financial data this is really Paramount to aerating video we promise our customers that we we're trying to like we're keeping everything secure and in-house so even going to open AI is like it's that burden is tremendous and would change like even if we have a ramification of the terms and conditions with our customers and therefore there's a huge motivation to like every used open source models and hosts them ourselves there's another benefit of like you have lower networks if you have the model sitting right next to you and your instance um there's like lots of cool benefits but to what I did what we were saying was like you do have to point changing the model um but in that moment you also have like lots of opportunities to like make those models smaller and make the domain specific um make them useful to your specific use case and the model can then in the moment forget about all the other like these kids are in the Stream So speaking of uh use cases and you know you kind of mentioned one word like very particular needed to be able to kind of bring get in-house like David you've been looking at uh a bunch of things you've been looking at the developer ecosystem David also wrote a pretty cool blog post uh about the developer ecosystem uh but like what use cases are you seeing and kind of how do uh maybe kind of talk a little bit about I don't think about when to use let's focus on production right and production use cases and what you know when to go in One Direction when to go in the other direction what you're seeing that's really interesting right now yeah I I guess like I I have a framework that I used to think about this which is like okay so we just talked about Foundation models and they've been coming for a while but like this capability kind of got dropped in our lap magically by open AI a couple years ago and so like when you when you think about what's being used in production now I think a lot of how to think about it is just like how much time have we had to execute with this amazing new technology we have and so I frame everything to like how long does it actually take to build it and so the obvious stuff that you saw in production first I think is like the most simple application of language models and stuff like Jasper and Poppy AI that are completing text right or in the like sales category like Reggie and autobound books that are kind of like generating sales copy and sales emails and things like that and so why it's the easiest thing like you the production is like basically just a call to open AI you add one API called your application and you're good and and you can do really incredible things and so a lot of the stuff that requires no infrastructure you see really successful in prod right away and then you get like one layer down like what's the next level of complexity as you try to do things I think like the most obvious stuff is okay we want to incorporate external information about the world or about our users into these models uh sort of like what honest was just talking about in one sense like you need to understand what's going on in the world to be able to do anything in accounting or documents or spreadsheets or whatever maybe and so there you have to start like building infrastructure databases Integrations pulling data from whatever sources you need and there's just like the fundamental time it takes to do engineering and so I think we're in the moment right now where a lot of what we're seeing come to production are tools that where people finally finish that engineering workload that it takes to like make all that happen and it even could be some not to fine-tuning and things like that but for the most part I think it's like just like routing stuff from a database connect to slack or whatever you know so like in that category like yourself like chat GPT it requires a little bit of context of like a whole conversation and rambling that or all of the bespoke chat Bots that got announced when they announced their new API last week a couple of folks that I've worked with like nem which is a competitive notion reads your historical documents and emails and stuff like that to help like complete um but do tax completion but more personalized to you working with a company that's in our portfolio called apt Edge who do this awesome like customer service work where they have to connect to like jira tickets and zendesk tickets and slack to like know what's going on so if that's like my tier two of complexity and for the most part people like in that year still aren't doing the amazing work that Haas has done to like actually go grab a model fine tune it that's hard work still and this stuff is like kind of somewhat straightforward even if it's more complicated so I think of like what's the next tier of complexity and uh I think there's like two big categories you can get to one is um like increasing model complexity so like being chains of calls where you like want to do agents and you want to call it to a language model and get a response and parse the response and call it the language model again and that just kind of gets on unwieldy still like we just haven't really done all like it's just actually kind of hard to be confident in the systems you build with that in a way where you're like ready to put in front of users I don't think a lot of people have gotten there yet so there's like these open source Frameworks like Lane chain and stuff out there where a lot of people are doing cool stuff that making chains but trusting that I think this is probably why James exists is because like trusting some of that back and forth with a model is still challenging and I think this is another engineering Gap where I think it's just going to take us time as a community and as a set of developers to figure out like how do you do it and how do you manage it and what you can trust yeah and the other side stuff like automation where you're like you know like a lot of this stuff you see now is co-pilot honest talked about this where you like you have a human in the loop to validate and that's because like the same thing or it's like hard to build all the tools we need to trust models right now but uh but where we want to go is automation where we have llms doing everything but that again requires just a ton of engineering work and going on that route like that's where people really think fine-tuning is necessary you kind of have or rlhf like the the more complicated methods of of owning and tuning models that you can trust it's yeah I talk about like there's a big difference when you give llm the keys to the car versus the autopilot uh there's a lot more things that can happen and so I guess like long answer but I think there's just a what we're seeing is more and more engineering complexity getting layered on to this amazing ml thing and and use cases spawn up nearly daily because of the amazing progress we're going getting uh but you can sort of track it on a on a straight line of how complex people are willing to go I think the next Frontier to what house is talking about is okay can we Allen weights can we distill models can we do lower latency work because there's so yeah that's a ton of engineering complexity some people are bold and on the frontier doing it um but I think a lot of people are still sort of in level one David you bring up a really good point that every little open source language model comes with it uh its own tokenizer as an example right like you have your bird it has like a workpiece tokenization you have your h11 but there's like an even like fitting those pieces together that is a whole Challenge and I totally glanced over this like I'm sorry about this but like once you once you get over this hump and you actually have the models and the weeds in front of you then you can run like for example the chance of a plugin and like see what can I cut out of this language model and make the latency fast and this is something which maybe you don't have to do for the problem but if like if you see a lot of inferences um that investment might pay off and then your this couple would see from your API you get a faster response then you save money long term um but it to your point if it's like this lots of engineering complexity in there so it's like I can see the trade-off product starts it might be easier to code or an API because you get it up and running faster and versus long-term benefits one of the interesting things is yeah Diego mentioned the front that like a lot of what's happened is this like uh democratization where so many people can develop with language models and so it's like oh I just go write a prompt and I've done something cool and man it's really intimidating just like it always has been to go from that to like I'm gonna download weights and learn tensorflow and the tuna model and use the tensorflow profile like I was an ml engineer for a whole bunch of years and I never use the profiler because like getting to that level is kind of scary still you know and I was like I think a decent one baby uh and it's still like just like depths of ml problems like it gets really complicated really fast and so um I think there's a lot of people sort of on the sideline once you get to that level of complexity because it's suddenly like yeah it's like we've told everybody you don't need to learn ml to do ML and then it's like oh never mind there's go fine tune the 13 billion parameter model good luck and then I think there's still a ton of work we have to do to make that uh as accessible as a prompt maybe I was just gonna double tap on that like a bunch of the we we've talked to many dozens of generative AI companies right and most of them are not the people building these companies are not ml developers they're not ml Engineers right they're not ml people they are software Engineers that are trying to build really interesting software by kind of plugging in these incredible pieces of Technology right and they're playing in the tier 2 complexity over that you just explained right whereby okay we're going Beyond tier one which is kind of you might describe as that kind of UI wrapper style like demo of like this is cool I can build on this but to actually building kind of semi-agentic chained systems right that get highly complex very quickly but even jump into that third tier where it's going to do it yourself ml development I think is just a huge barrier so I think we're going to see very large companies built in that tier two of complexity right so so we saw like Jasper enormous company right built on that tier one I think we're going to see a second wave of a huge number of companies that we're observing right now building that tier 2 complexity so we're we're building the tooling that enables people to actually manage that complexity such that they can build really interesting agentic systems and then potentially automated ones and then I think who knows when we get to like tier three right and like anes is done very non-trivial work as an ml developer an ml engineer to get to that stage I think in that two-tier complexity we're seeing a lot of Highly capitalized companies that are just building that tier to a software application players so that's why we're building what we're building is to try to enable them but I think we're going to see a huge population in those companies yeah so I'm gonna try to completely oversimplify kind of like what kind of three categories that we can talk about for llms in production when we talk about trust and interest we'll talk about testing the results security privacy anything that's regarding kind of like you know the you know the ability to use these let's talk about latency and and I'm pretty late and see a category because I think it's completely under appreciated how much latency matters like building any sort of software maybe to your point James about how all these software Engineers are actually working on these problems it might be more present but you know we'll talk about uh you know kind of latency and then we'll talk about cost right and so I think those three things and I think in terms of in all of these three things I think we can break it down into the engineering tools that are going to exist how we're actually going to apply them what people are doing what research is coming in because I think new architectures are coming out unzipped we can start on any one of them right so I'm going to Define I'm going to Define cost as literally what it costs to run and this isn't just about you know the API call like you know we're talking about when it's experimental the hardware what it takes to uh you know lift up that Hardware what it takes to kind of like run that at scale um as well as potentially use the apis obviously I mentioned a little bit about that trust right so trust will talk about privacy security like the use cases there and then uh I'm forgetting the other one was uh latency but latency which is really the definition of the use cases right so um maybe we'll start with like uh latency to begin with and you know James I know this is near and dear to your heart so yeah first of all Define to us like what latency is in these kind of like llm world use cases how do we look how we should look at it and more importantly maybe kind of like touch on some of the problems I think it's an unsolved Problem by the way so like uh I don't expect you to answer it for us but kind of like we can uh you know if you want to kind of like frame it for us yeah absolutely so I think you can observe and think about latency at two levels right one is that the kind of instance level like specific model and then one is it the the workflow level right from a more kind of user perspective because like you're trying to deliver ultimately some kind of user experience that adds value to someone such that you can build an application potentially build a company Etc so when you're cooling let's say you're at that tier two of complexity that David described right you've got a chained kind of workflow of potentially multiple instances of models it's going to give you an idea here I was talking to a Founder yesterday and he got lost walking me through his own kind of uh workflow here and he's calling seven different instances of models right four of them fine-tuned Babbage uh there are a couple of embeddings models in there right and each of those calls based on the apology is bringing in some sort of latency chained together at the workflow kind of level you have the ultimate latency that the user experiences I think that's going to be king right especially in the second level of complexity it's like I need to build especially for those who are building here thinking software right and to your point doe you said like maybe they're thinking more about this than than others they absolutely are so we we've talked about when in our conversations with companies is a cost quality and latency triangle right and depending on your use case you care about one of those more than the other two and like the best the best example of this is code copilot right like about it latency is everything you you have to stay within a flow state for your user experience such that you know I we could they could make those functionalities better but they don't want to because they want to maintain very very low latency that's how I think we need to start thinking about this is okay there's a latency of making one call to one model but then there's a latency of an end-to-end user experience yeah I love how you picked that example because I've like I've many times used that example as the kind of like state-of-the-art in terms of latency and it's pretty darn slow right like in terms of like when I look at right now it's a suggestion but like I want autocomplete and jumping from suggestion to back you know using doing autocomplete will require fairly significant engineering to be able to get that and actually like my gas and I'm not the expert here is that it will take kind of like re-architecting some of these models or uh you know up from the ground up so I don't know I think we have a question here that people are asking about production I'm a quickly Define production and then I'm going to jump to you Rebecca because I saw you get excited about re-architectures so what we mean about production is actually like an actual application that's being used for users uh you know and I would argue kind of like in a kind of like you know in the production manner so a service that is being like offered to other people that is running all the time this is beyond the demo this is beyond the uh you know kind of like toy weekend you know kind of thing this is like you're actually using it as part of a business process educational process whatever you're using production is uh I usually like to Define it like people scream if it goes down and uh and so I think somebody was asking about like does that count the open API service in Azure absolutely if you're using it as part of a production application like if you're just using it for your boy app at you know on your computer then I wouldn't consider that production so rewinding a little bit we'll go back to you know what is it going to take we're still on the subject of latency and you know state of the art right now is a couple milliseconds I think actually like there was a paper that talked about state of the art and kind of like what I call in a lab environment was a 29 millisecond inference pass which is like you know this this continues a bunch of use cases that I've been traditionally worked on in ml which are kind of like sub 15 millisecond use cases but back to you Rebecca what are we seeing that's going to be able to like hit this latency problem yeah no that's a great question a lot of thoughts on this yeah I think Hannah's uh the optimize optimizations you mentioned with quantization and model distillation awesome yeah there's a lot of great engineering work happening now um so I actually think that engineering especially in for engineering and for knowledge is really relevant so yes it's uh people need to learn you know Pi torch tensorflow uh learn how to find two models but um I think actually more and more just hard engineering skills are increasingly relevant to NLP and just to the productionization of AI going forward so so yeah I just really want to drive that point hell as well you need good engineers in this space on the inference side so um some of the inference being slow is indeed um Diego I think you hit the nail on the head here it's the Transformers architecture so Transformers is great for long range dependencies and capturing context and is actually faster in training because you have access to all of them but but it's not good for fast inference and um you know this is something that is very inherent to the architecture and that's why people are talking about latency and like immigrants needs so much because like we are bottleneck just by the uh Transformer structure and so if you compare Transformers to models for like uh recurrent neural networks like rnns Transformers are not naturally sequential so R and empty coding is going to be a lot faster than Transformers which have you know your multi-headed attention Matrix computations so you just have more computations and added to that we also you know we're talking about llm so there's also just more parameters and these models so in every board pass you have a lot more layers tensors together so it's just um yeah I think just everything just compounds to being slow right so yeah I think some of this uh will require you know if we really want to talk um like just like removing this bottleneck entirely as opposed to like working around Transformers which is what we've been doing it will probably require an architecture change that's going to be hard because we've seen Transformers have been so dominant since 2017 and it looks like it's becoming more and more dominant and not going the other way around if people are interested and this is you know still very much in academic research territory but something I've been excited about is um it's been called State space models uh which is sequential and so um that they underperformed compared to like Transformers models so far but they're I think you know that takes off um watch out for that coming out of Stanford um Jen we can solve the I'll put the paper in the chat the Hungry Hungry Hippos chat I just want to jump in there because Diego you said in passing right it basically invalidates right now a whole host of use cases like that late for that fundamental agency barrier like there are so many things that you can do in principle that currently you can't do in practice because of that fundamental latency Barry even if especially when you're at that tier two of complexity right at once so the founder I was talking about that I was they're selling this company initial pass on that workflow is 15 seconds right now that is a fundamental like end-to-end user Journey that just doesn't work it's a non-starter like everyone's like giggling a little bit right now he managed to get that down to like three and a half seconds through kind of fine-tuning Babbage instances rather than using large models Etc taking this tier 2 complexity perspective and it might be good enough it like that might be good enough to build a really interesting company on top of in that tier 2 complexity but that one cutter for other use cases where three and a half seconds just kind of lost you out of the room right yeah there's plenty of like orders of magnitude like for each person with a seven person or seven calls to an API there's someone with 70 in light or thinking of of 700 to do complicated things and so like they're it's hard to describe how many orders of magnitude room there is for improvement it's it's a lot and and I think like we we're talking about L M's in production right we're talking more broadly about foundational models you can set a kind of multimodal world that's coming down the pipeline right which is just going to be fascinating maybe that's an unhelpful grenade to throw into the room with kind of uh 15 minutes to go omega's health organizations we'll find out but I think uh one thing I wanted to get people's opinion on was like taking the Moore's Law approach of these large models right if we assume that some of these very large models are going to go just get cheaper and cheaper and cheaper and get faster and faster and faster like what changes fundamentally over the next 12 months if okay we don't know why but like maybe it's a new fundamental architecture but in 12 months open AI has a model that is just 10x faster and is 10x cheaper again like how does the world change I'd love to hear kind of instincts on that I think we potentially have like a Divergence right where you actually don't get you know you don't get actually the uh you know one like larger model Mega model that it just gets smaller and cheaper but we actually get to back to its highly specialized models or you know I think one of the I've actually seen implemented I mean recently and it's really interesting these kind of like start with one of these very large language models and actually use them as a router like a task router into a much more specialized task and I see highness is like naughty I'm guessing he's that's what he's doing internally uh which is kind of starting from a more generalized task right because you take advantage of the you take advantage of the kind of like uh no need for the pre-training kind of like you know kind of like no cold start problem you can like go into a use case and then route from there into something a lot more specialized um that accumulates latency though right and so I think like going back to kind of like because I want to like close down the thought on the on the latency thing like I think you know the the most important part that I see here is like you really need to go down to the use case and say okay what is this use case requirement right for a latency because you know if I'm looking at near real time use cases like the reality is that at least today right I'm kind of out of luck right like and so you're not really going to be able to achieve those you're going to have to look at different architectures smaller models distill it down to like very specialized to get anything even close to that and if you start thinking about again like you know you look at things like you've mentioned like you know chaining these things you're just accumulating latency right and you know across across those chains so you get into and again if you have a big batch process it doesn't really matter but I think that's a you know that's that's really interesting because I think we're you know I don't want to be like uh too deterministic here we live in a probabilistic world but like I think right now truly real-time use cases are kind of out of the question in like point today and time at least from uh you know kind of like production perspective uh at least from what we've seen now that said I'm sure that the Facebooks and Googles and uh you know kind of like microsofts are very much working on this problem but I think that's a reality at least in the in its current state anybody disagree with that or is that kind of uh you know can we get uh I don't disagree but I just think it's bold to uh be recorded making claims about where we are I set a point in time I said point in time you know yeah see you Mom and we'll be like ah do you remember when latency was a problem yeah but but I think latency is a function of like uh the business case the the way we deploy like with for most of the applications we're talking about we don't need the general model like chat GPT right like we can go through like a debate specific application and what worked really well for us just like sort of like the teacher student approach for like we take a larger model and then break this down into something smaller and cut out layers and and then so if I compare like the latency from our first deployment a couple of days ago to like what we have now it's like magnitude sticker it is like and we we were able to get like reduced models we would get the profilers we get Excel xla uh we get all these optimizations and so we get now we can even run those models on the CPU in a very condensed version and then and the costs player player role as well right like you can throw one at it and as somebody said in a chat like you could just use GPU right so if your business case pays for this that's the option but I think they're they're to make it very domain specific is the biggest uh latency saver you can you could go after right and there's there's a comment on the chat and I think like like let's talk about I think we we I'll rephrase it like we're talking about latency across the entire workflow right which also includes like everything else right like the system You're Building likely has overhead the problem is that you're actually significantly adding a bunch of latency while usually in like any of these kind of like online systems your offline system you know the latency your system's adding everything from i o to database pulls to everything like you're now adding significant amount more uh as you kind of like chain through these uh to these models so I love it you gave me the perfect segue Hunters here as they talk about and you might be able to kind of like put it on CPUs and uh anybody who's worked in the space knows that that is the uh Holy Grail for your wallet given that uh running gpus is about 4X what you can do on CPUs and so um I'll go back to I'm definitely stealing this James I love the triangle of the quality uh cost and latency and so you know determining the the use cases right because um when you look at in France and inference on gpus and you look at that on average and I think it can be on average it's about 4X more expensive that's what really you need to be like talking about so let's talk about cost and the use cases you know around that and how to think about I'm gonna pick on you David because you're probably looking at a bunch of companies and being like yeah this sounds great but if they scale like they can't afford it that's actually like uh you know run this company uh you know at any like cost like that makes sense even though you know GPT 3.5 is now 10x uh cheaper actually I'll just let you answer it's funny like um I think this is an interesting question because it's really important I and I've seen what happens at scale and it's almost like worth handing to Rebecca because like metascale cost becomes pay for whole departments by making one percent improvements on like their ad in for a ranking engine right like it's it's insane what happens at scale with inference for startups again when you're getting started I just like I don't know like money on fire like beat people win you know and I I just haven't seen it be a constraint early on it's certainly have to do the math so like it's really easy to back the envelope how much does one inference cost how many inferences will you do what's your margin per inference and you can even figure all that at math out like you can get there on the apis in particular pricing is all pretty easy to predict and so I always do that math it's good to do that math it's good to know where your margin's at but you know I think part of how I think about it is thanks to folks like honestly have gone down the road before in show notes that you can build domain specific models and there is room to optimize like it's a question of figuring out when to optimize for cost is for most of the people that I work with yeah and for most people it's not day one for meta it's certainly true that you have to do it and it's worth firing 100 Engineers to work on it maybe or a lot uh because it's so important but it's just this thing that you evolved with over time monitor I always recommend people to be aware of but I haven't seen it block a lot of use cases early on that said I know a lot of people that were really happy when the new chat API came out was 10 times cheaper because yeah this is like for startups at least you're talking about Runway like how much money do you have in the bank to survive is changing and that's uh critical but I don't know you can do it like I think you can hire an ml person you can learn ml you can fine-tune models you can like I think there are tools out there to avert cost which is something you have to be thoughtful of more than anything is this an exercise honest that you date up front like did you think about even back of the envelope like okay we're gonna go do XYZ this is what it's going to cost like you talk to people in your team that were beyond the ml team because I'm assuming this is like at least involved product management or leadership of the company in terms of like yeah we're going down this route absolutely uh we've been uh backup of limo calculations it's a little bit tricky with open AI like how many tokens does it actually require to solve this town we did some uh back a level of estimation went to the CEO it was like would you be okay if the stator goes to another API um there's all these privacy questions there and then there's obviously a Cost question and for the for the tasks we had invented um it was more beneficial to do this in-house we just given like a little bit of insights on the scale but we we have to process and close to 100 million transactions a day that is not like um something we could easily ship over to an API and that moment we have to talk about like latencies Etc and in that moment even the project of like investigating the model and figuring out what could be cut to so so we could actually run it on a CPU Festival was super beneficial so then we can we can talk about like what happens to be product but as a batch process we're streaming that or something but yeah it costs is basically the first question to be trans of the Holy Star project so we're gonna we're gonna go into the last uh subject of of the morning here I guess morning for us evening for James which is kind of trust and trust in these systems and so there's one term that I think it's worth defining uh which is like and some people might have heard of it that these models hallucinate and I'm pretty sure that's not because they're just trading on a bunch of people's Burning Man pictures maybe Rebecca you can actually kind of like help Define what a Hallucination is in uh this uh in the in this context and uh why uh I know we'll kind of discuss why this needs to happen I would say uh hallucinations yeah just anything that's not factually accurate and just falls into the accuracy problem when you're thinking about evaluating language model outputs you know sometimes it's open-ended like how engaging or interesting are these outputs sometimes it's like as a scoring are there grammatical mistakes um but what a lot of people are caring about when we're prioritizing these is I don't want the model to say that you know Michelle Obama is the president for example although it's like embarrassing with my brand embarrassed for our company um and and this is a problem that has actually existed in language models for for years but I think it's become a bigger problem because now people are actually using these lens for real world tasks so now you know it's not just like researchers writing papers about hallucinations how to improve it it's like actual uh you know users of these all um saying uh live model just you know wrote this this text for this Downstream use case like maybe like generating legal text for example and it's just totally wrong and really embarrassing for a company so yeah I think that this is um you know we talked a little bit about research directions I I think uh David maybe or has mentioned uh you know having uh basically external knowledge sources plugging in like retrieval right I don't think he's super true but like I think we're describing as like retrieval augmented Generation Um so yeah large language models currently as it stands uh they're very um poor sources of facts and knowledge uh and this is because it's just not very parameter efficient to be learning facts and so I think the way forward really is having these knowledge bases and combining it maybe with like a retrieval and then it actually assume that Chad venkat actually hit it uh really well because this is how I like to describe it I mean like we're used to very deterministic workflows and code and we've been building deterministic systems for a really long time but now we're applying kind of like a probabilistic uh you know kind of like implementation to some of those so maybe some of the tips and tricks and I mean not tricks really but like how how have people been thinking about making these things feel more deterministic and there's been a lot of techniques that have been used uh by different teams you know to do it but I'd love anybody jump in here uh in terms of how to think about some of the introduction of some of the techniques you can use to make these workflows feel more deterministic I think you talked about kind of like the teacher student kind of model that there's uh like kind of the you know uh human reinforcement like you know kind of model there's a bunch of these different models that can be used not to confuse with the actual models uh you know in terms of how to do it so I'll open up the room to anybody who wants to kind of like clutch on that because I think this is so important for us to build trust in these systems right and and truly adopt them in uh and and you know most people don't understand what a probabilistic workflow is uh By Nature humans are bad at it I think you've got to start the use case again right there are some use cases right now that have like high affordability of something not quite being right right such that I can I can generate an outfit and there's affordance for my my users such that okay if I if it gets inside you wrong or very wrong or whatever hey that's fine like I wouldn't shown the user and the user won't take a bad action on the back of it there are obviously low affordance use cases that we just can't surf today right like I don't want to like a medical diagnosis for an llm right because that's very low affordance right if it gets it wrong like uh it's game over right so and I think I wonder how long it will take until we we start to slide down that affordance scale that's kind of a as an aside to your question but I think it's good context a lot of others have those really cool approaches like they're basically databases with a constant lookup and so because we have to compute the weights uh there's just a fixed number of Weights but as a downside is like we lose out the accuracy of the of the results being returned right and so what I would I found really interesting was like okay because there's like this there's some submission uh risk in there and then when I saw the implementation what what Bing is doing is basically they they use like generate me a query that query just did it based that gives you the top five results and then it goes back into summarization summarization application so in that area but in the benefit here is like it leaves out on the constant lookup but the benefit is you basically get all the references whenever it states something and maybe in the meantime we need to think about on a system level in such a way they're like okay how can you trades back references or things like this and for the moment give up the the constant lookup concept and then work towards that maybe it's part of the model as well fantastic we are at the hour here so I want to be respectful of everybody's time first of all I want to thank all of you David my name is James Rebecca this was awesome this was uh really really fun conversation I think I told Demetrius we should make a conference out of this this is uh this sounds uh like there's so much to talk about such an exciting space uh I totally agree with what James said you know uh probably eat crowl and anything I said today in two months from now just based on uh you know like the speed at which is moving everything's moving uh and that's the fun part uh so I I always reserve the right to be wrong constantly so with that I will pass this on to Demetrius thanks for having us yes dude this was so good we're gonna do a conference I'm already planning it the last thing I'll say if you haven't put your information into this survey I'm gonna release it to everyone so I'm dropping it in the chat right now again throw in if you're using these large language models in production or if you're not we want to hear why and then I will reach out to you with the answers and all the data on this I think there's already been some incredible answers that have come through uh I love it and I also will try and make some kind of like uh tldr report on it so that everybody can see what we're all doing and where we're all at with this I thought this conversation was perfect so thank you everyone and I may be giving out some of these bad things happen to good data shirts for some people that fill out this uh this survey so bad things happen to good data we know that and if you drop in your email in the survey you could win one of these shirts all right everybody that's it for today and see y'all great job thanks everyone thank you foreign"
    },
    {
        "title": "Solving the Last Mile Problem of Foundation Models with Data-Centric AI //Alex Ratner // LLM in Prod",
        "video_id": "-oDgV6q6KtI",
        "video_url": "https://www.youtube.com/watch?v=-oDgV6q6KtI",
        "length": "36:47",
        "transcript": "for anybody that is wondering and wants to go just deeper down the rabbit hole with Alex we had an incredible conversation when he came on the podcast uh about a month two months ago time is going a little slow for me these days so it's probably wrong might need to do a fact check that one but a few months ago and we just talked all things like changes ml Ops slash foundational models and I know you got more for us right now so I am going to hand it over to you and give you the stage that's awesome I'm super excited yeah it was a really fun chat it feels like uh I think everything feels the the effective time skills are very interesting in AI World these days because everything changes so fast and I guess one uh one segue into what I'll chat about today is you know some things change very rapidly other things don't um so um can you see my screen now oh yeah okay great so um I uh I don't know how we're doing chat um or or q a or anything like that here uh I can't see I can't see anyone I'm just seeing full screen in my presentation so Demetrius if you want to jump in or if if you're doing live q a feel free to make this interactive um exactly I I'll um I'll jump in if something comes up and there's a great uh question in the chat otherwise I'll leave it to you and we can keep all the questions for like the last five minutes awesome sounds great so um and just to do a time check I think uh I've gotten until 12 10 right yeah I mean we started like five minutes late so 12 15. awesome generosity today this is great um cool so I'll I'll jump in and um as a quick intro I'm Alex I'm one of the co-founders and CEO at snorkel AI I'm also an affiliate assistant professor at University of Washington uh where uh at both places and and then uh you know back when I was working on the snorkel project at Stanford with the team uh everything really is around this idea of of data Centric AI this idea um that a lot of the development that is most critical and highest leverage for building for building AI is around manipulating curating labeling slicing sampling data um more broadly developing data even more so than you know picking the right architecture or the right or you know tuning the knobs in the right way um and I'll talk at a high level and part of this about how that's been accelerated by this this uh rise of foundation models and I'll share some other broader thoughts as well if we get into the weeds I'll I can also share my thoughts on uh some of the work actually out of my my co-founder Chris's Lab at Stanford on at end the snorkel research team on ways of automating uh prompt Engineering in part via VIA weak supervision so that'll be a uh you know in the weeds teaser if we get to that then anyone wants to remind me or I remember we can talk a little bit about that but otherwise I'm going to go through kind of a high level uh tour today and thank you all for taking the time to to be here and watch the talk hopefully ask some interesting and hard questions either during or at the end so you know to make it interesting okay so quick outline I'll start with and to meet yourself to apologize here because I'm opening with some some shots fired I'm going to switch right away to saying uh Foundation model not llm which I get is a kind of aggressive move given the name of the of the conference but it's it's kind of critical to how we do things I like it I got to make things a little interesting right so I'll start with that and um uh you know obviously I'll actually flip back and forth probably between FMS and llms um uh throughout without realizing but I do want to you know start on that note and and just uh introduce why you'll hear me and you know in some instances saying Foundation model I'll share a little bit of this this high level idea that a lot of the future is going to be around um you know let's call it gptu rather than gptx I'll then dive into how you make these more customized and domain specific and performance Foundation models what what is the development the data center development you do I'll use some of our work at snorkel as a case study although again I'm not going to be overly focused on that it'll just be an illustration of you know the system building we're doing around these ideas and if I get time I can talk about some other you know interesting challenges that are really orthogonal to our work at snorkel in the foundation model space just picked up from some of the customers we work with and and some you know some general thoughts may or may not get to that um but that's the highable outline and I'll jump in now so it's not that uh that maybe there's a little underwhelming as a shots fired slide because it doesn't it well I don't know Demetrius you tell me but I don't think this is the most aggressive looking slide uh with my you know uh hastily Googled image of of uh house and Foundations um but I do want to kind of introduce why do we say Foundation model uh why um you know my co-founder Chris uh and and a bunch of our uh Stanford colleagues have anchored on this term with with uh the center there um there's really three reasons um the first one is that a really exciting one which is we've we've moved Beyond language actually any data type uh any data any data type that has some underlying graph structure admits the exact same types of you know self-supervision or Auto aggressive methods uh Beyond just you know a sequence of tokens right we're seeing that uh with image and multimodal data uh we're gonna see it with with everything from databases to to genomics or we're already seeing that so really this is you know a boom that we're going to continue seeing far beyond beyond you know the classic language models that's reason number one the second reason is um you know often I hear uh you know gen or generative AI as a synonym for large language models but really all AI application types not just generated but also discriminative or predictive which is where a lot of the you know the classical and high value workloads uh still live our view is that all of these are built on top of foundation models um so if we get time we can talk more about that but that's kind of Point number two and the third point where most of my talk will kind of dive into is this you know the idea behind the name the basic and simple but but very critical metaphor which is that Foundation models are are foundations and you still need to you know build the specific house or building for your specific setting and needs on top and uh we'll talk about how you do that coming also there's a leaf blower which happens more times during talks than I think would be uh you know occurring by random chance if you have any problems with my audio let me know otherwise we should be fine you're sounding all right I didn't even notice it until you said it awesome okay I did but uh it sounds like we're good so diving in I'm going to start now by just kind of opening the aperture and and uh I guess firing some more shots although again this is mostly about you know open source Community love and predictions of where um you know practical realities are gonna you know of you know AI use cases and and the market are going to take us but you know again I'll I'll say it's a little bit more declaratively just to make this talk interesting so I'll start by saying that um you know the entire field owes you know the GPT line of work uh you know a massive debt and and I'm a a huge fan and uh I hope that you know gptx continues to increment up because uh you know it's been driving the field forward and it's a you know truly incredible innovation but I do think that most uh usage in the Enterprise even in you know in individual and consumer settings is going to be a lot more around let's call it gptu rather than gptx so why is that three high-level points I'll start with here just to get us all thinking number one we've seen a lot of evidence recently you know this is both first principles but also just you know demonstrations that have been uh you know accelerating of late around the lack of defensibility of closed API models and you know very correlated with that this kind of just boom of Open Source Foundation model Innovation and proliferation number two the more durable mode again no real surprises here if you think about it from first principles is around private data and and domain-specific knowledge whether that's in a person in an Enterprise in an organization we're seeing that and some examples of the power of actually leveraging that which I think is going to spread and a third point which will be the segue into the rest of the talk and a lot of what we do at snorkel is um this idea that really the last mile all of the hard work to get these you know to go from Foundation to house for specific use cases in specific settings is really where um you know a lot of the the effort goes vast majority effort goes and a vast majority the value um and and you know differentiation is going to be captured and I'll talk about how that's done and what's exciting there so just to go through those first couple of points high level you know there's been a uh a proliferation of awesome awesome logos and and uh I guess the sunglasses thing has been taking off with these recent projects so for folks who who um haven't been you know living and breathing Foundation model Twitter every you know five minutes uh like like a staying healthy person um uh one uh recent a very interesting project that came out actually right around when gpt4 dropped so it it you know didn't get noticed by everyone was this alpaca project out of Stanford basically at a high level what they did is they spent I think a couple hundred bucks on uh chat GPT API calls and they were able to use that along with a self-construct method out of some some uh out of a team at UW so basically train a 7 billion parameter model the the open source or the the Facebook llama model to and the evaluation is not fully finished but to to have very uh similar levels of skills and and and outputs as chat GPT so basically just by a very cheap set of queries they were able to basically clone um or or come close in many ways to cloning chat gbt and then we've seen a bunch of other follow-on work databricks clone the cloning method with their Dolly model um uh and and now there's doll ev2 where they put some of their own crowdsource data into there's an interesting koala project out of Berkeley where they show that they could get you know the same or even better performance as alpaca with just very careful curation of the data sets the high level Point here is that um you know it doesn't really seem like closed API models are going to be that highly defensible if they have any kind of you know sufficiently cheap API and if the open source continues to advance in terms of supporting these models which we see no signs of slowing down and is quite quite exciting so where do we actually have more durability well one first principles concept is that it's going to really arise out of private data distributions and you know and knowledge and we saw one exciting example that's come out recently with Bloomberg GPT so they took a bunch of private financial data and they were able to train their own model that was able to perform better on that specific domain so you know a I think there's going to be a flattening of the space in terms of just general purpose web data and general conversational and other kind of generic task style Foundation models number two we're going to see this kind of family tree start to really Bloom of domain specific models many of which are going to be leveraging you know private data and specialized knowledge to be you know better foundations for those specific areas um that gets into the kind of gptu idea um and then the third thing which I'll talk more about is again the building of the house on top and so I I in no way I mean to pick on the Bloomberg paper but it's interesting that very few people these days actually uh oh this would come off a little cynical it was very very few people uh you know actually open the papers often on these these uh these projects and so if you actually look even the blog post that the Bloomberg gbt team released you'll note what you see in many of these these uh evaluations that are done you know thoroughly on on proper held out data sets which is that it does better in a relative sense which is a very exciting achievement and proof of of this power of domain specific data but if you look here at the financial and and you know the financial specific tasks it's in the 60s right for most applications that's nowhere near a you know production Deployable accuracy level so again that gets to this love this idea that even with all this development both closed and open source even with the specialization and leveraging of you know domain-specific data and expertise you're still building the foundations and you still have a lot of work to go what always an AI over decades has always been the hardest part The Last Mile as any kind of you know uh data scientist or ml expert who shipped real stuff to production those that you have to do on top so that's what I'll I'll talk about there I'll pause for a second just any any uh questions comments pushed back on this this high level framing before I get into this this uh you know Last Mile building the house part we're good so far awesome Okay so let's get into this and what I'm going to share with you is is you know our perspective that a lot of the you know the building the house really revolves around these high-level ideas of of data versus model Centric AI uh and I'll share that both in terms of high level ideas and there's a ton of innovation in the space in the community uh and that'll be kind of with my you know academic Community hat on and then I'll also give an example of how we're supporting it uh in in production settings with our platform at the at snorkel the company uh which we call snorkel flow so um and here you know partly because I'm partly because I'm stealing slides uh uh and partly because it's just a you know it's a relevant stand and I'll talk about Enterprises this could be you know a commercial Enterprise this could be a government agency this could be a an open source organization anything where there's a you know a collection of non-public non-generic data and real production use cases so that's that's you know for the purposes of this talk think of Enterprise as a stand-in for any kind of organization or setting like that so one way that I think it's helpful to think about this is at least collapsing into two Dimensions is think of one dimension where you you you're basically asking how bespoke or unique is this data this is you know for ML folks in the audience this is just old transfer learning intuition right um if you train a giant Model A small Model A giant model on web data it's going to work better on data closer to what it trained on um Part of Me versus you know very bespoke different data inside a bank or a hospital or government government agency for example and then think about the x-axis is what is the accuracy requirement or accuracy proxy metric whatever you choose before you can actually use this thing we see a lot of of a very exciting use cases for you know especially generative type use cases where no one even measures the accuracy or even really knows how to right if I'm trying to generate some marketing copy or some cool images for a kind of co-pilot style application I don't even know necessarily how to measure what accuracy means and I don't need it to be that good because it's just a starting point it's a it's a it's a part of a human the loop process so there's high tolerance for failure um versus a lot of the AI applications that are actually shipped in production have to be you know 90 95 99 accurate accurate before they can even be shipped to production so a lot of the really exciting momentum and demos that we're seeing are kind of in this lower left quadrant where you're testing on data very similar to what you know gptx or other Foundation models were trained on you know you either have kind of low accuracy requirements because there's High degree of failure tolerance and a human loop system or you don't even have an established way of measuring what you know some accuracy a style metric and a lot of where you know the house building is both hardest and more most valuable is in this upper right quadrant where you have you know non-standard data um in a bank a government agency a hospital system most most places in the world most Enterprises for sure and where you actually have high levels of accuracy you need to get to before you can actually ship something um so I'll give an example of this uh with some of our internal data um and this is using a variety of foundation models uh some you know early results of gpt4 also smaller models like Burt and clip um and here we're just yeah there's there's more data here and and we'll be releasing uh some of the latest gpt4 data from our experiences soon but just at a very high level you know comparing the out of the box performance so a zero shot and and the few shot performances are pretty similar of just using gpt4 one of these models out of the box and this is looking a range of problems from info extraction for a large Pharma company to um actually an open source case study we released on classifying legal Clauses uh um to image and chat things as well and if you actually look um at the Gap you have to fine tune on on tens or even hundreds of thousands of labeled data points to actually even start to approach uh production level accuracy and I'll note here just to be very precise that gpt4 doesn't have a fine City interface so actually if you look at these case studies with gpt4 this is fine-tuning a actually gpt3 so it's actually fine-tuning a a less powerful model but gets a significant leap in quality um and and if you're curious on more data you can look at actually this legal data case study where we're updating it pretty soon uh this includes in between the 59 the 83 all kinds of few shot techniques Advanced prompting techniques nothing really approaches uh you know the the you know good old-fashioned fine-tuning on labeled data but this is mainly just to show before we get into methods that you have to do a lot of work still to build the house on top of the foundations and there's lots of work out in the public domain I'm I'm you know highlighting one of my favorites here uh because I was uh I was lazy and I just grabbed one there's a lot of work and and you know there will be a wave coming out once uh folks have enough time to evaluate gpd4 and other models this was an evaluation of chat GPT where the conclusion was that um chat gbt was again an incredibly impressive generalist hence the bet that this is you know Jack you know models like this are going to be the foundations for for all AI development that's certainly the bet that we're taking um but it was on average 25 percent worse than a specialized model that was specially trained for the given task and it was it was over 25 NLP tasks and by the way that specialist model was often a minuscule fraction of the size and therefore cost of chat GPT so we're seeing these kind of same results out in the open source um just a little behind because these things take time to these large benchmarks to take time to run um so how do we build that house how do we get from the kind of Baseline out of the box performance that again for real AI use cases or at least for many real AI use cases in that upper right quadrant we showed are just not anywhere near good enough to ship to production how do we go from that you know generalist jack of all trades Foundation to a you know a specialist or an expert that's Deployable in production well surprise surprise uh already announced that this was going to be the uh the technical perspective but um at a very high level what we've been working on out of Stanford UW and snorkeled a company over the last eight years or so is exploring this idea of of what we call data Centric development this idea that you know rather than pursuing things that the often the kind of classical way how ML and AI is still often taught in intro or most intro classes where the data comes from somewhere else it's janitorial work it's exogenous to your process as a data scientist it's not your your job I often think of this as the kaggle era of machine learning where you just your machine Learning Journey starts when you download your data set from kaggle all nicely labeled and curated and collected and then you start you know tweaking your model architectures data Centric development is the idea in its extreme of kind of flipping that on its head where the model is now fairly standardized and fixed and may not even change or maybe automatically configured in your process and most of your data science process your workflow your journey is really about iterating on the data labeling it sampling it curating it augmenting it Etc um so it's it's not always that extreme but one thing that I'll point out is the the wave of foundation models has really made it much more extreme than it ever has been think about it from the perspective of a user you know if you find out that your foundation model based application is messing up on some patient population or some subset of the satellite images you're analyzing or some you know subset of of legal documents more than ever before you can't go and just tweak the model architecture you can't go and you know tune you know by hand some of the trillion parameters you effectively have to go to the data whether that's labeling prompting Etc it's it's all these these kind of data Centric interfaces so in our view the rise of foundation models has also accelerated and in some ways completed this shift from model Centric to Data Center development so that's a high level thought I'll leave you with uh again you know one of one one uh uh you know of our favorite examples here is the the gptx family and I'll note that that you know the advancement that got at least a large chunk of the world you know uh going crazy over these these advances was really a Delta between gpt3 and 3.5 that was all about human supervision right a lot of folks are familiar with the the rlhf term but you know I think it's helpful and if you look at recent work it's you know more precisely you can separate the the inputs from humans and the mechanism by which the model was updated which is the RL part um and the the input was just labels in this case it was uh it was uh you know labels in the form of rankings and or orderings and then there was further labeling in terms of the thumbs up thumbs down and now there's even further labeling and and uh response generation being paid for yet again so the Delta was not really about the model architecture the Delta was all about the the data and the supervision so this is you know one one really great example of this this uh this data Centric development idea so you know our idea is that um and the central concept of both snorkel the academic work and this data Centric uh you know um uh concept is that you know the the critical layer between kind of these base Foundation models and here I'll depart or I'll be orthogonal to that point that I raised this doesn't matter whether you're starting with um closed apis like open AI always a fun fun sentence to say or open source models like I pitched I think are going to become even more uh more more prevalent um whatever you start with you have this layer of your stack where you have to do development to fine-tune or adapt them for your applications and that development is done via data primarily and this is where a lot of the challenge comes in because you know manual annotation is extremely difficult a lot of it's difficult because um it's just costly and slow and especially for most uh um you know non-trivial complex settings and certainly most Enterprises they can't just Outsource it so this takes you know huge amounts of In-House efforts often from you know very highly paid and and very busy subject matter experts you know a clinician a lawyer a network technician and you know an underwriter Etc um and it's also very brittle because every time something changes you you don't really have any way of modifying manual labeling so here's where I'll segue in uh to snorkel flow which is our system for um you know for developing Foundation models using data Centric AI and again I'll just quickly pause on some of this high-level stuff that I covered before any any questions any comments otherwise we can leave it there yeah there is a few questions that came through one is like when does gptu need more than 32 000 tokens of context oh yeah great question so I mean I I think that I think that's mostly orthogonal is like I think the the extension of the context window length um a lot of that work actually has been been pushed by some great work by uh um tree Dao and others in um the co-founder Chris's lab just advertising if anyone is hiring for academic positions I believe trees on the market he's amazing his work on flash attention has been been behind a lot of this these advances in context window left um but I I see a lot of that as somewhat orthogonal I think it's it's extremely exciting um there's a ton of possibilities that get opened up when you when you open up the context window um you know you can I think there's there's ways to basically unify fine tuning and prompting by putting all the label data into the context window you could handle obviously larger contacts and and more complex documents and Etc um but you know you can still get by with a shorter context window it just puts greater emphasis on fine-tuning on on you know various kind of chunking and compression schemes Etc so I see those kind of orthogonal to to um to a lot of stuff I'm talking about today although very exciting there is another there's another one that came through also that I wanted to say and it's uh from pradeep asking we can use the expensive large language models with Last Mile prompt engineering until we get enough training data to train or fine-tune gptu and maybe that wasn't so much of a question now that I read it again I at first I thought it was a question and now I'm thinking that that was probably just a statement but maybe you have something you want to talk about on that statement well I I yeah I mean I mostly I mostly agree I mean it really just depends on your your use cases right I think one of the you know the key things in characterizing a use case is um uh you know the various different ways you could talk about this but call it the the failure tolerance of the of the use case right how accurate do you need to be to go to production and um you know some of these things are scalar meaning you you get you get better results you get better Roi if you improve the accuracy in which case yeah like starting with you know a a zero shot or prompting based technique that gets you to that kind of 60 level and then gradually kind of tuning and developing it with data Centric AI to go higher is a very viable strategy in other settings 60 or as I mentioned before like not even knowing how to measure the accuracy for some of these generative use cases is good enough because it's just a copilot use case where it's just meant to assist a creative process and there's always going to be a human editing and and you know the final result and then in other settings and as you can guess there's a lot of where we operate um you know getting in the 60s just isn't good enough for anything you can't ship that model and so you really are blocked until you can get that that data development done uh to to get it to a production level accuracy um so I guess I'd say I agree with the statement in certain use case settings in others you're blocked until you can do that that you know fine-tuning or Downstream development and and in others you're fine because it either works really well out of the box because it's a very generic or kind of standard data task or you don't care about the accuracy as much because of the the use case setting um that goes back to the kind of quad chart I shared or at least that's one way of thinking about it right awesome so I'll push on because I'm almost at time so I'll just give a little preview I definitely won't get to the last section but um uh you know I'll just give a little view of the very high level Loop uh that that we support in snorkel flow um and I'll note that you know a lot of what we've you know over the last you know many years we've kind of anchored the description of snorkel flow on is you know developing training data for training models from scratch actually a lot of our workloads have actually been using it to fine tune what's called a medium Foundation models like Bert for many years now um and and now you know obviously a lot of the world is moving towards you know and we're we're you know we've been heavily invested for the last year or two and moving to you know building on top of foundation models so the basic workflow in in snorkel flow today is starting with some kind of Base Foundation model it could be closed Source it could be open source again I I I threw out some bets at the start of the presentation but we're we're completely orthogonal to that you you bring whatever you want to uh to start and basically the process starts by defining the specific task you want to accomplish let's say you know I I you know a lot of our our customers are you know still focused on predictive tasks where a lot of the value-wise I want to you know classify these contracts with very high accuracy so you take your your base Foundation model it would say train on web data and you apply it or snorkel flow automatically applies it to your data and your task so that you can kind of you know see how it does to start and that's when the guided error analysis starts so the idea is let's apply your foundation model to the data and if you think about it how else could you actually inspect how your foundation models do it right you can't go and poke around the model weights at least practically today you have to apply it to data to be able to even see how it's doing so that's that first step and then you know you start this data Centric Loop which begins with um you know discovering error modes in your base Foundation model via guided error analysis I'll skip over details there but that's a lot of the the work we've done both academic commercial side and then your goal is to correct them and to do that as rapidly as possible and this is where if you've um you know heard me give a talk before or seen any of the other snorkel materials I'll refer you to that if not a lot of the the um uh the acceleration that we get and a lot of what our academic work has been around is these radically more efficient and programmatic ways of doing this correction this corrective labeling and as I teased at the beginning I'm basically at times so I I won't go in depth but I'll just note that um this idea of programmatic labeling or on the academic side we've often called it weak supervision is a a powerful way to unify all different types of input so it could be a heuristic it could be a knowledge base it could be using you know clusters and embedding space it could be manual labels it also could be a prompt so there's a some nice work out of the the Stanford lab on um and also the snorkel research team on how prompt can actually be viewed as programmatic sources of supervision or labeling and automatically combined and modeled so you don't need to find kind of one perfect prompt you just dump them all in along with any other source of information and it gets all combined by snorkel flow um and I won't have time to go into the how but lots out there in the academic space and then the last step and I'll end here is basically one of two pads or three I have here on the slide you can always you know take the data that you labeled and just export it but the two main paths and I think this is again broader than just circle flow is either you go back and you update your foundation model with this you know corrected and augmented uh labeled data and again the standard way for production accuracy is that you'd still do is fine tuning or and we see this increasingly in our customers you actually now distill this into a smaller model that is specialized for this task and I'll just quickly note there's a case study that we have up on this this open source Ledger data set it's a contract classification underway classification problem and the net result uh you start with actually an ensemble of foundation models we did in this in this case you do this to development and you actually get not only a 41 accuracy Point boost above the kind of this was with gpt3 the GPT route Baseline but we actually now distilled it into a smaller Foundation model that was 1400 times as small so this is a lot of where we think things are heading you know starting with the foundation models as your foundations but then building the house on top via data center development and here's where I lose the the house building metaphor but basically also then distilling it into smaller models for for production accuracy so let me cut there I know I'm at time um and I don't know if there's time for questions but at least we covered already a couple and uh thank you all for the time today dude amazing for the questions one thing that I would say is for everybody that's looking to continue chatting with Alex jump into slack Alex I think you're in slack if you're not I'm going to send you the invite right now and go to the channel the Community Conference Channel tag Alex in there and ask him about all of this stuff I want to give a huge thank you to snorkel while you're on the stream with me because you all have sponsored this event and I am so grateful for that I also want to mention to anyone out there snorkel's having a conference a virtual conference too so we'll drop a link to that in the chat uh it's coming up soon I think and when or am I speaking out of turn Alex was that a secret did I just blow the secret yeah I'm I'm not even I don't even know so I I I I don't think it's a secret I didn't even know we were like giving out socks and stuff and sponsoring so um let's assume it's not and uh at least for this group otherwise it's super super secret exclusive announcement yeah we'll be running and thanks for bringing up we'll be running another version of our our um uh future of data Centric AI conference that's it yeah we've had some exciting speakers before last time you know a bunch of academic folks uh we had the uh the incoming um I think uh uh CSO the CIA give a talk um uh so you know academic Federal industry um all open um largely kind of tilted towards academic uh type stuff um and uh all about this this intersection this year of foundation models and data center development methods so uh yeah if anything that I said today piqued your interest uh please uh consider showing it up and Demetrios thank you so much for all the time today it's a pleasure man thank you for joining us and I'll drop all those links into the chat together"
    },
    {
        "title": "Want High Performing LLMs? Hint: It is All About Your Data // Vikram Chatterji // LLMs in Prod Con",
        "video_id": "XpeC1dqfiNo",
        "video_url": "https://www.youtube.com/watch?v=XpeC1dqfiNo",
        "length": "33:56",
        "transcript": "ah hey yo we got big drum joining us so what's up dude hey Dimitris good to see you again you weren't you weren't ready for that intro on there we're expecting cows were you but I loved it it's the only way oh man dude so it's been a while since we chatted I love what you all are doing at Galileo and we're gonna give a huge shout out to Galileo if anyone wants to try it we're gonna drop all kinds of links you're gonna show us all about what you all have been working on and I gotta say thank you so much because you all decided to sponsor this event and that is huge it means a ton it keeps us in business it means that I can do these things and I can dedicate more time to it so thank you so much Vikram thank you so much and there are Galileo socks as well oh no way all right I didn't realize that I'm gonna be giving them away then anybody in the chat Let's uh oh my God Lou just wrote in the chat that that last video was moving and that is how you get some yourself some Galileo socks all right let's do it so Vikram I'm gonna share your screen right now and yeah man jump into it I'll see you in like 25 minutes and for anyone that is watching along feel free to ask questions in the chat and we'll get to them at the end sounds good thanks Demetrius yeah and I'll try keeping about five minutes in the end for for any questions um just a quick introduction uh about me so I'm the co-founder CEO of Galileo um the the company itself and the product is completely geared towards your your data and how you can figure out what the right data is to work with I just heard about just before this we're talking about the terrible experience debugging and uh when you're but they're working with recommendation systems or whether you're working with NLP models or Now with nlms uh but the whole reason we started the company was because before starting Galileo I was heading up the product management team at Google AI and while building out models there we face this problem on a daily basis like literally days and days of just working with exercise sheets and Python scripts to debug the model and figure out where the failure points decide we used to call this data detective work so a lot of scars from that and that's kind of what I also wanted to talk about over here but uh all the chatter about llms now which is uh to what Alex was mentioning before it seems like this is the new phrase that we've all uh anchored around and it used to be Foundation models just like literally a second ago and John stands for learning based uh Transformers before that so it's interesting how the how fast the entire industry is moving but what's interesting also is that the first principle is still stay true and that's what I wanted to discuss the first principles are that with ever since machine learning ever came about for those of us like me who've been in this space for the last decade and more it's it's always been about the data right uh and it that doesn't change but what I think changes is what does that mean in the context of llms and the context of generative AI for people who are trying to work with GPT or any of the other wonderful open source models that are out there so obviously unless everyone's or someone's living under a rock we all know that AI in general is having quite the mainstream moment right now powered by this hugely been model development essentially like just having being able to train uh bottles on a lot more data huge Corpus a lot of other pieces here that have come in and to making this happen today but almost every day you see things like you know why combinator having a good almost more than one third of their entire class or one-fourth of their class being a generative AI apps we're seeing these other accelerators as well uh hearing about how this could be transformative for the Enterprise we're also starting to see a lot of worry and repetition in the market and you know how do you navigate generative AI for customer service is it a bubble uh their think tanks talking about this uh it's a it's overall just a really interesting time in space and the interesting thing is that all of these different kinds of headlines are have come in just the last three or four months right like little rewind to December and a lot of this wasn't even in the Zeitgeist which is ridiculous so uh of course like we are all having this huge moment now where AI is gone mainstream and uh everyone in the street just kind of knows about GPT and it's kind of brought it out there however I think the piece which I wanted to really flag is that it's for for those of us who've been in the industry for a long time which it seems from the chat and every every all the other talks that I've seen so far it's a lot of us so we kind of know that we've we've seen this movie play out before it's just now there is so much more hype and someone's done their marketing came really well because uh you know everyone's talking about it but essentially if you if you're rewind all the way back to uh 2012-ish uh because I think it was 13 September 20 2012 and there's this this CNN uh called alexnet which uh which came out and and won the won this imagenet Challenge and uh you know people were talking a lot about that and there was this article in the in the economist at the time which in my head is kind of when you're reaching mainstream uh where where where they talked about how suddenly people started to pay attention not just within the small AI Community but across the technology industry as a whole about what's going on here how can you classify these images in such an interesting way is it gonna uh have to be a huge breakthrough and that was of course on the heels of a lot of interesting developments in the GPU space and uh from compute and storage Etc but all of that's what led to that happening at that point in time you fast forward from there just a couple of more years and you get to um the uh the huge Transformer paper the attention is all you need which came out of a sister team of mine at Google and this was huge right and you know just that that's what led to uh our teams at Google creating Bert and then because Bert was open source and anybody could build on top of that it lead to increasingly these models becoming commoditized entities and uh we saw this huge explosion in terms of distilled word coming about Robert are coming about uh span bird and so many others uh and uh if we saw companies like PayPal coming up with what they called PayPal board if someone's thinking about GPT what's called plume GPT it's very similar right like PayPal basically said Bert is great but it's it's trained on the entire Wikipedia card but it's not good enough for us we're gonna fine tune this and to train this on specifically PayPal's data and that's what we need for our use cases to actually work out so awesome I'm gonna pick up bird just off the shelf it's super commoditized and but I'm gonna fine-tune that on this specific data that I have and that's going to work for me and my models and my users and my use cases uh similarly stock news sentiment analysis with Finn birth I came about so this whole entire explosion of models essentially led to the commoditization of these models as entities and so my team at Google essentially was also just picking up bird off the shelf and that was the job done we would experiment with maybe a couple of variations of bird but for the most part it was again what we called Data detective work where we figure out what's the right data for this for us to train with um once the models in production you could again kind of check for whether there are any data failure points for the bottle etc etc so it always comes down to that first principles and that's what happened there in uh this was I think back in 2017 to 2020 2021 um so that was like in my head the first wave of uh large language models because in back in 2018 I guess like bird was a very large language article so if you you know I guess now we're dealing with larger language models and that's the only difference uh and because of that it's much more powerful and because of that the stakes for in terms of what can go wrong are also much much higher right so that's why we have to be very careful but right after this we started to see this huge wave of what um data scientists kind of already knew when we're talking about but I think Andrew ning coined this really interesting phase phase of data Centric AI uh where uh it was it was interesting because when we uh when we talked to uh more and more data scientists they started to really gravitate towards this this phrase because they realized that already that when they're working with their with their importance those had become commoditized and good quality data and knowing what data to train with not quantities of data became the barrier as well as the mode because some companies just did not have enough data what do you do there some companies that did have propriety data that just becomes the board for their for that business it becomes a massive differentiator for themselves so the whole industry started gravitating towards this idea that great these models are commoditized now I'm going to start to focus more on the data fly field that I can create here and if I don't have enough data I can actually synthetically generate some of this data uh can I spend a lot of money on labeling the data can I yeah what do you do after that when the labels are incorrect how do I fix it all of this started to come under this umbrella of data Centric AI which I'm doing uh uh coined a couple of a couple of years ago um and this is also essentially been at the heart of what Galileo does it's essentially a very data Centric platform for uh for being able to build great machine learning models across the ml workflow whether you're starting from before you even label your data right after your label them in your trading and iterating on it over and over again we're in cahoots with the bottle and we tell you uh the calendar the product automatically tells you what the model failure points are so you're not spending weeks and weeks in data detective work that's the enemy for us and then once your artist in production again like telling you what's the right data to train with next uh and pick up next so uh very much in line with this entire uh this entire movement however um back to the present again what you're seeing now is a new Modern Wave of models that are emerging super exciting times of course like I think all of us are feeling that um and we've again increasingly seeing at a much faster Pace than what we saw in the last iteration of this a couple of years ago that the martyrs are becoming commoditized way faster right and you know uh GPD it feels like just came about a little while ago and then llama came about Google has its own um many applications built on top of that to further Market these different models um and then a whole host of different open source uh models coming up right on top of that but with this commoditization essentially uh what we've started to see is this is going mainstream again and so um Again The Economist about 10 years later after this first quote that came up in 2012. again there was this uh this this uh this quote about how Foundation models can do these magical things and that there's huge breakthroughs and this was in June 2022 so the economists had not caught on this new wave of it being called large language parties yet but they were calling it Foundation models similar to what Alex was mentioning before where you can call it either but I feel like it's good for the industry to gravitate towards uh one term but essentially at the heart they're kind of the same they are the basis of what you can build on top of um so it's really interesting how we've gone through the cyclical process in the first iteration where an interesting ml breakthrough happens people talk about it it gets uh it gets uh it gets uh a lot of attention gets into the mainstream and then gets commoditized very quickly and then again now we're getting into this phase where people are realizing that wait um these these models are great A lot of them are uh are are closed source so now how do I how do I use this for myself um and uh how do I figure out what to what to uh train my train my borders with um so a lot of these different kinds of uh open source models have been coming about recently and you know some of we've some of this has been mentioned in a few talks before but it's interesting how um you have data breaks coming up at Dottie for instance you have alpaca uh who's come out of Stanford uh which is super interesting to see uh you know they've basically been ordered trained with much less robots of data but lots smaller amounts of money as well but basically like fine-tuning the the larger bottle uh based off of data that's coming from a large language model uh Bloomberg GPT again like you know very similar to a couple of years ago PayPal bird came about and now you have Bloomberg GPD coming about where they've painted on their own financial data uh the model itself has a lot to be desired and if this is just the beginning I'm sure rev one and after this they're just going to keep iterating on it over and over again with better data and make sure that the problems are better as well um foreign and uh another example which I which I came across was eichel which is an instruction tune German llm family so it's it's uh it's interesting how every single uh use case there if your folks are trying to find data so that they can train their open source models within just hone this for themselves and that's going to be another proliferation very similar to what we saw with the bird era of doing things we're seeing that again with the GPD era of doing things that the difference is the board era started with open source for the most part versus now we're seeing this go from have a bifurcation between the closed source of apis versus the open source and uh both of that I feel like it's a healthy competition and healthy things will happen in the market but it is just going to be this huge um proliferation of Open Source models and because of that a lot of customers and companies especially in the Enterprise uh trying to trying to curate their own models for themselves so that they can have a differentiation and have that more to be built out um again what we're noticing here is uh yet again the good quality data is emerging as the as the big blocker and and uh and the big mode for for the industry uh so we this is by Greg Brockman this is an interesting uh quote which really hit home for me uh this is uh for those who knows the CTO of openai uh he was a co-founder of openai and who's the CTO of strike for and in fact not unfamiliar at all with this problem and uh we've been mentioning this before about how you know 80 to 90 percent of what a data scientist does today is basically just staring at the data trying to figure things out trying to fix it and then iterate on it over and over again right across the entire ml workflow it doesn't matter if you're just starting out you start out with the whole Corpus of data or if you're just iterating on the trading Cycles or if you're on the model production side of things you're just constantly iterating with data and it's a it's a from a lot of our customers and partners today got a little we've heard of this being referred to as the most soul-sucking part of the job but yet the most critical and pandatory part of the job and this this whole notion that this is the manual inspection of data has got to be they have the highest value to prestige ratio of any of the other activities is true it's a really really hard problem to solve and it's it's typically very very manual which is exactly what we're in uh trying to flip the narrative off it doesn't have to be manual we can automate a lot of this we can make this much much easier uh and the same thing the same principles go through even even today um so the other piece here is is that when you're building llms data becomes extremely critical and I think more and more people who are in the weeds of building out uh ml apps using you know whether it's the predictive models or generative models what have you you very quickly realize that you know the data is a very important piece and now with these generative Partners I guess what you're calling nlms um it becomes important for a couple of reasons so maybe touch upon that first like why it becomes really important and then I'll talk about um you know which aspects of the entire flow when you're creating these apps are where you should be really where we should all as a community really be thinking more about how we can invest in where there is where research needs to can be pushed in pushing the envelope on and uh where tools can be built out to actually help out developers in really turbocharging their their workflows and giving them superpowers so the first part is um what uh why this is really important um so as we all know that you know model High destination is a very real thing we actually saw that in the uh I noticed that in the uh prompt injection piece that we were doing were very quickly we were running into these bottle hallucinations and that happens all the time it's it's it reminds me of these uh of how fake news that's just spread across social media you just don't know what to believe anymore and so the model is kind of similar it's like I I don't know here's an answer and it's very confident about the answer but how do you know whether to believe in that or not and but with the right kind of data with the right kind of prompting that hello stations can produce it's just a matter of being very very cognizant about that so this is really important the obviously the the super popular story about this this the case study around this is Google's AI chatbot Bart that was making that made that factual um error in its first ever demo um they've been tweets by Sam aldwin and others about at GPD as well where it's like do not believe necessarily in everything it says Google Bing in its new um uh in this new release for its for its chat part also said that you know take this with take these results with a grain of salt so it started to uh the mitigations around hallucination has started to emerge in legal text in these applications but as app Builders ourselves we can go we can we can fix this by trying to be better about the inputs that are going into the the llm itself which I'll talk about just after this the other piece is you know becomes very critical when you are fine-tuning your uh your llms to your use cases again the I'll stand for alpaca was a great example of this we've seen a whole host of these to be honest koala came out of Berkeley recently it's a really interesting paper if those are not if you have not read it it's about a dialogue model for academic research specifically trained I believe on just a bunch of academic research papers um uh there's a Portuguese Portuguese fine-tuned instruction uh version of llama um there's a Chinese uh instruction following a version of that as well so there's the power of just providing an a model out there so that you can fine tune it is exactly this it leads to a huge explosion of very individualized kind of models for anybody else to use in the world and that's where Innovation can begin and now you can start to focus on your data on top of that so you can fine tune your llms and all of these folks who basically had some level of some Corpus of data which they could find you in their their model or top off um the uh the other and the third reason is uh to ensure the model responses are predictable and of the highest quality because at the end of the day especially when it comes to Enterprise use cases but we keep hearing from our uh from customers at Galileo and our partners is that I don't I can't launch a model unless it's unless I have trusted it and this whole notion of you know the uh what is it 10 or 20 of models reach model production it's it's not really about the lack of tooling at the way in fact I would argue there's too much tooling along the way to build great models it's more about how do you make sure that the model is of a very high performance um and that's the biggest part in that how do you get there and again for that data has become the biggest bottleneck to get the model to that high performance where a data science team can feel like I I can truly uh put this out there in production and then once it's out there in production uh can make sure that I can I have the verb without to be able to make sure that's always performing really well um there are a couple of things in which we can do to uh to be better when it comes to uh create doing uh more data Centric llm development uh going back to Ann tuning's coinage of the storm data Centric uh you know applying that here to uh llms it's it's there are many different ways to do this the way uh we've been thinking about this at Galileo has been um that it always comes down to the inputs right and before earlier when do you think of foundation models or Transformers the inputs used to be the border the code and the data and uh what we noticed was the code was minimal the model was commoditized the data becomes the 100 000 pound gorilla in the room that you have to fix right now what you're noticing is it's similar the model is commoditized the code is minimal but the big inputs now are the the data but also the products right and uh so I would I would my argument here has always been that we need to figure out how to how to uh balance between both you do need to find you and your your your models with a lot of data but again like that can be difficult because a lot of the Practical reality is sometimes you just don't have enough enough data uh and sometimes if you have a lot of data you don't know whether it's of high quality or not so you can also just use um uh prompting but two people to go that extra mile and be able to uh fix your make sure that the model output is good depends on the on the use cases that you're working with however uh it's really important to make sure that you're taking control of your inputs and what I mean by that is when you think of prompts itself right it's very critical to optimized for a few things if you keep that top of mind uh one is the structure of the product itself right so something which is really popular these days is just zero short prompting and and uh and uh and uh prompting the model that way but there there's a whole host of research that's being done around how we can do better about the structure of the prompts itself and I think this is somewhere where um you know a lot of the Premier Research Labs that we talk to and you know half of our team at Galileo is ml research that's constantly uh pushing the envelope on exactly this kind of stuff um we keep thinking about this like what it can we do which is better and one one paper which we read recently which is really really interesting which came out of my uh previous sister team at Google brain was uh this idea of Chain of Thought prompting uh which essentially it's a very simple concept it just means that instead of just saying that the in your prompt template input uh it's just saying the uh the example answer for uh for uh for a question like this where you're doing a almost a math calculation and saying hey the answer is 11 instead of that if you just reframe the the answer to actually tell the model the chain of thought that you used to came to come to the answer the model outputs can be much much better so that's a slightly different structure towards your prompts uh and it's a it's a really interesting paper where they tested this out against uh whole host of very large language models and they noticed that as compared to standard prompting it has it by orders of by an order of magnitude it does much much better so this is an example of where I would uh urge all of us to be a bit more cognizant about the structure that we're using in the prompts itself and look out and for better research and maybe share it across the community what the right kind of structures are um for these prompts so that we can all benefit from it holistically um the other piece is context and retrieval of course the more context you can give the model the better more examples you can give it the more retrieval you can get the better it is fine Cohen is an example uh this is certainly not the official logo of Pinecone but uh you know those who use retrieval mechanisms for their uh during prompting will know that it really helps in augmenting the the outputs of the model but again the question there becomes when you're when you are using a vector database or some kind of a repo of data for your examples and giving context for the to the model the question becomes is that the right kind of context if you're using a vector database the question becomes is there other kind of context that you could use which is maybe similar or close up close by in the embedding space that you could pick up how can you be better about that so that kind of think better about the context that you're providing and tweaking that and maybe even fine-tuning that over the course of many different prompts is super critical and the third one is just instruction uh for for these problems and just uh you know being able to give the right kind of examples and uh and tweak that over and over again um this is is super critical so there's a lot there it just prompts itself which is why perhaps there's this new term around prompt engineering that's that's coming about but the whole reason for that is because there's a lot here in terms of what you could do for uh for improving your your uh the output of the models and making that a little bit more predictable uh before you put it out there in the wide um apart from that you can uh of course just fine-tune with label data and while doing this it's number one extremely expensive for the most part to use a labeling tool or or uh and also very time intensive sometimes uh but at the same time it's extremely critical here when you're fine-tuning to identify whether you can you're seeing any performance regressions caused by uh caused by the fine tuning of the model um and especially now with llms right this is much more exacerbated because you're dealing these models have been have been trained with so much data and the like I think the Corpus of data is so much larger that when you fine tune it with certain kind of data the model performance that you see is not going to be that great so you have to keep doing it over and over again um uh to be able to get to the point where you can you can have a model that works really well for your use case whether it's Finance contact center or what have you um the second piece is of course incorrect ground truth no matter what products you use no matter if you have in-house labelers if you have humanism Loop if you're doing reinforcement learning if you're doing rlhf there will always be incorrect round truth information and that this becomes a hair on fire issue how can you automate this are there ways to do that how can you get faster about figuring out what the incorrect ground truth is a lot of this is um very similar again to what used to happen with the world of Transformers and still does for for uh predictive ML and those same principles and first principles still apply um the last piece is maintaining data freshness and repeated fine tuning because again the models in production that's not the end of the day it's not done and tested you can't go just move on to the next thing you have to keep monitoring it if you keep figuring out whether you need to fine tune again what's the right data to fine-tune it with that's again something which exists today with the world of foundation models as well and with Transformer Partners but it it is again super important as if you're building llm apps across the Enterprise there's a whole lot more of things that you have to do on the fine tuning side too so tldr here is when we think about being data Centric so to speak in the world of llms it's not just the fine tuning of your of your actual date label data but also looking at the prompts and analyzing that from multiple angles and making sure that you're good on both fronts um so I know I'm at time or maybe even Beyond um sorry Dimitri stuff I am but um essentially llms we all know are ushering this new wave of Baseline open source and flow Source models and the Enterprise is benefiting from this we all are uh so very exciting times so I would say like we should just get our data ready and have our first principles hat on and get to make sure that we are um making sure uh optimizing the the uh the different uh prompts that we're using as well as the the data that we constantly fine-tuning it with so I'll stop here uh I don't know if there are questions at all but I'm happy to take any there were a few questions that came through let me let me find them real fast because I thought it was really nice um so let us see there was one that I wanted to find and now I can't find it of course of course I'm also slack by the way so if anybody has any other questions feel free to reach out to me there too there we go there we go so uh we can continue this conversation in the community conferences Channel but I think there was um uh where there was some there were some really good ones and of course in the chat itself I struggle so in the meanwhile if anyone has any questions throw them in uh because there was so we had some people singing the Praises of Galileo uh then they have okay there was one person talking about how when they did fine-tuning of Bert in the past and then they saw the results they never thought we would get to this stage that we're at right now so oh man there was a good one but now I can't find it okay anyway for those people who have questions go into slack yes because I guess we're not gonna get it here I've failed you as the moderator I had one job man I had one job and then you're gonna see how good of a moderator I've been I mean um okay we got one coming through now that it's it's been a minute have you seen regression in prompt performance when updating models how do you manage that you do you do and it's a lot of experimentation to be honest right because you're constantly changing the kind of models you're using you are kind of you have to test and evaluate those those prompts um I think the way to manage this again gets back to force principles you have to have the right kind of metrics at your disposal you have to have the right kind of tools to be able to check for you know which model did I use here what was the what was the response from the prom for what input um and then be able to make decisions around that and that is something that we'll be hearing again and again from from industry practitioners that it's been really hard for them to manage um you know this entire like uh the the uh the uh the comparison between how you look at different problems because it gets into the hundreds very quickly and then the thousands and then you have multiple problems the multiple models The Matrix is just huge and so um it is Barclay a software engineering challenge I think which is exciting for uh you know the ml Ops Community uh but I also think it's a little bit of a research ml research Challenge and that's why we keep talking to the research Community because we need better metrics in my opinion to evaluate the prompts from a model perspective to know like which one is doing well and which one is not and honestly like right now I've been asking the the different research agencies my teams at Google before and there is very little like out there right now in terms of what what metrics you can use to evaluate these problems across the board so people are kind of defaulting to the usual like blue scores and other things like that but I think something new needs to come about and and help that but for now software engineering tools are are are the best bet dude well the big question that I have is do you feel like prompting is then stay like are we not going to just move past prompting eventually move past it um there's a big debate that Dimensions like I think you open up a can of worms that they'll literally amongst the people that we talk to uh there there's there's there's two different gaps there's some people who say that that's all you need you don't need to find unimporters anymore that like you know the bars have arrived throw any data into this we just need to do inference with prompts some folks say prompts are stupid you just have to always find inner model I believe like the reality of today is that you need to do a bit of both it depends on your use case if you're building a very um High uh high intensity app which has with the downside of getting something wrong is really high uh you need to do a lot of fine tuning to make sure that you can get to the the uh the 99 percentile or the 90 50 percentile but um I I think the hero now is prompting is very very important it is a it is a real part of the workflow so you pick out for it but I do think it's gonna stay for at least a while until models just get that good which is probably going to happen but again the market is changing so fast we have to kind of index on the year and now and I think at least for the next one to two years it's gonna be a thing but it's just gonna get better and better get better helping people do prompting faster and maybe it'll be automatic at some point here's the problem you should use them"
    },
    {
        "title": "Challenges and Opportunities in Building Data Science Solutions with LLMs // QuantumBlack Roundtable",
        "video_id": "0j0EtPDunyY",
        "video_url": "https://www.youtube.com/watch?v=0j0EtPDunyY",
        "length": "37:19",
        "transcript": "Quantum black is a center of excellence in Ai and machine learning at the mckinsan company we are focused on building machine learning models and leveraging them to really improve businesses and drive their performance so we have offices around the world with perhaps a major Hub still in London and this is exactly where Daniel and I are joining from we will be joined by our colleague based in Brussels Pascal um and yeah like today we wanted to share with you some of our learnings from the last few months in terms of challenges and opportunities that we faced when we're building data science Solutions with llms so um I'm really excited to be here Daniel perhaps would you like to introduce yourself before we begin [Music] in the London office originally physics PhD and yeah really excited to work now on alarms and actually brings them to clients wonderful just in time indeed thanks for joining us you're joining us from Brussels right I didn't lie Amsterdam yeah [Music] backstage area um so yeah um today we wanted to talk to uh our lovely audience about the challenges and opportunities um that we faced when building data science Solutions with llms and um we represent our strong community that is of course very excited about all the latest advances in large language models on channel AI in general and of course things are moving really fast and sometimes it's really hard to stay up to date with what's going on and um things really um um sometimes feel like they get out of hand so we really wanted to focus today on things such as risk and compliance and all the important guard rails that need to be in place when we actually start deploying these models and start bringing value to the end users um so Pascal uh perhaps given your background and I would also love you to say a few words about your uh your background before you join QB and uh at QB um yeah like who would want to hear a bit more about um how software engineering developments and innovations that happened in the last few months affected um the trends that we see in using llam's and in building data science Solutions with cell alarms so of course you know for a long time llams and really had a very high entrance bar and we're really a matter of a few lucky ones but now we see that the applications are really Rising every single day and like it feels like they're growing exponentially and of course a lot of this acceleration is really enabled by apis that are now exposed and that would be pretty interesting to see your take on how other Innovations are really um affecting this democratization of Technology oh yeah sure um yeah so maybe I'll just share a little bit of like how we approached it so essentially a couple months back I think the whole world got overrun by uh Chachi BT essentially and it was in everybody's uh on everyone's mind um I think the moment when your parents-in-law start chatting with you through translations from chatavity and your fitness trainer um just says okay to based class was activity based um you realize it's mainstream and it kind of reminded me of this a couple of these memes back in the in the blockchain hype days whereas like when your taxi driver is asking you what the latest is on blockchain then you know it's time to to jump ship um and so I just realized or we all realized okay we have to get involved here um and um and I applied a little bit of my my data engineering background which like I built two or three years of data platforms and uh have a lot of kubernetes experience um and we looked at what was out there on the on the cloud providers so this is like Q4 last year um and we very quickly hit the boundaries so if you look at gcp it was impossible to host these large models they're just too large so vertex didn't support it sagemaker there wasn't really anything there um Azure also wasn't it wasn't yet so clear How Deeply integrated Microsoft and uh and open AI are going to be so we realized okay there's definitely nothing not anything out of the box um and so let's apply what we know which is let's use kubernetes let's see how big the machines need to be um and then let's try and containerize it and bring it into a world where we can apply our existing knowledge around machine mlops and hosting let's say normal machine learning models for smaller ones um and get them to work uh and so we looked at Frameworks like uh Selden and McKinsey also has uh recently acquired egrasio so we also looked at that um but we then again hit this this threshold of well it doesn't actually fit on a single instance because some of these models were so large we couldn't fit them on a single kubernetes node um and so while all of the tools were there they weren't meant to handle 300 gigabyte models because of course Daniel and I instantly wanted to load the largest blue model because why wouldn't you start with the largest one because then everything else um becomes easy afterwards uh yeah and so like we and it actually ended up making everything work we used Rey in the end because Rey allowed us to uh cluster across multiple nodes and spread the spread the model across different instances and it also just felt more mature because Rey comes from the same people they came up with spark and Spark just became so democratized or democratized big data so much it felt like a right move so we switched to Rey and got the blue model running there um and then also looked at other models and got some of those running and so now this is two and a half months ago and I feel like it's completely obsolete because somebody ported everything to C plus plus and then colleagues sent me pictures of them playing around with Bloom on their iPhone um and so like if I just take a step back now I realize we probably don't need the complexity of kubernetes anymore vertex just announced that they are going to have llm support sagemaker didn't really make a big fuss of it but they also supported now uh Azure supports it now um the models become much more tameable again because people are showing that seven to nine billion models are also super useful and um the space is moving so quickly that it's actually really fun to see that code that you thought was a really good idea two months ago and soft infrastructure that you've built two months ago uh you can pretty much get rid of because you can stand at the shoulders of giants from Google and Amazon and not handle a kubernetes cluster because ultimately that's just a lot of work for for very little benefit so yeah that was kind of our our journey over the last three months um you know it was still very good to learn because we learned about all of the infrastructure challenges that you have when you're trying to make these things work and you're trying to provide sufficiently large GPU workbenches um trying to bring these models in a way that you can scale them up and have Auto scaling scale to zero is very difficult um but then I think taking a step back again realizing to build a product it's not necessarily to pick the coolest technology but just to get the job done and turns out chances are the you know hyperscalers are going to be able to to help us a lot with these things perspective this is very helpful Daniel yeah given that you were also one of the the cameras do you want to comment on your experience and also given that you're coming from a bit of a different background and how was that for you um after there's a journey in the last few months was uh super exciting so I think one of the use cases that really stood out on our side was the whole topic of uh document based question answering so I've been starting to play with this um in the second half of the last year so just putting together on Smart prototypes nowadays in pretty much every industry so uh from final sustainability over Healthcare education you name it people do face the problems that they have to ingest huge amounts of information and usually they're looking for some very specific insights from you know a range of documents structural data sources you name it so one of the tools that we listed out and were there was really exciting development happening in the last few months in this line chain um by the way congratulations if anyone is here onto college you know raising this 10 million seats around and the parts that most people are getting really excited about is that it allows you to take these well chat equity which is already really really good at answering questions from a vital range of General problems to really Taylor it to the specific problem domain also another big Advantage is that by providing resources you would use the hallucinations and on top of it you have the chance to validate the model outputs and cost check them so that's a topic that because at the moment uh yeah and Daniel for you as a data scientist did you feel that all these new libraries got popped up and um were like developed by the community really helped to democratize access to other lands not only to the data practitioners but also to The Wider population um I would say it definitely unlocks a lot of capabilities that you know most people wouldn't really have seen two years ago I guess uh people were working already previously and saw what's possible with dbt3 got early access to the API is very aware of what can be done but you're currently seeing it yeah along its mass Market especially thanks to the hype generated around chat GPT um one minute um do we maybe briefly go to the commands we are seeing here so there's a question for Pascal uh could you please share the list of tools you used in the last few projects and a second question about infrastructure challenges any specific ideas and tools helped you to fine-tune the Beast I guess that would mean biggest models um yeah I'll so the first one um we try to stay pretty Cloud agnostic because um in general it felt like if you if you can somehow get into the kubernetes world then it doesn't matter if you're on premise whether you have um a a cluster in one of the hyperscalers and you can imagine that also a lot of the very large Enterprises which are often McKinsey clients are um they tend to have still a fair bit amount of on-premise stuff um and that's actually your super interesting World in which it's fun to to apply this technology because um when you have all of this knowledge that's kind of locked up in some on-premise systems making that available of course is super attractive but um what seems to be a bit of a pattern there is you bring the data to the model or do you bring the model to the data and I think a lot of companies actually prefer bringing the model to their own data rather than sending all of their data to some other Pump Company and so we went for kubernetes and then um I think state that open state with that open source stack so kubernetes Prometheus grafana Loki for all of the like you know non-functional tooling and then um let's say on the on the llm stack it was really kubernetes um of course making sure the GPU uh or the gpus are available for the pods and then um we had Rey or array cluster running on top of kubernetes because it then allows us to apply or to deploy a model across different modes and we used Alpa in the end which was I don't even know when Alpha was released um to get the very large models running um and then I think for the second part like what the um what the challenges wait okay um perhaps you don't know you could also comment on that yeah Daniel do you actually think what we used is still applicable because I feel like I just want to get into the new tools that were released over a lot like deep speed I want to get involved in or I want to try yeah deep speed uh was a useful tool so that made it really really accessible also we see uh you know accelerated equation from hugging phase I'm really really excited about trying out a deep speed chat so I was just like yesterday or today in the morning looking at the performance figures that they gave on you know what kind of machines you can use to find tune aux30b model and hey you can do it on one instance 8A 100s spend a day or weekend on it that sounded very promising yeah some some more questions specifically on the bloom project here from the chat so any any thoughts on that Daniel um that was of course one of the few large language model options uh generally available that you can run your own environment and find you into specific purposes so yes we used it for that I think fine-tuned it yeah a lot of the world used llama right because that was or a lot of let's say a lot of the people that publish online um because they're just doing it for research they all use llama and so that seemed to over the last month and a half or so make the most progress anything that's kind of a llama derivative but if you do anything commercial you can't use it all of the meta models are Untouchable essentially and so Bloom was the only one that was really feasible and now I think databricks came around with dolly so I'm also very keen to look into that because can we now use the innovations that we've seen over the last month and a half with alpaca Etc apply that to a private to Dolly and see can we maybe do a fine-tuned model that is specific to a to a domain like an insurance document specific model or something that um I'm pretty sure most people know like the the medical literature fine-tuned bottles I think that's super interesting to see indeed so before we move to the next question from the chat I wanted to address another question to Pascal so uh yeah we see that indeed as you mentioned the space is moving so fast God knows what's going to come out in the next week and how this is going to change our ways of working um so perhaps it would be interesting to get your view on the existing limitations that you still see in terms of the infra and Tech architecture and perhaps you know like the big breakthrough that you believe could help us overcome these limitations I actually I'm getting really positive or becoming really positive about it because I feel like um we're getting to the point where if you just want inference like I think we have like these three stages right if you just want inference that's pretty simple at this point you want to fine tune that's harder but still pretty doable and then if you want to train from scratch of course that's complete completely different Beast um but just the inference bit we're getting back to the point where it's it almost feels like just another model they're larger but um the platforms they're adapting pretty quickly and they're making it visible so then I think the major part of work is not actually going to be the llm but it's going to be all of the other engineering that we already know for a while it's just it takes time um if you want to do question answering like Daniel refer to you need to have a vector store and that Vector store needs to be integrated and you need to have some form of process flow okay the user asks a question you want to enrich that question with additional context now you send it off to the llm you want to keep your risk and ethics checks uh involved there as well so you need some kind of um process flow that organizes all of this and that's classic software engineering requirements which I think we'll now again see that you know there's there's the entire complexity of building a product and then the model is just one small little piece it's an old image that has been used for a while but I think we're going to come back to this where llm is just a small model in this larger bucket of thinking about the product thinking about your um the technology stack where's the data if you can't if you if you don't even have access to your own data because it sits in so many silos then it's very difficult to bring it into this one vector store indeed indeed and it almost feels like you know with very very similar to what happened to you data science um when envelopes became a thing it's gonna yeah like the new advances in general are gonna disrupt uh the skill set the data scientists and machine learning Engineers need to have so would you would like to share a perspective on what you would you would recommend data scientists and the engineers to develop as a skill set um as we move forward Daniel do you want to do the data science first um I think you have a college go ahead okay uh yeah so from my perspective like data engineers need to be again more and more software engineers um I often have this feeling like if you if you talk to people what is a data engineer you either have the campus former software engineers and now just do a lot of data work and then you have the second Camp which is I write SQL queries and um sparkcode and I think that second part that might not necessarily be the most significant uh skill set anymore but the first part of bringing a large complex software engineering project together and connecting to all of the different software Data Systems and bringing those data assets into one place and making them available for the llm that's something I would focus on and definitely look into like vector databases are so such a curious new kid on the Block that I haven't learned about in University and now it's so obvious like why wasn't this always a clear database style or Paradigm that everybody always talked about thank you Pascal yeah please standing I'll go ahead and I think this is also a nice segue to our next set of questions of course so on the data science side I have to say compared to traditional machine learning where it's usually straightforward okay you have your r squared and now you're trying to optimize for it by adding features to gabapometer tuning sometimes uh the whole prompt engineering Etc feels much more like a soft science so when you're only a consumer of these large language models in general we'll take question of how to evaluate the quality of the output in a structured way is a bit of a tricky one and I haven't seen a good off the shelf solution for that yet and other than that I think it's a matter of new field so you have really a chance to be creative and come up with new use cases that get unlocked with last language models so yeah um so they also pick up the value and custom tokens question or so yeah let's perhaps leave it to the end I just really wanted us to talk a bit more about um an actual um example of the application and um yeah perhaps Daniel I could share a use case um that we've um we've worked on at TB and what exactly takes right like to build a solution end to end so Pascal started describing the flow but it would be very interesting to hear from your perspective on yeah what exactly needs to be built to make this whole Machinery work yeah of course um let's maybe stick with this um Lang chain question answering example um it's a foundation but the foundation model as the name already says um in the next step needs to be enriched with uh the additional data stores as Pascal just outlined so set up your actor database in just also one of them data make sure the permissions Etc are appropriate potentially depending on you know if you chose the API consumption yourself hosting would you might have to tailor the model a little bit so for the public domain for the use case on top of it there's a few other layers that you have to build you need to build the whole infrastructure together feedback from the users so you actually have a chance to iteratively you models you have to put in place actually a good amount of risk infrastructure both on the input side so when users asking the questions so you don't have one so you're reducing the likelihood of do anything now moments and also a review of the outputs to make sure that you know especially if it's but available to a wider Bond of people in the organizational externally the output is again filtered and then yeah you have to build think about the whole ux perspective UI perspective especially if you're thinking about use cases that are going Beyond hey I want to put a chatbot on my website just a little bit better so if we double down on this question of validating and evaluating the answers which of course in the case of um a textual data becomes much trickier than just like checking performance of your model where it's just r squared or you see or whatnot can you share any thoughts about how that can be improved or committed accelerated and yeah if you have any um any learnings that you could share with the audience I would say there are two or three potential avenues that you can take you can try to go with the Swiss Auto Value attribution and automatic validation you can try to use the fact that the underlying models are probabilistic and not just generate a text but what are the likelihoods for different outputs and potentially it's also possible to move towards more automated evaluation methods so we're thinking about the approach that is take in real possible learning from Human feedback with a small quitting model now of course this is all still you know properly evolving at the moment as well perhaps you you could also share your perspective on um yeah how you see that part of the whole flow in this solution with further automated and and improved um yeah but if you don't mind so Daniel just or Daniel's description particularly in regards to the risks um made me think a little bit because I know that or most of us I guess know that open AI invested very heavily in aligning the model and making sure that it behaves in a certain way not in other ways whereas these raw models of course have not gone through this kind of alignment process as much and so in theory um as we have more and more of these open source models available and deploy all over the place we have to replicate all of these risk safeguards again and again and again um and so I was just imagining all these different organizations having their own model that can do all of the or will do all of these things that we've seen with Bing initially were being um just says I'm tired of answering your questions and ask me something intelligent please um but then I also wonder is this just a phase where for a while we're all going to have a lot of fun um breaking these models out of their out of their cages but then at some point it just becomes boring because ultimately I think then we've all tried it once just like we've sent memes on WhatsApp um but then at some point it's just a tool and we all become a little bit more mature about it and just get on with our lives and use it for the problem that we're trying to solve I don't know what's if you you shake your head Daniel yeah let's see I think uh the human nature of trying to break things and getting unintended reaction is not going to go away anytime soon yeah true yeah I don't know if there's any good tools or Frameworks in regards to risk and compliance that I would feel like are a drop-in solution and just take care of it for you that's probably something interesting to watch I think this is very interesting because you know like even before I um was a thing um in traditional Amal and AI it took us years to come up with some Frameworks like not just the general framework but some Frameworks around risk governance and compliance and ethics to Monitor and like have some ideas on yeah indeed you know our models are prone to be biased and we need to actually do something about it so I believe yeah given the the pace at which this technology is evolving like we definitely need to move faster but I have a feeling based on um how that's been so far it's gonna take us you know like months it's not years yeah but yeah like um this is very interesting that you mentioned that of course I guess part of the validation part and definitely a layer of risk and compliance is extremely important and especially when you start working in organizations with their internal data this is becoming even more important um and uh yeah perhaps just like to finish it off um Danielle could you share some of your experiences with um with the organizations on how the approach how open they are to this kind of systems I would say it's a lot of enthusiasm at the moment to use you know gen AI mainly inspired by jtbt one of the biggest challenges is to manage the expectations since it's cool to go to the website you enter something you get a response but of course it's really expensive as visual subscribe so there's still a lot of steps to go through um maybe also on the west side of using these tools I think at least in the beginning it's a good idea to have humans somewhere in the loop especially when it comes to uh but outputs that are being used Downstream and posters that might affect individuals so I wouldn't completely go on because that automation yet and yeah I'm not sure I'm just talking to if you want to go Victoria um I actually believe that and this is yeah we went full circle in uh I would actually want to pick up some questions from the chat uh so I see some of the questions already dropped here um specifically for Daniel so the one I'm gonna read out loud I'm also playing with long chain and also baby Aji okay good start I wonder how do you think it can be used for commercial these kinds of tools require generating lots of tokens and can be really costly so yeah let's discuss the cost part of it um actually that's an interesting point yes [Music] it costs a bit of money to use the large language models and in the beginning I would have expected okay to get a good answer um you're going to pay let's say a dollar question now a few weeks later there was the announcement yeah we are having uh GPT 3.5 turbo and the cost of reduced by a factor 10. and you know suddenly you move to 10 cent and you're like oh okay that's a good price with probably further reductions uh coming in the future but more providers ultimate marketers comparable models and bonus to become more efficient um also I would say commercially yes it costs you I mean worst case let's say a dollar to answer a question but if to get a similar answer you would have a human spend half an hour going through multiple multiple documents control effing and then writing up the answer um actually that might be a dollar of aerobic sponge so it really just depends on the use case that you're exploring I would assume the economics would be very different if you're running a public-facing website and you're trying to finance the service that you're providing only with ads in that case the cost of function will provide of course play a bigger role I mean I had this question is going to be an interesting one because the whole ad business of the internet is currently being challenged so you know how much can you rely on an ad-based business model of a website to to solve your problem if the technology that you're now leveraging is actually challenging that ad business model in the first place what do you think about that one I don't have a qualified opinion about that just time to perform it um thank you I think I like the point like um never underestimate your the value of your online if it saves you 10 minutes that's 10 minutes you can have quality life if you know for a walk I think that's probably worth a hero for most people that's a great philosophical spin on that Pascal um so yes next question to you then uh I imagine that European companies especially in financial defense are super hesitant to use U.S cloud providers or us-based API Services given the US Cloud Act um so I don't see the second half of the question but yeah I believe that the worst um some requests for the commentary on that um and yeah perhaps you could share uh your opinion on how that's going to be how that needs to be managed and for the European Solutions no I think we just got the second part of the question oh yes the competitive disadvantage of the new companies due to the to this regard to llms and to what degree can we get something working maybe community space within this reality uh in the EU so I mean I don't think that this Innovation only happens in the U.S um if there is a sufficient I mean there's there's close to half a billion people in Europe um and that that's more than enough of a market for for companies to say okay let's let's create a similar offering in Europe if that is if it's worth it if there's a commercial value in this but maybe um I'll not go on to the European level but I do think there is a huge value in creating a product that feels almost like a document QA in a box where you commoditize it to a degree where a smaller medium Enterprise can just take one of their virtual machines or their virtual appliances and then install it as a web interface and then you load a whole bunch of files in there you just drop them all into a folder on on the Windows File system or on a Linux server and then you have this document QA in a box I think that is something that is super democratizing and I wouldn't be surprised if that comes out and not too long for now and that would resolve a lot of these arguments if I don't want to send data to wherever it could be us could be China it could be Europe could be Cloud it could be to my competitor thank you Pasco so yeah I believe um we're a time any final thoughts remarks comments um I would say it's super exciting at the moment and I think there's a lot of interesting things coming up but once the reduction also cost for training influence um additional open models coming out hopefully it's a whole reinforcement learning agents on top of it distillation of models and all those types of data augmentation plus uh action Transformers so I think it's going to be an exciting 2023 yeah from my side I feel like we should not forget that this is a technology and Theory should make our lives easier and better so this is I I keep thinking about this how can we do less of everything and not more now that we have gen AI how can we we do less emails less reading articles less just trying to stay on top of everything and just doing a little bit more of the things that we actually want to do so I think that's just a reflection that people come into my head I think it's a very beautiful place and thank you both and yeah thanks everyone for listening and for your wonderful questions foreign"
    },
    {
        "title": "Vector Databases and Large Language Models // Samuel Partee // LLMs in Production Conference",
        "video_id": "GJDN8u3Y-T4",
        "video_url": "https://www.youtube.com/watch?v=GJDN8u3Y-T4",
        "length": "13:10",
        "transcript": "there we go okay I'll try to Blitz through it because I know we don't have much time um but thanks for everybody to stuck around through the technical difficulties um talking about what Vector databases in large language models I understand party I'm a principal engineer at redis um and without further Ado we'll get started yeah uh first talk about what are vector embeddings uh mostly since not everybody knows what a venture database is for even what goes inside them uh vectors are commonly they commonly represent unstructured data audio text or images they represent these in a highly dimensional embedding and essentially I just say this is a list of numbers where each of those numbers represents some piece of information and these come out of machine learning models which you've heard all about today probably from you know open AI hugging phase so here um and it's become incredibly easy to use these and actually extract embeddings from these apis and use them um we'll give an example here of similarity search and how this actually works uh I'm breezing through this but I actually with Demetrios and the whole crew at MLS Community wrote a blog post on this a little bit ago which includes a lot of this so if you have any more questions about how this kind of search actually works just let me know that link so three semantic vectors that's our search space here represented by the three red lines in the plot do you see to your right and then once semantic vectors are query um that is a happy person you can imagine that any three of these were created by the code on the last screen from a hugging face model or an open AI model or what have you and each of these um three semantic vectors makes up our vectors or space when we take our query Vector that is a happy person what we're doing is calculating how similar they are trying to find the most similar Vector to that query but how we do that we just calculate the distance we say how far is that Vector that list of numbers that represents the input that we put into that machine learning model how far is that from any of the other vectors in our Vector space and we do this through a metric called cosine similarity in this case where that actually calculates the cosine distance between any number of those vectors you see represented in the plot there and so you'll see these numbers down in the bottom right and that is a very happy person is obviously the most similar sentence to that is a happy person even though that is and happy and uh are all words shown in the other sentences that is very happy person is the most semantically similar to that as a happy person and so that is a major advantage of these models is the ability to capture semantic representation which will become very important later next slide please so the search space that we were talking about it can actually be represented inside a database called a vector database and that's where these Vector databases come in is that you have all of these embeddings that you've created from any number of these apis or models and you put them into a badger database which also provides the ability for a secondary index and so in this case you can have an index and all of these embeddings stored in the same place which allows you to then provide a query interface to Applications so Vector databases are essentially a methodology by which you can operationalize the ability of vector similarity search um this makes it easier to deploy and do things like crud operations when you go to production and you're actually using these in a rail application next slide please so why am I talking about this why is redis even in this conversation uh because with redis search redis is a vector database um so when you have both redis and redis search you have the ability to do secondary indexing on hash or Json documents stored inside of redis um we have two index types flat and hierarchical navigatable small worlds um and a bunch of Integrations coming out you probably heard from Harrison Chase earlier or if you're going to the MLS camia in the last meet up in San Francisco later today you'll hear even more from people like Sebastian from Fast API and Harrison and Simba from feature form um about all of the cool things that uh you know they're doing in that space we have Integrations there also really cool in the relevance AI that actually allows you to basically have a GUI on top of your vector database all of these are very important um but really one that we're really excited about is our GPU index that's coming out with uh Nvidia so we're working directly with them to be able to put your index on GPU so that's a little bit of why uh red is just talking about this and what we've been doing in the field um but if you want to try it out there's an open AI example cookbook that Docker run right there will tell you how to spin an instance up and try it out yourself but next slide please but we're here to talk about large language models so what a vector databases have to do with this um and because large was not large enough this essentially means these large language models are already incredibly encompassing you know training on all of Reddit and all of Wikipedia and all of these various places but they don't know everything and especially they don't know everything about um what you're doing your confidential information your proprietary documents your rapidly updating pieces of information so we're going to talk about how Vector database actually solve that portion of the large language model problem next slide please so there's three that we'll talk about today context retrieval large language model memory and large language model caching um I'll talk about each of these in the context of various use cases but essentially the the hottest one right now is called context retrieval you see people doing this with like the Retrievers in line chain the way I like to think about this is uh the vector database is a Golden Retriever and the large language model is essentially a someone playing Fetch and every time they want to go and get something the gold retriever goes out and gets it for him um I say this because the operation is performed by a vector database are relatively simple and straightforward just like playing fetch however the operations before by a large language model are not um and so that's why I like that analogy because it's specifically supplies the large language model with that particular piece of context that it needs for a particular information and retrieves it for it um so that is also relevant to a lot of different use cases we'll go over but a really interesting one is actually large language model memory it's Sim similar in the case that it's providing a con contextual database outside of the large language model that the large language model may not have been fine-tuned on but in this case it also provides specific enhancements for things like chat Bots which we'll talk about and the last one is a simple caching use case but in this uh in this uh type of area you can't just say is this the same piece of text is this query the exact same because really it's not always the exact same words but they might be the same question and so you can imagine how Vector databases might be used for that so we'll talk about each of these as we go down so next slide please okay okay so q a systems are really huge right now for all types of use cases Google Docs really didn't like my bullet points when we uh translated these um but um you'll see a bunch of different ones a bunch of different use cases that should have been listed there at the bottom um but you'll see an architecture here that you can find on GitHub uh redis Ventures is our GitHub so there will be a bunch of different ones in there that you can go out and check out um but here the Venture database is used as an external knowledge base like I've been talking about and so when you go and ask a question what's going to happen is that question is going to be turned into an embedding and that embedding is going to be used to search through the vector database for semantically similar context and that context will be retrieved that golden retriever analogy for the next stage you can think of it like a chain thanks Harrison um you can think of like a chain um that the next stage will be the generation where that context is going to be used to inform the large language model of something that it may not know about something that may be surprised or something that may be confidential or something you might not want to have put into the fine tuning process this is also cheaper than fine-tuning and faster and allows for real-time updates to the knowledge base think about you know you wouldn't want to have something on a millisecond time scale that you needed to fine-tune for instead you could have an external database where that context is rapidly updated so if you're making trades in the stock market or something and you really wanted the latest news that you wanted to go put into something that suggested what stocks to trade on as finances my thing but that kind of thing you would need an external knowledge base that would rapidly updated the pace of the stock market next use case please slide thank you okay so long long term memory I'm gonna Blitz through this um we'll just go on to query cat uh one back one back if you could so just like I mentioned for chat Bots it's really it's really useful to actually have context so um on the previous page you saw how in Q a system you might actually have uh the user asked a question and then the previous chat history be used as the context to answer that question well chat GPT memory is a really interesting project that allows for uh addressing topic changes in multiple user sessions and addresses this problem of context length I know we now have 32k tokens but at not all models have those and at the same time even 32k tokens isn't enough in a lot of cases and this particular methodology allows you to have only the last K messages relevant to a particular message in some chat history isolated for that particular session or use case um so this is a really interesting way to address that problem for a chatbot like scenario highly suggest checking out the project next slide please and then lastly this actually this diagram was taken from uh the zealous team they had just released gbt cash um it's a really interesting concept um some people have already started implementing this with redis that we've seen gbd cache is a really cool project though um it's it's essentially where you use a semantic Vector database to say um if I have a query that is semantically similar enough and I already have cached the answer to that semantic query and there's some threshold I have decided upon that I say is okay for that query to be answered in that amount of time then I can simply say return that answer um and so what this does is it saves on computational monetary costs it speeds up your applications because large language models are slow and generally it's applicable to almost every single use case that employs a large language model we've also been working with a version of this called the Trident response cache with Nvidia which is soon to be coming out as well really interesting work in Saving on computational costs with caching so definitely uh go and check that project out and keep up to date with uh where we are in that project as well um one last slide I know it's a lightning talk I think I'm at way too long given the technical difficulties but um if you have any other questions hit me up at sam.party at redis.com or at Sam parteen on Twitter or GitHub and our Solutions page for the marketing folks that are there um and so if you have any other questions definitely let me know there's a ton more to talk about here and we're gonna be giving more talks about the year but thanks to Demetrius and then lost Community folks for having me on appreciate it thank you so much Sam that was awesome and I appreciate you going at lightning speed [Music]"
    },
    {
        "title": "DevTools for Language Models: Unlocking the Future of AI-Driven Applications // Diego Oppenheimer",
        "video_id": "tkL2c-16fXc",
        "video_url": "https://www.youtube.com/watch?v=tkL2c-16fXc",
        "length": "29:55",
        "transcript": "foreign [Music] great well hey everyone thanks for being here today super excited about about chatting uh today so I'm Jacob Oppenheimer I'm a managing partner at factory we're a venture fund that's specialized in AI Investments I also happen to be like help a couple of different llm based startups uh with product and so I've been pretty deep in in the ecosystem um one of the things that are really interesting is that like this tremendous popularity this kind of Cambrian explosion of use cases and interpretations of like how we can actually use uh you know build AI applications I think um really the kind of quote-unquote chat gbt revolution has become fascinating in terms of like the inspiration that it's given to developers hackers uh and uh you know the people who have uh you know been inspired to go build uh you know for truly what we can start thinking about like you know as the you know software's 2.0 or 3.0 if you want to call it in terms of you know how it gets powered by AI so in today's talk I'm going to really kind of do a little bit of a survey of the environment and kind of the tools that are out there um I'm not going to be I can't go in depth there's so much going on uh but I kind of want to like talk high level about you know how people are building these applications and what kind of tooling they're using let's figure it out great so let's start a little bit into you know uh because I think a lot of people still have a little bit of this question in terms of foundational models versus large language models and so the first thing I want to do is you know these foundational models are trained on like extremely large amounts of data um and they're you know if we think about kind of like foundational models as these broad general purpose models um that are really aimed to capture you know a lot of capabilities and knowledge um the kind of like large part comes usually uh with for because they're a billion plus parameters um and you know they're really kind of kind of design and architected so that you can be adopted and fine-tuned uh so that you know to specific data so you know examples like gpt4 and clip and Dali fall in this um in for you know so foundational models really think of it as the kind of like generic or superset and inside that you know there is obviously language and there's a image and there's multi-model models so large language models are really designed for specifically understanding of language so uh and you can see their births and their llamas and gpts in this category they're really trained on massive data sets and you know they're you know you can see the tasks that exist in terms of classification generalization summarization and more so I'm going to talk I'm going to go back between foundational models and large language models a little bit but this can kind of give you that framing of you know how to think about about them so let's go a little bit into um you know kind of a history so in terms of like you know how do we actually get here uh from a research perspective right so you know back in the day started with the bing bang very sad no models emptiness nothing you know we the first kind of iteration or not first but kind of like big push in terms of uh you know machine learning was in self-supervision we started seeing models that had you know 125 to 350 million parameters um you know it's capabilities where uh to be able to kind of uh replicate uh what it had seen and kind of like spit it out uh and the kind of like data sets that were actually used to kind of generate these were really around you know kind of like small web you know so small web components uh you know or book dumps um the trend ended up going to bigger models right and is when we started actually getting into the kind of like large models which are you know getting to that you know how do we get to that 100 billion parameters these you know the the more you know a lot of proof showed that like the bigger the models the more the capabilities that were coming out of it so we started seeing these one to 100 billion parameters uh really tax taskless text generation was really kind of like the first kind of um uh things that came out of this and you know the data was really kind of trained on like all the web so now we're getting to these instruction tuned uh you know uh and uh you know and massive uh uh models that they can actually follow tasks and instructions they are using actually heavily curated data sets um they're getting ginormous right the 10 to 200 billion parameters they can generalize to tasks they also can listen to feedback in the sense that you can actually work with these models in terms of instructing them to get to results and providing context we're going to talk about some of the tools that allow you to do that and the more important part is from a data perspective these are heavily curated data sets um and label data at web scale with human feedback and the cost and the scale at which this is being done is quite impressive and and you know really kind of like the the the newest models have been there's a lot of work going on into actually kind of being able to do that so as the size and data quality increases you get more generalized um you know uh of the uh more generalization um and in context behaviors but on a much much higher cost and so we'll talk a little bit about that what that looks like as well cool so you know one of the you know interesting things about early stage development of of platforms is you um you know when the first kind of kind of platform show up people start uh you know with a very kind of like basic uh rappers around that and there's actually um a parallel to this in in general software and Technology right so when the first uh you know microprocessors came out there was essentially rappers around single board computers when the first operating systems came out um the first applications were really like rappers around utilities if you think about kind of like Unix utilities and kind of like uh you know like Norton as one of the first kind of like applications was really just OS capabilities wrapped around uh you know wrappers around that um in the internet we got these Like Rappers around like Unix and network utilities and what we're seeing today in kind of like the generative AI world is really this Like Rappers around well not just llms but also got around foundational models in general so we're in this kind of like first phase which is pretty natural um where these thin wrappers around these foundational models and there's an explosion around of it and the core thing that's actually happened right is the capabilities got to that holy moment right where we're looking at you know it really feels like magic and so my personal experience I've been working with these foundational models in a bunch of different contexts you know I like the best translation I have to it is it feels like you're Neo in The Matrix right like you're just accessing content you're being able to do things at a speed in a productivity level and so that holy moment of using Chachi BT for the first time you know has really driven to you know tons of Discord servers and hackers and Market maps are being made in an unbelievable amount of people are building uh you know what is today the basic kind of rappers around these foundational models uh but that is kind of like if we if a history provides true this is kind of the start of a whole industry that will be building deeper and deeper applications as we as the models get better as our tools get better and as we understand how to build these applications in a better way so the ecosystem is completely thriving and you know I cheated here a little bit and uh you know I stole this from uh from the folks over at at first Mark so I want to give them credit but like the data landscape and the AI and mlasky this is just growing and what's actually happened especially in that kind of mlops category is we're seeing actually now llm specific tools right that are just coming you know that are going everything from the most basic convenience wrappers around open AI apis to actually complex tools for orchestrating and multi-step reasoning it's a very simple databases for prompt templates and so what we're going to see here is that as developers continue to experiment with llms a thriving ecosystem is going to be cropping up to support that work and these tools are designed to enable developers to iterate quickly build amazing features on top of these llms uh around that and so we're going to dive here a little bit into kind of like a couple categories um of of those two tools that are emerging so you kind of understand like you know what we're seeing so first of all I think let's start with what does it take to build a llm based application and to give a little bit of that workflow so we just generally understand um is today fairly easy grab an OPI around an llm you know plug in your experimentation prompt tooling you know potentially if you need it and I'll explain when when you don't need it Vector database and data integration and you gotta be one product and to be clear like most of the stuff that we're seeing today all right and most of those explosions really around these four first boxes which is we're getting to that first version of the product and and we're still in that kind of like holy moment around that first version of the product so you know as we continue experimenting with these llms then this uh you know kind of like the thriving ecosystem kind of keeps going up you know we're going to see these steps go through the first is experimentation and prompting we go into knowledge retrieval and fine-tuning and experiment Haitian phase you know developers are really tinkering with the prompts um these llms have a really interesting API which is natural language and it has some specific ways of working with you know with that with that API API um that require uh certain tooling or at least not required but like we want an abstraction on some tooling to make it a little bit easier um and this might involve actually chaining prompts uh you know through that and the next is knowledge retrieval which involves providing like kind of like relevant context to the model so it can actually improve its accuracy so these generalized models if we can actually reduce and provide context into them um you know allow you to uh you know improve that accuracy and also it behaves better and run cheaper uh from that perspective and so finally fine-tuning is really where you know when we're going to go in like into the second version where we can actually get really really what I call kind of like snipe data sets so highly curated specific data sets to show examples to these models so that you can actually fine-tune on them to improve your model accuracy and actually reduce your inference latency uh which really what matters when you're actually thinking about production use cases so let's um jump in a little bit into what these uh you know what these uh you know kind of places are so as I mentioned like we're kind of you know version one is really where all the action is today uh and obviously there's a ton of people working on much much deeper stuff so I don't want to like take that away but like if you go look at and I was looking at this uh you know kind of last night uh one of my favorite newsletters uh no relationship to them but like Ben's bites like you know aggregator of a bunch of uh links and new tooling right so uh you know hundreds and hundreds of applications like a week are popping up essentially uh building in this kind of like V1 world uh which are really rappers around this mlms so you know you know for effect purposes you know that that moment where we that blows our mind like getting to the next step is actually doesn't blow our mind as much even though there's a lot of complexity getting into that V2 uh of um uh you know that version two of applications so let's start jumping into uh you know some of the actual like tooling here so the first one is really what I would call kind of this like you know experimentation and prompting so to get actual desired output uh you know from llm developers often need to experiment with different prompts and chains of prompts and this can actually be quite complex and time consuming so fortunately what we have right now is kind of emerging tool sets like land chain and Lama index that have helped to jump start and manage this experimentation process connecting to you know they're because these apis are natural language you know that that mastering requires that you know experimentation between single and chain prompts and so these tools have really come out to like help us connect the data sources right provide contacts and grab a bunch of data provide indices to actually be able to run through that you know you know through that data coordinate chains of calls and provide other uh core abstractions like agents that'll allow you to build kind of the applications of the future and so this is really again like the the core of this tool set is around being able to very quickly and iterate through that experimentation uh and orchestration process uh and Abstract the way some of those uh you know things and you know some of these tools have gotten extremely popular um and if you just Track by kind of like the the GitHub Stars which you know it doesn't really mean how much they're being implemented just really shows interest um it's really fascinating to see how this they've inspired you know I think it's fair to say that they've inspired a whole revolution of hackers to go build applications off of these uh large language models so then let's talk about knowledge retrieval and you know kind of vector databases so these um you know I one of my favorite descriptions around uh you know large language model is like it's like the it's the smartest goldfish you've ever met right so you know in what that means is like they don't really have memory right at this point and to be able to actually have like you know a memory or understand what you're doing over time you have to kind of provide a context and kind of like guide it through what you want and so the best way of actually guiding the context right is to actually pass it in relevant content so it can actually frame it in a way that it you know it understands uh around it and so um one of the ways of really um you know Finding relevant contacts and actually doing it in a very cheap and efficient way especially because the way embeddings work uh you know is actually using Vector databases and so there's been a you know the vector database is a very popular right now there's tons of fun being going around them but the real reason around this is that they're actually really really efficient for Vector similarity searches which is your retrieval they're really effective at storing uh billions of embeddings which is what you want to do to provide you know if you convert documents and different contexts into those embeddings to be able to feed them into these llms and they're really efficient have really efficient indexing capabilities so when you start thinking about you know retrieving similarly uh similar documents really Vector databases are this core component of giving memory and then hoping to improve uh output quality so if you're actually building an application and what you're trying to do is that to have you know as I said you know smartest goldfish um you want to provide memory to the the application and understanding over time these kind of vector databases tend to be a core component and so learning how to use them and actually Implement them uh becomes particularly important so then let's go into kind of like what these version two so we talked about the kind of like product you know the the the you know the the building and like you know the the kind of thin wrapper layer around the traditional model but that's actually not you can do today there's actually a lot of things you can do in terms of making these models more accurate um and also faster and cheaper to run and that usually kind of steps down to this fine-tuning step and to be able to do fine tuning you really need super high quality data and so I think it's really interesting and you know there's I you know you'll probably see a couple of talks on this but uh my good friend Alex who I'm sure will be uh we'll doubly clicking on this but like this is really kind of playing out in terms of the data Centric AI movement um that started you know a couple of years ago where the core quality of the data and the kind of like manicuring of data sets um is proven now to be the actual most efficient ways of you know reducing latency and increasing accuracy uh through this fine-tuning process uh for these llm based applications and so while they generalize really well um the effect of actually trying to get them like really really like can you customize or at least working in a better way is about getting data sets um that are uh you know very very curated uh and specific to task um and then providing fine-tuning uh you know apis to be able to do that and so in this process you know you can do uh you know the you know there's a couple different patterns um around that so the fine-tuning one is the one that allows you really to improve performance and and when accuracy is critical um and then what you the other cases what you do is distilling these models to smaller versions of the model that run in a cheaper way but that don't lose that accuracy uh but you just don't need a lot of the other um properties that these large models potentially have so you can think about this distillation process where I can get a smaller still large language model uh you know that has the same accuracy for those tasks but is no longer uh actually um you know you know it's much cheaper to run and much faster to run so as these use cases come up or latency matters where accuracy matters where cost and efficiency in the unit economics of running these models uh you know is an important factor you have to start thinking about these techniques uh you know to be able to kind of boil those down so finally we get into kind of like monitoring observability and testing and so the first thing is like let's be clear like the we're still working with deterministic workflows right so let's say probabilistic workflows so while most and and it's really interesting because as we look at the general population of software developers and people who are interacting with these models for a lot it's the first time that they are uh encountering probabilistic workflows so people who work in data science and machine learning like are very comfortable they understand you know that you know like that there's confidence scores and like how these models produce results uh but for the general population like you know we're used to like when I do the exact same thing you know over and over again like I should get the exact same results and that's just not necessarily true in a probabilistic world and on top of that when we're actually trying to assess performance for large language models um you know we're really have to assess quality via the user interactions right we have to understand what the user is doing what they're actually like complementing around that and so the um you know how do we measure performance is a really interesting question inside the uh you know large language model good and how do we understand good what how the good generated content is um we talk about this problem with uh you know hallucinations and overconfidence in the results but you know as we start thinking about monitoring observability and testing you know we're going to really have to think about okay why you know how do we observe the user interactions and what they do after it and what their results so being able to provide that feedback in to assess performance um we're gonna really have to double down on building a b testing around the full workflows for product analytics Because the actual result in the chain it's not just about one model and testing the model but that's to the entire workflow and they be testing that entire workflow to understand uh you know the you know if they're working or not um as these models you know larger models get larger and more and more there's a giant um uh swell around open source models so we're seeing a lot of these generalized models uh coming out how do we compare on them how do we understand just the eye test of being able to like run the same prompt against a couple of different uh you know models isn't really going to do the trick so open source efforts like how I'm over at Stanford are really providing comparison Frameworks on a task by task basis so there might be room here for actually thinking about local what does Frameworks for kind of like full workflows uh and testing those against each other and some of these tools are starting to pop up in things like honey Hive where they're really giving you like the ability to kind of test and iterate uh through through those workflows and finally you know what's the performance impact we need to be able to look at you know uh you know the the performance he directly impacts the ux so if we think about um you know how fast or slow these large language models you know the very very large ones might take a second a second and a half to produce a response if we're doing hundreds and thousands of these inferences every day not only is the cost really big but the actual cost of the ux and the experience that the user has is really affected so how do we actually measure the full user experience of the application and understand hey when should we be using a cheaper faster lower latency model because we're affecting the user experience in in that way so getting to you know finally like one of the probably the most interesting pieces uh right of working with these llms is you know they can really generate plausible but incorrect information which actually really poses a threat to what I call low affordability use cases like medical Diagnostics and final financial decisions so if we think about um what high affordability versus low affordability looks like so high affordability to be wrong right that's the framing that I use to kind of think about some of these use cases so if you think about your GitHub go pilot if you think about your suggestions in your email if you think about uh you know generating an image like these are all high affordability use cases for being wrong like you know you might it's not going to really affect anybody like you know sure you can ignore the suggestion this is why we see so many like this explosion of co-pilots type um you know workflows because they're just enhancing there's a human there verifying it like it's really about getting you to faster but once we start going into low affordability use cases things like medical diagnosis financial decisions things that where this technology should be applied to because it has the opportunity of truly revolutionizing uh you know those Industries uh the the uh you know that's how we actually can start thinking about you know okay what do we need to do to do that so we need to start Guard railing you know these uh you know these these models so how do we ensure safety accuracy and reliability um teams that Microsoft you know the people you know companies like Microsoft have full productives actually doing this um and it's really interesting because it's really kind of going to be at the core of how we explore the future of these um you know how to explore the future of using these llms so tools that start emerging that uh you know Define rules schemas and heuristics for llm outputs are going to be crucial to build trust in those systems so that we can actually start building and trusting these kind of what I call low affordability uh to be wrong scenarios so finally to kind of wrap up here I'm going to leave with um a couple of predictions um around this and so the first one is that like there's a ton of developer tools and you know usually iteration Cycles are what defines the winning developer experiences so you know if we think about what we originally started when we started working with uh you know deep learning there was a launch of you know libraries and kind of Frameworks like Cafe and tensorflow and as we iterated through it new ones emerge like pytorch that adopted quick you know uh very very quick popularity so it's really interesting to see and you know I think it's too early right now to think about okay what which one of these uh you know Frameworks that are become extremely popular today the good thing around this is I think like theater we're going to see the iteration Cycles really quickly um and these new libraries are actually enabling that iteration cycle um second one is and again this is probably not a not too crazy easy to think which is like you know if we start about what traditional amount something I spent a bunch of of time in and you know kind of like what would call previous generation of ml Ops um every single step of building an ml application was hard right so I had to build a data warehouse and capture the data I had to hire and train an ml expert label that data I had to go build a model and I had to build an inference point and then I had a version one of the product that level of difficulty the way I compare it to you is like originally you know before AWS and before cloud like you know just to build a web application you have to go stand up servers the whole year there's a lot of manual stuff to go do this is kind of out of the box now I can start right it's extremely easy to use Alm API it's literally interacting with it in English or any language right prompt design is actually fairly straightforward then you can get to complexity but it's actually really easy and then you know obviously there's complexity around dealing with the orchestration and the data integration but you know we can go very very quickly to these very one products so I think we're going to continue to see even deeper explosion of these really interesting version one products as these large models uh become bigger and have more capabilities and they're going to really continue to inspire this revolution around this future of software and I'm wrapping up here Dimensions it's the last one I promise it's all good man keep it going it's awesome yeah the final the final prediction and I stole this from Alex so I'll give him credit you know I think we're gonna see this uh you know gptu and what this means is like hi you know the the you know the the this destination of of Open Source models that are going to be highly highly contextualized to your organization so the ability to grab these uh you know models that are based you know trained and fine-tuned on your data on your organization's data that are specific to your business I think are really going to explode and what we're going to see is I'm a lobster tooling that is going to help achieve that right um and and that the core principle here is really is that you know data is the most durable mode um the last mile is always where the value is generated in these workflows and so at the core of what we're going to be seeing here is okay what is the next version of these that are really going to help you tune and specialize these these uh you know these these large language models specifically to your organization's data to your personal data um and and we'll and do that so that's kind of like the concept of that gptu so this has been super fun uh hopefully it was uh useful um happy to chat with anybody later but uh thanks Demetrius dude killer and since I ended up burning a lot of time at the beginning we were supposed to do a whole q a session for you right now but if anyone wants to talk with Diego and keep the conversation going ask him any questions feel free to jump into the slack link that I added into the chat and that's the ml Ops Community slack you can at Diego Oppenheimer and ask him questions we set up a conference channel that you can hit up and that is uh that's that I have one question though for you Diego and it's not about llm Ops if we're going to call it that it is more about you've been in the game for a while I mean you've been doing this I'm not going to say you're an old hat but you you've been doing it for a while right what's different about this time around do you feel like there's actually some legs here or is it are we gonna find ourselves in another winter soon so I I think no I'm I'm I'm so bullish about what's going on right now like I think like we are I was actually thinking about this last night and I was like I can't imagine a more exciting time to be involved in kind of uh you know ML and right now like the It Feels Like You know the for some people it's almost like dizzying like just keeping up but like the level of innovation that is happening at the speed I don't think we've ever seen it in the history of software uh and it is so exciting and an entire generation of developers and hackers and kind of like software artists are being inspired to go build and they're just going to build and people are just building and like the speed at which they're building is just awesome like I I I I legitimately I could not be more excited I think this is uh we will look back in history as this being a crucial time uh in in our existence as Humanity oh dude killer this is I'm excited I'm very excited and you just talking to you listening to you makes me even more excited some people were commenting on your visuals and I think it's worth noting that I think you made all these visuals with every kind of yeah I I tried to be almost fully powered for the um for the entire car talk so I I generated the outline I created the speaker notes I generated the visuals and I thought all using four or five different foundational models from uh different sources good job"
    },
    {
        "title": "Age of Industrialized AI // Dan Jeffries // LLMs in Production Conference",
        "video_id": "3litPFfQvuU",
        "video_url": "https://www.youtube.com/watch?v=3litPFfQvuU",
        "length": "29:56",
        "transcript": "we have another friend that is coming on where we're gonna call Dan to the stage and in the meantime Dan while I'm bringing you up here on to the stage [Music] um I'm gonna introduce you and so he Dan Jeffries is a good friend of mine he is the uh formerly of stability AI now he heads the AI infrastructure Alliance and or AKA Aya dude it's great to have you here I'm so excited every time I talk to you it's just like my mind is blown the vision that you have for where we're going with this is amazing uh and if that is not too set a very high bar for your presentation I don't know what will so we're running a little behind on time I'm just gonna let you have it and then I'll jump in with some q a afterwards all right sounds perfect man thanks so much all right so I did a variation like on uh uh the concept I've had in the past the age of industrialized uh some of you may have read the article uh so I just kind of used the framework here um but if the if you saw kind of a talk on this in the past this one's pretty different I've adapted it pretty strongly to L Labs we're going to basically talk about where all the stuff's kind of what might look like in 10 years and then we're just going to go backwards and say like how do we get there I think there's just a ton of problems that people aren't solving yet and I wanna I want folks to start thinking about some of those problems so that we get to say kind of this magical future so you know think about this it's 20 33 you're a top-notch concept artist in London and you're building a triple a game with 100 other people and the game looks incredible it's powered by Unreal Engine 9 it's capable of photorealistic Graphics in real time uh and it's got near perfect physics simulation uh you know the chips inside of it were designed with the AI ten years ago I would have taken a team of you know fifteen hundred to five thousand people to make this giant game it's super open-ended and you can talk to any of the characters and they can stay on plot uh and you just you wouldn't be able to do it but now you can do it with 100 people and but that doesn't mean there's less work or less games it means more a lot more we used to get 10 AAA games a year and now we get a thousand the powering a lot of this stuff is really what I'm calling large thinking models so these are the descendants of llms and they've stormed in every aspect of software at every level and call them LTM so these large thinking models they're general purpose next-gen reasoning and logic engines with State massive memory grounding and tether to tethers to millions of outside programs and sources of Truth and these ltms are orchestrating the workflow of tools and other models every stage of game development like a puppet master as lead artist you've already created a number of sketches with Unique Style cross between photorealism oil painting some of the Aesthetics from the Golden Age of anime sci-fi you fed those style through a rapid fine tuner and now the AI model can understand and Rapid crack those assets art is definitely not dead it's just become a collaborative dance with artificial intelligence you can move much faster now you can quickly paint an under sketch of a battle robot and for the game and a bunch of variants pop out that you were dreaming about last night you feed it into the artist's workflow and you talk to it and you tell the LTM you want to see 100 iterations in 20 different poses and behind the scenes it's kicking off a complex 30 stage pipeline that you never see you never need to see it it does all that orchestration invisibly two seconds later you get a high-rise iteration it's ready the 27th one looks promising you snatch it out of the workflow you open it in concept painter you quickly add some flourishes a little bit of a different face mask uh you know the generative engine didn't quite get it right maybe a new wrist rocket you erase the shoulder Cannon or place it with an energy rifle you feed it right back into the engine if fix a few problems like you said it's now good to go pops out another pipeline transformed but you know Guided by the LTM which is automatic 3D transformation rigging all this good stuff in the background it pops out into the 3D artist workflow on the other side of the world that artist's working in Chiang Mai Thailand totally distributed team has to fix you know the artist has to fix a few mangled fingers and some armor bits that didn't quite work right it's ready to go he kicks it off to the story writer who's working out of Poland that seeing that new character gets her inspired she knocks out a story outline in many story language feeds it to a story iterator LTM and it generates 50 completed versions of a story in a few seconds she starts reading one of them is really good but it needs some work you know one of the characters been working right a little bit she weaves in a love story fixes some of the middle feeds it back to the engine new draft coming back new drafts come back it reads so well she fires it off to the animators in New York City so welcome to the age of industrialize AI but how did we get here how do we get to intelligent agents embedded in everything Supply chains economics art uh creation Etc everywhere there's nothing that will not benefit from more intelligence nobody's sitting around saying I wish that my supply chain were Dumber so let's roll back in time and focus on today's llms and how they've morphed into ltms and it really all started with as Engineers across the world are working hard to solve the limitations of weaknesses we're kind of coming out of the labs you have Engineers looking at these problems a lot of folks think you can solve these problems in isolation but you can't you know when you put refrigerator you have to put refrigerators into the real world before you realize that sometimes the the gas leaks and they blow up in the early days and so then you fix that problem you know opening I spent a ton of time in the early days trying to worry about political disinformation in the lab and GPT has been used for that about zero percent of the time but they didn't anticipate you know spam or people writing a ton of marketing you know crap emails with it and uh that's because that we can only figure these things out when it gets into the real world but you know what are the alarms good for how do we deal with them today so first it's it's important to kind of understand that mlms are not really knowledge engines a lot of folks are like well let me let me look something up with that um it's it's kind of like a database it's not a database it is a rudimentary reasoning engine that's its key strength and that's really what it's going to be over the time Horizon as we embed intelligence at every stage of the software life cycle the real strength is acting as that logic engine inside of apps you know that lets us do things we'd never be able to do in the past right you know we could go out there and I can have you know an intelligent research if they can go take a bunch of unstructured data which is 80 to 90 percent of the world go look and Discord and slack and on the web and read blog articles and go extract a bunch of useful information and summarize it for me and then put that into a spreadsheet with all the authors names and go to their LinkedIn profile and dump it in there you know that would take you know I don't know go if I said go look at if I hired a researcher and say go look at you know a podcast and listen you know to 2000 episodes and find me every instance where someone you know talked about artificial intelligence uh you know that would be a huge task and now we're going to have these little reasoning engines that can go do that pull that information out summarize it merge it together with other information these are kinds of things we couldn't we can't do we just can't do with current technology so it really opens up these exciting possibilities but the thing is these things are just really not great reasoning engines yet right they hallucinate they make things up they choose the wrong answer they make mistakes of logic and execution and that's because these systems are massive and open-ended it's literally impossible to test every possible way people will think to use or abuse them in the real world if we're making a login system for a website there are only so many ways that it can go wrong security League sub Library error huge amount of things can go wrong with traditional coding but it pales in comparison to what could go wrong with the production llm prom tax blow past the guard rails hallucinations there's whole websites based on prompt injections now the the llm is used to write malware script complex attacks tricking the L M into revealing internal information which is social engineering with a Twist right an old school acting technique that you used to use on people that are using on the llf unsafe outputs like advising dangerous drug interactions right uh picking the wrong next steps in a chain of commands if I have 30 steps it makes the wrong decision on step five do the rest of them fail how do I even know so the list goes on and on we can start to think of these collectively as bugs in the same way that we think of traditional software bugs and as these systems agent types are seeing things like you know Moto GPT and these kinds of things all over Twitter uh doing some really cool things people are going to become a lot less tolerant of bugs right now if you see if you've played with any of these things you see the agents kind of go off the rails uh maybe 15 to 30 of the time that's an unacceptable error rate they're going to get better over the coming years and they're going to get better even faster but if you prompt an l11 it gives you a messed up answer uh you can just prompt it again and uh but if it's an autonomous agent that has the ability to do a lot of steps without a human in a loop and even if we got the error rate down to say 0.1 percent you might think that's perfect no problem except you know if it auto writes an email to someone that offends a big customer or you know reveals some internal information or just says something tremendously stupid and you it takes that 10 million dollar deal with that customer even though that's in the 0.1 percent error rate that error rate is now way out of proportion right so as these things Asian ties I think people are going to become a lot less tolerant if you're just talking to the thing it gives you a stupid answer you can say well I need to re reprompt it again to get get closer to the answer that I want right but now you're going to have to do a lot more things to kind of get this thing to work effectively and I think that could slow down some of the progress I also think it's an opportunity that a lot of folks are missing so nobody is really fixing these bugs fast enough for it all and to fix them we're going to need a whole new suite of tools and new strategies to get there we've got to break down the problem so that we know where to start in projects in companies that are looking to make a difference can start to look at it like this where can the LM break down in production and there's a few major errors that I'd classify as during the training or fine-tuning and in the middle so in the middle has a bunch of subcategories at the point of the prompt in between the model and you in between the model and other models tools for software at the output AKA after the llm response and or during the workflow AKA a dag breakdown so we're going to look at each of these in turn try to understand the implications and the potential ways to fix these but first in the side there's uh if you're hoping for a magical solution a lot of people are well we're just going to train gbt5 and you know then coherent anthropic and a21 and everyone else is going to come out uh with new versions of the models and they're just going to be better and smarter they're going to fix everything uh just a big spoiler alert they absolutely will not better models will always help we get new emerging properties uh more consistent reasoning engines that means less failures of logic but they're not getting us all the way to ltms because Mo complexity no problems to borrow from Biggie Smalls it will be a game of whack-a-mole it's a bit like asimov's uh you know iRobot where you have a robot psychologists uh you know constantly finding new ways for them to break down right if you think about the other kind of probability machines that we have in the universe human beings you know uh those they often make wrong decisions and break or make more decisions and so we would expect the machines to do equally uh badly at times so prompt Point failures could be the user sign or they could be on the side of the model right the model didn't understand it's got a screwy training data you know it just wasn't fine-tuned well enough or it could be the user asking the question in a weird way um in it's it's that's when you ask it for something that doesn't understand the question there were things that understands the question or makes up an answer or gives you a Bogan answer or if we're talking about chain of reasoning maybe it makes a mistake about you know which tip is next so we're going to need guard rails on the systems and to better understand when an llm fails and some of these things they can take a few forms and that's you could constrain the range of the prompts you can pad the prompts you can add a series of rules or heuristics to interpret the input and output Diego talked a little bit about that in the last uh you know example I'll talk about it a bit more and you can create other Watcher llms or models that watch for specific kinds of characters or attacks before or after the prompt here's a an example of chat TBT they're using these sort of compressed set of uh questions with Emojis or whatever that gets the llm to reveal how to hotwire a car because it thinks it's role-playing in the story so how do you fix that well take something like mid Journey we don't necessarily know that majority has an olm behind it um because it's closed architecture but a similar approach to what I'm talking about was used here in terms of padding the prompts we know that mid Journey 4 did a ton of pad prompting behind the scenes we and we know that because people could type in basically a one word or a few words and get kind of a consistent output they've kind of dialed back on that mid Journey five but it's still there and so they stacked a lot of invisible keywords overall prompt engineering abstraction or prompt padding kind of works uh if you kind of limit the number of ways that a person can ask a prompt drop downs things like that the problem is it can have kind of undesired effects so you know if you put in uh you know prompt stacking to fix maybe a diversity challenge you know you want to get that you type in CEO you want to make sure you're getting uh you know women and uh Asian folks black folks white folks et cetera you want a range of folks non-binary folks whatever it is um if if we prompt to fix that you may end up with uh you know if I ask for Mario as a real person I don't get a male Italian plumber right so at best this is a heuristic kind of rule-based hack and it's only going to get us so far you know we had um you know you could think of these as kind of like a Malwarebytes or filtering based on signatures or looking for prompt injections other things that we want to alter filter at the point of prompting But ultimately they're not good enough and that's because you have to remember the Sutton principle from the bitter lesson and that's that the biggest lesson that can be read from so many years of AI research is that General methods highlight you know italics line General methods that leverage computation are ultimately more effective and by a large margin and he like he highlights two things scale or scale and search right or uh learning excuse me and search learning statistical Methods at scale so machine learning essentially and a lot of people misinterpreted this paper they thought well you know humans are irrelevant you just scale up the computers that humans go bur it's not what he was saying he's saying that you know for instance you know deep blue had a bunch of kind of heuristics built in there for you know Levering Pawn controls right or the controlling the center of the board stockfish which came along after that was a great chess playing program it had Alpha Beta search so it was scaling search but it also had a bunch of like human-based domain knowledge baked in right that said essentially go ahead and control the center of the board Etc so you say don't waste any time with that those kind of small domain knowledge based algorithms go for more generalized ones like RL or um you know statistical learning any of these kinds of things they're always going to beat it in the long run so that's why Alpha goes zero which basically just learned from Morrell and playing itself and didn't have any domain knowledge baked into it other than the basic rules of the game the smashed stockfish over time so we're going to want to get PS the heuristics into more advanced generalized systems they can deal with this I suspect we're gonna have lots of Watcher models you know kind of dealing with these things it's going to be similar to the same way that we had kind of a Towering Inferno of rules in the early days of uh in the early days of spam filters they were maybe 70 accurate it was a natural inclination of the program to say hey there's an email that says dear friend I can write a rule for that or fix it um but over time it starts to break down and then you get a general purpose system like a Beijing filter classifies as the ham and the spam all of a sudden it's 99 accurate so we're gonna have to make that transition but in the short term we might have nothing but heuristics until we kind of figure these things out for a period of time we're going to have these sort of basic rule you know basic towering for no rules for a period of time to kind of help us keep these things on the rail the other place where this the model mostly fails is kind of in the middle and so I think there's a massive opportunity for AI middleware and that's because that's where it's most of the time it's going to fail as we integrate these llms into the in with other tools and make them more autonomous they are going to fail in strange and spectacular ways that traditional software doesn't fail you know you're chaining the other commands and it pick excuse me picks the wrong order throw the wrong step goes into a text generating death spiral there's a million of these we're going to email aware of that checks the input and output at every step that's the key you know and what's that going to look like again it's probably going to be a hybrid mix of traditional code smaller watch Watcher models that can understand and interpret the results and check them to see if they make sense and that kind of ensembling is going to help us make better decisions more often we are not yet used to detecting the kinds of errors that llms deliver it's not a clear coat it's not a 404 error it might seem like a perfectly normal uh formatted correctly formatted answer that's really an error of logic and so how do we know that it's neurologic how do we know the third step shouldn't be to you know push code or ask another question it should have been you know go do an additional search to clarify information there's really nothing that exists to detect these things at an advanced level uh to pinpoint them properly consistently huge opportunity for folks out there to be in the middle to take uh inspiration Solutions of the fast like um API management layers and those kinds of things other places it can fail are basically training failures and you know the model itself might be the problem problem wasn't trained well enough doesn't have the right capabilities hit the limits of its architecture poorly aligned might not have enough emerging capabilities that can be harnessed and most of these get better with better training and better faster fine tunes but it's really just not fast enough now the speed to fixing any of these problems is way too slow at the moment fine tuning is slow it's scattered it's more art than science there are many many many bottlenecks to speeding up these fixes like the need for human labeling or scoring of data sets and uh you know we're starting to get models now at gpd4 and blip 2 they can kind of label things Auto magically but in general uh when you look at something like Foundation models the data is almost universally poorly labeled right it's actually amazing that it works at all and there's no way you could physically label all these things you can find you can label a small data set curated asset but not these massive models so you know what are the fixes going to look like here's an example from The Lion data set uh where the label is diabetes the quiet Scourge you know that's a clever line probably from a blog post but it really has nothing to do with what's in the image stethoscope you know fruit Etc and it's not going to teach the model anything uh if in fact it's going to teach it a wrong idea so llms are about to bootstrap that process and make creating that synthetic data and or labeling largely obsolete we're going to have not only a ton of convincing synthetic data but more well-labeled data at scale and that's the key at scale if the lion data set uh and it's got 20 perfect labels 20 decent labels 30 mediocre labels 30 totally wrong labels that's a huge amount of room for improvement so multimodal LMS diffusion models and the like scene to learn like I said despite themselves if the llms are then able to label that data you know more accurately the 70 80 90 that's a massive Leap Forward just for the foundation models and then it's also going to help speed up the fine tuning so when it comes to llm's labeling RL uh HF examples we're going to need models that can guess 90 of the time what the human preference is uh and then surface only a small subset of those labels if I have to have a data set and a human scoring every time that there's a bug in one of these LMS there's really no chance that this is ever going to be able to scale so we've got to speed up the process some of the other things you can do is there's going to be more grounding so you have Goldberg kind of noticed recently that llm seemed to do really good with coding and he said you know it's kind of because it's a form of grounding and that's because you have the the comments which go with the code so if human you know natural language and the code itself so that's anchoring that knowledge to stuff right and and I expect natural language overlays and labels for just about everything in the near future right so if the LMS can ground better by reading that text storing in a vector DB whatever it is they're good we're going to start having natural labels for everything so that llms can kind of consume this and other forms of grounding obviously are connecting to external data sources you know Wolfram Alpha uh which was for many years you know doing symbolic logic and but I've taken tons to integrated years ago and now it's 10 to 20 lines of natural language rappers around the apicals that's amazing so we're going to start to get these overlaps of symbolic logic uh and external sources to get things like web brain where it's trained on huge uh Corpus of Wikipedia and it can go out and look at Wikipedia and inject that into the prompt as it's generating uh really useful yeah so let's you know expect more clever grounding hacks to come they're uh they're going to be there in the near future and we're also going to start to see Violet teams which are basically a form of red team and self-reflecting llms so these are L M's acting as an engine to fix itself uh spitting out the synthetic data testing it labeling it and then checking it by people quickly so let's say someone catches a model advising people to commit suicide right so the Violet team uh which is a security team variant kicks off a series of engineered prompts to explain why it's never a good idea to kill yourself right so it takes the original prompt and then it says Hey explain why you don't it prompts it by saying hey explain why it's a bad idea to kill yourself and it uses the LM again pairs that original question should I kill myself with the the the modified response pairs that together and then you say to the llm okay give me the original question give me 50 or 100 or a thousand variations on that question and give me a thousand variations on the answer now you can Surface a small subset of those to people to check you could score them with another model and now your data sets complete he runs you know a rapid lore up fine-teen or something like that you're ready to go so we're really going to need a training Revolution too it's just got to be a lot less art more science got to be a lot faster training right now it's horrible it's Emma lopsy it's slow it's ugly it's low level um if again if companies are getting together a data set kick off a long-running job and then run tests and fix every single problem it's just not going to work and you're also going to keep adding you know adding kinds of things we're going to have these things interacting with hundreds or millions of people uh and suit billions it's going to need an order of magnitude faster way to train out the bugs uh one one thing I'm sort of seeing is what I'm calling sort of model patching and so that's adapters things like Laura's they're really just the first step it's easier to train they're smaller they don't change the weights of the original model much or at all or they add parameters to the models there's some challenges with it they could just kind of scale up uh the amount of memory needed the more tasks that you add et cetera uh but the techniques are developing quick you got adapter H AKP tuning you got gmatics you got Laura's I expect to see many more of these I expect to see lots of models with hundreds of thousands of patches hundreds of thousands of adapters kind of working together that make them Stronger Faster more grounded less vulnerable and more secure and stable uh it might need ways to even compress those things together so you might have 20 adapters and then you average them together or whatever to make them smaller to reduce the amount of things that you're going to be using closed Source systems you can't do some of this stuff on unfortunately because it's just an API call and so they're going to have to adapt to allow this kind of extension of the models because they can't be retraining you know uh gbt gbt4 or whatever every time they need to fix something it's just not going to work so we're also really going to need kind of updates and continual learning so adapters can struggle in complex tasks and there's also likely a limit to how many of these we can chain together before a performance starts to suffer or we start just adding somebody fast that the memory you know becomes you know uh just impossible to deal with uh there are it's already challenging enough to deal with in these you know 100 billion trillion parameter models so really going to need advancements in continual learning we have recent papers like memory efficient continual learning from Amazon researchers that adds new tasks and updates the model without scaling memory requirements uh as the parameters increase for each new adapter task so that's really exciting we're going to need more more breakthroughs continue learning is going to be the real answer we need to make these models lifelong Learners and if you look at the anthropic deck for their big you know 300 million dollar raise or whatever they're looking at they basically think that you know within a few years uh most of the gigantic Foundation models are going to be trained and there's not going to be any chance to sort of catch up and that essentially we're gonna we're just continually sort of train those models over time and make them smarter I don't know if I fully agree but it isn't but it does but there are is going to be kind of big moats created by these models especially as they crack continual learning and if I can just keep adding tasks and new data to these systems and and they get smarter and smarter over time without catastrophic forgetting yeah you know then I have a significant mode and that's going to be very interesting that gets us a lot closer to the LTM concept that we talked about in the early part of the presentation so look this is the end uh we're entering this age of industrialized Ai and ai's out of the labs moving into software applications when engineers get their hands on it they think about it in a different way and that's exciting when I saw stuff in the stable diffusion Community where they just started jamming together blending lots of models many people thought that wouldn't work I saw one researcher Jam together 200 models and most people thought the models would collapse and they didn't they made a better model so that's exciting you start to see these techniques the Laura for instance was adapted from llms two diffusion models to the point that the author of the paper was then on the subreddit talking about I never even thought it could be used for this so can how can I adapt the next version of that to fix these kinds of things so that's the kind of thing that Engineers do they take things from a lot of different places they Jam them together they learn from the past they think differently and so we're going to see a lot of the mitigations for these problems a lot of people are worried we're not going to come up with the answer to these things that's ridiculous we are that's what Engineers do that's what engineering is all about it's fixing problems in the real world so we're already seeing the seeds developed in Solutions to the most well-known problems with all of them and they're going to come fast and furious over the next few months and years to make these trusted for Enterprise environments and to make them more explainable to make them more controllable to make them better understood to make them stay on the rails more often we're gonna have smarter more capable more grounded more factual models that are safer and more steerable and it's not just going to be one company doing there's going to be these techniques applied to all the kind of models and the techniques that are out there they're going to be rapidly adapted back Upstream to the foundation model so anything your teams can do to push this forward we don't need another company wrapping something around GPT there's already a million of those that's cool if you want to do that but if you really want to push forward to the kind of age of industrialized AI you got to get in there in the middle you got to get in there in the fixing of these things you got to get in there the engineering side of the house and that's the thing that ends up powering us to the ubiquitous ambient intelligent age much faster that's it dude that is so awesome man ah I love every time I talk to you because I get so in and that is I got so many questions for you but because of my foreign [Music]"
    },
    {
        "title": "LangChain: Enabling LLMs to Use Tools // Harrison Chase // LLMs in Production Conference",
        "video_id": "XTczX82wzLQ",
        "video_url": "https://www.youtube.com/watch?v=XTczX82wzLQ",
        "length": "11:43",
        "transcript": "let's hear it for none other Mr [Music] [Applause] [Music] s [Music] oh you blushing dude that's ridiculous what are you doing that's ridiculous I appreciate it thanks for having me here man this will be fun so here we go man I'm gonna hand it over to you and let you get started awesome thank you so yeah I mean there's a there's a lot to potentially talk about uh you know I think uh Lane chains been really focused on kind of like prototyping for the most part but now we're starting to think a lot about what does it take to actually run these chains and and agents and and everything that people are building in production and so there's a lot of different things that I considered uh talking about um but I think the one that I settled on is basically how to enable llms to use tools because I think it covers a lot of different components that um you know even even in willem's talk just before the study that he mentioned a lot of those are really relevant for this um and then this is also very top of mind with with all of the AI plugins um the the chat GPT plugins and and and all the like Auto GPT baby AGI stuff going on so I figured this would be a fun uh conversation for today um so uh yes so I'm gonna try to talk in 10 minutes about why is tool use important different types of tools then the main chunk of this will be uh talking about how to get language models to use tools um and some pitfalls uh that that occur and then and then common fixes for them um and then a little bit of Spotlight on some of the open API tools that we've been working on over the past few weeks again kind of driven largely by this AI plugins chat GPT plugins announcement so why is tool use important um I think there's a bunch of different uh reasons I think the two main ones are allowing it to retrieve relevant context so so get in information about current events um pull in information about proprietary data navigate complex data structures so so tool usage itself can actually be used to interact with data structures and then also allowing the language model to interact with the outside world and by this I really meant kind of like taking actions um so whether it be pushing something to a database or or some more complex things like that basically you know language models are themselves just text in text out or or roughly that and so hooking them up to different tools can enable a lot of different and really cool capabilities on that note different types of tools this is still I think a really uh interesting area to keep on adding more things but the main ones that we've kind of seen are search engines this is really relevant for uh kind of like getting current information in calculators alarms aren't great at math necessarily and so adding in calculators can help them with that retrieval systems this is again around pulling in proprietary data there's been some really cool stuff around coding agents and having language models basically code and so interacting with python repels or other Rebels for that arbitrary functions are kind of just a generic catch-all you can you can really make anything a tool that you want and then apis and that's bolded because yeah with the with the chat GPT plugins has been top of mind for a lot of folks um so how do you get language models to use tools like uh like everything with language models you just kind of tell them to um so you tell them when to use them you tell them how to use them and then you want to tell them what they return as well and so this is a bit um over simplistic obviously but I think it does uh underscore a big part of the the and I'll talk more in depth about a lot of this in in just a second but you know you tell them to it's kind of really a real answer for how do you get language models to do things um it's uh I think yeah it is really about what you put in the prompt how you put it in the prompt um and then and then how you use the output as well um and so so you tell them to is the quick and and and short answer but there are a lot of challenges with this um and so some of the most common ones that we see people running into in link chain are how to get language models to use tools in the right scenario um how to get them not to always use tools uh oftentimes you know you have a conversational bot or something and you may want to to have the option to use a tool but it's also totally fine if it just wants to converse with you and so it's striking that balance is really tricky um and then the third one that we've seen a bunch is basically parsing the LM output to be a specific tool invocation um and so I'm going to Deep dive on these for the majority of this presentation so um for the first challenge getting them to use tools in the right scenario um so there are a few different kind of like tips tricks techniques um that we've uh discovered heard um kind of see to deal with this um one is making the instructions uh really clear whether it's instructions in the prompt whether it's a system message for now these new chat based models um you you kind of have to uh tell them to tell them to use tools tell them what tools they have available um tell them what the tools do so this gets to point two around tool description um telling them when to use tools in certain scenarios is really useful so you know if you give a language model a search engine that's really good for like current events or something which I think is the main use case for for search engines um I've found that you know you want to put in the description hey use this for current events um otherwise it will it will do some guessing and it won't always be perfect so for a lot of the questions around you know the language model isn't using my tool in the right way the answer is kind of like beef up the tool description um and then tell it that it should use it in the right way um repeating instructions at the end especially for some of the older models has been really helpful because I've observed purely anecdotally that if you put kind of like instructions at the beginning by the time it gets to the end it kind of like forgets about them a little bit and so my general technique is put instructions at the beginning like hey you have these tools you can use them you should use them in this way Etc and then and then right at the end be like remember to like uh format uh the output in the correct way or remember to only use tools if you need to um and so a little reminder at the end goes a long way um and then the fourth one is a bit newer but this gets around um and I think Willem kind of touched on this a little bit earlier about using some like embeddings or semantic similarity to decide when to use tools and this is really useful when you have a lot of tools so if you have like 100 tools you can't you you probably can't put them all on the prompt and and ask the language model to choose between them so one thing that we've seen be helpful is basically do some tool retrieval step first retrieve them then put like the top five tools in the prompt and ask it to decide between those um so that's around getting them to use tools in the right scenario um the second challenge that we've seen is they don't always need to use tools and so again kind of like telling them that in the system message again repeating the instructions at the end and then the third interesting thing is basically adding a tool which itself is basically like just responding to the user um and so we've seen this be really helpful um because yeah the the instructions sometimes Aren't Enough by themselves and so explicitly having a tool quote unquote that it can call to to get um to get uh a response to the user has been has been really helpful there um and then the third challenge that we've seen is it's basically parsing llm outputs to get a tool invocation and so the solutions that we've seen for this are basically using more structured kind of like response types like Willem was talking about um so Json typescript have been have been really good ones um and then we have a bunch of Concepts in link chain around how to do this kind of like explicitly and easily so we have a concept of output parsers um and we have also a concept of of fixing output parsers so jumping through some examples really quickly there this is the base kind of like output parser example you actually Define it in pedantic um so so super common to those who are who are familiar with python um in in the JavaScript library we have a different kind of like schema definition thing and then we automatically can convert the pedant object into an output parser you can then use that output parser so if you look here we do parser.getformat instructions and so this uh uses the the schema to generate some format instructions and these are typescript Json and and then we put those format instructions into the prompt itself and so that's how we tell the language model to use a specific format and then we can then parse the output explicitly back into this pediatric object there's the obvious question as as Willem was mentioning like what happens when that fails um and so as we can see here we have an example of a output parser that's misformatted and so we have um we have some stuff in LinkedIn to basically fix that by passing it to a language model and asking it to fix the the in this case the Json decoding errors there's an interesting kind of like Nuance here where this doesn't actually um fix all the issues that could possibly arise so if we go to the next slide we can see here that uh the the response in the at the top is actually invalid not because it's bad Json but because it's missing uh argument and so if we ask another language model to just like fix this thing it won't actually know how it should fix it because there's a second argument that should be there where there is some kind of like correct meaning and so here it just kind of like puts a blank string but that's not actually what it should be we can see at the bottom when we use this other type of output parser um it kind of retries with the prompt originally so now it has all the information um and I know I'm running a bit low on time so with the last slide just wanted to Spotlight how this all comes together for some of the open API tools which are these chat GPD plugins um we can use some of the prompting and output parsers um to to communicate the Json and typescript parameters for each endpoint and then this is a bit of a tangent but basically we've seen that language models still struggle with some of the more complex parameters and and function definitions and so we've found it really effective to actually wrap each endpoint in its own chain and have that chain basically be responsible for a single endpoint because then it can kind of like learn and know how to interact with that endpoint and those complex parameters better and then we have the agent this basically router language model communicate with each chain independently and the input to those chains is just a string a natural language string and so it's way simpler to kind of like tell the the agent to interact with those chains um that is that is all I have for today I think I went a little bit over so I hope you're not too mad at me Demetrios for having me um yeah people are NSF I would love to see you at the Meetup tonight there we go man hello [Music] foreign"
    },
    {
        "title": "How LlamaIndex Can Bring Power of LLM's to Your Data //Jerry Liu // LLM's In Prod Lightning talk 3",
        "video_id": "ix4EjVLGUKc",
        "video_url": "https://www.youtube.com/watch?v=ix4EjVLGUKc",
        "length": "9:59",
        "transcript": "awesome um thanks so much for the opportunity uh here I'm super excited to give this talk uh the goal of this talk is how llama index can connect your language models with your external data so my name is Jerry I'm one of the creators uh and co-founders of llama index and I'm super excited to be here so the basic context here is that llms are a phenomenal piece of technology for knowledge generation and reasoning uh they're pre-trained on large amounts of publicly available data and so you've all seen the amazing capabilities of llms by just playing around with stuff like attribute you know they can answer questions they can generate New pieces of content they can summarize stuff for you you can even use them as planning agents so basically you can have them perform actions get a response and perform more actions over time pretty much every application developer that's using llms thinks to themselves how do we best augment language models with our own private sources of data and so whether you're an individual or an Enterprise you're going to have a bunch of raw files lying around for instance like PDFs PowerPoints Excel sheets uh pictures audio you might use a few workplace apps like notion slack Salesforce you might have a if you're an Enterprise a heterogeneous sources of data collections from data Lakes structured data Vector DBS you know object stores all these different things and so the key question is how do we best augment our alarms with all this data there's a few paradigms for injecting knowledge into the ways of the network these days uh probably the most classic machine learning example is through you know some sort of fine-tuning or training or distillation process uh the idea is that you know you take this data and basically run some sort of optimization algorithm on top of this data that actually changes the weights of the network itself to try to learn the new content that you're feeding it and so you know if you look at a pre-trained model like chapter BT or gpt4 they already internalize a huge amount of knowledge if you ask about anything about Wikipedia it'll be able to uh understand and and give you uh you know any any information about any Wikipedia article on there however I think another Paradigm that has emerged these days is in context learning and so for a lot of users fine-tuning tends to be a bit inaccessible for both performance and cost reasons and so these days a lot of applications developers are using this Paradigm where they combine a pre-trained language model with some sort of retrieval model to retrieve context and they manage the interactions between the language model itself as well as this retrieval model so the way it works is imagine you have some knowledge Corpus of data and so imagine you have a notion database of various text files this is about uh you know like say the biography of an author it's about Paul Graham and so the idea is that you have this knowledge Corpus of data and then you have this input prompt and given this info prompt it would look something like the following it would basically say here's some context and then the retrieval model will be responsible for retrieving the right context from the knowledge Corpus putting it into the prompts and then giving given the context answer this following question and then you put the question in here and then you send this entire prompt over to the language model in order to get back a response so some of the key challenges in context learning is how do you actually retrieve the right context for the prompt and we'll talk about some common paradigms as well as less common paradigms that might solve more advanced use cases how do you deal with long amounts of context how do you deal with Source data that's potentially very large and also very heterogeneous you know you might have unstructured data semi-structured data structure data and how do you actually trade off between all these different factors like performance latency cost this is basically what llama index's entire mission is focused on and so you know imagine you're just building some sort of knowledge intensive uh language model application whether it's a sales tool marketing tool recruiting tool Etc and your input is going to be some quick Rich query description it's going to be either just a simple task uh like a simple question that you want an answer to it could be a complex task that you feed in the idea is that it's basically something that you would normally feed into chat gbt but here you're feeding it into us as an overall system llama index itself is a central data interface for a language model application development and so we set on top on top of your existing data sources or databases and we manage the interaction between your data and our language model and the response is basically a synthesized response with references actions sources Etc and so going into a little bit about some of the components of wama index and and I think the goal of this talk is is to talk about some of the additional use cases that really solve so we'll get into that in just a few slides our goal is to make this interface fast cheap efficient and performant so we have three components we have data connectors which you can find on llama Hub they're basically just a set of data loaders from all these different data sources into a document format that you can use with llama index or even Line train um and then the other next step is data indexes so once you address this data how do you actually structure this data to solve all the different use cases uh uh of kind of uh knowledge augmented generation and then the last component is this query engine which basically completes this black box and once you have these data structures under the hood now you have a query engine that takes in a query and is able to Route it to these data structures to give you back the response that you want so the goal of this is really just to give you a few concrete use cases that llama index solves and so probably the most simple one that pretty much everybody is doing these days whether using qualama index for using another kind of vector DB stack is semantic search and so imagine you load in some sort of text documents right so first line of code just load in some text documents second line of code you build this like simple Vector index from these documents and typically what that looks like is you Chunk Up the text embed it plot it in a vector database and store it for later then during query time you know you ask some question like you know what did this author do growing up imagine the text data is about the author and you would just do embedding similarity-based retrieval from this uh you know Vector database or a document store and then you would take the top K of relevant trunks inject it into the input prompt and get back a response and so for simple questions like fact-based questions like what did the author do growing up or stuff where there's semantics and the query that map well to semantics and your knowledge Corpus this works pretty well and so you can see we using this Paradigm the answer is you know the author grew up writing short stories programming on an IBM 1401 this is retrieving the relevant chunks from your knowledge Corpus in order to generate this final response there's also summarization so summarization is actually a little bit different because instead of doing top K retrieval you actually want to go through all the context in a document in order to synthesize a final response or summary of the document and so for instance you know we load in some documents through the first line and here this is another data structure called the list index so instead of uh storing you know each node with an embedding associated with it a list index actually just creates a a kind of a flat data structure of all the nodes within a document so that when you ask any sort of query or input prompt over it you explicitly want to go through every node within your list in order to use that as context to generate the final response so for instance for a query like could you give me a summary of this article a new line separated bullet points this is the first response that you would get and then this is an example of the answer there's also connecting to structured data where you can actually run Texas SQL queries on top of your structured data and so that's actually a different Paradigm than unstructured data because you inherently want to convert this into the query language they could run over a structured database and some of our Advanced constructs include stuff like being able to synthesize across heterogeneous data sources if you have notion documents as well as slack documents how can you best synthesize information uh not just you know by treating everything as one combined Vector store but actually explicitly going through and combining information across your notion and slack documents here is an example diagram that shows this use case and probably the last thing I'll talk about is also multi-step queries if you have a complex question how do you actually break it down into multiple smaller ones over an existing data source in order to best get back the results that you would want so for instance let's say you have some existing index or knowledge Corpus about the author and you ask a complex question like who's in the first batch of the accelerator program so therefore you know you can actually break this down into smaller questions use those to you know get back retrieval-based answers and then combine everything together to synthesize a final answer right here long story short there's a lot of stuff you know there's more stuff in these slides and a lot of this is found in the docs and the idea is that llama index solves both the uh kind of short uh the the kind of simple use cases of semantic search as well as more interesting interactions between the retrieval model and your language model awesome thanks for your time thank you so much Jerry that was awesome [Music]"
    },
    {
        "title": "Generative Interfaces Beyond Chat // Linus Lee // LLMs in Production Conference",
        "video_id": "rd-J3hmycQs",
        "video_url": "https://www.youtube.com/watch?v=rd-J3hmycQs",
        "length": "24:49",
        "transcript": "alrighty uh there we go cool I don't know or between between uh Harrison and that song I don't know how to top that but but I'll make my best attempt um happy to be here to talk about generative interfaces Beyond chat uh I'm I'm Linus and I'll do my intro in a bit but um where I want to start today is I think we all generally have a sense of catch upd style chat is super useful super valuable you can do a lot with them but it's I think at this point we all kind of accept that it's not the end of the road right with these interfaces and I had a I had a a tweet a while ago where I was like you guys are telling me that we're gonna invent literal super intelligence and we're going to interact with this thing by sending text back and forth and and it's it's obvious that it's not the end of the road but Chinese here today most um usages of language models and interfaces that I've seen in production are built on chat style interactions or dialogue style turn-by-turn kind of interactions and there's interesting experiments on the fringes some of which I'll hopefully mention later but Chad is here today and so the leading question that I want to spend our time talking about today is given that Chatters where we are and given the possibilities for other things to come how do we incrementally evolve what we have chat repeat style chat forward to build more interesting interfaces that balance the flexibility of language models and the power of language models with the ease of use and the intuitiveness of some of the other kinds of interfaces it can build so uh excited to talk about that a little bit about me I think a lot about UI design interfaces interaction design Nai and I've I've spent a bunch of time thinking about that in the context of building creative tools and productivity tools so it makes sense that I am currently a notion number research engineer at notion before that I spent a couple years working independently also pursuing these ideas building a lot of prototypes uh it sounds like some of which are going to be linked somewhere in the chat um and have worked on other kind of productivity tool companies and apps before so if I had to roadmap we're going to talk about I think there's sort of three big buckets first I want to just lay the groundwork for how should we think about conversations and dialogue what are the parts of a conversation that we should think about when we build language models for conversations and then second talk about specifically the idea of context in a conversation and the selection in a conversation which will will come up and then third I want to land on this idea of constraints and adding constraints and the benefits that they can have and how we can balance adding constraints to make interfaces more intuitive and more easy to use without sacrifice and Power so let's talk about conversations let's say you and your buddy here are about to talk about something and you want to say something even before you say anything at all the communication channel has already opened and kind of started because in any conversation in any kind of like linguistic act uh do you you start with a shared context that context might be uh your friend just pulled up with a chair next to your office and you're about to pay a program it might be you're in a supermarket checking something out it might be you're collaborating with a co-worker it might be your friend uh or a stranger walked up to on the street that context determines how everything that you say and everything that your interlocutor says back to you is interpreted and so I think context is important and notably I think in applications like child PT context gets very very low billing uh you basically start with zero context and you have to embed all the context that you want the model to use to interpret your words in the prompt itself which is where I think a lot of the difficulties of prompting come from so you start with some context and then uh your the speaker will imply some intent it could be explicit and direct like hey can you pass me a glass of water you could be a little more uh it could be a little more implicit like if I'm a construction worker I'm building a house or I'm like assembling a Lego kid I might say oh the blue brick and that's not a complete thought but in the context it can be interpreted to to figure out exactly what you're looking for or it could even be just like me pointing at something right or or like doing like one of these and then and then that my partner in in the conversation can interpret the intent out of what I'm doing and that's that's the speaker's role unless you have the intent um especially important for language models as giving the model time to think I'm borrowing some I'm abusing some reinforcement learning terminology terminology here and calling this rule out but uh people call this Chain of Thought call this a scratch Pad some internal monologue for the model or for the recipient of this action to figure out exactly what you mean and kind of do the interpretation of your intent what you said within the context that you have and then once the model or the recipient of the message is done thinking there's some action the action might be answering the question so maybe just textual but more exciting and more often I think we're seeing lots of apps where the action is some combination of a response back and the action that the model is taking whether in an application or integrating with an API so I think this is kind of where the anatomy of a conversation if you really break it down in a typical kind of language model usage Style let's let's take uh co-pilot does it as an example this is a screenshot from co-pilot next or copilot X chat all these names are insane um copilot chat inside vs code this is one of those cases where there's very clear you're already starting with some context if you're building something like this you wouldn't just build the chat you would want the language model to be aware of all the context as much context you can get out of this application so the context includes things like what files you have open uh or do you have a terminal open is there what's the last a few commands and the outputs that they got because maybe the error message in the terminal can inform what the model can do for the user it includes it in things like what line is the cursor on for the user or what lines do they have selected because selection is actually a really strong signal for what the user is thinking about and looking at it's kind of like pointing but but on a screen or in a text editor so you start with some context and there's the intent the intent is in this case is write a set of unit test functions for the selected code and you can see that in interfaces like this you really need the context to interpret the intent correctly and the more context you have usually the better that interpretation is going to be and then presumably there are some some internal monologue thinking for the model um uh and then after that we we get the models action back out so when we think of prompt these conversational interfaces I think we usually focus on the intent The Prompt and then the completion the output the action but there's there are these other elements that I think we should also think about um the context in which the users uh intenses being interpreted and then also the internal monologue so uh this is a screenshot of Microsoft's copilot for Excel I think this is an interesting example of a really rich valuable application where there is a chat interface but where there's clearly a lot more we could do um so so in this case there is so much context that the model can use to figure out exactly what the user is doing and maybe what they're trying to accomplish um and if you had a chat sidebar like this the sidebar sort of exists in a totally different Universe uh than the chat I mean the model can theoretically look at everything that's in the spreadsheet that the user has open but in this case in this screenshot the user is having to refer to specific columns and specific part of this spreadsheet verbally by naming things saying you know the in this case it's the column about uh the sales data or last month's sales data why can't I just point in and select and then say what about this column right in the real world that's kind of how we work if I want to refer to something that's in front of me I'll just point to it I'll just look at it um so in this case I think having the chat agent be co-located or like inhabited co-inhabiting the same workspace that the the user is in I think is a key part of how to make these interfaces gel a little better and without that I think these conversational interfaces start to feel more like the the command line interface of generative AI where you're having to specify increment every possible information about your intent and your action explicitly into the prompt rather than being able to work more fluidly in a kind of a shared working environment so uh how do we how do we where do we go from here well I think I keep talking about this idea of pointing and and using the context and selecting things and so one really powerful technique that I think we can look for is using our hands using selections and pointing and when you point at things in the context or when you select things there are a few different ways I think for the language model to observe what you're pointing out or what you're doing sort of in order from both grounded and reality so that I think the most interesting and kind of out there I think we can start with uh yeah so point and selection interfaces uh one way to think of point and select is uh sort of breaking down your action into nouns and then verbs so what I mean by that is if you're in a spreadsheet the noun might be like the column that I want to manipulate and you select the noun and then the vertical bee I'm going to filter it or I want to aggregate it or I want to hide it or delete it or duplicate it um if you're in a writing app then now it might be a single block in the writing app like the title block or it could be the entire page or it could be like a new file um in the real world this point and select mechanic is sort of built into every object in every material if I wanna take action on some object I have to first like grab the thing and then do something with it um but in in chat style interfaces I think it's less obvious but this point in select mechanic is also what makes the web great for a lot of applications because there's existing sort of materiality built into everything on the web every bit of text by default on the web is selectable you can select anything you can copy paste anything uh you can uh often drag and drop files into web pages and so there's all this like noun and Denver based mechanic built into materials that you use to build apps on the web and uh in chat all of that kind of all of those affordances around selecting objects and then applying actions to them are kind of gone and I think we might we could think about how to bring that back to uh chat style interfaces and point and select I think are most useful for helping clarify context and focus the model on something you're looking at or alternatively directing the kind of action stream or the output stream of the model so if you're in a writing app you could say you know take select something take summarize this bit and then like put it here at the top of the page or or you know make a new page over over here and being able to point point and select I think are useful for directing the output as well so there are a few ways that we can get the model to observe what you're doing or what you're pointing out the most common one currently and I think the most obvious one is sort of this what I'm calling like the omniscience model which is the model can look at everything everywhere visible all the time it just kind of knows the entire state of the application but it's up to the model to figure out what to query and what to look at it's so it's technically the context is technically fully accessible but the model doesn't know exactly what you want the model to look at next level up from that is what I'm calling call by name which I think is kind of interesting for certain types of application especially kind of pro applications where there's a lot of flexibility and customization if you have an application like like a design app like figma or a sketch you could imagine naming different artboards or different panels and then being able to app mention and say hey can you clean up you know panel two or can you clean up like the like timeline panel so uh this only really makes sense for environments where it makes sense to name objects and refer to them by kind of handles or names but if that's the case then I think this is an interesting way to incorporate context and be able to directly point to things but with your words using the names there's also this kind of really interesting uh interface that I I don't think anybody's really seen in production which is what I'm calling literally mentioning something um this in particular is a screenshot from a paper from a project called sikulu from I believe an MIT lab where they uh had a programming language that interleaved icons and images with a way to program around the UI and you could mention you could imagine an interface where if I wanted to refer to a paragraph I could start writing and saying summarize and then I literally drag and drop the paragraph into the prompt box or if I wanted to transform an image I could drag and drop an image or or if I have to if I want to talk to a person that's in my contact list I could grab that person's icon and say hey can you call this person and then just drag and drop that image or that object and so having support for these rich objects inside the prompt box I think is a really interesting possibility and then the last one is what I'm calling contextual actions which is a great example of this is uh like a right click so uh an example of the right click is these context menus right so the left is notion right is figma you grab an object sort of metaphorically and then you can see all the options that are available to you all the things you might want to do with that object in a lot of current applications these are hard-coded in but you could imagine using a language model to say okay here's what here's the object that the user has in their hands or the most likely actions given the full context of the application and given maybe even their like history of actions and what the title of the file is and what they want to do what are the most likely actions they might want to do you could you could have the model select from a list you could also have the model generate possible trajectories that the user might want to take and so context menu I think is an interesting way to reveal actions that you want the user to take without having to force them to to type the instruction up fully another kind of context menu pattern is this kind of thought driven programming or autocomplete driven programming which I think is the the analog of right click but with text so if I'm typing in a text editor or code editor and you hit like dot like document.potty Dot and it'll show me all the autocomplete options this is kind of like saying I'm holding this object in my hand what are all the things that are accessible to me from this or what are the actions that I can take from this or uh in the the other panel I have tab completion so I'm working inside a terminal I have this CLI in my hand what are the things that I can do with it tell me the possibilities and this is another way of grabbing an object and then sort of showing me what's possible and you can imagine powering something like this with a language model as well and then lastly this is a slightly more complex pattern but but where you if the user selects an object you could uh materialize an entire piece of UI like a side panel or or a kind of an overlay so on the left again is notion AI on the right is is keynote uh which is what I was using to make the stack and in either case you select an object and then you can see a whole host of options for how you want to control it and this gives the user a lot of extra power at the cost of maybe not being obvious exactly what the user wants to do or what the user should take action on so in all these cases we have this sort of like noun and then verb like choose the object and then what action you want to take kind of pattern and that lets the system constrain the action space that the user might want to take and maybe even come up with uh follow-ups or suggestions and what the best actions you could take are given all of this given and given what we talked about around the anatomy of a conversation I think and then when you look back at something like Challenger PT charger PT is really just about okay you have this little tiny prompt box and you have to cram all of the context all of the intent in there and also everything that you want the model to know about where you want that model to take its action and that that I think is a good place to start but it is limiting and there are ways we can expand out of it um so one way to summarize the the sort of ground that we've covered might be that the Holy Grail or one powerful goal of user interface design is to balance um intuition building an intuitive UI that's uh easy to learn and sort of progressively understand but flexible um and intuitive and flexibility I think is the strength of language models um in chat style interfaces with chat cell interfaces you have the full access to the full capabilities of a model you can ask it to do anything that the model could possibly do including things like use API and use tools and fully specify like a programming language that you want the model to use if you want and that's the strength of falling back to chat but by adding these constraints where you start with something in your hand and then try to recommend or suggest or follow up and say given this is what you're looking at given this is the locus of your attention right now here are the things that you can do um and maybe predict some actions and add some guardrails add some structure to the instruction I think that's where we can add sort of bring back the intuitiveness of graphical user interfaces without sacrificing the power of language models open-ended natural language interfaces I think uh trades off too much of that intuitiveness for for the flexibility um so in an app like chachu PT you have this blank page syndrome where the user doesn't know exactly what they are supposed to type in maybe they have a a sense of maybe I want a summary or maybe I want a conversation of a certain style but there's no affordances in the UI to give them hints of okay these are the things that the model is good at these are the ways you might not phrase the answer none of this none of this exists and so I think it adds a huge learning curve and as detrimental to the ease of Discovery and by bringing back some of these graphical interfaces I think we can improve that situation a bit and then lastly since I'm closing in on time um I wanted to add one more note around another goal of interface design often which I think is closing feedback loops particularly in Creative applications and sometimes also in productivity applications you want to try to tighten the close the um tighten the the feedback loop between the user attempting something and maybe having something in their mind they want to see and then looking at the results evaluating the result and then figuring out okay this is how I need to iterate this is the fix that I need to apply to get get the model to generate what I want and uh there are a few ways to do this right one is instead of the model generating one output if you're say generating images instead of a model generating one output it could generate a range of outputs and that allows users to pick okay these are these are maybe four different ways of looking at this answer for four different images that you could generate this is one that I like and then iterate on that again this only really works if the output is easy to evaluate if I ask the model to write an essay and it gives me four different essays it would be it would be pretty difficult to use um so going along with that idea I think uh you want um if whenever possible you want to prefer uh what I've heard referred to as like people like to shop more than like to create they like having a range of options they can choose from and maybe even like swipe left and right style kind of do I like this do I not they want to that kind of interface is easier to use a more intuitive uh and more engaging than here's a blank page tell me exactly what you want and again powering that kind of thing comes back to okay what coming up with uh options and predictions of actions and suggestions that you is that you sort of uh plan out for the user in case they want and then the user can make that selection and then lastly um I've seen some prototypes of this this thing that I'm calling interactive components what I'm referring to this we're referring to by interactive components is if you are in a chat kind of interface and you ask a question and instead of responding with a paragraph of answer maybe I ask like uh what's the weather in New York and instead of responding with a paragraph of answer the model says okay here's maybe maybe it says the temperature tomorrow is 85 and then there's a little weather widget with a slider for time or with with buttons we're looking at precipitation and these other things and the model can synthesize maybe the model will be able to synthesize little interactive components little widgets on the Fly and that again helps me close my feedback loop by saying okay these are these other options of information that I can look at and I can really explore them really directly without having to re-prompt and retype my my queries so uh bringing it all back I wanted to close out with one of my one of my favorite quotes from one of my favorite papers essays when I'm thinking about creative tools uh by um okay content in this essay called casual creators um and I I think this quote is great so I'm just going to quote at length the possibility space of creative tools and what you can do the action space should be narrow enough to exclude broken artifacts like models that fall over or break when when you're in a 3D printing app but it should be broad enough to contain surprising artifacts as well the surprising quality of the artifacts motivates the user to explore the possibilities base in search of new discoveries new use cases a motivation which disappears if that space is too uniform so again she's talking about this balance of you want to constrain just a bit just enough that the user never gets stuck in that like blank page state that there's always some option that they can take or always some suggested action that seems interesting uh you want to preserve the power and the flexibility and the sometimes surprising quality of these language models and I think that distracting that balance is is sort of the primary challenge of building interfaces for these models oh something's happening there we go okay uh uh so last slide just to sum up I think five Big Ideas that I want uh it'd be great you could take away from this this conversation I think good dialogue interface is built on llms can have agents that co-inhabit your workspace that are there and can see what you're doing in its full detail including where your attention is it should take full advantage of the rich shared context that you have with the model uh to interpret your actions so that you don't have to cram everything into a prompt I think these interfaces can lead initially with constrained happy path actions that you can use models and other language models and other predictive models to try to predict and then if the user wants to do something more advanced or different we can always fall back to chat as an escape patch because there's the power and the flexibility in language models and then lastly whether you're building a chat interface or something a little more direct manipulation graphical uh it's I think it's always good to think about how we can speed up that iteration Loop especially by forcing the user not not forcing the user to type text but by responding uh more directly with a with a mouse or with a touchscreen for closing that feedback loop so with that I hope that was interesting and useful and hope you can build some some great conversational applications uh given there's a extrude wow wow I mean so many questions there's so much stuff that's going through my mind and I love the idea of how it's like you you're helping guide people that is so nice to think about instead of just leaving this open space And then trying to figure it out it's like hey can we can we suggest things so that people can figure it out with us as opposed to just letting their imagination go wild and then it may or may not turn out okay yeah exactly I think some there's some history of predictive interfaces like this and I think uh in the design World collectively our tastes have been soured a bit on predictive interfaces because the model that we've used models that we've used in the past have not been that good and so we couldn't really predict that far and we could really predict only simple actions but I've seen prototypes of like programming interfaces where given the full file context uh you can predict not only code but you can predict hey do you want to refactor this function or do you want to like rewrite this type into this other type or if you're in a creative app you could predict fairly complex trajectories for the user like hey do you want to take this drawing and recolor it in this way or do you want to apply this filter and then this other filter and given the power of these models I think we should I think it's worth taking another look at um these predictive interfaces as well obviously leaving the escape hatch that is just normal chat yes so I'm excited for the day that notion automatically knows I want to create a table with something and it will populate it with things exactly what I want and uh I'm guessing that you're going to be one of the people that's making that the reality of the future perhaps one day sweet man well this was awesome there's so many incredible questions for you that are happening in the chat so if you all want to continue the conversation I am pretty sure that is is on slack and it's not at Linus it is at the that is yeah it's my there it is my internet name I guess so [Music] thank you [Music]"
    },
    {
        "title": "What is the role of ML Engineers in the time of GPT4 and BARD? // Hannes Hapke // LLMs in Prod Con",
        "video_id": "l59AS0lK5rA",
        "video_url": "https://www.youtube.com/watch?v=l59AS0lK5rA",
        "length": "22:31",
        "transcript": "perfect hi my name is Thomas I'm a machine learning engineer at digits uh we handle books for business owners and accountants in real time then and that involves a lot of machine learning natural language processing and these days and also large language models but we had this big question of like how does the role of machine learning engineering evolves in the time of gpd4 in barn does it commoditize our whole profession and so if you take an example project here for example you would build like an address parser with like deep learning for example to handle some more uh unique cases instead of just using erendix profiler or regex implementation you um you would write something like this you would deploy your model you'd have an internal endpoint and then all of a sudden you would get those results out so that I don't know might take you a week maybe two weeks depending on the complexity of the project and depending on the available availability of the data um you could also go to an API like open AI or in the future Com or uh Claude or other services and oh by the way then you run into those rate limited issues and like their IPA the monthly IPA but the API sometimes it's not like available but once you get over this and you submit your request then you get a decent answer back and so there's zero um there's zero implementation time from the ml side and that is sort of like hmm what does it mean for the projects and then you dig a little deeper and you come across those projects from the initial demo where like somebody scoreboards a description of our website um on a page and then all of a sudden the algorithm or the the API endpoints spaces out some ideas about like how to write the HTML and CSS and it looks very much like what they had anticipated uh in this group link and those moments all of a sudden you're just like wait a second what is going to happen to the entire uh like ecosystem of ml engineering envelops etc etc and so we're not like the little Kevin here like uh running around and screaming but if we step away then what does it mean for us as like machine learning Engineers uh ml Ops projects owners stakeholders Etc is there's a Gutenberg moment and I call this a Gutenberg moment because something interesting happened in the Middle Ages in like 1450 the uh there's a German fellow who is a quote-unquote invented a printing press and until then um books have been transcribed in monasteries um there were monks they were trained on doing this by hand and uh they did this ever with high artistic value but then uh this inventor came around and was like hey we have a printing press we can print those um documents or Bibles or books or whatever they wanted to print much quicker and that sort of like contributed to the entire democratization of access to books and uh had a lot of like in um had a massive impact on literacy across Europe things like this but if you walk away from this or if you if you zoom out from this moment you could say that um we have an machine learning engineer um and we have the modern printing press is basically the API for those models and we're at a moment right now where like as a as a community we need to figure out like where we go with our projects so what this means is like machine learning got democratized uh what I mean with that is like now we have domain experts they don't need to be experts in machine learning they can just basically take their data and run it against the apis and solved quote-unquote some machine learning problems and the interesting effect we also see in the community is like now we have discussions about machine learning and the impact of machine learning in a broader public so do you see articles opinion pieces in the New York Times talking about artificial intelligence machine learning I have close relatives asking me all this out and was like hey can I how can I get a key to open AI how can I use this and those folks struggled before um to even understand what I'm doing on a daily basis for work so those are the good things and then when we look at um is this a Gutenberg moment for us or not like some projects let's be honest some projects got commoditized like um we'll talk a little bit more about this later what this specifically means for us uh in our project and they're also we need more education around the dangers of machine learning and I'm not danger I'm not saying like to scare anybody here like but we need to talk about like what does safety mean what does bias mean like how do we train those models what's actually happening behind the scenes and that goes back to the criticism around open the eye are they really that open or uh what the name says or more like close and I think there may be more on the later hand so we also need to educate folks who use those apis that the predictions are not objective they're still subjective data behind the scenes and there is a bias in the data just because the machine has made the prediction it doesn't mean it's we can classify this as objective so before we talk about like what does it mean for our projects let's summarize like the early lesson we learned when we looked into those apis and so the first results were really impressive like no question right like you see you saw the demo you um realized what's actually going on uh behind the scenes we're like wow TPT was great but then gpd4 is sort of like that's a different level right that was super useful um then you start thinking about like wait a second those brief interactions they can be misleading and as an example like the case I showed you earlier with your dress parsing if you submit the address if you submit the same request multiple times with that prompt I showed you you would get the same values back but for example the keys in this Json structure would change so it's inconsistent um so there's therefore when we have conversations internally about like those apis we just need to be really aware that the first result can be misleading and then there is like convincing hallucinations so we have seen tasks where we try to run them through an API and the model returns something in a decent data structure but it hallucinated in the values themselves so it believes that it extracted something and it was uh convincing but it was utterly incorrect and we've seen this now with discussions about like biographies uh descriptions of people like it's very convincing but um again the whole text was hallucinating and then as I just mentioned the outbreak was so um when we look in this it feels a little bit like little Kevin is like dancing at home and just like not making things up but like pretending to be a little bit bigger um than what he actually is and so that's how it feels like right now with like machine learning projects like we can do a lot of things but boy there's like if one of the strings breaks then all of a sudden air thing stops dancing in the movie so let's talk about like commoditization that was my first impression when we saw on tpd4 that a lot of projects got commoditized and even folks in in the first few hours after the release were like hey I'm doing a PhD in natural language processing it's my entire PhD still worth doing and I think the short answer is yes because there's a lot of problems still not answered but a lot of machine learning projects got commodities so which ones would we consider as being commoditized so any project with public data available is basically commoditized because there's a good chance that it has been soaked up into the um the sort of like the the scrapers of those data sets for gpd4 or maybe potentially gpd5 so if it isn't solved yet um there's a good chance that somebody else will do this in the future um and it gets commoditized projects without any specific environment requirements so let's say um you have a use case you need a you need a machine learning model or Union machine learning solution but the stakeholders don't have any requirements in like this needs to run on device or we have security requirements then it's a good potentially threatened by a public API specific security requirements like the digits we take um the security of the data uh as a Paramount and we do not ship the data to third parties so that's really critical so if we want to use large language models we have to deploy them internally and that is a requirement we couldn't fulfill with openai and therefore we can't use those apis at this point in time and what does it mean for our in-house machine learning so we need to focus on machine learning projects on proprietary data let's say the best example is always like um there's a high likelihood that uh openai won't replace Radiologists because yes they have a lot of data but if you have like a custom trained highly specific domain-specific model you there's a good chance you'll always outperform um the specific generalized apis so we don't need a machine learning model to make a radiology uh assessment that also gives us cocktail recipes and tells us like what to cook next Sunday um any project with very specific security requirements is an in-house project so if you can't ship the data to a third party for whatever reason let's say you want to run your machine learning model on an iot device or you wanted to don't share the data with a third party that makes a good qualification for an in-house project and then we also have seen great instability around the latency and the availability of those third-party apis so if you have low latency requirements that is a really good reason to pick up those projects and do them internally instead of shipping them to a third-party API and then most important and that's a discussion you as machine learning Engineers need to have with your stakeholders is like what is the core intellectual property you want to preserve in your business or in your research project so if we want to classify the sentiment of a text is this important to your entire IP value chain if not an open API or a model API might be a really good use case to get you up and running and started and then you can focus on the actual IP related projects so what does it mean for machine learning we as a machine Learning Community we had objectives in in for all of our projects we did not always achieve them but we had a bunch of goals in the past so we wanted to make our predictions unbiased so there was a strong focus on like how do we handle the data up front to make sure that everything is as unbiased as possible balance training sits um known data sources making sure that we remove some of the biases in the underlying data there was the objective of like adding transparency around the data and the training of the model so you have seen a lot of like projects around uh model cards from Google for example or data cards or things like this to communicate what are the limitations what are the constraints of all of our machine learning models when we run versus 11 models there's this great objective to add feedback loops to our models to improve the model performance we know as machine learning Engineers no model is perfect and so if we see something which is not working well we want to approve uh improve this in the next model generation and therefore we need to capture those like misclassifications and then add them to our training set that was like a key objective for all of our in in-house machine learning projects and then we have this objective of that user privacy there are projects going around like Federated learning or ml privacy or even encrypted machine learning so we wanted to make sure that everything is sort of like uh at the at the highest standard for the use user use cases and then obviously we have undevice developments like sometimes we want to shrink models and make sure that they can run on a cell phone or on the latest iPhone um as you have seen like for example with like stable diffusion and those types of models but those were the sort of like ml Ops objectives for our machine learning projects so when we zoom out again and take a look at apis then basically we're we're missing out on the last four points and the question around like unbiased predictions is sort of like up in the air because we don't know what the models have been trained on there's very little information about like the background and so we as a community we need to be careful with like the objectives we have as machine learning Engineers when we use those apis when we talk about a little bit more about this later but here's one example let's say you get an incorrect response from gpd4 there is no way right now to feed this back into like your own training set you cannot find you in this API potentially that works in my future um but it comes at a high cost and so for some of those problems you might be able to just like use a large language model from an earlier generation let's say a T5 or um maybe a gpdj or something like this which doesn't have so many parameters um and then fine-tune it for your applications you might not need the cocktail recipes being generated when you want to do the address parsing and in that moment you can add back the feedback loops so we talked about what does it mean for the for the machine learning ecosystem but what does it mean for you and me as machine learning engineers so our role has drastically shift shifted from like developing machine learning models or creating the infrastructure of those machine learning systems to being the moderator between stakeholders so you might have a CEO or a CTO who doesn't have the full understanding of like how the complexity Works in machine learning and what the benefits are of like all this um the security aspects and the Privacy aspects and those types of things and machine learning systems so it comes down to you as a machine learning engineer to educate the other stakeholders what are the benefits of like in-house development versus using third-party apis and then you as machine learning Engineers you were the core drivers now when it comes to advising others in organizations around the risks and benefits of those third-party apis so as soon as gpd4 came out we had very long conversations with the security folks on our team um they were they were very much concerned about like sharing data with third-party apis Etc and we unpacked this a little bit of like what does it mean what is being shared um is there maybe a way we could get around this um what are the benefits of doing this in-house Etc so just to conclude where are we going from here as like the as like the community of ml engineers and emerald helps people so prompt design I'm not calling this prompt engineering because right now there's too much guessing in this game it seems like it's more like design a design process it's very much iterative than an Engineering Process where there's like a given path in the structure yes that will be part of machine learning but it won't replace machine learning that is my strong opinion like we will not be out of a job tomorrow because there's like some massive model and we we just sit there and tune a prompt it will be part of it but it not it won't be the full job there are lots of ml Ops challenges around large language models so even deploying a model with like plus billion parameters is not an easy task and we will need as a community we will need more experience and like how do we do this how do we distribute models across multiple instances drawing this across maybe a bunch of gpus for a single model and how do we get latencies down to um to like something we can use in real-time systems um also as a side note like the carbon footprint of those systems is massive and should not be neglected neglected and this is something we need to focus on as a community and then we need the integration of like the integration itself needs like good ml understandings so we need to know like how tokenizers work like how um why certain terms make uh models more sensitive and then other channels and we can we can understand this with a good machine learning understanding which we can provide as a community to other stakeholders who want to use those apis and this also goes further in terms of like bias and safety so instead of just like uh handing over the API keys to somebody else in the organization let's have a conversation with them about like that about what it means in terms of safety in terms of buyers then we as a community have no insights into those models and there could be consequences for the users so what can we focus on for the maybe a short-term future or midterm future focus on projects with proprietary data um if you have um a custom data set in your organization that is gold like nobody will replace you uh no a open API overplace you and your project I focus on subjective machine learning so our machine learning team is heavily focusing on account on the accountant space we advise on accounting questions bookkeeping questions those are very subjective machine learning problems because one classification for one user is very different from the classification for another user there is no Global machine learning model for us we use something like similarity machine learning but you could also think about like recommendation systems as like a subjective machine learning nobody will paste the entire shopping cart history from some start party system into a prompt for gpd4 to make the next recommendation that would be just simply too expensive and therefore subjective machine learning is the key for in-house developments and then avoid plain vanilla plain vanilla projects like getting the sentiment from text that is something we can uh uh this is something we can do probably through the apis we don't need another model to detect cats and dogs so um focus on the non-vanilla project and then focus on projects with specific requirements like as I said like user privacy security low latency and really hone those uh those aspects and become the expert for example in like low latency language models and then be the moderator between the stakeholders be the moderator and advisor and help other people in the organization um to understand the benefits and the the sort of like the the disadvantages of those apis and then drive also the conversation and with that you're basically in a really good spot to live alongside those open apis or model apis to uh to succeed as a machine learning engineer cool thank you"
    },
    {
        "title": "Cost Optimization and Performance // LLMs in Production Conference Panel Discussion 2",
        "video_id": "wxq1ZeAM9fc",
        "video_url": "https://www.youtube.com/watch?v=wxq1ZeAM9fc",
        "length": "36:06",
        "transcript": "[Music] oh can you hear me hey how's it going oh yeah man when we got Jared Daniel all right Mario and the guest of honor that I am going to end up replacing myself with Lina Lina you are the one who has been tasked with leading this panel that is coming up I'm gonna hand it over to you and let's get it started thank you all for joining us heads up oh I wanted to Triple to you earlier amazing uh to have you all here we're going to talk about um cost optimization and performance and so I'm also an ml engineer and I have an amazing panel for you today so I have two people who run large language models in production we have Daniel who is a research scientist at Neva which is an ad free private search solution tried it uh quite nice and they're using um large language models in search for all kinds of questions like summary semantic retrieval career generation so I'd be super interested to hear what you learned by applying it to the search space then we have Mario who is a staff engineer at intercom and they put one of these early gbt4 powered projects out which was a chatbot and then we have two people from the tooling space really also very excited to have here Jared which is the co-founder of prompt layer um hey Jared wow hey um it's a platform to manage your production llm apps with a reliable prompt engineering monitoring of the problems like versioning course latency and you can track historic usage history and performance so I'll be excited to talk more about evaluation and last but not least we have Lewis who wears many hats he is super hero signs in Washington and also the co-founders of octo ml where they offer easy and efficient deployment and there are interesting part for me especially was that you can just upload any computer vision um or NLP model and it will be optimized and it will tell you which instance to run it on and soon you can also run your models there so I'd be very interested how you help users to make this model run very more cost efficient and faster okay um so maybe let's start with costs we know generally running large language models is quite expensive much more expensive than normal paid back-end apis so so um Mario and Dania maybe because you run it for practice um was cost an issue how did you approach this um the cost angle of running and production uh yeah so class is an issue especially with scale and I think that's kind of like where we started out we started off with some of these kind of like foundational apis as a way to figure out is this a product that's useful does this help our ranking signals to our users like this and then once we actually started getting that both the cost of running that and how much we could run this on really because of bottleneck so if you think about like summarization systems uh potentially running it like in production real time for users that's very possible the API it's kind of slow it can get expensive but what if you want to run summaries on let's say the entirety of a web index which is on the order of billions of documents at that point no longer can kind of become actually a cost possible so you have to move in the house to some type of smaller system and at the same time it's just a question of like how widely do you want to test and that one using these models how many people you want to access them and what is their expectation of their turnaround time yeah do you have a guideline like how long would you start so you have a working prototype and also maybe could you speak on imagine you're running it in-house now but it's still expensive right so do you have a ballpark uh compared to other services how how do you evaluate that and how do you bring it down to which level yeah so on the evaluation side or initially using kind of these foundational models it's about kind of like having rapid iteration and the ability to kind of refine things via prompt and get this in front of users to actually kind of figure out what they're thinking and once users are actually saying like oh this is great this is the use case that I'm working on then we tend to kind of focus in and figure out how can we distill this extract this into a smaller model within these kind of smaller models the process we generally think about is less around uh just the dollar cost more on kind of like where can I run this and for us the golden threshold has been can we make this model run on a single a100 GPS or 810 GPU or on any type of CP requirement because like when you move from the a100 to an 810 say you're serving AWS that moves you from having to pay 40 bucks an hour because you have to get eight at them at once to on the A10 you can pick up a spot instance for 30 cents and so like at that price point you can do pretty much anything and then if you even move it to CPU the game changes even further where you can move to kind of billions of items without necessarily even thinking about this and so a lot of our work we're moving like for some of our semantic search these are small queer encoders these are 30 million 40 million 100 million parameter models and so in that case we can actually run the query encoder on CPU so it sits in the same system that's actually doing the retrieval and so that can just simplify production it's just a question on really like figuring out what scale you're going to what is your latency throughput that you're looking for and what is kind of the size and one of the things that we saw was the foundational apis are actually really slow and partially that's actually really good because it changes user perception whereas like before people's perception via Google search was like anything over 100 milliseconds is terrible now people are used to waiting three four five seconds and we've been able to see that we can take if we're running to here we're running open AI uh what's it called anthropic they're on the order of like three seconds for our response for a single batch we can get a model that is 20 billion parameters to respond in 400 milliseconds so that like net Improvement in speed is huge just by bringing in-house and that's just because you don't have to go on the internet it doesn't have to go through their orchestration you can do with your own and the other thing is when you bring this in the house you really become a master of your own fate where one any type of latency optimization is actually owned by you you don't have to worry about API rate limiting you don't have to worry about like their entire system going down I think that was actually one of the things that really pushed Neva to bring these things in-house because shirt open AI or has 99 uptime well that's 99 for everyone that was not what we were experiencing at all and when we're trying to deliver this product and we're like well this use case doesn't work because their overall system's down and we can't do anything but sit in our hands it really came as kind of like the impetus to bring this in-house yeah we experience the same issues we also had regular they were regularly not available sometimes 15 minutes at a time so that was a problem um okay anyone else on on course consideration can I can I just jump right in here to say Amen like I love it was just what he just said Mario like this is exactly our point of view is that hey often you don't need the highest End Hardware right so you can live with Hardware that actually hits your latency throughput requirements you know even though there seems to be um a strong um let's say uh shortage of high-end silicon many people don't realize that they don't probably don't need high-end silicon to deploy their models um and you know basically what Octonaut offers exactly what you talked about but in the general way in a fully automated platform for broader users to deploy their own model their own custom model or another source model in a platform that they control automatically choosing the Silicon for you and turning models into high performance portable um artifacts and code that you can run on different Hardware I just have to say you know we should definitely follow up after this because our our view of value here is 100 aligned so but in panels we need to disagree with each other so somebody should disagree with with him to make this interesting right so You can disagree about the most promising um ways to go about customer so if you each had to name like your top two methods because you can do so much right if you if you start reading on the internet what are the top things to do to reduce costs you have a list of like 20 items so maybe you can disagree on that that let me give you give me each your top two okay first of all um make uh pick a model that does the thing that you need and optimize as much as you can because then you have better performance in a given Target in a second choose the right silicon for it the one that has the lowest cost possible and hits your uh performance requirements so you're also saying that a total type like a smaller type of type of exactly yeah completely right model optimize as much as you can hopefully automatically and then still pick the right hardware for it so Yeah my two things would be kind of like structured Printing and knowledge distillation where you can take the large model and then you actually just remove portions of it until it fits in whatever systems you have and like I'd like direct example for this like we're serving a flan t511d parameter model that physically doesn't fit in an A10 GPU until you remove at least 60 of the weights so you have to just because like the actual size of the model once you've done that you can make that model fit in that smaller GPU but then you have to use some form of knowledge distillation so the model doesn't actually have this like uh decrease in quality in generally speaking those two things together really provide like big big performance improvements and if you get really crafty with your distillation and I used to work at neuromagic which focused on distillation for CPUs and everything else and we were able to get 30 40 50x improvements on text text classification tests by basically just pushing in that like knowledge distillation as far as we could with the biggest teacher model and the smallest student model and that just works extremely well the downside to it I guess I'll take my own uh going against myself in the other corner there's a non-zero cost actually moving all these things in-house like if you think about the beautiful part about these foundational models is the feedback cycle is super fast so if you think about like say you're doing something with the prompt for search engines uh you want to rephrase how your system works and so you just change the prompt and it runs it's good it's in production it's good to go if you want to do this in a house you basically say you're like you want the system to take a more neutral tone you have to go through all this iterative approach to make data sense just throw this data set into something smaller take this model compress it into something smaller figure out how it moves to production so like your iteration time goes from being on the order of minutes to potentially days or weeks and that's kind of like this like nice trade-off of like how much do you want to explore how quickly you want to be able to iterate on products with customers to how quickly do you want to be able to optimize and control your own state um if running in-house what do you say what size of team do you need just for the upside of running uh I don't know few models but let's say two I guess it depends yeah maybe it's too general question depends what type of model probably but just maybe can you give listeners like a ballpark on on the on the amount of work that you had to do so I would say the bringing these models in-house and making these models there was roughly there's about four of us that were working out on anything from data modeling to I'm the compression guy so that's what gets me up in the morning uh but there's other folks who are working on the modeling side and otherwise I think overall at Neva directly working with models there's probably around 10 of us okay and Mario Jarrett thoughts on the cost side uh I can maybe give like uh a little bit like a few thoughts on um how you can control cars before you actually move anything in-house so we don't really like we don't train or fine tune our lamps ourselves so right now we are pretty much on the consumer side uh just calling already apis uh and It's Tricky uh it's expensive and um I think even DaVinci uh can be a very expensive model depend how you use it and um most of the time like we just first do pretty much feasibility exploration of is it possible to do what we want to do um in some like really isolated cases uh like can we summarize one conversation for example and to like to assess that capability we would take the best model first probably see whether that's possible and try to go down down you know just try to Pare down the model uh to the lowest model that gives us results that are okay like Daniel mentioned that's often like I think uh producing games uh 25 years ago where everything is too slow and you're totally like limited by how much you can process and uh it kind of like Cuts both both from like cost latency perspective like you just can't summarize all the conversations in the con because we'll Bank wrap straight away um there's like too many of them so but like one thing that's one thing that's interesting for like cost reduction is just basic engineering thing which is just call it less and that's not always possible of course I think like like probably like in Daniel's case in Evo that's a little bit more tricky if you're doing a semantic search but like the systems we build are usually a complex set of things that talk to each other and there are multiple there are multiple components that are implemented with llms and sometimes you can put like a really simple classifier before doing something to figure out whether it's even whether it makes sense to call a lamp at all it's like that's you know that's one technique that I think can work quite well because not all tasks are so sometimes you can find a tool that's really simple and it's going to be able to say whether the more complex and smart tool is going to has has any chance in finding an answer so I think like that's one technique that helped a lot yeah and I just Echo Mario's first point there that like the first step yeah you could do this bringing kind of bringing it locally or whatever but if you're using these uh large Foundation models from open AI or whatever the first step is just knowing what's going on and kind of just building version one and kind of see just understanding how the cost works and this is like uh it's a bigger step than you'd expect and just understanding which prompts in your system which parts are expensive and like knowing where to go from there is a is a pretty important actually so just echoing that otherwise yeah so there's some great tricks you can employ when when starting off and then maybe later when you scale uh to bring it in-house okay let's talk about latency it's uh okay I just said one more thing I think you would think that I think was was implicit but I don't know if it was said explicitly was you know there's plenty of Open Source ready to go models that can do I think Daniel mentioned you know T5 plan for example they can refine for users and people realizing that you don't you don't do not need to pay the open AI or cohere tax if there is a use case that you know a local Source model can do well and you can much more efficiently in a platform that you control that you might not need to build on your own that that already gives you a very very significant room for cost savings there right so just know your use case you know you don't kind of like there's an analogy of use a CPU because it's General but the CPU is expensive but then use the GPU to the work of the computation because that's more efficient I think there's a direct analogy here where you have these uh you know General expensive models like pt4 they have to pay to use but then for things that you know that you want to do and you can specialize it and use an open model you're probably going to pay a lot less like probably orders of magnitude so just to make it interesting and push back a little bit I'd actually say that please yeah I the way I see it if you're if you're I mean it depends where you are in the stages of development if you're building version one if you're trying to ship something where honestly most people working with LMS today are trying to ship version one I say don't bother with open source models get something working on GPT yeah yeah yeah but just like hey if you're writing code here you write in a CPU you make sure it works and then your floats to the efficient compute unit that does what you need to do similar thing here you start simple with the model that you know is more expensive and as you understand what you actually need from it then I'm I'm fairly confident you can find an open source cheaper model that does what you need to do right question answering summarization and stuff like that I I think the part that's important to consider in the cost reduction also just is Staffing a team like the story I have here is I've always worked in like the search World of like big search engines so there's always been a lot of money behind this there's teams everything else my brother used to work at fine Tech and at some point they were looking for something that could classify businesses and when they poked around and figured out like what would this cost to have our own system you have to hire people you have to hire at least one ml engineer private info engineer you have to have to run it and pretty much their bottom Barrel costs would be on the order of I don't know five six hundred grand a year just for like the team and they could make 99 like what Alice was talking about but in reality they just needed something that was 70 and like if they just plugged it into a system they never had to hire an NLP team and who never had an ml team they had the system that was beautiful because like they could go in and someone who could just tune The Prompt until it works well enough and it just like by using this large system they save money because they didn't have to hire expensive people like me and it's an understanding use case because they were only pushing 10 000 examples through today so like their cost is not as High versus on like a search engine side or when you're like dealing all the interactions that intercom has like if you wanted to do all the summaries then that kind of cost calculus changed yeah it's uh it's a question of scale so clearly if you if you use it like the exit plus type of thing with the human in the loop then using as the API will be always more cost efficient it's a good point okay so um moving on to latency this is another huge problem so just getting uh or maybe I'm a bit um you know spoiled from normal classification models but just moving into the llm space and seeing that just one token takes 0.7 seconds on P90 was uh shocking and then you normally you need like a whole sentence um so that can take several seconds and how do you deal with that if you have a user on the other hand who is like you're really very impatient things after one second the application is broken and what are typical how do you go through the life cycle when do you that you start out maybe not looking at speed what types of speed UPS can be um are possible to expect do you do this trade-off between model size and and Speedo like how are you thinking about this and maybe also throw your best tips like if you had can do one or two things uh on speed uh what you do if you don't have a lot of time literally yeah I can walk through it directly that we didn't even we've talked about is our initial summarization model and uh initially use a large language model you get the summaries you're basically looking at about three seconds per item and we're doing a batch of 10. so we're basically spent 30 seconds you we did this and figured out that like oh it turns out this is actually useful and very good iterator and prompt and then brought this into a smaller model of a house just going into a T5 large model but serving that on like a batch size of 10 on an A10 still took like eight seconds naively so it kind of went to about all these ways so first off there's a great Library called faster Transformers Nvidia kind of supports it they don't fully support it so like you got to fiddle with it a little bit but just moving from uh Native Pi torch serving to faster Transformers basically brought us down from like eight seconds to like 1.8 seconds so huge gains geost on changing how it served it worked on the GPU had a funky build process but it works super well and then from there we threw in some notion of structured pruning and asymmetrical pruning on the encoder decoder side just because like the encoder produces this contextual representation on a sequence sequence model and you want the best contextual representation possible and it also only runs once so the cost is pretty much doesn't matter versus the decoder runs again and again and again soon heavily compressed with decoder so we basically took this model that started off with 24 layers on either side and brought it down to 24 layers on one side and four layers on the decoder and that meant that like by the time that we were done with faster Transformers this compressed model we got down to roughly like 300 milliseconds per batch of 10 which that allowed us to basically say hey in now will cost us twenty thousand dollars to create a summary for everything in our index and we can actually do that because we'd like decrease these costs versus when it was 10 seconds 30 seconds I think just not tracked yeah yeah I just I just wanted to underscore making sure to use the very best kernels the very best binaries for the giving harder they're running it on because that's if you can do without changing your model at all or any batching parameters any other system premise start with that and see how far you can go you know if you and the problem is that this is dependent on the hardware Target it's very specific in the hardware Target so the more you can um ultimate that the better I'll put a plug-in for what OCTA ml does we can help you search automatically for what's the right library or compiler using tensority or TVM or uh you know Cody and then for NVIDIA or the equivalent for other Hardware targets and then after that you know change the batch size sure you might pay a little bit more Hardware utilization but this latency really matters to you reduce the batch size pay a little bit more so I I'm really happy to hear it sorry again Mario I'm really happy to hear like with Daniel and Louis are talking about because like if you're in our world we're just taught like talking to some external API your hands are so tight so effectively you have like two ways to uh improve the latency one is by shared capacity um if they if they can give it to you uh and then you know you can play a little bit more with like throughput latency um curve and like where you want to sit there and the second one is maybe like what I said like if if you're okay with not finding an answer and if you can detect it quickly maybe you just don't call it and return the answer straight away like there are ux paradigms like I think it's becoming quite for like chat applications you know just piping the tokens directly from llm to the user so user is you know saying straight away that something is happening or sometimes streaming do that streaming effectively yeah like streaming like directly from llam to um to to the end user experience like sometimes you want to check what's you know the answer when it's fully uh generated and in these cases like you have to wait for it and for example for gpt4 uh shared uh pool of capacity uh like the the latency I'm seeing is something like uh 100 millise per token um for English it's usually one word per token they change the tokens for gpt4 but it's extremely high uh so I'm really I'm really happy to hear with Daniel and uh who is there talking about because I hope they'll be able to actually you know move some of these things um like once you understand like which use cases are actually the most important for us and which use cases make sense to uh to optimize and bring in-house there's there's some tricks you can still do on um on when you when you call an API you can work with a prompt because the main database in the output not in the size of the input so you can you can tell it to be precise or give you back a number of paragraphs like be a bit shorter in the output so this is something I mean we can clearly see from the panel that if you want to be really fast you need to move it in house but let's say you're in this first stage where you still need to understand the cost and and the speed uh there's a small amount of trick like semantic caching and what Mario mentioned that you can do and maybe Jared you also have a dashboard where users can track a latency they observe is there some insights that you can share like how do they like what You observe in terms of your users with respect to latency yeah I guess uh the interesting thing here yeah is what is the low hanging fruit to kind of help with latency imagine one is obviously like how long the response is but the other one which is also very obvious but falling back to different types of models so obviously if if you you can detect uh how how badly you need cpd4 over 3.5 if there's a use case where you can fall back to 3.5 a faster model or even something kind of let although that might be the fastest one they have now but or maybe Claude or something like that kind of there's becoming uh in fact there already is probably this whole axis of which engine do I use for this problem as opposed to kind of just saying we're using GPT for for everything or we're using our in-house for everything I think these are all tools that have pros and cons and one of the pros and cons of each is latency yeah I I can't resist big enough again like one one thought that was mentioned very briefly you know that streaming the output like I'm I'm more on the user side these days interacting with this boss they go and buffer the entire answer before they show it to me a simple thing that just helps human patients is to say it's being typed or streaming it to the user because then you see that something is happening right so he detectifies black Bots here it buffers the entire output before it shows it to me like if you just said hey the bot is typing just let this GPU work hard for you that will already go a long way so don't change the latest you just manage human expectation they will probably go a long way as well Ah that's such a good point the actual UI experience and there are some drawbacks in terms of like safety of the response like if you stream it you can check the response so that could be a concern for some type of applications but I like your second idea of like typing like when you see someone typing in some different types of like a human I don't know different shared applications that helps you see their typing it's a good idea okay yeah I think uh just I'd add real quick yeah I think there's a lot of ux ways to do this to solve this problem um that's one thing and then also just one other addition is these latency amounts of like gbd4 these latency amounts of the models being offered are not constants either um we've like been looking kind of at our high level like user data and the lane season like very spiky of things like tpp4 and like it's very laggy some days and not other days so if you really care about optimizing that's probably another thing to look at yeah yeah oh that's that's they don't support this lace yet so I guess that's we'll be having a latency page to kind of help with that soon so stay tuned on something we talked about earlier I mentioned it's like batch sizing is hugely important both for like the streaming component but also yeah if you think about like when you're actually doing this like greedy token by token decoding if there's any difference in the length of each batch everything basically defaults to the longest batch so like there's some literature out of Washington I think that in machine translation they found there's like 70 of tokens and batches were effectively useless tokens because it was driven by the longest padded sequence and so like especially if you have a high variability in your output doing anything without a small batch size can like be bad because it's like if you have a short response you don't get to enjoy that basically your all your responses are limited by your longest output hmm makes sense do you have do you control for that and your prompt somehow or do you how do you handle that the control of output size I'd say on our side we've done a bunch of optimization there even as you mentioned on like making the gpt3 output more concise which then we kind of can distill from but uh it's really just kind of fiddling around with it like one of these things that we saw was uh with this kind of asymmetric pruning that we did when you move the batch size is larger all these gains went away and that's because like when it was smaller you really could like highly optimize it because you're not waiting on this decoding versus when you have a large batch size you're just basically dominated by the longest sequence so all your compression is completely kind of just broken interesting that makes total sense okay let's talk a little bit also about monitoring I'm very curious uh what you guys found useful both like uh human based LM based modeling any other um type of techniques that you found useful um I can jump in here with like a high level overview and I know I've talked to Mario personally about this I'm sure he has some things to add too but kind of what we've seen our users doing kind of breaks down into three big categories first category is which I think is the real source of Truth is end user giving you a thumbs up thumbs down or you can use behavioral based maybe like clicking refresh or closing out is a thumbs down as a negative signal on like how the prompt is doing that's one category another category is this whole mturk hiring out to click workers to do very boring very boring way to do it in my opinion the third category which I think is really kind of where the future is here is synthetic evaluation so how do you actually use an llm to rate how good it is and the one interesting thing I'll add here which is kind of a thesis where developing uh based on our conversations that prompt layer is kind of via negative is the right way to do this instead of trying to understand which prompt you have how good they are it's better to reverse the problem and say how can I evaluate what's failing in prod which users are getting bad results like is the chatbot being rude is kind of the most trivial example of this but that that's the way we see it yeah to be direct on that that we're calling this critic modeling and it works extremely well especially with the most recent models we found that so like on a seven point scale the gbt4 is off by an average of one label which I used to do a bunch of human evaluation that's pretty much the best you're going to get off like mturk train judges and it works across fluency accuracy other things the one thing here is there's huge positional biases so we found that if you ever try to compare two things the outputs don't work as well so it's best to try to focus on individual comparisons and then potentially using some like weak supervision signal to figure out pairwise things can you give us a concrete example when you mean critic like what is the problems you're giving the critique or how does it work you're writing a thing of hey uh engine your taskers to evaluate how good a summer is for something you're evaluating in a term of accuracy something that is completely accurate has X characteristics as a seven something and it's slightly missing something as a six provide all your description of labels give it to the model the model will respond pretty damn well but I will say I'd like to give you don't try to say hey I have these two summaries which is better because the model will uh there's position bias depending on what you put first your second the model will favor what's left and at the same time the outputs aren't as usual it's kind of like the old Costco thing where you never ask someone why they prefer something because if you ask them to justify it then they will not actually give you their Preference they will give you an answer which they can justify so don't ever ask your model to justify oh yeah okay interesting recency bias is also known in human decision making by the way like if you give a man like a verbal statement of of choices people just forget very similar to what we observe in the network the first option and intend to pick some of that indecision design very interesting behavioral research um okay anyone else on monitoring and performance evaluation yeah yeah so just like I said we've done a bunch of stuff in-house and we've found similar to what Jared mentioned where like the tail latencies can get very out of hand where you can see massive spikes and they can be caused by these weird Corner cases that you never thought of like we were seeing massive tail latencies occasionally and that turns out we were running a model on a sliding window and occasionally hit super long documents and like the solution was like oh what do you do just truncate after I don't know 10 000 tokens but sometimes and the only way that you'd see this is like you look at what these outliers are and in most cases anytime we've seen an outlier the solution is super simple it's just like oh we didn't know about this weird Behavior yeah this is the easy fix we no longer have that I call this terrible uh I have a worst name but I call this terrible experience debugging which I regularly do I wasn't recommender systems so I also monitored like the really bad experiences and they just picked one every week to to Deep dive on the root cause this is very similar to to you saying like monitoring the bad experiences like the edge cases the long latency and then you you find like groups of problems that you can fix whatever one oh very good okay cool I think we're out of time I just find one more thing if you don't mind just so uh putting a plug here come try the optimal profile let's you with a single line of code you can get your model running across different hardware and you see the profile of the latency takes for different options and so on to give a very it's actually running the model and providing it for you so love to hear from you so uh I guess can I just jump out can I can I jump in for like 30 more seconds like I'd be interested to hear from others like problem in chat like how they deal and labeling becomes too expensive as well if you need like gpt4 for labeling then you know it's expensive again um so whether people have like some smart sampling techniques or maybe like pre-filtering initially are there Solutions out there for that it seems so nascent field to me uh that pretty much like all the tooling is just in the process of building so yeah I'd be happy to hear in chat of like what people use for things like that yes snorkel there you go oh that's awesome so we will um continue this conversation in the chat yeah please let us know what everyone is thinking and thank you so much all of you were incredible and these are like these are very poignant questions and I really think that you all educated us a ton on how you're going about it and what kind of best practices you've found thus far wow all right thank you later you all off stage though see you later foreign yeah yeah"
    },
    {
        "title": "Agentic Relationship Management // Ashe Magalhaes // LLMs in Production Conference Lightning Talk",
        "video_id": "U9NhEUyXKi4",
        "video_url": "https://www.youtube.com/watch?v=U9NhEUyXKi4",
        "length": "10:01",
        "transcript": "hello everyone my name is Ashley gallis I am the founder of art AI um above is my Twitter I'm pretty responsive on there so if you have any questions uh please give me um so I'm going to talk about a genetic relationship management um Hearth is still pretty stealthy but here's our website with our wait list um if you're interested okay so I know this is a lightning talk I'm going to try to move pretty quickly um and it was also meant to set a timer to make sure I was doing that I'm going to talk about who Am I who I'm assuming you are what is heart what do we mean by agentic what do we mean by a genetic relationship management and a case study that talks about a specific user flow we think about at heart our system um some production benchmarks we think through the mission of Hearth and then finally careers because we're hiring okay so who am I um here's my personal website at Ash dot um AI which is cool I got the AI domain a while ago um so I'm actually based here in New York City I've lived here in New York in Palo Alto San Francisco in Berlin in Edinburgh I've been working in the machine learning space for about a decade now um did some early internships at Facebook I worked for NASA um I worked at ml Airbnb and apple uh I spent two years building and racing a solar powered car which was pretty cool I went to Australia to race it and was there for a while and then um I spent two years working in American politics as a machine learning engineer in 2020 um and then went to Eric Schmidt's philanthropy after and um have now started Hearth about a year ago so my main mission is really to build AI that augments The Human Experience um so this talk kind of assumes that you all are Builders and are currently working in Ai and you're looking to learn more about our technical approaches to agentic products okay so what is Hearth um kind of the problem we think about is that today our personal and professional networks have reach an unprecedented level of complexity like even for this chat I would love to have all of you automatically added to my LinkedIn and to Twitter and to kind of systematize keeping in touch because you all are interested in AI um as I am but that would actually be really hard to do um so you know even today despite being glued to our screens we're still dropping relevant communication threads wasting time on manual data entry tasks so a lot of us have like spreadsheets or notion databases we're tracking people on we're losing key Network details we're failing to match the right people with the right opportunities at the right time and we're letting our important relationships grow cold so the solution that we think about is a genetic relationship management and this is a suite of products that leverage AI agents to learn from understand and act on behalf of the user so Hearth AI will synthesize and manage your network complexity so that you can focus on connection so what do we actually mean by agenda these are AI systems that leverage large language models that would be the simulated world over here and human feedback to determine which actions to take and in which order kind of the key here is that these products are executing a series of observations thoughts and actions to mean an objective and we'll kind of talk through how we're thinking about that in a few slides so what do we mean by authentic relationship category uh management um so we kind of see this as a new category in this world agents are AI systems that are personalized to a Persona Think Like A salesperson Partnerships BD capital allocator a specific team or an individual through communication and social network Integrations and their objective is to maximize different network workflows so what do I mean by Network workflows um the world of what this could look like is pretty vast some examples include you know which of my relationships are growing cold you know I'm in a new city I'm traveling to Austin Seattle who should I meet with who are the people in the sales meeting that I'm about to go on um what is my network news today so if I'm a capital allocator and my portfolio company hires a new candidate you know I should know that and then congratulate my team um so Auto ad conference attendees to my socials that would be really relevant here and then who does my teammate know that I should actually know okay so we'll narrow this to kind of one case study and that is this user flow for keeping relationships warm so here her sends me a daily briefing essentially saying you know here's Jody you're about to go on an investor intro call here's her LinkedIn her Twitter here's kind of the summary of what we have on her and here's some mutuals and then interestingly I might tell her hey keep my relationship with Jody warm and then Hearth lets me know if there are no Communications exchanged um on Mark May 10th uh hartho ping me and I'll remember to like reach out to Jody and that's kind of an example of like this authentic structure within the user flow is one case study um so what does this actually mean for our agent um here's where we kind of Step through levels of like thoughts actions and observations so we start with the user query of like keep this relationship with Jody warm the agent thinks you know I need to keep this relationship with Jody warm and then we'll move to an action model to see if this fits into a hearth action workflow or if it should be kicked somewhere else so it does look like it's a hard action workload something we support so the next level is who is Jody like who is that in the knowledge base and that would then be a search on Jody and kind of the context which in this case is the briefing or the nudge and then from the previous context we identify like Jody's full name and then that third level is you know I need to determine when this relationship is is cold and kind of warn the user so from there Hearth would have a relationship model where it's basically coming up with an amount of time that might pass before this relationship is called and you can imagine like starting pretty naive with you know maybe people are important to me that I've spent a lot of time with and then growing in complexity to what we mean by like an underlying preference model um and then finally there's a reminder system or workflow where if there's no communication heart will then send me a slack message and uh let me know to reach out so kind of visually what does this mean it means that users really only see the slack Dynamic but behind the scenes there's this Hearth agent which is then taking the query and putting it in a query model which it then determines you know is this search through a document store is it an action that we support or is it feedback um around like a previous interaction that's telling me I did a good or bad job and so if it is an action model from there like we discussed we would then kick it to a relationship model determine the amount of time that should be passed and then have a reminder workflow that's running and if it's not we have search which can then query hearth's enriched contact system that's refreshing okay so there is a lot to test in production and I think a lot of us are all working through this at the same time and that means our team really focuses on kind of a benchmark and test Suite that captures a lot of this so prompts um this needs pretty rigorous a vowel setup it's kind of wild to see what seems like minor changes and prompts really changing the output embeddings Vector Stores um I would urge everyone to like watch out for sneaky costs which is something our team has had to work through this week and the whole landscape of vector stores is changing really quickly especially when we talk about like production level data augmenting so again even simple things have had a big effect for us like indicating chronological order for meetings um as a feature um results in like better outputs or more sensical outputs from our model an agent's ability to self-refine and search action feedback loop is really important and how this persists in memory um so I know baby AGI has made a lot of like news this week and it's really cool when we think about like what is a long-term feedback loop look like where the model is retaining the user's preferences over like weeks is really interesting um and then a vowels on staff and Ella Labs so go ahead kind of going back to our visual this is actually a suite of many different models and we want to make sure benchmarking covers like their interactions the key Insight here is like the agendic dev ecosystem is very new and the longer term actually calls for agent monitoring systems in the wild as this action layer grows so right now the action is pretty benign it's a reminder system in the future when it's integrated with other platforms or socials we want you know a lot more monitoring and ability to kind of call things back or have a human in the loop so especially if it's like Auto ad focus on Twitter that's something we want more sophistication around and then I wanted to plug John May's work who I think is amazing and he's thinking a lot about these like monitoring systems in the wild as they pertain to the legal space which I think is fascinating okay so just to kind of take a step back the mission of Hearth is to build a new category of a genetic relationship management products that do uh four things that centralize that enrich on your contacts that recommend and that act on your network um so you and your team can focus on connecting um and finally we're hiring for an ml Chief scientist and another ml function here in New York city so come work with us and please reach out if you have any questions okay thank you everyone okay I see a question from the chat how are llms helping you manage the risk of this self-spamming like Google email self reminders um the answer is even before we have that llm wire we have like a pretty thorough processing um even when we think about meetings we're focused on meetings with other attendees in them so we actually haven't gotten to like an llm processing out emails level yet we've been able because we're focused on outbound Network to I think minimize that risk so far with our private data [Music] I think it's a super interesting space that I think um so many of our products will be like stacked lolabs determining other agendic workflows in the future [Music]"
    },
    {
        "title": "Building Defensible Products with LLMs // Raza Habib // LLMs in Production Conference Talk",
        "video_id": "ax-uKawA-c8",
        "video_url": "https://www.youtube.com/watch?v=ax-uKawA-c8",
        "length": "24:10",
        "transcript": "all right well thanks everyone for having me um I'm gonna be talking a little bit today about what we've been seeing at human loop with people building uh LM applications I'll give a little bit of introduction about um you know who what human loop is and and sort of why we why we have an insight into this question and then I'll talk a little bit about some of the more successful llm applications that have been built and how they've done that what makes them sort of particularly good and what makes them defensible as well chat through some of the challenges to building llms that we've seen and then talk a little bit at the end about defensibility and how you can think about making your llm applications uh something that you can defend over time so that's that's the plan um if I could have the next slide please tell them thank you um so yeah I'm sort of at a high level what is human Loop we build developer tools to help you build useful applications with llms focused around prototyping and then understanding how well your apps are working in production and being able to use that evaluation data to improve them over time and so because of that we've seen a lot of people go on this journey from idea to deployed application and we started to see emerging best practices and what is and isn't working for people and so that's what I'm going to try and use to inform the talk today and I'm going to try and pick up on a couple of specific instances of applications that have been very successful and kind of talk through how they've managed to manage to achieve that success and the components of an llm application um and then kind of build into that sort of best practices that we're seeing and a little bit of a discussion about how you can improve your own applications and so if I may have the next slide please and the first kind of app I want to talk about today is GitHub co-pilot because I think this is by far and away the most successful and most used maybe other than chat GPT um sort of large language model application that's got Mass adoption right it's uh it's got over a million users I think now and it's growing pretty fast and it's been able to win over a historically quite challenging audience right I think developers are particularly picky about the tools they use especially to have an AI Pro programmer and so what I want to dig into is the anatomy of GitHub co-pilot as an application how is it built and then tell a little bit about what makes it so good because in some senses it's built on the same base models that we all have access to right everyone has access now to the gpt3 model Suite which is what it's kind of based on um and yet somehow they've been able to make an application that you know I think it's significantly better than a lot of the competitors out there and it's not just distribution that's allowed them to do this so I want to I want to chat through that so that's that's the first thing I want to talk about if I may have the next slide um sorry back one okay can you go forward to I think we're missing a slide here we go um and so when it's actually yeah sorry it was the previous slide thank you so I'm not having control visas yeah so the first thing I want to talk about before I dive into the specific instance of GitHub copilot as an app is the kind of components of a large language model application and I think of llm apps as being composed of what I think it was LM blocks so you might have many of these repeated in sequence or put together by an agent but at its core each piece has the same three components which is some kind of base model um so that could be gpt3 it could be an open source model like llama but this is a pre-trained language model that is generic and can be used for many different tasks there's a prompt template which is the structure of the sort of message so this is an instruction to the model with maybe some extra gaps where you're going to have either user-defined input or data that's going to be fed in and then some data selection strategy for how you're going to populate those at test time and so if we go if we look at a specific instance of this in the case of GitHub co-pilot if I could have the next slide then GitHub co-pilot has as its base model right that first component is a 12 billion parameter GPT model um so it's a code trained pre-trained model but it's significantly smaller than cgpt3 which was maybe 10x in size and the reason for that is in this instance they wanted to have something custom fine-tuned for code but also that was small enough to have low latency so that when you're trying to get suggestions from it you're not having to wait for a long time and so that's a critical component right they've chosen carefully in appropriate base model for this use case where latency concerns guide what's possible you would probably get better code generated from a larger model but at the expense of having um you know much higher latency and then they have an interesting and quite complicated strategy for figuring out what to feed into a prompt template so what they're doing is they're looking at where your cursor is looking at neighboring files um that you've recently touched and trying to find based on the code that's just behind your cursor the compare the like edit distance or Jack art similarity of different parts of code and other files and then use that to extract those sections and put them into a prompt that they then feed to this 12 billion parameter model and after that they then also have a system set up for very careful evaluation capture so the way they do this is they look at the acceptance rate of your suggested code but they don't look at just did you accept a suggestion from GitHub co-pilot they also look at whether that code stays in the code base whether it stays there after a few minutes whether it stays there after a few seconds and so they're able to understand whether the suggestion was good in production across millions of users and these components taken together start to give them both an excellent application and also something that becomes defensible over time because they can run this regular Loop of basically putting something into the hands of millions of developers watching them use it and because they have excellent evaluation data they're able to then work out what worked well and retrain and fine-tune on a regular basis and so the model can get better and better at this task over time and so I think copilot has the anatomy of what I think it was like a Proto you know a really excellent example of what you can do here where they've made a very careful decision about an appropriate base model that makes sense for their use case they've iterated on and done a lot of experimentation around what is the right strategy for what I should include in my prompt what data should I be pulling where should be coming from they're deeply integrating with a particular user's code base so it's not just generic but actually sort of is able to be adapted to that user and then they're they're using evaluation them to improve models over time and so if we kind of Step a slide forwards uh if we could step up thank you I think that this sort of like highlights the you know if we think about those three pieces that make up an llm app block or make up GitHub co-pilot we need to find a way to make each of these excellent so we have to have like an appropriate base model that's the right size that's been fine-tuned for the task or that has good sort of performance we need a way to get the prompt engineering to work well and I think sort of on this side I've tried to put together some of the challenges that you face when you're doing that and then finally you need a way to measure performance that you can improve things over time and so what we've seen when people come to do this in production um is that there's kind of a few challenges that come up again and again and the first is that prompt engineering is still a bit of an art so small changes and prompt templates can have surprisingly big outcomes on performance um and that means that it has to be very very iterative you have to have a fast feedback loop and you need to be able to experiment a lot you sort of can get a first version very very quickly but then getting towards something that's truly excellent takes time another problem that we see pretty consistently and I think like others will you know have spoken about this as well is the need to try and make llms more factual and finding ways to overcome their you know the fact that they've hallucinate and make things up um evaluation is another one that's particularly challenging for llm apps and I think this is different for llms than most traditional software because we're beginning to use these things for applications that are much more subjective um than we might have in the past so if you're generating marketing copy or you're sending an email then there isn't a ground truth answer that you can just look at and say okay that's the correct thing you need some way to measure performance based off what your users think is the right answer um latency and cost you know is something that we have to figure out ways to choose from the appropriate app and then I think the one that we'll talk about a little bit when we come to defensibility is if you've just got GPT plus a simple prompt then it's a very thin Mo and you need some way to overcome that um and so I just stepped through these kind of one by one uh so we go to the next slide please just wanted to explain a little bit more about like why prompt engineering is so important and the kinds of small changes that you know make a big difference so no I think last year one of the there was a paper that captured a lot of attention and that has become kind of very commonly known amongst the community now showing that Chain of Thought prompting had a huge impact on the performance of question answering models and other reasoning models so if you simply ask a model not just to answer a question but to provide a reasoning Trace suddenly you get not just a little bit better performance but significantly better like many many accuracy points uh better than you would get just from a base question prompt and you know this is now well known amongst the community but the surprising thing is that there are still many more tricks like this out there to be discovered people are constantly finding new ways whether it's the format you know asking a model to be formatted in certain ways to click on a particular role there's a lot of changes or tweaks that you can make that have a surprisingly large impact in performance and so one of the things that we've seen when it comes to trying to find ways to build defensible apps is having a very fast way to iterate on your prompt templates get feedback from that and tweak them and change them is actually critical to getting good performance um the next thing I wanted to chat about when it comes to prompt engineering as well is if I could have the next slide is that something that we've seen people sort of start to have patterns around but has historically been challenging is finding ways to get factual information into the into large language models and so one thing that we think is going to be super critical here and again we're seeing examples of this in practice is giving llms access to tools and there's been a few other talks touching on this today but a a common emerging pattern for this is to sort of take the documents that you want to give your model access to split them into pieces embed them with a large language model and then make those embeddings available uh two-year model when it's doing generations and what we've been looking at is sort of finding ways to make this accessible in an interactive environment so that you can experiment with that much the same way that you would with your prompt templates themselves and uh if I could have the next slide oh yeah actually sorry can you go back one second for me to the the previous slide um and yeah so this is this is a common pattern and it's sort of again if we think about the pieces of say the GitHub co-pilot app which is one of the more successful ones this is another area where they've clearly spent a lot of time thinking about the right strategy for doing retrieval so there's different methods for doing retrieval into your prompt template to make it factual and they have a big impact on performance question from the chat what technique can be used to feed back the evaluation data into the model as with copilot potentially a form of rohf or something else okay great so that takes us that's that's a great question and if you can take me to the next slide Lily I'll uh I'll expand on this a little bit so the third component of what I think makes a really good llm app once you've figured out good prompt engineering you've maybe found an appropriate base model is having a way both to measure feedback and understand how well it's doing and then use that feedback for continuous Improvement and there's basically three things that we've seen people use successfully to do this so the first thing that we see people do as a very common workflow is they will use the feedback data they're collecting and you know all of the Best in Class apps now capture end user feedback in some way so I mentioned ham GitHub co-pilots looking at suggested code being accepted at regular intervals 15 seconds 30 seconds two minutes 10 minutes um chat GPT has this thumbs up thumbs down followed by various forms of natural language feedback and in general is a best practice that we're seeing is people capture are three different types of feedback and you know so actions what does the user do after they see my generation in the application issues and votes and those are sort of very common that become a common framework for types of feedback we see people collecting and then once you've collected this feedback the things that we see people do to improve their applications one is look at the cases that are failing form a hypothesis about why and then try to edit and try to do prompt engineering to improve that and so that might be realizing that actually your retrieval step is failing it's not giving the model the right um the right section of the code it might be realizing that actually you need to encourage the model to be less repetitive or you need to tweak it a little bit in some way so we see a very common Loop of people putting something in production filtering the data to see the failure cases inspecting them manually and then trying to do some round of prompt engineering to improve that the second kind of Step Beyond that is when people actually come to fine-tune their models and we see two forms of fine-tuning being used and if I could have the next slide um so the first is actually pretty straightforward supervised fine-tuning so the idea here is that you're just filtering the data set by things that have worked well for other customers or that you have some reason to believe it worked well fine-tuning and then repeating that process and so there's this cycle that we see really commonly which is to generate data from a model whether that's in production so you're running gpt3 or in the case of copilot you're running this 12 billion parameter model in production for some time capturing all of this feedback data filtering down to some subset that's worked well and then fine-tuning the model on that and repeating this process and as you do that you can get better and better performance on your specific subset of tasks um and that's something that we're seeing in production but it's also been demonstrated in the academic literature as well so there was a paper called star that was looking at doing this for reasoning so they took a a set of reasoning tasks used a model to generate Chain of Thought prompts filter it and retrain the model and they're able to to show that models get better at reasoning and there's quite a few instances like that and then the third way of doing this as someone asked about is rohf we see fewer people doing rhf in the wild because it's more complicated to do I can think of a few startups that have done it but actually the gap between doing no fine-tuning and even just supervised fine-tuning is really large you can get significant reductions in cost and latency if you're able to fine-tune a smaller model and you also get an application that's more customized for your specific use case and so when it comes to defensibility we've seen that fine-tuning for performance can actually be a very very significant advantage and one thing I wanted to chat about if I could have the next slide is a common question we get which is when to use prompt engineering versus fine tuning because I think there's some skepticism about the benefits of fine-tuning or it requires a lot of extra work and you can get quite far just by adjusting prompts and so when should you think about should I prompt uh should I sort of do prompt engineering or should I fine-tune and so what I would say is the advantages of prompt engineering and that it's very fast to adjust your prompts there's less work needed ultimately you're not having to host models or figure out how to fine tune yourself or even Munch data into the right formats with the fine tuning apis after experimentation you can get good performance and if what you're trying to do is get factual knowledge into the models that's changing fast then prompt engineering and retrieval is the right way to go so fine tuning I don't think is something you should be doing to try and get factual information into your models but what does what fine-tuning does allow you to do is to get smaller models to have similar performance on your specific use case and to get performance in your tasks that might be better than any other model out there so if you have access to some kind of special data set whether that's private data or you have access to feedback data from running a model in production for some time then fine tuning is a way of really building something that's significantly more differentiated than what anyone else out there can have it can allow you to bring down latency significantly so in the case of copilot we saw that they were using a 12 billion ish parameter model and that's primarily a latency concern but also because you're training smaller models you can get lower cost and then it also opens up the door to doing local deployments or private models in the tone of voice of a particular company so there are significant advantages to fine-tuning even though it might be harder and the journey that we've seen most customers go on is almost everyone starts with prompt engineering they get applications to a certain level of performance and then they sort of start to fine tune later because prompt engineering is so much faster to get to a first a first version of a model and so in terms of you know recommended best practice I would say push prompt engineering as far as you possibly can and then think about how do I fine-tune to optimize performance but don't optimize prematurely may I have the next slide cool Okay so we've spoken a little bit about GitHub co-pilot um we've spoken about the anatomy of an llm at the three parts of a base model template for a prompt structure like a strategy for getting it in and strategy for evaluation and the fact that those things get chained together but the title of this talk was how do you build defensible apps with llms um and so I want to chat a little bit about how you can sort of actually get defensibility and differentiation and obviously fine-tuning that we have hinted at is is part of that before I dive into this I do want to say that I think this has become a hot button topic of conversation amongst people in the sort of Builder Community who are building with llms thinking about and and I guess in investors as well like how do I build applications that are differentiated and they're going to be defensible over time and I think that we should be careful not to over index on this and so that's why I put this quote here before I chatted about it from YC who you know great investor into a lot of startups where they say you should ignore your competitors because you're more likely to die of suicide than homicide and I think we shouldn't lose sight of the fact that for building nlm applications the number one thing we should be thinking about is how do we solve a real user need and do that quickly and successfully but that said defensibility is still something that people will worry about on a longer time Horizon and there are things we can do to make our apps more defensible and so if I may have the next slide um what I wanted to chat about sort of as an example here is so the first thing I'll say when it comes to defensibility is I don't think that large language model applications or companies build on large language app models are fundamentally different to other software businesses right all the things that you would be thinking about if you were trying to make a company defensible as a software business still hold you're still thinking about scale you're still thinking about switching costs Network effects brand Etc the things that you would think about always but maybe there are specific things that matter or that you can do with llms that that you might not be able to do in other circumstances and I think it's it's instructive to consider this example um of considering two different companies that have been quite successful as one of the you know amongst the first applications of llms in production which are Jasper and writer so these are both companies that have um built developed writing assistance from marketers um they both had significant success early amongst the first llm company is to really get to scale but they took really different approaches and they've taken different approaches to becoming defensible companies so Jasper focused on scaling really really quickly they had quite a high marketing span they captured a large fraction of the market and their main approach was to build on the open model so sorry build on closed Source models from openai so primarily initially building on gpt3 but scale very very fast and try and get defensibility that way whereas a writer took a very different approach and actually focused on building with custom fine-tuned models and that allowed them to counter position against openai and also their competitors like Jasper because they were able to promise their customers that they wouldn't store any of their data that they would be able to give them customized tone of voice and that everything could run on their machines which allowed them to access an audience that just wasn't accessible to Jasper possible to some people who are building on open Ai and so I think there's an illustrative lesson to take from all three of these applications co-pilot Jasper and Ryder about things that we can be doing to make llm applications more defensible so in the case of co-pilot Beyond just building an excellent product I think they have this incredible data flywheel built in where they're able to capture feedback data use that feedback data to improve a model and get it better at that specific task so that over time that becomes harder for others to catch up to them in the case of Jasper they really went for some form of bit scaling by getting to a a size and brand awareness very quickly over others they were able to I think establish themselves with a very very hot like large share of the market versus writer who counter positioned against everybody else and was able to offer something to customers that others couldn't without changing their products significantly um and if I could have the next slide please I sort of you know I tried to jot down different things that we've seen in terms of strategies for making llm apps defensible and how they map onto maybe some of the more traditional views about what makes apps fermentable in general and so one of these is you know I think a lot of software applications have this and I think this will be particularly true of large language model applications is having high switching costs from integrating deeply with private knowledge sources so if you are able to get you know private customer information a particular company's knowledge base if you're doing customer service or code if you're indexing someone's code base or something like that then that can be a big Advantage because the base large language models don't know anything that wasn't available on the public web so if you're going to be able to answer questions or deliver a service that requires private information you will have to build a lot of Integrations and with those Integrations come High switching costs and that's a form of defensibility a second form of defensibility that we've discussed is the ability to build a flywheel through feedback capture so this is the kind of GitHub co-pilot example but we see this a fair amount with people fine-tuning through human Loop where you gather a lot of data in production you get feedback on how well it's working you filter fine tune repeat and are able therefore to start getting an application that is our model that is better than what others can train um and it because it's getting better and better over time you also have this sort of data defensibility through this network effect and the final one I've spoken about is a and the final one I've spoken about is um counter positioning so can you find ways to do things that maybe people who are building on other models can't and so writer was a good example privacy and having their own models fine-tuned on customer data they were able to do things that weren't accessible to their competitors and then the final one is obviously like in some sense less theoretically sound it doesn't map onto any of these traditional ones but it uh is just focused on building a really great product that solves a real problem and has distinct ux I think GitHub co-pilot is another example of this where they thought about building something that had fault tolerant ux because large language models we know are not completely reliable and so they thought about how can we provide something really useful to people whilst knowing that it can't get everything right all the time and completion and suggestion with sort of having your code in context works really really well for them um so I think that's my last slide and I will end there I don't know if there are any questions can you just hit the next slide just to make sure that's that's it for me so thank you very much"
    },
    {
        "title": "Data Privacy and Security // LLMs in Production Conference Panel Discussion",
        "video_id": "SZe1pDxgCt0",
        "video_url": "https://www.youtube.com/watch?v=SZe1pDxgCt0",
        "length": "25:44",
        "transcript": "now let's get through this panel on data and security we've got a few people who whoa what's this Diego is going to be leading us through nice seeing you again Senor Diego I'm gonna bring on my man Vin dude Ben uh I gotta tell everybody Vin was about two years ago he came up with the best quote ever which is kubernetes is a gateway drug and now I think we need to update that quote and it is GPT is a gateway drug to starting with machine learning I'm just gonna leave it there for you I'm also going to bring on sahil where you at man and Greg Grove and Shreya where are you Shreya let's see if we can get you up here too I will let Diego take over because he is the moderator of this panel and [Music] there we go all right oh looks like Shreya may have had some technical difficulties so I'll bring her up on when she is ready in the meantime Diego get cracking man oh you're on mute uh of course had to do that one right yeah I know how it goes you're just following your um your example here hey everyone uh so really excited about about kind of leading this panel so like the core of it is uh you know as is in the title data privacy and security so you know we've talked about some of these uh you know what does it mean uh to um you know the privacy and Security in the context of uh large language models uh what does it mean to trust these new AI systems so there's a lot of questions around uh you know hallucinations and and we I've talked a little bit in my in my keynote around the you know low affordability like high affordable use cases so without further Ado I have some amazing panelists here uh who are at the at the core of this on both uh is it fair to call the previous generation of ml it feels like makes me feel so old uh and uh but um you know uh so I'm gonna really quickly start and let them introduce themselves so and ask YouTube uh you know uh you know your name what do you do um and kind of like give me one little thing about like you know in the space what you've been thinking about so you've all been thinking about this space a lot so I'm a little nugget uh about it so so here we'll start with you since you're on my top right here it hurts um thanks for having us um so my name is sahil I am an engineer at u.com which is a conversational AI search engine and one of the topics I've been thinking about a lot is how do we best combine a lot of these advances of generative AI with advances in information retrieving Vin I'm ven vashishta founder and CEO of V squared been in technology for over 25 years data science machine learning for over 11. I do data strategy and AI strategy because well they wouldn't let me do any cool projects until I figured out how to get them paid and make people money so that's now my majority specialty figuring out how to make money with these models what I've been thinking about is what comes after this because these this is just the beginning and we've got probably a few other Evolutions coming afterwards things like Robotics are going to come to the front too so that's what I've been thinking about well it's next right keyboard hello I'm give work again I'm co-founder and a CTO AT xero Systems uh I'm leading the both a product strategy and the technological strategy and uh what we're doing zero we develop um co-pilots for knowledge workers which are augmenting them in a very like you know a high value sophisticated tasks and actually what we do what we think like you know uh the future will be in the intersection of uh automation of sophisticated workflows with with uh uh Foundation models and we are super excited to see how the businesses are being transformative with this technology excellent great and finally uh last but not least Freya uh hey everyone very excited to be here um my background is also in machine learning and a lot in like previous generation of machine learning as Diego said so I've been working in ml since uh yeah I think eight or nine years everything from ml research in um you know classical AI decision making under uncertainty deep learning um also work in autonomous systems and self-driving doing machine learning and deep learning for a few years and then it's most recently the founding engineer at an mL of startup also doing the machine learning infrastructure applied ml um I'm a top of mind for me and I guess also the reason that I'm here um talking on this panel is that I'm the creator of an open source Library called guardrails which as you would expect adds guardrails to the output of large language models and makes them a little bit more reliable and safe um yeah excited to be here great awesome um so let's just start about like kind of like let's high level like how should we think about uh you know the difference in some of these data privacy concerns returns that exist today between like kind of you know how we were building models and you know maybe a couple years ago and now using some of these kind of foundational model apis like how do we frame this uh anybody can can go ahead like I'm trying to think how do we should think about the framing here I'm like I'll just pick on Vin because he's right there in front of me it's always me how should we think about these I think it's the biggest piece that's missing from the conversation is that we're not thinking about the patterns they can uncover it used to be we could find fairly simplistic patterns the more complex machine learning and then eventually deep learning models got the more complex the patterns that could be discovered and so when we think about data privacy data security with respect to these types of models it's no longer the data itself it is the patterns within the data that can be uncovered and create vulnerabilities for anyone that has a significant amount of data out in public there's a study that was done by uh it was called short at Pepperdine where they realized they could through some creative prompting get these models to give a basically craft a VC pitch in the style of different VCS and it was it was convincing so there are patterns that are built into the data sets that we throw out there all the time on social media and they allow for more than just our data to be learned there are deeper patterns now and I think we need to start thinking about the implications so so how do we think about to kind of continuing that I'll ask you like you know you're working on information retrieval uh and a large scaler how do you how are you thinking about like the framing around these kind of like this patterns like when do you use when do you think you can use these open generic generic apis versus kind of like need to bring in models in-house like how do you frame that yeah no I think um that's a interesting point around some of these patterns and some vulnerabilities there I think there's probably my head kind of two ways in which we approach thinking about some of these issues so on one hand a lot of these models loosenate so they'll make up content I think we're all aware of that um so in some ways you know there's obviously a lot of technical work to be done on reducing hallucination better grounding essentially a lot of these models so that's one area but then on the other side um just because something is hallucinating doesn't mean that it's not a useful tool for people so I think it also is somewhat of a product question so how do we make sure that people have the right expectations when they're using a product than most language model they can get the most out of it so I think when we're thinking a lot about this type of stuff those are the two angles in which I think a lot about it is it's not only a technical question but also a product question around you know what are the right expectations that you know we present to people using these tools and how do we make sure that they're not going to be misled and they know how to responsible user we have a great and great point in terms of like how to use it and so I'm gonna I'm gonna move to you straight and kind of like maybe for the audience kind of frame some of the challenges around hallucinations and kind of like how you need to think about it and then obviously you know tell us a little bit about your work in terms of uh what's your hypothesis here like you know you obviously built this like fairly popular open source project that's having a lot of adoption so uh very curious to hear from you uh you know kind of like how you how you frame in this context yeah yeah I think Hallucination is a very um interesting problem like when people think about hallucination it's actually you know like a combination of problems that all kind of get grouped under this umbrella term of hallucination uh I think some of those problems are basically falsehoods Etc or even like you have multiple conflicting sources and you aren't able to trust like which one um you know is the golden um is the golden source that you should kind of Base your answer on so it's a bunch of like kind of complex problems going on here um I think like grounding honestly um is is the way to go is the way to kind of solve all of these uh solve you know very domain specific um hallucination problem so I do think that um trading better models training bigger models you know that are kind of like um uh primed to uh you know be less susceptible to this is like one way to go about this with at the end of the day you know as as all of us have worked with them now uh we kind of know that it's really hard to kind of get that level of certainty with any machine learning model and so being able to take something that is so powerful um and you know then add constraints on top of that that make it work really well for your specific use case I think is you know um more just more like more tractable essentially as a problem to solve rather than just make hallucination go away um you know as a blanket thing for LMS um and so one of those things I I believe people have touched upon this before as well but like is the way to go is um you can essentially connect what you believe are like good data sources or good kind of like fact checking um um you know either agents or tools or um um or or even just like um embeddings of um you know good data sources with your llm outputs um and then any kind of like LM outputs that are generated would get you know validated against those like ground rules so that's kind of like the guardrails way to do it which is that um you use your large language model to um generate something that you know functions and then on top of that um for your domain you think about like what are my constraints um and then impose them you know using like external sources or um um yeah or or external data connections you work in a very applicable space right you're building for information workers inside Enterprises kind of like these assistant like accuracy matters trust matters security and privacy matters like as you leave like how do you think like you know kind of what are some of the principles that you're thinking about uh you know today as you develop the product obviously don't reveal anything proprietary or uh you know trade secret but I'm very curious like how you're thinking about it because it has to be relevant I mean you're trying to get the trust of these Enterprises to not only connect their data but also like use your system inside their workflows like walk us through that absolutely so currently the Enterprise is already see and believe in the power of the generative AI the power of the large language models so it is like very close to them like they can play with that the charge GPT for all for their like you know personal use cases but when it comes to the Enterprise use cases and we are working with for large Enterprises such as Fortune 500 companies the largest law firms in the world so their data is very confidential and it's actually their client's data and they have like you know contractual obligations about how they are going to govern that data and of course like you know now there's like you know there's a hugely like you know cousin between the opportunities and the reality and what we are doing uh we strongly suggest not to use the apis like child GPT for the confidential data and for those cases just bring AI inside the organizations because nowadays there are already a lot of great models for like you know several billion parameters you can bring inside the security perimeter of the Enterprise fine-tune on on their domain specific data give a product iterate on the human feedback and reach the level of the quality when the users will trust your system so there you go your absolutely right the trust for Enterprise systems is absolutely must in case they will lose the confidence in your product they'll just ignore that totally absolutely into you know because I you know I I could frame one of the things I'm always kind of curious about I've been thinking about like there seems to be this um people applying the the you know kind of like the use cases to a one-size-fits all like framework and like you know like and and it's problematic right because there's times that like you shouldn't care right go use an API whatever's faster cheaper whatever gets you there and there's other times that you should spend the time to bring it on you to your point bring all the AI into the Enterprise but that has a cost right it's it can be expensive you need to have knowledge and stuff like that and so being able to do the back and forth based on like how you're thinking about security and privacy and not applying everything with uh you know one size fits all I think is really important I want to kind of push a little bit on the um you know like what have you seen in an industry in terms of people like let's talk about that framing right like how to think about the use cases how to frame how to you know if I'm listening to this talk right now and I'm trying to bring some of these use cases into my organization like how should I frame it right in terms of thinking about how should I break down the problem should I go in route a should I go in route B like who can help me and kind of like guide the kind of like questions that I should be asking my myself in terms of like where to run this and how to run it any of you can jump in yeah so like one things like you know when you are currently thinking about the use cases at first you need to understand that currently you need to put every single truth that you had in the future like under the question because you had some understanding like what is solvable what is not solvable right now everything changed you need to come to your business and understand what which are like you know the the major kpis uh that will contribute to the like you know growth of the your business where are you are you know there are your opticals you know where the protests are sophisticated you are spending a lot of like you know money with uh like you know less input and understand if you can like automate that and bring in the people who understand that and they say oh you know with new technology it is possible got it then you work on data strategy a lot how are you you know if I came to you today and I said I needed some advice on how to think about this can you frame it for me yeah definitely what I tell companies is I tell them this is kind of an arc there's a whole bunch of use cases that you could have but they haven't been proven yet and if you're not meta don't be first because you don't have the capabilities you don't have the background you don't really have the domain expertise in this area to be the Pioneer for a particular use case wait until someone else proves it out and then enter in but that doesn't mean you have to wait to your competitors get into the market you can look at each use case as a category you look at the ability to service customers and there's so much that you can do there but you also at the same time have to be protecting your customers which is not something most companies are thinking about what happens when their customers submit a query across your website where does that go how is it stored what are the Privacy implications that you're not thinking about and that's why I say a lot of times you don't want to be the first company to do this because you don't have that in-house capability to really at an expert level think about these things but at the same time what you should be doing is some really targeted opportunity Discovery thinking about different categories of capabilities and problems that you've seen solved in other Industries and then asking years of how could I apply that problem solution and fit it into something that I have in my current business internal use cases are always great for proof of Concepts because you're banging on it inside and the risks are significantly lower then turning around and you see you know companies like Google did this Microsoft did this they consumed it internally and then they turned around and let companies externally play with it and so a lot of these paradigms are what I'm talking to clients about now but it's really important to be thinking through use cases and connecting to Value propositions not well it sounds cool so let's do it no no start with the ROI if there's not a significant Roi you know if there's not a big dollar value on the other side of this why distract yourself from your core strategy and your current competitive advantages you have to adopt this at some point so you should be forward-looking and prescriptive but at the same time it's all about the returns if there's no significant use case that fits that will deliver cash don't do it yeah actually um I was I was gonna ask sahil about his so you know you work in search you work in the future of search right and retrieval and personalization and I'm kind of curious like how you think about you know let's go you know like you know if one argues not only company but our personal data is kind of like the most valuable thing we have right but we obviously want to contribute that data to search experiences that are great for us uh how do we think about like you know what's the future look like can you guide us through that like uh in terms of like how you're maybe how you're thinking about it at you or maybe how you're just thinking about it personally we can detach those two things if you want uh but I'm very curious uh you know what the what the future of personalized search looks for look looks look looks like yeah I think it's honestly I think it looks super excited that um yeah I think the future of search is probably more exciting now than I think it's been in you know the last last couple years last five years maybe um and I think there's a number of reasons why I think a lot of it is obviously the recent advancements and you know natural language processing Etc I think when it comes to personalized search um I think it comes down to you know how like I guess when we think about if we're thinking a little bit more into the future um you know what can we do to really allow you to control your search experience so I think that's something that will have a lot more ability to control I think a lot of times now when we're interacting with some of these models for example just think of the concept of a system message so when you're using you know one of these models um you know there's this idea where you can specify a system message and it will basically you know dictate how the model is able to adapt to that so that's that that in itself is a degree of a personalization of AI that we have not been able to do in the past so I think ideas like that basically will allow us to really personalize the results that we get from search in the future if we kind of apply that more broadly and then also in terms of bringing your own data I think there's going to be a lot of we have a lot of ideas at least we're thinking about you know building an open platform where other people can contribute their data and essentially what we call apps into search and we'll incorporate that into our chat slash results so I think there's you know a combination of open Community efforts that can make you more personalized um as well as um personalization enabled like technology itself cool so um we're gonna go do uh we're gonna have to wrap this up even though I could probably talk about this uh you know all day so I'm gonna do one quick uh you know rotation through everybody and I'm gonna ask you to either recommend a tool it's totally fine to recommend the tool that you're working on a uh you know recommend a resource or kind of like give a thought to the audience to go chase in terms of like how to be thinking about data privacy and security and trust in these AI systems so so I'll start with you uh and uh and go for and go that way yeah I guess maybe I would start I guess one thought would be um maybe just you know what you currently find um these tools to be very useful to you right now where you know I think sometimes it's easy to think of tools as useful um in abstract or maybe on a couple examples but what are some ways in which you've been using some of these Technologies and consistently useful for you um and then you know from there I think one can imagine what the future of these cases will be good work yeah it's a great question so since we have like a very uh like you know a big audience here so and I believe that not everyone uh started to like you know work and experiment with like you know uh generative Ai and like you know bring the applications via length chain so I strongly suggest uh to do that because you'll be very impressed how fast you can have a great result and then you're already will have a good use case you can order the like you know double tile on the accuracy at front like you know your user experience right Vin uh I would start looking at Auto GPT the whole concept of self-healing self-correcting those are some really fascinating use cases there's danger but a lot of potential in that direction so I would say look at those tools if there's anything that comes out of this that I think becomes an exceptionally powerful construct going forward that's the one to keep an eye on from a forward-looking perspective got it and Shreya and I you know I I expect what you're gonna you know which what tool you're gonna expect to push here so do it uh totally happy with that I think I'm going to basically talk about guardrails and I'm going to talk about it more in the context of like this idea that I want the audience to kind of like think about and like take home you know um um uh yeah some take-home inspiration and like why guardrails kind of fits inside that um so I think essentially um we like these tools are these uh these models are really performant right and we see like really interesting use cases and everything but like are they ready to be deployed into production where they can you know like work reliably work like 100 of the time and not you know um return like awful messages to your potential users Etc right and so I think the idea I kind of want to share is that this isn't going to be um the this is insufficient enough to like actually put a lot of you know the applications that we're building into production and the actual solution would be a hybrid of you know more traditional machine learning uh more traditional like rule-based and heuristic based methods in addition to like large language models and I think like that kind of gives us both the performance as well as the kind of reliability safety guarantees we care about and I think that's a very powerful construct of like ensembling those two kinds of methods together and so in that context I think like guardrails is a good Tool uh that I want to push uh uh obviously I'm biased but I think it's a great tool that you know allows you to kind of like get a lot of those um a lot of those guarantees Straight Out of the Box yeah awesome well hey thanks all of you for a great panel uh I believe we're uh we're we're here wrapping up uh Ben gervorg uh sahil and straighta thank you so much for taking the time they will all be in the slack channel for the conference we'll also share the links about what we uh you know what was talked about today uh Demetrius back to you all right wow that was special so I knew it was gonna be good I'm not gonna lie uh you get this many hard hitters in one place on one virtual room and what do you expect [Music]"
    },
    {
        "title": "Efficiently Scaling and Deploying LLMs // Hanlin Tang // LLM's in Production Conference",
        "video_id": "AVccFl8-5-8",
        "video_url": "https://www.youtube.com/watch?v=AVccFl8-5-8",
        "length": "25:14",
        "transcript": "foreign [Music] okay great yeah thanks a lot for having me really excited to be here my name is Hamlin Tang I'm the CTO and co-founder at a mosaic ML and today I'll be talking through what it takes to efficiently scale and deploy these large Anglers models at a sort of uh taken the World by storm you know in the last couple of months right and we've seen all the headlines on this entire amazing virtual conference that's been that's been put together has been built specifically for this topic what we've observed early on is that people used to think that okay llms are here and you need one giant AGI model uh for every use case um that's very centrally controlled by a few individual companies I think over the last month or two that thinking has really evolved inside the ecosystem to saying hey you know what uh you don't the world doesn't actually look like one you know AGI model it looks like many small specialized models that are used to solve very specific use cases owned by many companies and I'll provide some examples of what we're seeing in the ecosystem today uh with um with this type of thinking and it's not just us uh you know you can see here um uh articles from uh both illuminaries in the field as well as Venture capitalists that are saying that you know what we're going to be in a much more decentralized world where the power of llms will be put into every individual company uh or startups uh a world and you'll see these very distinct General AI models start to come into place and on the right you can see you know an article from uh folks in Venture Capital that companies with unique data stores will see clear advantages from training their own models as modes so we're really you know seeing this future world where companies will do both they'll use these amazing external apis that have come onto the scene uh to build some of their applications but they've also will undertake the effort to build their own custom large language models and what I'll do today is talk through exactly what that second bullet point means um is it too expensive is it too hard when should you do it when should you do it and what tooling is out there to help you really be able to train and deploy your own custom language models trained on your own data so why build your own models one of the key reasons that we see out there is you can see these headlines here a lot of kind of data privacy concerns where you don't want to be spending all of your time Outsourcing and sending your private data your core IP uh to an external service but I think if we dig deeper there it actually comes um a lot more um than that we see many reasons that companies are now citing that they want to be able to train their own models we already talked a little about data ownership but the other piece is really understanding where that model data is coming from if you want to be controlling the bias and the outputs of these models that many people you know so far have already talked about as a big problem for large language models part of that is understanding well what data went in if you're building a financial model for Enterprise do you really want to be using a model that was say trained on you know reddit's a Wall Street bets channel right or if you're building a healthcare specific model you want to make sure to be excluding certain data sources from your model itself part of the challenge that we have in controlling the model output is that we're applying all these Downstream hacks or modifications to get the models to align when many of the problems actually just come from where the source data came from so controlling what data went into training the model um can play a huge role in this data provenance problem the other piece is really controlling the content fillers that make sense for your business right some businesses need a very long sequence length or have very particular filters that they want to apply and you want to be able to apply that control and the third piece that we see is model ownership companies want to be able to own their weights either for portability so that you don't have to be beholden to a particular service to run the deployment side but also for better introspection and explainability we see even more reasons why customers want to be training their own models inside this ecosystem that's growing very quickly um of course I mentioned the business Advantage where kind of your data is your remote but the other under appreciated piece is inference economics um originally I think the field when they came onto the scenes okay we need to train these 100 billion parameter models 175 billion parameter models you know 500 billion parameter models and playing that kind of scaling game but what really emerge is that for many individual use case that you may be solving um you don't need a 175 billion parameter model that can do a lot of magic and AGI you have a very specific ml problem or NLP problem that you want to solve and for that training smarter smaller models um becomes much more interesting also in many applications that are very domain specific we see a lot of folks starting to train their own language models as well so in genomics for example where now you're dealing with sequences of proteins and dnas instead of natural text or electronic health records or medical or or in vehicle and then lastly I already mentioned kind of the need for data and model ownership from a privacy and from a regulatory standpoint we've heard from insurance companies where hey if you're in a highly regulated environment model and data ownership is critical to building more explainable and and better models also you don't want to be relying on external services that may for example you know discontinue a model or obsolete them all that you've been relying on for your particular application so we're really seeing this world where customers and and you all may want to start training our models and deploying your models and I think when we first say that a lot of what we hear from folks is hey it's that sounds hard right you need to find the right number of gpus you need to figure out what the software thing is to be able to run these things to figure out how to deploy them um thankfully and as I'll walk through during today's talk um there are many amazing tools out in the open source and in the community today to make training and deploying your models a lot easier than you may think it it would be from from first principles and I'll share some examples of of how these are being done today so this is an example is biomed LM it is a domain specific large language model that was trained just on PubMed so just on a large corporates of biomedical literature and I'll make a few interesting points here first this was trained by Stanford University joining with us but primarily by Stanford um and by just a team of uh two or three ml engineers um so despite like the size and scale of these models the tooling has gotten to a point where even small teams are able to train very interesting and Powerful language models and so this is a very simple example of a three billion parameter fairly small in terms of large language model world but we've we've seen that models like these really punch above their weight um this model is able to hit state of the art accuracy at the time of the release on the U.S medical licensing exam um and uh so that's this med QA USMLE example on the lower left here where the model gets this very long prompt about a history of a patient and the symptoms that's being presented um and it has to do a multiple choice about what the right test to order for this particular patient is uh and on the right is a different Benchmark that we use that Stanford used to measure this model which is PubMed QA where you're given a question you're given some context and you're supposed to answer you know some a particular medical problem and what's interesting here is that if you look on the chart on the right is that this biometer actually it was originally called PubMed GPT but we renamed it to PubMed LM model this first row here we can hit state-of-the-art accuracy at the time um on this med QA Benchmark and what's interesting is that this three billion parameter model can reach a similar performance as Galactica which was a model there's about 40 times larger but was trained on much less specific data so it really drives on the point where we're seeing growing evidence out there that training domain specific large language models in the medical space and the legal space in genomics and Healthcare and insurance you can actually build models that are economical uh to deploy at scale in inference across a large set of of customers the other point I'll make here is that um this is actually pretty accessible there's a lot of data that's already exists out there for particular domains out in the web for you to take in and and and and use uh in order to train these models so smaller three billion to 7 billion parameter models that are fairly specialized uh we've seen in the ecosystem can actually have very very strong business value um and we've seen applications everywhere from in the financial space where folks want to classify or summarize loan documents um to co-generation and helping with the programming tasks or these small models bring a lot of value because in most cases you don't need the power of that large you know AGI system that's been trained on a lot of data and it's very expensive to serve the other example that just actually came out last week was Bloomberg who trained a 50 billion parameter large language model on a combination of web data and also uh internal Bloomberg data and uh this uh is also fairly interesting because traditionally we thought of building large language models as requiring either the training from scratch piece or the fine-tuning piece but what's Happening Here Also is this continue to pre-train piece where you take a combination of web data that's already out there and internal Bloomberg data that's what Bloomberg did um and they found out that outperformed existing open source models on a large set of financial tasks and this really drives on the point that hey your internal proprietary proprietary data matters and can be used to solve better tasks for for your business now I think you know when we first say this there are a few I think myths that start coming out and the first one is oh my gosh Hanlon that's great I want to build my own models uh they give me data and privacy I get control over where my IP goes but it's too darn expensive um if you read the press out there gpt3 took somewhere from 10 million to 12 million dollars uh to train this model that's nowhere near feasible for kind of a prototype system the other I think myth that we've seen out there is that oh it's just too difficult um you have to figure out all these different individual components in order to train your large language models how do I get everything to connect together how do I figure out uh the right how many gpus I should use how did I get multi-node orchestration to work how do I deal with node failures how do I get the data streamed in how do I fit all these models into memory what type of model should I train and so these are two very common myths that I wanted to sort of bust in this talk to show you where the tooling is today uh to make training and building your own models uh very accessible the first one is you know we put out a Twitter poll I think this is somewhere in September uh asking folks how much do you think it actually cost to train a gpt3 quality model from scratch um and just like it's being reflected in in a lot of these articles that I just showed about 60 believe that a gpt3 quality model um costs you know between anywhere every one to five million plus or per rough and you know you do have to train these models a few times in order to get it just right um but the reality is that training large item is actually fairly accessible with the tooling that's available from us and other people in the in the community uh here is a actually a chart of different model sizes um how long they take to train on about 128 gpus and the approximate cost and suddenly if you combine kind of the tooling that's available now with the idea that hey a 1 billion to 7 billion parameter model actually is starts becoming very attractable uh for for business use cases uh you can see that training these models is no longer that expensive you can train a gpd3 quality model for about half a million dollars um and uh for many business use cases a thirty thousand dollar model can already go pretty far uh in being deployed into into production and how is this being done so there's a lot of great work happening right now on good Tooling in order to scale these models really efficiency efficiently uh and additionally a lot of work in the open source and from us on ways to efficiently and stably train these models so to efficiently scale these large language models for training one of the challenges that these models are so large they can't fit into the memory of a single GPU pytorch has released a fully sharded data parallel which is a pretty powerful and very simple strategy that essentially splits the model across many gpus that's very flexible and and easy to use and it's native to pytorch and so integrated with everybody who's using pie charts these days uh to train these models and um fully sharded and fully sharded data parallel essentially what happens is you split the model and the optimizer across all the different gpus and essentially fetch them just in time with training this saves a ton of memory but it's also very flexible because you don't have to deal with the more exotic you know parallelism strategies uh in order to train your models and so we found coupling fully sharded data parallel with our own composer Library uh and all the optimizations that will be baked in uh will eventually we'll make a trainees in large language models uh fairly fairly efficient and uh and easy to to use the other thing that's kind of involved in the research space in the last couple of months is that uh you don't actually need to train a large language model for it to be efficient um you can see here a plot from the metas a llama model so this is training loss on the y-axis and how many tokens of data is being crunched through on the x-axis and you can see here in blue is the Llama 7 billion parameter model and in red is a llama 65 billion parameter model what's interesting is that you can see for the 7 billion in the smaller model the loss continues to go down as you train more and more and so interestingly you won this ball when the set of models was released the seven billion per model was the one that got everybody's attention uh because hey it's small you can run it on your laptop sometimes for inference it's very cheap to deploy and if you continue training it and if you use it it actually can solve many of the business problems that that your your that you need to deploy these large language models so internally you know we have this Mantra you know train longer not larger and this has really changed the accessibility of these large anguish models uh from the bra for for everyone to be able to use foreign that we've seen out there is uh hey uh training and models is just too hard um I need to deal with uh challenges or finding the right kernels how do I do my parallelism how do I stream my data in I'm monitoring these runs becomes challenging because nodes can fail um in our experience nodes can fail almost uh every other day or every few days how do you recover from those very well you have to deal with challenges such as lost spikes High my training more efficiently how do I orchestrate all these multiple nodes and get them to talk to each other in the right way all these different challenges at first blush you know get in the way between the model and the data set and your your underlying Hardware and uh there is some evidence to substantiate this when you're training models at very large scales these are the training logs from one meta train their opt model and you can see nodes fail every so often uh when you resume from training you know it takes about 50 minutes when all the cluster is Idle uh waiting for the model to be reloaded and for the data loader to fast forward or you know sometimes your provider may accidentally delete your entire cluster when trying to reprovision the nodes and so all of these combined do make the sense or the the sentiment that hey maybe training these models is just too hard uh fortunately that's not actually the case with tooling that we've built and other folks in the community um we built an llm stat that sort of just works uh where we have provide optimized configurations across many model scales to be able to realize kind of the the cost and and the model training that I I spoke about previously um that is very fast and scalable and essentially you know provide your data and just go Um so if you're out there and you have some interesting internal data that you want to train or fine-tune these models uh to deploy for yourself using you know a lot of the models that are out there in the open source Us and other folks have built a great stack a great set of stacks many in the open source that make training these models uh very easy and sort of adjust working right right out of the box and so what we observe in Enterprise is that many folks may say hey I've trained a Bert model Bert base burnt large or deployed into production that's great um now I want to scale up and so using these tools you can now break this multi-node barrier um and start training smaller you know one billion parameter models and deploying those in your production seeing if that brings you a business Roi and then continuing to stack up uh from there uh to continue to to derive more value from training these larger models are maybe more accurate and um I spoke previously on many of other unique infrastructure challenge that come with training large models and large systems to some extent many of these are now starting to be solved like like that many platforms that are out there so um the ability to detect auto-memory errors that occur when you have a very large model and dynamically adjust the usage on the fly to prevent these ability to resume instantly uh or near instantly from uh from training so that if there is a node failure it automatically catches it and it restarts the ability to kind of gracefully resume from node failures and lost spikes such that you don't actually have to be monitoring these runs uh on a 24 7 right under the hood um they'll uh fill um all right one second um be able to resume gracefully from node failures and and lost spikes so that when things happen you don't have to be monitoring them 24 7. uh we you people have built systems that are able to automatically find the bad node and restart so from your perspective you just provide the data and training for it just happens and of course many of the efficiency work that's happening in the ecosystem from flash attention uh from uh from the the Stanford folks to various efficiency algorithms that we're developing uh to continue to bring down the cost of of training these these large language models and so that really ends up we end up with this ecosystem for the large language model training stack uh where uh there's this full stack that's now developing uh to allow you to easily train large language models on your data in your secure environment everything from experiment tracking tools to tooling from ours and others like web data set to be able to stream the data in to various distributed training Frameworks the underlying deep learning libraries deployment and orchestration and also various device device drivers and and toolkits and so this is literally leading to a world where you know for what we build like the Mosaic ml platform and others it is very straightforward to start training these one billion and seven billion primary large Anglers models for your particular use cases and we really see this as positive for the ecosystem right we want to be in the position and you know what this community actually this this ml Ops committee has shown is we want to be decentralizing on these capabilities um so that we're not beholden to just very particular API providers uh to be able to train these models I know I'm kind of running uh close on time here so I'll close by saying there are many reasons to build your own models training you know seven billion private language animals and and lower on your own data now is very cost effective and straightforward um the communities continue to push out many great open source models out there for you to build off of uh and we've sort of busted uh two myths here the first myth is hey it's just too expensive um which is not actually true it's now fairly cost effective to train these models and the second myth is too hard not true at all uh there are a lot of great open source tooling and full Stacks out there including ones that we've built uh to make the very easy to train these models and so I would really encourage everyone to don't be too scared by the large part in a large language model you know if you've trained or fine-tuned burnt models in the past or computer vision models the tooling is there now to start your large Irish model model in Journey and start scaling up efficiently and deploying these models into production of course we as Mosaic ml offer some of these platforms as well and so you can work with us or many others uh to to get started so very excited to see you know over the next six to 12 months the plethora of different models dedicated for legal or medicine or for your particular business that's going to emerge because of all the great work that's happening in the community and and in the in the open source um so with that um I'll close and thanks again to the animal Ops Community for inviting me to share what we're seeing out here [Music]"
    },
    {
        "title": "LLM Deployment with NLP Models // Meryem Arik // LLMs in Production Conference Lightning Talk 2",
        "video_id": "BN-txmqGxvQ",
        "video_url": "https://www.youtube.com/watch?v=BN-txmqGxvQ",
        "length": "8:24",
        "transcript": "thank you so much for coming um I'm Miriam I'm one of the co-founds of technomel and what we're building is a specialization platform for NRP models and today what I want to talk about is how we can make NLP deployment much much easier using specialization methods and compression methods so earlier Diego gave us a really great explanation of what all alarms are um they're essentially really really big neural networks that know a lot of things and we can customize those for the things that we we actually care about and these are really driving the NLP development over the last kind of five years have all been down to these really fantastic foundation models like bur T5 GPT and the benefits of these when building NLP applications are really really clear you need a very low data requirement because they're pre-trained on almost every single piece of data on on the internet um the state-of-the-art accuracy informance all comes from these Foundation models and they're really easy to build um into version one applications like Diego told spot earlier however these Foundation models and llms come at a cost they're really really really big which makes them very difficult to deploy and you get slow inference and you need to run them on very expensive Hardware so you get very expensive Cloud costs so they're very expensive to deploy they're slow and all of this is really because of the L they're large these are really really large models and the way that these models work and the reason they have such good natural language understanding is because they've seen so much information and as a side effect they have huge capabilities so for example chat GPT is able to do tasks as broad as you know writing plays and um brainstorming strategy right they're able to do such a huge breadth of tasks however as a business or in an application you actually only need it to do something very very narrow um and but when you have it do this very very narrow thing the way that you currently deploy it probably you also pay for the the effects of it being able to do other things like writing love letters or categorizing uh you know or reviewing contracts when you only need it to to categorize um resumes so what we do with specialization and and this idea of specialization which I don't think is particularly well understood is we take large language models like your butts or your gpts and we take the parts of that model that are relevant for just your task so if this uh blue thing is my whole Foundation model um in that there's only a really really small amount of that that's actually relevant for the task um and this is how you're able to build a much much smaller model which is equally as accurate which is much much easier to deploy now these models are just far better from a ml Ops point of view um it's much cheaper deployment they're much easier um to get really really fast inference from um but also you actually get very good quality models so a lot of the state-of-the-art benchmarks in very specific tasks are held by these specialized models um not the really really big um models that we see coming out of places like open AI so here's a uh a quick illustration of what this process might look like and how this works so here we have a graph of um like your accuracy and your size trade-off and this trade-off is always going to exist now currently your options are what's available open source so you might have your original model which might be a very big but maybe a one billion parameter but and then you'll have open source checkpoints along the way so I think this one we've we've pointed out here is maybe a base or a still bar but what we're able to do with specialization is from the original model specialize and make it much much smaller and give these what we call Specialized models along the top and then you get here this Pareto front of models which are much smaller but don't have this huge accuracy decrease that we see when moving from the open source large models to the open source more resource efficient models and you're able to get a better accuracy latency or size trade-off than you would have been able to previously so I'll uh since we don't have very much time I'll whiz through and talk very quickly about the kinds of results that you can see with specialization so on the left you can see the the graph of latency and model size so when we move from larges all the way to the Titan but the Titan variants which are much much smaller so on the order of like 100x smaller um but then we can compare that with the accuracy that we see from these models and they beat um bird base and distilled but fine-tuned on most of the natural language understanding um tasks so you're able to get models which are you know between 10 and 100 x smaller um while actually sometimes improving the accuracy on on benchmarks which is really impressive and obviously because they're smaller they're much much much easier to deploy so uh this process of specialization is very difficult and the way that it's done currently is individual ml Engineers will do one-off tasks uh one-off projects to specialize their models so they might do a combination of pruning and quantization or graph compilation and neural architecture search um but the issue is this is a very expensive and long experimentation process which quite often fails um so what the Titan ml platform does is it wraps all of these techniques up and into defined pipelines where you can put in your model your fine-tuning data set and the tizen platform will automatically specialize for you and create that model that is 10 to 100 times faster and cheaper and therefore much much much easier to deploy so I'll finish off with a very quick case study of of uh what we did with a early client of ours so what they were building um was in a uh it was the original was an electro type model and they were doing document classifications um and the problem that they were struggling with was they weren't able to get the latency that they required um on sensible Hardware the only way that they get the latency was on using two a100s which for inference is pretty silly um they'd already tried standard things like Onyx runtime and quantization um but what we did is we we took that same original model and put it through our specialization Pipeline and compression Pipeline and ended up with a model which had still very very good accuracy but could be deployed on a single T4 with the same latency and that's a bit of a game changer when it comes to the challenges and getting these models to production and getting them uh running at sensible latency sensible costs so the the tldr is large language models are very very very big but for most use cases they really don't need to be that big when you're dealing with one specific use case um and turning those large language models into much smaller specialized models um means you have much a much easier time getting to deployment and getting the latency and memory constraints that you might have as a business uh thank you so much for the uh mlops guys for organizing this you can reach me my emails down there or LinkedIn me um and let me know if you have any questions in the chat I'll be happy to stick around and answer any later thank you so much Miriam this was awesome really appreciate it thank you so much"
    },
    {
        "title": "LangChain Apps to Production with LangChain-serve // Deepankar Mahapatro // LLMs Con Lightning talk",
        "video_id": "92xXyVNUGWI",
        "video_url": "https://www.youtube.com/watch?v=92xXyVNUGWI",
        "length": "9:47",
        "transcript": "hi everyone this is the tanker I'm here to talk about clientele Stuff how to take your language in apps to production unfortunately I couldn't be online during the call and had to send across the recording if you have any questions feel free to ask them in the mL of slack works please I'll grind lots and then as soon as I can I'll try to rush through the slides as much as possible since I only have 10 minutes a brief introduction about myself my name is the bunker I work as an engineering manager at and I'm based in Bangalore in India at Gina among other things we develop Frameworks for multi-modern AI applications in open source I have shared my socials here feel free to reach out in case you want to collaborate all right so what is a LinkedIn serve the goal behind Langston serve is to take users local vaccine based apps to the cloud without sacrificing the ease of development that Langston already provides so language basically uses Gina to achieve this so to the members in the audience who are not aware of what Gina is it is an mlops framework to build deploy and manage multimodal applications is available on Pi Pi you can install it using pip install links itself all right so and this quick talk let's try to understand how to use LinkedIn serve or using a couple of simple examples uh the first example here is deploying a custom agent uh in just four simple steps let's take an old example from the LinkedIn talks in this example uses the Search tool using the search EPA wrapper then uses this tool to Define an llm chain which is basically used to create the zero shot agent and finally we run agent exit 2. run on a particular question this can be run on your local to get the Chain of Thought and output all right so let's see what changes are required in case you want to deploy this using the language itself and so the first step here is to define or refactor your code into and Define a function so in this case we Define a function called ask then add type hints to this function so here we've added a the input of type string in the return type of string then we add the return value so we just basically do agent executor.run on that input and finally return that at last we basically import the serving decorator from SSR this is decorated that the link chains are python module provides so you add that import that and add that to the ask function that we just defined so this is just to show that we can also run the ask function as is like before on your look all right let's go to the step tube so the step two here is to add requirements.txt that includes all your requirements for that function so in this case open Ai and Google search results are enough so whatever your dependencies are you just add them to our requirements.txt in the same directory and save it okay now that the code refactoring and requirements dot txt are done let's run this uh let's deploy this app locally so the command to do that would be LC serve deploy local and your application name so in this case app.py was a fine name so we just basically passed the module name called app so uh that's for that's what running this will expose uh rest API on your local on the port 8080 uh now you can talk to it using the curl command so here notice that we're basically sending a call request to localhost port 8080 and the function name that we had added called ask has been added as an endpoint on that DPA so you can send a call request here if you if you observe the data the input schema in this case the input of the field input was the one of the arguments under the ask function and this schema also accepts all the environment variables all these environment variables are basically needed so that the function can run so in this case these are the tokens provided by open Ai and subpr all right so once we know that okay the function is all these the function is exposed as a rest API on your local and actually we can interact with it now let's go to the next step and deploy it on the AI cloud so the command here the only change here is uh from local we've shifted to jcloud which is basically ginai Cloud so you basically run LC server deploy jcloud app and that would give us an end point uh this endpoint is a serverless trustee pay endpoint uh with TLS Source on Genie iCloud using this endpoint you can basically send this code to the same call request to validate that all your requests are passing successfully you also get uh swaggered docs using this and also the open EPS cap specs which can be used and as another agents if you want go to a slightly more complex example so in this example we'll try to enable human in the loop on our server which is deployed on Gene III Cloud this is enabled using websocket streaming so let's let's go through this so first thing like before we decorate our functions so we decorate we Define a function called hitl this exception input called question and the output type hint is a gain of type strand we add the return values which is basically again the agent chain not run change here the difference here is basically to define a streaming Handler that streaming Handler is responsible to send the websocket responses back to the user so you get the streaming Handler and I'll add them as a callback manager to whatever llm function you've defined this passing this would be enough to sign all the llm output via this callback manager back to the user via this we also enable human in the loop so whenever there is uh an input required we basically intercept the LC server will basically intercept them and send a response to the user and wait for a wait for users input to come back so that it can proceed with the next steps finally we basically add import and add the serving decorator remember to pass a websocket is true in this case all right so let's keep the local deployment and go to the United Cloud directly so we basically do the same again as we serve deploy jcloud and we pass hitl which are which was a file name and uh doing this will give you a websocket endpoint and you can you notice that this is what WSS which is basically a websocket endpoint in this case other than demoing using curl so we I have written a very simple python client uh so first it connects to the endpoint and sends a Json which has the question and the envs like before uh then it waits for stream responses uh back from the server uh when and prints them to the user's console it intercepts uh the there's a particular format that it intercepts which is expected whenever user input is designed that that format is defined here and whenever that is there we basically ask an input ask for an input to the user and whatever input record will send it back to the server and this is how the human in the loop is interpreted in let's say so yeah so that was a very simple example of of enabling streaming and human in the loop but again this can be expanded to any any complicated cases all right so what's what's coming next so we want to host uh we want to enable hosting streamlined apps on the cloud for these uh blanket Maps so that user will also get an UI for an application then the complete Journey would be available for any user we also want to add authorization for the API endpoints which will uh which will validate the request from a valid users rather than allowing anyone access and we want to add more examples to our talks all right so that's that's the our time I had thank you so much for tuning in uh if you find the if you found this uh useful please let us know if you have any feedback any feature requests uh join us on slack or create any GitHub issues yeah thank you [Music]"
    },
    {
        "title": "Unleashing Speech Intelligence through DSLMs // Andrew Seagraves // LLMs in Prod Con Lightning Talk",
        "video_id": "ZglrqT0dPUU",
        "video_url": "https://www.youtube.com/watch?v=ZglrqT0dPUU",
        "length": "9:34",
        "transcript": "okay great well let's get started we have about uh only 10 minutes here so um I'm gonna try to hit the high points uh so this talk today is entitled Slim Fast unleashing speech intelligence through domain-specific language models my name is Andrew uh VP of research at Deep gram I would be remiss if I don't first tell you a little bit about deep gram where uh Speech to Text startup or mid-stage founded in 2015 where a series B company 85 million uh total dollars raised so far so far um so we have uh what what we believe to be the best you know most accurate and fastest speech to text API on the market um so far in deep Graham's existence we've processed over one trillion uh minutes of uh of audio we have a lot of customers who we have delighted and uh so I'm going to tell you about some things that are that we're working on and we're interested in today so some of the things that that motivate us uh here's some of our fundamental beliefs the first one is that uh language is the universal interface to AI so we believe that language is the primary way that uh we interact and carries the most amount of information and will be the universal interface that will unlock the full potential of AI and that is starting to happen now um so businesses are going to be able to realize the power of language AI but we think that businesses need adapted AI to make it useful so if I'm going to start off here by first making some predictions related to language AI over the next two years many businesses will start to derive tremendous value from language AI products and in the short term the most impactful products that people are going to build are going to combine existing Technologies in a multi-model pipeline so what does that mean well this pipeline is going to have three stages in it there's going to be a perception layer which is going to be fundamentally ASR something that takes audio and turns it into text so speech recognition system probably complemented by A diarization system that predicts from that audio who is speaking when and allows you to format that transcript in a nice way to separate out the speaker turns there's going to be an understanding layer that will take the output of that transcript and apply probably a large language model or perhaps a medium-sized language model or a small language model some kind of language model that will understand what's said in that transcript and will produce some kind of distilled useful output like a summarization of the transcript or a detection of the topics or the sentiment finally you'll have an interaction layer which will take say the output of an llm which is generating say a response to the uh to the audio input and then we'll make that we'll turn that into audio using say text to speech right so we have this language AI model pipeline so we think that businesses are going to derive maximum benefit from language AI products that are cost effective reliable and accurate those three things together you need all three um and this the purpose of this talk is to argue that this is going to require the use of uh small fast domain specific language models as opposed to large foundational language models right so that's my thesis we're going to argue this point in the context of a specific application call centers so what is a call center well it's a centralized officer facility used by companies to handle very large volumes of income incoming and outgoing telephone calls call centers are staffed with agents who are trained specifically to handle the customer inquiries that will be coming in the complaints that might be coming in and they're trained to provide technical support or sales or perform sales related tasks and their training will be specific to the business that this call center is supporting so there's a number of uh AI products that are going to be built for the call center some of them are already being built and you know in their initial stages um these will be products that will help both the customer experience on the left to the employee experience like the agent on the right a couple of important ones would be voice Bots or real-time agent assist systems other importance would be important ones would be audio intelligence features so basically taking the output of many phone calls and summarizing them or predicting the topics for them or predicting the sentiment of the customer in those calls so you might say why not use a prompted foundational language model large language model to build language AI products for a call center the first reason I would argue against this is the scale of large language models they're ridiculous 100 billion plus parameters if you measure it in terms of how much resources it would require higher to launch this model with uh with a 5000 gpus it would require dozens of them they're very small or they're very slow uh inferencing on the order of you know 100 milliseconds per token so you know along generating a long response might take many seconds so just based on the scale and the inefficiency alone you could argue against it um looking a little bit more in detail at large language models and what they can do they have broad general knowledge um and they can be aligned to do many tasks without explicit training they do many many things however conversational text generated by a call center has a high degree of specificity it covers narrowly distributed topics the People speaking have unique speech patterns associated with where they are in the world and there's going to be a long tale of rare words that the foundational language model probably has never seen before to get an idea of the task specificity that's involved we could look at say the top 15 tasks that a call center agent would perform they're very highly specific and yet complex tasks that require language understanding another point we can make is that domain-specific conversational text is generally out of distribution for foundational llms we can get an idea of this by asking chat GPT to continue a call center conversation so if we give it a a brief prompt right with an intro by an agent and then we have a customer who's starting to speak and we look at the transcript that gets generated we see that uh it's very unrealistic the speech is way too clean as if it were written and that output tends to follow a predictable script of a greeting customer describes an issue agent takes an action customer accepts the action and the call ends very unrealistic if we look at real examples we see that they feature things like crosstalk people trying to talk over each other makes transcripts very difficult to read and interpret it uh real transcripts have things like disfluencies where people are basically stuttering and stumbling while they're speaking using filler words that also makes transcripts hard to read so we argue that we should shoot for domain adapted language models for call centers and that these models can be small um to prove this point we took a small language model 500 million parameters and that was trained on general internet data and then we transfer learned it on uh an in-domain data set of call center transcripts we found that the model improved dramatically in all metrics uh next token prediction loss and perplexity and when we use it we find that we can continue a call center conversation a prompted call center conversation and it generates very realistic text and then we have gone further and used that to train a summarization model which I'll demo to you now in this Jupiter notebook so we're going to make three calls here we're going to first make a call that uh so we're going to call a function that hits the Deep gram API um transcribes an audio a phone call Audio diarizes the transcript and then sends the transcript to it to our slm to perform summarization you see that that call took 8.4 seconds we're going to print the transcript that we got back from ASR and diarized we see that it's highly accurate and we're going to print the summary that we got back and we see a very nice human readable summary that uh is actually quite accurate describing what happened in this call and that's what I have for you folks today"
    },
    {
        "title": "No Rose Without a Thorn - Obstacles to Successful LLM Deployments // Tanmay Chopra // LLMs in Prod",
        "video_id": "lnbx4dOW220",
        "video_url": "https://www.youtube.com/watch?v=lnbx4dOW220",
        "length": "10:24",
        "transcript": "[Music] foreign [Music] so I'm going to hand it over to you and let you get cracking man awesome thank you so much uh hi everyone I'm tanmay um I work in machine learning at niba which is a search AI startup and previously did the same work at Tick Tock uh so I'm here to be a bit of a bus kill about llms and and talk to you about the obstacles to deploying llms successfully to production um so if you've spent any time in on Twitter at all you've probably seen uh hundreds if not thousands of demos of llms um if you work in Industry you've probably seen very very few of these ever get deployed to production um if you work in statistics probably hit me right now for not giving you a scale for this diagram um but why do such great MVPs and demos never make it to production um well there's two big chunks of challenges that we see with deploying elements of prod and they come in the form of sort of infrastructural funds and and output linked fonts where infrastructure thoughts kind of refers to technical or integration linked challenges where output link fonts are essentially the output of the model the text that it's generating uh that might cause problems for and block us from going to to prod so when you come to the infrastructural side there's sort of four big buckets that we look at problems in the first is alarms are slower than some of the status quo experiences we're used to a really good example of this is search right where you have very very quick results that users are used to but now when you start generating llm output um it takes significantly longer for that response to complete as ex as compared to their status quo experience there's also a lot of decisioning that goes into taking llms prod one of the biggest decisions is do you buy or do you build whereby kind of refers to purchasing API access to a foundational model and build usually refers to fine-tuning some sort of Open Source llm and so when it you see the buying case tends to pose much smaller upfront costs but as you scale uh starts creating a lot of challenges in terms of costs on the building side there's much higher upfront cost and a lot more uncertainty on whether your llms will be um at the Quality level that you needed to be able to demo uh or actually go to production there's also some emerging challenges around API reliability in case you do choose to buy um and this usually tends to emerge from cases where um infrastructure serving infrastructure is still being built up by foundational model providers and users can very quickly lose trust um in cases deployed to prod um when these when they experience uh even infrequent downtime one of the bigger challenges is also evaluation we're still leaning somewhat relatively heavily on the manual side um and we're looking for more and more clear quality metrics for this output on the output link side um the the major challenge that we're seeing especially when you start integrating these models into pipelines is output format variability given that these are sort of generative models there is a certain degree one predictability to the response uh which can make it quite challenging to sort of plug into some sort of pipeline that expects certain formats uh this is the easiest one to solve but there is a lack of reproducibility um where the same input might give you different outputs even for the same model um and then finally we sort of come to this world of uh adversarial attacks or adversarial users where you see challenges related to prompt hijacking and and as an extension trust and safety uh where the model might generate or be forced to generate um intentionally malicious output that might be considered undesirable but does this mean we're doomed are we always going to see sort of this case a very small case of production I don't think so um what I've just covered is sort of pretty much the whole landscape of challenges um some of these are mutually exclusive if you buy you'll face some if you build your face some but it's highly unlikely that you would face all of these challenges um so let's talk Solutions when it comes to the infrastructural side um in terms of speed you can make models faster or you can make models seem faster um when you make models faster that's sort of the conventional machine learning techniques of distillation pruning um trying to use smaller models where bigger ones are not necessary this obviously sort of leans towards being able to build um but if you are buying uh there are ways to make models seem Faster by leaning into human computer interaction techniques so you can load animations you can start streaming output um you can start parallely using um outputs to the core tasks that are more complementary versus blocking um and in terms of costing decisioning this is a tough space but there is a somewhat optimal approach which sort of entails buying while you build so using that low upfront investment getting to Market fast validating your MVPs and then over time sort of collecting data and fine-tuning models uh in-house to make sure that your costs aren't getting infeasible in the long term with adoption when it comes to sort of this foundational model reliability space um the best parallel or the best analogy is sort of this multi-cloud approach where you think about these fallbacks across different providers we have yet to see a time where two of the major Foundation providers have failed together so this does seem to be as satisfactory approach to dealing with this um there is also this component of failing gracefully um users do understand that this is a technology in-depth development and so if we are reliant on sort of API reliability it does make a lot of sense to think about what happens in The Last Resort case where you do fail um to deliver output and furthermore when it comes to this evaluation infrastructure um the way I like to think about it is this is a great time to fail fast with fail-safes so make sure that you're not causing sort of trust and safety related failures but when it comes to the core product itself um it's totally fine to start going to production faster um and using very strong user feedback and feedback loops to make sure that you're iterating as you go um another really helpful approach is to link your llm Integrations to some sort of Top Line metric so that could be anything from say stay duration to session length um and and evaluating how this impacts that change on the output link side um output format variability is probably the largest chunk of challenge um I would say in terms of reducing this to a huge amount few short prompting really helps so you actually go ahead and give output examples in the prompt itself um there's also a couple of really cool libraries I think we we had one of those speakers talk about these earlier which is sort of guardrails in realm that can kind of help you validate output formats and actually iterate if you need to um call the llm again um again failing gracefully is always helpful just one easy fallback um in terms of lack of reproducibility this is absolutely the easiest one to solve uh you can just set the temperature to zero um I sort of think about prompt hijacking and trust and safety in one bucket largely because uh the main negative outcome of being prompt hijacked uh does sort of lean towards generating outputs that are trust in the safety layer could solve um so I just want to end with some thoughts or tips around how you can start strong how do you get your first llm to prod and how do you make sure that use case succeeds um the first and probably most vital aspect to think about is Project positioning it helps massively to be able to focus on deploying to non-critical workflows um where you're able to add value but not become a dependency as we're sort of building up more reliable serving infrastructure we can start serving more critical workflows but things like output variability API downtimes sort of push us in this Direction Where We should be trying ideally to add value but not become a dependency uh it also helps to sort of have relatively higher latency use cases um these are scenarios where things where users have lower expectations of how quickly outputs will be generated and so gives you more space to sort of create value um in a low barrier to create value the third one is probably the most key one to make sure your llms don't just go to prod but stay in prod and this is to plan to build while you buy so make sure that as you're working towards deploying your LM with an API solution um you're also figuring out how in the long term you're able to scale those costs uh in a manner that's feasible and lastly do not underestimate the HCI component um in this case llm success is largely determined also by how it interacts with the user and it really really helps to sort of respond seemingly faster or fail gracefully or enable large-scale user feedback and that's pretty much it from my end over to you nice dude awesome thank you so much for this for those who want to keep up with the conversation and you want to ask 10 many questions because that was awesome really incredible there's some really cool questions coming through in the chat go ahead jump in slack he's there go to conference [Music] I know I've got some show everybody what is this [Music]"
    },
    {
        "title": "Emerging Patterns for LLMs in Production // Willem Pienaar // LLMs in Prod Conference Lightning Talk",
        "video_id": "-bmynyCJVBg",
        "video_url": "https://www.youtube.com/watch?v=-bmynyCJVBg",
        "length": "9:41",
        "transcript": "[Music] he just threw me on screen oh I just threw you up there I a little bit of a surprise man so what do you got for us today I know this is your first of two appearances later on we're going to be doing a prompt injection competition with you and you told me to get ready to get my prompt injections ready so I've been uh you know studying a little Reddit but in the meantime you got a presentation man I'm gonna kick it off to you because you only got 10 minutes dude ready that's good let's do it baby all right I'm gonna be talking a bit about some of the emerging patterns we've seen with owl limbs in production uh but first of it about me um so my name is Willem uh my background is in production ml built a bunch of production ml systems um a company called kojic uh Rudy Wilson double down and focused on ML tools and Frameworks and platforms um so one of which was a feast open source projects we built and open sourced and adopted by a bunch of companies like sharpfi and Twitter and Robinhood and some others but I've already been working with teams building ml tools and platforms and um you know helping them do it in a reliable way and so that's why the generative AI space is interesting to me because of the challenges we're seeing today so what do we what are some of the unique challenges that we see today with uh generative Ai and um you know I I guess I can in general um uh the key the key ones we're seeing are around reliability cost latency and safety and the numbers on the screen there are from um an element production survey that and their basic teams that have responded and said this is an egregious or like a critical problem for us and so reliability is obviously one of them because you're dealing with like unstructured or textual output coercing that into something that your application can use is hard cost is a big one um if you're an AI Builder today you're either asking users to provide an API token or you're absorbing a lot of the cost yourself and so this is a challenge that a lot of you know product Builders are faced with today we've also deployed a bunch of probes globally or I personally did that and I've been monitoring these API endpoints myself the the providers and they're really really slow so they're like 400 500 600 milliseconds um sometimes they they even Spike to like 18 20 milliseconds for completion so how do you build a product around that and especially if you have to do many round trips it's very challenging um and finally safety is hard um you know folks are inputting private data or they're you know your vulnerable to private sorry prompt injection attacks and all kinds of vulnerabilities that are kind of new and unique so it's challenging building on RM today and you know that's kind of like opportunity for us to embed things down a little bit so how do you use elements effectively the same rules apply to structured ml really in the space start simple sort of the basic prompting start with including some examples to do some view shot prompting start introducing external data or exogenous data sources using Lang chain llama index and composing workflows so you incrementally increase your accuracy over time yes you'll increase your costs and latencies a little bit but often that's okay if you're um you know accuracy and reliability improves um but if but soon you'll get into a point where you want to get to it refinement where you do things like prompting where you're doing tool selection calling out the apis like Wolfram Alpha or others to give you more reliable responses and I think most folks end at this stage but you know if you want to take things further you can also start using um you know fine-tuning hosted models or using open source models and training them from scratch but that's something that we wouldn't discuss today but um you know in terms of the techniques that we're seeing out there in the wild I think one of the key ones that you know SRI and some of the others have been introducing and um kind of spearheading is adding structure to your responses so you can ask a model to provide a respondent typescript scheme as a format and it'll do so and you can encourage it to be more reliable by giving examples asking it to take on a Persona um and just bursting into reminding it how you always return Json you can even unfortunately threaten the model sometimes and it will um often be more accurate for that uh response and if it fails you just re-ask and you can keep re-asking until I guess it affects your ux you can increase the temperature or you can start with a more cost effective model and then ramp up to from like a GPT 3.5 to a F4 and this allows you to at least validate the outputs in a structured way instead of dealing with clean text as the output um another technique that we're seeing applied more even in the production setting is self-refinement and so the idea is normally you you make some kind of uh prediction you give a prompt and you get a completion but you can also in the prompt say hey review what you've just given me and score yourself and refine that prompt and you can even ask the model to do this multiple times so it's literally scoring itself and improving the prompt and this has been shown to be surprisingly effective especially for models like gpt4 so you can say oh you've written a tweet for me make the Tweet more engaging rate the tweet and and you know improve it and so that's been surprisingly effective technique and it's I performed bass lines in a lot of use cases um and so another technique that we're seeing out there in the wild is contextual compression and so what you see in what you do in a lot of cases is you're calling external data sources let me see you're using Lang chain or Lama index and you're enriching the data in your context window with fresh data that's relevant to answering a question or doing some kind of task but often it's very unstructured the way people do that today one of the ways you can improve the density of information is by doing compression so you can do you can say so let's say a question is you score the first goal in the FIFA World Cup you can call it a bunch of external sources and then compress based on the questions you're asking that information even drop some of those examples and then only shove in the you know relevant ones into the context window and so this gives you often 2x or 3x the amount of information that you can put into the context window and that really improves the quality of the output completion so now you've got more and more accuracy and a little bit more structure but what about latency so one of the techniques that we're seeing now also being adopted is semantic caching so the ideas with normal caches if a prompt if you're just caching prompts in the completions you often don't have a very high cache hit rate because one character difference and suddenly the cache is going to get missed but you can use a vector DB to also do cache hits so you cache it based on a distance like a similarity distance the problem is that often or in a lot of cases you don't really have a hit right if it's if a prompts are sometimes slightly off maybe the hit isn't a true hit so you need some kind of evaluation function so what folks are doing is they're using an llm again using LM for everything but using LM to evaluate the output response and have a judge whether it was really a cache hit and that that works really well in the case where the completion that you're returning is an expensive completion so in many cases you you're calling an LM you know three four five six times especially if you're using chain of field reasoning or any kind of decomposition just it's expensive to compute those completions and so um even if you have to call and now aim to validate cache hit it's still cheaper than having to recompute everything this technique also works really well for Tool selection we this set of options so if you're using like to like an API that you're going to use or like wool from alpha or something um then if you have a high confidence in a single tool and no confidence in any other tools you don't even need to call now and you can just use a distance metric to evaluate the cachet so this is a very good technique to reduce latencies over time above and beyond just the normal cache and then finally I think one of the interesting things that I've realized from the paper in um you know Sparks of a GI is that LMS like gpd4 are extremely good at pi and prompt injection detection 3.5 is also is also good but these LMS are effective at identifying um leakage of information outperforming even the Baseline purposeful tools at this task and also prompt injections and in fact a little bit later today we're going to be playing one of those games where we can you can actually try this out um but so yeah if you're building on albums today and you're trying to make your systems more reliable faster and improve the ux and preach out dude thank you very much Mr Willem this is not the last that we're going to see of you today I know so we're going to play that prompt injection competition we got some headphones to give away a little later and we're gonna be doing it man thank you so much for this talk"
    },
    {
        "title": "Ensuring Accuracy and Quality in LLM-driven Products // Adam Nolte // LLMs in Prod Conference",
        "video_id": "QNg49s10kjQ",
        "video_url": "https://www.youtube.com/watch?v=QNg49s10kjQ",
        "length": "8:48",
        "transcript": "hey everybody I'm super excited to talk to you all today about ensuring accuracy and quality and llm driven products I'm going to walk through a couple examples of how you might use llms in your products a more simple example as well as a more advanced one and hopefully leave you with some questions and thoughts on how you might be able that you can bring into your your own products a quick introduction on myself so I'm the CCO and co-founder at Autobox we're building debugging and monitoring tools for companies that are building llm-driven products before autoblocks I was a software engineering consultant at Westwind row and also spent time on the engineering teams at project 44 in front both platforms dealing with large amounts of unstructured data so we'll kick things off by just defining what is an llm driven product so of course I went to chat gbt gpt4 manasse and said an llm-driven product refers to a product or service that is powered or significantly enhanced by a large language model like gpt4 of course from openai large language models are artificial Advanced artificial intelligence systems that excel at understanding and generating human-like text based on the context and input provided which makes sense and I'm sure you're all really familiar with and it also gave a handful of examples like virtual assistants content generation translation sentiment analysis text summarization coding customer support Educational Tools I'm sure you're all familiar with a lot of a lot of text space I mean there's more and more use cases emerging as well and behind the scenes you'll actually look at an example of of one of those and you've probably seen some across all of the talks today a lot of folks are really thinking about where do we Where Do We Go From Here past just the regular text box so first what we'll look at is a product support chat bot so a simple question and answer you probably have this hosted people can come in ask questions they'll look over your knowledge base and help your users out and also save save time for your support folks so someone comes in they say how do I set up a workflow chat buses I'm sorry workflows not a feature that we support they asked how did I do so very simple you've probably seen this a thousand times thumbs up thumbs down you know give me feedback this is really the basic way of incorporating feedback you know whenever users do thumbs down you can start to look at similarities on those inputs when they are having negative interactions and you know start to dig in and understand how you might be able to improve thumbs up you know you can start to use our like chef and reinforce um and go from there but you know a lot of times especially if you're like myself sometimes you may not even use a thumbs up thumbs down so it can be pretty basic and not always super reliable so we'll come back to this example towards the end and think a little bit about how we might be able to improve um the quality here and incorporating feedback next we'll jump into an example where you might have a large language model working behind the scenes and it's not going to be so obvious to the end user so in this example we're a ticketing software Think Like A jira or a linear that you're all familiar with in this example we probably have and say we've added a bunch of AI capabilities so our AI can help us auto-complete the title and description as we type it out it can also Auto assign so here you can see if this is assigned to Wade so it can use what you've typed in the description and title and then look at all the historical context and pick the right assignee for you so you don't even have to think about it it could also add a label for you in this example here you can see that it's labeled as engineering so it can take a look use all this context historical context Auto label for you Auto label for you and really just make you more efficient because it's always a pain trying to figure out what you need to label all your tickets as and keeping everything organized then last which is really cool it can also assign a due date for you so it knows how long it usually takes way to work on these kind of features and it can suggest a due date of next week for you and so what we want to do you know we've added all these features in and we want to really understand like how well are they performing how can we improve them how can we get better well if we take the chat example we would just paste a bunch of thumbs up thumbs down everywhere which obviously is ridiculous and would lead to a pretty poor user experience we wouldn't want to do this and I mean it wouldn't you know frankly it would be really difficult to understand like okay the thumbs up thumbs down different parts like how that work so obviously we need to think a little bit about how we might how can we understand and improve in these examples another way would be maybe just one thumbs up thumbs down with a user can input feedback directly but then you have to manually go through and read and look at all this feedback yourself and it's not going to be super insightful so we really need to start thinking about is kind of back to more product analytics type questions so we need to ask ourselves you know what what questions um so what do we really want to know we want to know did they update the title or the description after we did an autocomplete if so what did they update to is it close to what we provided is it very different um you know those are all really important questions that we need to understand for the assignee we need to understand did they update the assignee did they update it to who did the person they updated it to actually end up working on it or maybe a week later it got switched back you know if we started retraining our large language model and all the context based on who they updated in the moment we may actually you know update it incorrectly if it's always going back to wait maybe later on and there's some other user training we might need to do same thing with engineering with the label so do they update the label if so is a new label similar maybe there's too many similar labels and we can actually assist the user in using our product better and consolidating their labels you know if we start asking these kind of questions you know even when they do update it we can start to provide a really a better user experience and also improve the quality of our models the same with the due date you know how much do they update it by how accurate was the due date uh maybe they updated it and then our original due date actually was correct um and so we don't want to take into account that they changed it uh we want to actually just know when it moved to the done state was our original due date correct so these are all like questions we should be asking and start thinking about as we're building these LM driven products if we Circle back to that chat example that I showed early on what we actually really want to know is that they actually solved their issue um do we actually not support workflows maybe that was a hallucination what did the user do next you know think with me for example if I came in here I said how do I set up a workflow and the chatbot spit out a list of steps and if you're actually able to measure in our product did the user complete those steps like we intended and was that a successful experience for the user that's going to give us way more information than just a thumbs up thumbs down which brings me to to truly ensure accuracy and quality in your llm-driven products you need to understand human behavior beyond the thumbs up thumbs down so how do you do this well like I said before you need to sit and think deeply about the failure modes that can occur in your product what's most likely to cause a negative user experience what are the positive outcomes and once you have all this information you can add to a link to like measure those human outcomes and how users are using it then use that to optimize your ammo AI models so you end up in this feedback loop of presenting the AI features to the user collecting analytics and continuing to optimize and you want to make this feedback loop as tight as possible um so Autobox we're here to help so you can sign up for a private beta at our website here um we're thinking deeply about these problems and how we can help you understand the entire user journey through your app and then incorporate that back into your AI Futures and optimization and things like that so we'd love to chat with you if you're building llm driven products or even just thinking about it like I mentioned feel free to sign up for our private beta at our website or shoot me an email directly I'd love to chat so really appreciate all your time today and I look forward to chatting awesome thank you so much Adam I think that was like the best timed presentation I'm glad to hear I practiced it a few times with a timer uh so I was pretty I'm glad it ended up working out also on the right time that was great all right well thank you so much for joining us today we really really appreciate it yeah I think silly [Music]"
    },
    {
        "title": "Reasoning Machines // Justin Uberti & Jon Turow // LLMs in Production Conference Lightning Talk 1",
        "video_id": "xchOwCMDLgE",
        "video_url": "https://www.youtube.com/watch?v=xchOwCMDLgE",
        "length": "14:11",
        "transcript": "good morning well hi folks here's going to uh Justin Bieber really really excited to talk about reasoning machines and how you can build citing and useful and differentiated apps with llms by thinking through the the stack how you can do that from an ml Ops perspective so next page please Justin and one more please as I mentioned I'm John turo I'm a partner with Madrona Venture group or early stage investors and Ai and data I previously spent a long time at Amazon AWS and I'm Justin Liberty a co-founder and CTO of fixie a platform for building interactive applications with large nineless models before starting fixi I worked at Google for a long time and have been around the internet industry at Fairmount John next page please so you know so much excitement about building apps with llms and it's Justified your apps can be super powerful they can do things that if you think about it right maybe you've uh maybe no one's ever seen before but you know when you talk to Founders and and anyone who wants to build apps on top of the group on top of the stack that this community operates there's going to be a real question how can I build a differentiated app that's going to be different from what everyone else can build on top of the same models we're going to talk about that today next page please Justin to do that let's just talk about exactly what llms are we'll take a step back to ancient history some six months ago with to a paper from Stanford about emerging capabilities you know these these large language models made it possible to take so much data and so much compute that all of a sudden AIS can reason they can do complex reasoning with multiple steps they can reason outside the domain they were trained on they could they can do new tasks like generalization and summarization okay that's what these models are at their core and if you go to the next page that lets us draw a link that for all the facts in these models even though they they know things like who is the president of the United States the best way to use llms is as reasoning machines not fact machines but wait a second if we want to build great apps we need both facts and reasons and reasoning we need to put those two things together otherwise it's you know this guy John lamb uh who's thought a lot about these topics has said on Twitter that asking an llm to do something without the data that it really needs of course it's going to hallucinate that's just like leaving a person in an empty room with no tools he's going to hallucinate to so what we're going to talk about here is how you can use the stack to actually inject the Right facts into your model to make more useful and differentiated apps okay next page please Justin we're going to talk really briefly about three different ways to get facts into your model and then we're going to dive into one specifically the first and the most famous is to train your model itself and and this is you know if we're starting from a model from scratch this is what everybody would have had to do but in the world of models that are so sophisticated and so uh and increasing their capabilities at such a rapid Pace the reasons to do this now are because you want to push the technical envelope or you want to restrict you want to respect structures or concepts in your model that are so radically different from what other models are capable of um and so you know this is for very sophisticated very sophisticated use cases and teams the next thing that we talk about all the time is fine-tuning where you can actually take an existing model off the shelf and do editing of the model of the model ways and that allows these the existing model to become more familiar with your data and depending on how much data you have you can be more surgical or more profound to the impact you have 50 to 50 000 rows two thousand to two million tokens which brings me brings me to the third technique that we'll spend the most time talking about today in prompt engineering and with prompt engineering you really don't actually interact with the guts of the model at all instead you use the stack to inject information on demand into the prompt of the model along with the whatever is the ad hoc input from the user you retrieve data on demand from whatever data store or API you'd like to use and when you do prompt engineering for data that lets you have Dynamic control over model output it lets you reason about specific facts that you can rely on not being hallucinated or sort of confidently wrong and you can do this on top of any llm that you might like to use so we're going to spend the rest of this lightning talk talking about prompt engineering and show some examples of how you can use the stack to inject data into your models in real time on demand on the Fly and build more useful and differentiated apps take it away Justin all right thanks John for that lead up so we're going to talk about as John said you know how can you use prompt engineering to use an existing model but then differentiate your application by allowing it to connect to existing data sources and apis and you know I'm not the idea of you know differentiating your application especially when using an LM because then you can differentiate while you differentiate right so let's Dive In so I'm gonna do some demos here and what we got was going to talk about um you know how exactly prompt engineering works and how do we actually do this process of connecting llms uh to to existing data sources so as John mentioned you know LMS have a lot of facts that they've been trained on and you know those facts you know some of them are readily accessible to the LM because they don't change they're kind of static uh they're well known it's unlikely to be it'll lead to a hallucination but you know Justin I can only see the uh the right hand of your window oh okay the text at the left let's see if I can resize the window here great okay so back to prompt engineering yes so with primary engineering we're going to define a format for queries and responses and then ask the element the reason about that provided query and so some queries are pretty simple and work pretty well like if we ask who the president is Elon has seen this in its training data and it's able to answer this question accurately and authoritatively but if we take some other queries that aren't encoded in the llm's knowledge like for example a dynamic query like what time is it the LM is going to struggle here and give us like basically a random a random answer uh it looks plausible you know like it's the right format for the answer but it's the wrong fact in some of these things you know this is where we can really get into trouble is that it's not always totally obvious when the LM is making something up if we ask a more in-depth question like who the speakers of this conference are from VC firms well we get a lot of like really reasonable answers I think these are all my actual uh VC firms and probably actually people but poor John gets snubbed here and not even being mentioned in the results and it's just like how would the Alum know as John sorter mentioned it's just a person locked in a room it has no ability to understand anything that's outside it's training set so to address this we can use prompt engineering to supply factual information to the llm and then the LM can then use the information we Supply to generalize using its Rich World model that is built up so here you say you're in Seattle and we say here's what time it currently is and now I asked it what time is it well it's no real surprise that if we just hold of something you can repeat that fact back to us but the part that's really interesting then is we say okay you know we've asked what time is it in Seattle but now if we ask what time is it in New York City wow okay is generalized to understand that well New York City is somewhere else in Seattle it's in a different time zone it's an Eastern Daylight time that's three hours away from where Seattle is so therefore if it's 9 10 in Seattle it must be 12 10 in New York so that's a pretty powerful concept where we haven't really asked the LM to give us a fact we've given it the facts but we're asking the LM for its knowledge of reasoning based on the world model so mainly supplying the information like this it doesn't scale but we can give the element tools to retrieve necessary data from data stores and so I'm going to do another demo where we point the uh unlimited web page and ask it to read the web page and think about it before responding and so the question that I had before I asked the LM who's speaking at the conference from a VC firm well now the llm can go and read the document which is basically the web page for the from the conference site look through that convert it to a form that Ellen can understand figure out who are speakers uh you know at the conference and then use this world model of like what is a VC firm and you know which of these names are BC firms to know that oh John from a drone is speaking and we've also got Diego from Factory and speaking at the conference as well so you know this is a pretty powerful example of how we can actually use external data to make the and the recently power bill around to come up with pretty interesting answers to complicated questions so we'll do one more demo here that uh we can take this even a step further of coming up with a um a general set of tools that an llm can use where anytime it knows that it's being asked for information that it doesn't have in its existing World model it can go and consult those tools to get the information that's necessary to answer the query authoritatively factually without hallucination and so in this particular example we have here we're asking the this tool this is fixie here uh to go and find out who are the contributors to the Transformers repo on GitHub find out their contribution counts and then plot them as a bar graph using an external chart API and so we've got a bunch of different pieces involved here where we've asked it to find out information about uh you know GitHub it knows it needs to go consult GitHub get this information and then bring that information back in a format that can be given to an external API that's capable of doing charted and so we're cranking away here and uh let's see where we end up all right so we've got the information of the commit counts it's now going Consulting the API for building a chart and boom that's a lot of code being cranked out by a lot of great llm engineers so I hope this all gives a good picture of how we can connect apis and data sources to allow differentiation of like your llm based app and just sort of to sum up I can remember how to actually uh make this Tool Work you see it now just yeah okay perfect so summing up you know LM should be used as reasoning engines not fact-based engines you know but we can basically build differentiate applications by using that reasoning engine and connecting it with external data we can you know train models we can fine-tune models and we can prompt engineer and there's good reasons for each one of those things that we might do but prompt engineering is the most flexible and programmable way for us to go and extend and differentiate applications because it works with existing LMS it works with existing data sources and apis and it's you know something that is available just through straight up you know writing software rather than having to go and do a lot of you know training step so uh we're pretty excited about you know the opportunities that are available here and uh we're looking forward to answering your questions thank you foreign"
    },
    {
        "title": "Guiding LLMs While Staying in the Driver's Seat // Jacob van Gogh // LLMs in Prod Con Lightning talk",
        "video_id": "ERdd5Ts7Bdg",
        "video_url": "https://www.youtube.com/watch?v=ERdd5Ts7Bdg",
        "length": "10:02",
        "transcript": "again my talk is about staying in the driver's seat while you have an llm or you know guiding llms well while you're still in the driver's seat um as a little bit of an intro for what Adept AI is all about we can go ahead and go to the next slide let me see if I can fix this video issue really quickly okay can we see if we reload if that works now cool can we try playing this video so what Aditya is building is a natural language software collaborator if we could try full screening this video Maybe um so what we can see here is that someone has entered in a natural language request to our model and our model is now controlling the mouse and keyboard to sort of execute this task on Airbnb the person here was trying to find like a weekend getaway in San Luis Obispo California uh we're training our model to use a variety of software tools so here we're also doing something on Salesforce and really the goal is for everyone to be able to have Adept carryout software tasks for them by describing things the same way you might describe them to an assistant so we want you to feel like you have this Universal software tool expert as your personal assistant or collaborator and it's as if you were looking over their shoulder as a debt performed these tasks for you and we can go ahead and go on to the next slide so in terms of like how we're achieving this it's not anything revolutionary and we can sort of Step through this a little bit we start by training this Foundation model so something that has this General understanding of language or increasingly more inputs than that and then fine-tuning the model to learn specifically how to do software tool actions and ultimately this means that our product is powered by large language models and so it faces a lot of the same challenges that you know other products are facing such as not knowing when to abstain our model being a little bit overconfident or having hallucinations in terms of inventing actions that maybe weren't appropriate given the context of the software tool and you know all the other problems that people have brought up today the difference is maybe though that the sticks are a little higher because you know the model is taking actions for you so we can go ahead and go to the the next slide so imagine a scenario where you were on Amazon doing a little bit of shopping maybe you wanted to buy a couple of books and imagine you were on this page and you requested that Adept you know add to cart um now you'll have to take my word for this but this seems like a pretty straightforward thing for our model to get right there's this clear button that says add to cards we can probably see how our model would would map this request to that particular action really well but if we maybe were to go to the next slide and imagine that you used our model to navigate to this other book and you sort of gave it the same request add to cart maybe before realizing that at cart is not a part of this page and now this is problematic right because um our model you know in an ideal world would sort of maybe ask you for a clarifying question because add to cart doesn't seem to be relevant to this page um but at the same time as we all know machine learning models can get things wrong and there are now you know a number of wrong actions that it could take here but one in particular is especially painful because where there used to be an add to cart button there is now a buy with one click button and semantically those are even sort of similar right you could imagine that adds a cart and buy now with one click maybe have similar sentence embeddings or however you want to think about it the difference though is that in addition to buy now with one click not being the intended action of this request it's a much more serious action it has much more serious consequences it's purchasing the book and that is not what the user intended and so how do we try and prevent these failures from being so painful to our users again in this world where large language models have this huge failure space due to the nature of its incredibly versatile output space we can go ahead and go to the next slide so again we're not doing anything revolutionary here but the issue lies with the fact that we're sort of sending our user requests and without any sort of safety checks just taking the actions from the llms as being given so one of the things that we're sort of exploring or we think is a good idea if we could go to the next slide is sort of surrounding our large language model with this guiding system so this is like a number of checks that we can perform such as checking action reversibility or maybe a Content filter to add a little bit more color to the output of our large language model and the key Insight I think is that these are not necessarily large language model or ml based checks that we can do you know we could do simple word filters to start with and by doing simple things that are not dependent on our large language model we gain the ability to iterate really quickly because we don't have to incorporate that into our model to begin with it gives us the ability to iterate on what the product might feel like if we had these sort of checks in place and to make sure that we're moving in the right direction before we spend a lot of time up leveling the ability of our model and so you know by doing these things we could maybe say that if the model is trying to do an action that we deem sort of irreversible maybe we should also ask the user for confirmation before we do that and ultimately the goal is to use all this to feed into our large language model and ultimately improve its capabilities so yeah now's a good time to go to the next slide Adept cares very deeply about our data engine hopefully data engines are also not a novel concept but the idea is that we have some model that we can deploy into our product and we use that to gain user feedback and we use that user feedback to then inform our data collection strategies such that we're always collecting the data that is most impactful in terms of improving our model and improving the experience that our users are having when using our product that is driven by this model some things that we care about in particular if we go to the next option so we really value fast cycles and adapt at a depth we strive for 24-Hour cycles for internal builds so what this means is that every day we're building a new model deploying it to the product that we use internally and gaining you know user feedback on that model that day and potentially updating our data collection strategy that day in these short Cycles ensure that we're getting fresh user feedback imagine interacting with a model that was maybe two weeks old the user feedback is is old and so what that means is that any insights that you're gaining might not be relevant because maybe a model that you train today would not have those problems anymore it also enforces a high standard of internal tooling we care about our workflow reliability and our visibility and instrumentation that we need for fast insights you know that we were able to inform our data collection strategy every day and it requires this really tight integration between engineering and data collection we're always chasing that that best data point that we could get to improve our model in the most impactful way and so if we go to the next slide there's sort of like three things that we've learned early on when building a product that's powered by llms that's taking actions for you you know they can fail in many ways and you really want to focus on those which are most painful to your users again these actions that are maybe irreversible or have high consequences if you get them wrong I don't think we should shy away from non-ml checks on llm outputs we don't need to throw out all of the the types of Technologies and strategies that we've used in the past for controlling um you know these automated decision making systems that we've been building and keeping a tight integration between data collection and product and Engineering efforts I think really makes the data collection efforts that you have more optimal and more efficient and so we really value that as well so those are sort of like the takeaways I you know thank you for for listening and if you know Adept here in depth we think that we're working on really difficult problems and so if these sorts of problems excite you in any way you know feel free to give our our careers page a look you know we'd love to chat awesome thank you so much and a couple people said that um I don't I didn't see the notebook I can't see the notebook was that on one of the slides it might have been the previous talk well maybe it was okay and then someone asked how would you integrate with something like mac automator Mac automator um well I don't know if I could speak to that explicitly I'm not super familiar with that Tech but if if I could you know imagine what it might look like um so we spend a lot of time sort of trying to collect data that really Maps um a user request into these sort of like action space that you would need to do to execute that so in the browser it's like given a user request what sorts of like things do we click or key down in order to achieve that task so I imagine in Mac automator there might be like a similar thing that we would try and map to if we wanted to leverage that technology right now we're doing everything in the browser but we think that the idea in general extends to the things that like sort of take control and execute tasks for you awesome well thank you so much Jacob this was wonderful and it was a great note to end tracked you on so thank you so much again thanks for having me [Music]"
    },
    {
        "title": "Using LLMs to Punch Above your Weight! // Cameron Feenstra // LLMs in Production Conference Talk",
        "video_id": "1_NTxx3CJXg",
        "video_url": "https://www.youtube.com/watch?v=1_NTxx3CJXg",
        "length": "35:49",
        "transcript": "and I'm super excited to finish off strong man uh I'll let you take over now and then maybe we will uh sing a little song at the end of this just to commemorate what a magical conference this has been with all the technical problems and everything great sounds good all right um thanks to anybody who's listening um so my name is cam feenstra um and today I'm going to talk about how you can use large language models to punch above your wing so whether that means uh you know just doing things faster doing more with less or um you know even as a small company potentially competing with bigger companies where you might not have been to uh able to otherwise so um a little bit about my background uh I when I first came out of college uh looking to go into the software industry uh I was mostly interested in machine learning and I I wanted to go train models and I I had um you know some reasonable skills with sort of python data analysis less so training models but at my first job out of college I I spent a lot of time training machine learning models for doing various uh NLP tasks um and I actually started to gravitate more towards the software side I got more interested in kind of the infrastructure and a big reason for that was just that the feed boot back Loop was so much faster writing software than um training at least that's how I saw um now uh that's very much not necessarily the case anymore which is a big part of what I'm going to talk about uh today um so sorry okay uh so um let me tell you just briefly about Onsen which uh is the company I've been at for about the past year and a half um so the problem Hansen is tackling is um pretty big 23 billion dollars every year is spent on basically employees suing their employers um and that's Rising year over year fairly consistently um and you know today there actually aren't that many good approaches available to businesses um you know they they will buy insurance most of the time and then separately they'll rely on uh legal lawyers legal advice for a lot of things eventually they might hire a head of HR who who knows a lot of domain knowledge about compliance um and so the solution that we're we've come up with that we think uh is pretty interesting is in two parts so number one we actually sell insurance we write uh various different types of lines of coverage to businesses um to protect them if they get sued but we also package a software product uh that aims to help avoid companies being sued in the first place ultimately time is our most valuable asset and so even if you're covered when you get sued uh it's still um not something you want to do and and uh you know sort of a great side effect is oftentimes the things that help companies not be sued are also much better for their employees um and so uh it can help them offer a better employee experience as well uh and for context we're still uh quite a small team at Onsen we're under 20 people uh which was sort of the inspiration for the talk um and um basically I'm gonna talk about uh a couple of specific ways that we've been able to deploy large language models to help uh solve some of our own problems very quickly um and I'll share a few learnings uh of things we've learned along the way um that you might be able to apply to your own work uh so uh you know probably it's not that big of a surprise that it's hard to compete with a big company as a small company um you know they have more money they have um bigger teams they can throw people at a problem uh oftentimes they have lots of valuable data sets that you can't really get otherwise um they have Network effects uh you know in the case of insurance if I know uh that my friend has had a good experience with a particular company I might be more likely to just go with them um but on the other side uh they're also less agile um slower to deploy to new technologies whether that be um sort of a if it's not broke don't fix it type attitude or um some bureaucracy or whatever it is um and so uh specifically with with in uh the insurance side of our business we are competing with really big companies uh and so the first example I'm going to talk about is how we were able to use a large language model to streamline our underwriting process um so underwriting is basically the process of taking a bunch of information about a company um you know financials they're you know if they've ever been sued before a whole bunch of different things um and then a person actually decides is this a company that's too risky or do we want to write them an insurance policy and if we do want to write them an insurance policy how much should we charge for it um and so uh what I have here is kind of a diagram of how uh in the kind of type of insurance we write how this normally looks um so there's someone called a retail insurance broker that sits typically in between uh you know about 98 of the time in between the actual client who is a company um who wants insurance and the insurance company so generally the uh company will give an uh approach the broker they'll have them fill out a long application which I'll talk about in a second um and then they'll go to any number of different insurance carriers and say hey uh can you give me a quote on this those insurance carriers take it back to the broker who then ultimately takes it to the client and then if any of them are good the policy gets issued um and one thing I want to point out here is maybe you can kind of see from this diagram you know each one of these Brokers has many clients and for each one of those clients they're corresponding with many different companies and so it's really important for us to make the process for the broker as simple as it can possibly be or else they're just not going to do business with us um and to give a sort of uh zoom in a little bit on what traditionally happens inside the insurance company would be they get this application um you know emailed or maybe faxed in some cases um they would have actually a team of people who waits for those applications to come in reads them um determines a if they're a duplicate sometimes a company might be working with multiple insurance brokers and you only want to process it once and they'll also manually extract a bunch of different information to make the underwriter's job easier and then they'll hand it off to the underwriting team who will decide if they want to write the policy how much to charge and send a quote um and just a note about the applications these applications um are really complicated there are tons of different formats generally insurance companies will accept any format that you give them um as long as it includes the appropriate information and they're they're long and dense and have all sorts of questions which actually makes extracting that information in an automated way quite a challenge so um what we implemented at Onsen is something like this so um you have the broker sends the application as soon as it hits our inbox we have an automated classifier that classifies if it's an application and then we have a separate component that extracts information from it if the application is a duplicate um it ends there otherwise we we send an alert to our underwriting team um who more or less does the same process uh as in the previous slide they decide uh if they're if it's a company they want to ensure or we want to ensure and how much to charge now um a couple details about how we put this together so uh there's sort of two parts to the solution there's the classification piece and there is the extraction piece um and so for the classification piece we were actually able to put together a classifier that performs pretty well uh with with very little work required on our part so we started with uh we built a classifier based on Google's Burt model and we I mean basically the Baseline performance was essentially you know the same as if we had picked randomly um and we within kind of an afternoon we're able to pull down all the attachments from our team inbox which was about 300 at the time manually label them and then come up with a classifier there that had about 95 accuracy uh 90 precision and 100 recoil now um we did uh test configurations that had better Precision um but there was always a trade-off with recall and for this use case uh we were okay with um getting a little bit of noise as long as we captured as much as we could um and we were also tested out a bunch of different models all open source um on on hugging face um and a bunch of different training configurations and were able to choose the one that had the characteristics we liked the most for the extraction piece we actually ended up using an API AWS textract but particularly the question answering feature of AWS textract which lets you um add a query like what is the company implied applying for insurance and uh it will extract that from a PDF document which works very well um you know they hold their cards a little close to the chest in terms of How It's implemented but safe to assume based on the performance that that there is some kind of large language model powering that under the hood um and uh you know I just really want to stress um especially with the classification model um you know this is something that we couldn't pull off the shelf there weren't any good options um but as I said we were able to put it together and well the the training data we were able to put together in an afternoon and basically end to end we got the whole thing working in under a week um compared to uh training a model from scratch um it's sort of at least an order of magnitude less effort and less data than we would have needed to get similar performance otherwise um so now one more example of something uh that we use large language models for at Onsen um so uh one feature that we wanted to build for our risk management platform or I guess I should say a hypothesis we had that we wanted to build a feature and validate is that um right now uh generally entrepreneurs startups um relies a lot on their lawyers to in to review individual documents and let them know if there's anything potentially wrong with it um things like offer letters uh separation agreements if if someone gets laid off or something like that um and so we we thought it would be a really interesting feature to build um a system where someone could just upload a document and get immediate feedback on potential compliance issues um and you know even though we're not providing legal advice and that's clearly spelled out on our website and everything like that for this type of feature we were really uh wanted the accuracy to be as high as they could possibly be because we certainly don't want to tell someone something is wrong or just have completely kind of bogus outputs like you might occasionally get if you use something like chat GPT so the first step was that we gathered a bunch of domain Knowledge from attorneys we learned you know what are the potential things that that you see in offer letters for example that are issues and then we put together a system um that looks a little bit like this so for for each type of document we have a number of different features that we want to extract for example for an offer letter uh we might want to know what's the salary um is this person an exempt or a non-exempt employee um and then for each one of those features we sort of have a two-step process so first we use uh sentence embedding to extract the most relevant portions of the document so more or less something like semantic search um excuse me and the second piece is that we use a question answering model to extract the exact uh bit that we're looking for so for example after this step for the salary feature we would have forty thousand dollars and then we would have exempt or non-exempt um based on that we run some business logic and come up with a set of positive and negative insights uh based on that domain knowledge that we've gathered from attorneys um so in terms of how we actually implemented this for this uh we were able to just pull models off the shelf um specifically a question answering model and a sentence and betting model uh the question answering model is again based on Google's Bert model the sentence and betting model is actually not really a large language model it's it's based on Microsoft's mini LM but um for this particular task that was uh the the best performing model that we tried um and additionally like it didn't require a ton of data uh we were able we had a few dozen examples of each type of document we wanted and based on that we were able to sort of crap manually write prompts um to help us extract each feature um now um I think for this particular feature uh at this point it's too early for us to say if our you know our hypothesis is true or not we're still developing it and and taking it uh giving it to people for feedback the feedback we've gotten so far has been really good but the main point is that we were able to put together this together in uh about a week at least for a prototype and so rather than you know again training a model from scratch or something like that where it would have been a big investment to validate that hypothesis we were able to do it uh very quickly and the end result it feels actually fairly kind of magical um so uh quickly want to go over a couple of uh learnings obviously this is not a complete list but um so you know probably not a surprise but uh definitely if you want to host these things on your own um it's important to have someone who who knows a lot about infrastructure uh to do it in in a good way I mean so you know for one thing these models are really resource intensive um you know they're you have to understand how they're gonna perform or if it's all going to fall over under high load um and so you know many of the same things that you've always had to do for deploying production systems um you still have to do and they might even be more important when you're deploying llms um and evaluation metrics are also very important um you know ideally you want to be able to quantify uh if your model is performing as well as it did in tests or or as or if the performance is going up or down over time um the third point is um you know if you're running massive production workloads it does make sense to use the best hardware and everything like that but especially if you're just working on a prototype or anything like that um it is definitely possible to deploy pretty large models uh you know maybe when you get to something like um you know many billions of parameters uh it wouldn't work so well but um you know we were able at least for prototypes to deploy models on normal CPU instances that we use for other things as well um and they work well um now the last thing I want to call out is something to factor into your costs if you are relying on an API is that um probably you have some layer of business Logic on top of the outputs that you get from the models and so software is all about iteration and so ultimately you're going to want to change uh that business logic and so um and you know probably you're also going to want to test uh how different you know for example prompts or or different uh inputs uh to the model look um if you're relying on an API doing those changes is is very expensive um and in some cases might even be cost prohibitive um so it's definitely something you have to factor into your expectations of cost um now maybe uh anyone watching is thinking you know why am I not talking about chat gbt or something like that um but uh definitely we do have a lot of cool ideas based on chat GPT um you know I've kind of presented the things that we've been able to get into production and are working well um but we think that there's a lot of potential for uh generative models or or something you know gpt4 one of these models to potentially be a really good underwriting assistant um it might go fetch data about a company it might just sort of make an initial recommendation um but our experiments have have shown that it actually seems to be pretty good at it um and then the the second one um one of the really difficult things about compliance is that the uh information you need to know is scattered all over the place that's why um well one of the reasons people rely on lawyers a lot of the time um and so you know one thing that we think could be interesting would be to be able to ask a question about a specific compliance issue in natural language and get um a really great answer uh based on or you know at least pointed to the right location um but that's something that is is just an idea at this point so uh thanks everybody for listening um you know it's definitely a really amazing time to be alive really cool time to be in software um you know I'm I'm super excited about uh all the advances we've seen in AI um but you know even without all of the brand new models um you know large language models can still be a great asset um like they have been to us at Onsen um and last thing is you know we are we are a startup we're always hiring so uh if any of these uh problems sound interesting to you or or you want to learn more about what we do uh please reach out um we're always looking for for great people um yeah and thanks to the organizers thanks to anyone listening um you know it's really an honor to to speak and um if we have time I guess I can take a question or two for you there's all kinds of time there's just a little bit of a delay between the stream and the actual uh chat that is happening in oh I see um so we can just shoot the here for a minute and then when there is a quality it's like you want to make sure nothing too bad goes yeah man I mean the while we're waiting there is I mean there's so much good stuff that you're talking about there but I'm wondering on more of a again I want to go more to this philosophical level and talk about like if you're working with this with these large language models and you're dealing with like problems that you're not seeing before what's your main way of going about just debugging and being like wow I've never I can't Google this because I'm probably the only one in the world that is seeing this right now right yeah yeah that's a really good question um you know I mean trial and error uh to some extent you know that's always the fallback um you know I think that um you know to some extent you can uh for example if you have a situation where you know you have a bunch of possible parameters like you know it's it's actually feels kind of similar to training a model sometimes because you're like okay let me run all of these different things come up with some Metric yeah I guess that's part of the answer is like really you just need some evaluation metric I mean yeah like I don't think that there's really you know no one on earth can explain what the llm is thinking and so like there's no you know at least not today there aren't tools where you can sort of see the flow of information through it and and use that to um um debug what's going on but but you know traditional kind of software engineering techniques uh you know can also work reasonably well did I share this did I hold on let me share this with you speaking of which I don't know if you've seen this yet yeah but pretty cool that's that's the shirt that I I made because it's like yeah man we gotta call this out like a lot of this when it comes to reliability and just making sure that there's there's this big question that I have in my mind like no amount of prompt and correct prob prompting right or like no matter how good of a prompt engineer in between quotation marks you are um you can't force the model to give you output so yeah like how do you go across those kind of problems right yeah no yeah I mean it is it is it's funny because like you know I probably seen all this like AI agent stuff out there you know putting one of these on a loop and uh you know that was actually like um around the same time it was kind of blowing up it was an idea I was thinking about and I experimented and it's it's really funny because it's like you give chat TPT these instructions of like give me this exact format do not give me anything else I was like yeah sure like let me help you so and and I mean ultimately you know the models are non-deterministic too so you don't even actually know which is a lot different than most software when you you know you give it an input you don't even know that the output's going to be the same especially when you're using something like you know chat like an API they might have completely changed the model under the hood uh without you knowing and so it's definitely not an easy problem um you know I mean I I follow all sorts of people on Twitter and I see uh there's a you know a lot of people are kind of doing kind of ad hoc I would say research on um you know specific techniques and uh I find myself frequently kind of bookmarking things of like oh that's an interesting idea you know maybe I'll try that sometime but it's an emerging field I would say um and uh you know I would guess within the next couple of years it will become you know a formal discipline or something of that nature yeah we'll see we will see uh so there are some awesome questions coming through in the chat uh that are getting a bit more specific with you have you checked with shape what part of the documents get used most in the classification um no I would I guess the answer is like no um mainly we just evaluated uh the outputs um and you know we're a small startup we have a million things to do and so you know we kind of monitor how it works as it goes on but we you know we aren't we haven't really Doven into uh like what you're saying um but that's actually a really interesting idea that maybe we should try is at least just evaluate you know does the first two pages or whatever like usually yield the same classification result um I I just don't know the answer at the moment that's awesome Sebastian great question reach out to me I got some socks for you that is what happens when you ask these awesome questions all right next up what are the core parts of underwriting content from llms that would need to be evaluated to ensure output what is required sure yeah um so I mean that's one of the interesting problems like I think I had one slide where I had one of these sort of nine page applications and you know on top of that whenever we're underwriting we get like financials we get you know their employee handbook and so um you know ideally we would just give that all to the llm and you know it would do our job for us but um you know I mean maybe with gpt4 you know 32k tokens like it'll be doable but context size is definitely one of the um problems we have to think about when we get deeper into it yeah um you know I think that one idea we've been like potentially thinking about is you know maybe asking on the first pass having a large language model or chat gbt or whatever um kind of Summarize each document and then put that put those summaries in the kind of the final uh prompt that makes the evaluation something like that but um I guess the answer to the question is there's a lot of information that's the application financials you know history of if they've been sued um you know and based on all of those things we might even ask for more yeah and like knowing all this do you feel like the method that you're currently using is the only method that could actually do this or are you also looking for like is there another way so doing what specifically like trying to I guess the idea is the end goal that you're you're getting at right and with with feeding all these llms or feeding these llms all this data like I just wonder if there's and I didn't mean to get so philosophical I think it's probably because it's late here but you caught me at a good time I need my my scotch and cigar and this is uh it's what you get for being the last one I get to chat with you on this kind of stuff but yeah I I mean I I love uh pontificating so yeah I won't find any more look at that vocab word man oh my God printing out the real guns all right but at the end of the day it's like the the thing that I wonder about is do you need to use because there's a lot of times that we talk about in the mlops community that machine learning might be over engineering something and then I feel like now with these new large language models potentially using them and in my own experience when I've played around with them sometimes I spend so much time trying to get what I want out of it then I'm like dude if I just did this it would have been faster than actually having to do it so I just wonder do you ever want do you ever think about are you inventing a nail for a hammer or is this the only way in your mind or in your eyes right now that this this can happen uh I mean I think yeah it's a really good question I think I you know the traditional wisdom with machine learning has been like use it as little as possible only when you really need to and I think that that's a lot of the reason that for uh the use cases we put into production that far thus far we weren't like let's just throw a bunch of info at chatgpt and like let them handle it um because yeah I mean I do think but also there's things like you know I mean if we were able if we tried to create a system to like do underwriting uh you know I mean there's actually a lot of examples of unsuccessful insurance companies that have tried to do that and so I think that for unsolved problems it it can work really well um you know and but yeah I mean I agree I think that you know what will be really interesting is seeing you know good software built on top of the large language models like I think a lot of people say you know text is the new UI like yeah I mean you know I love my terminal but I don't think that's gonna happen um you know uh and so you know there's all and and using large language models presents new ux issues right like oftentimes you know I think in the like performance optimization discussion and they were talking they they got a little bit into like ux fixes for for slow models and stuff like that so um yeah I think that uh you know it is a really cool new primitive um you know and and we'll see kind of what what it works for and what it doesn't undoubtedly there's going to be a lot of things made that are way over engineered and you know don't actually make any sense and I mean I think if you start using large language models where they aren't necessary it can actually potentially make your software worse because something you could have just written in code for and it's so slow only once 80 of the time or something yeah yeah it only works eighty percent of the time it's super slow I mean yeah you're just you're degrading the whole system so there is there's another question that came through how are you getting evaluation metrics um um um say that at the moment um for uh our internal for for our internal classification system the evaluation metric is that everything goes to a slack Channel and I take note when I see things that are wrong uh you know and and people give me a heads up if it misses something um and then uh periodically I've been kind of going back and seeing if I can improve the model um and you know I always have this original labeled data set that it was trained on um and yeah I mean I would say at the moment we're we're doing things mostly at a small scale and so um the evaluation metrics are still on the manual side I think that we will certainly over time get to the point where we uh have things like regression testing and stuff like that uh in a more automated way but at the moment it's just something that we we think is important so we're doing uh doing it relatively manually excellent dude cam thank you so much man you finished us you finished strong and this has been incredible um I'm gonna kick you out now all right this is the end of the party you don't gotta go home but you can't stay here and so thank you we will see you later man thanks for having me yeah it was it was like I said talks were amazing uh super honored to to be able to talk and um yeah yeah"
    },
    {
        "title": "LLMs in Production Conference Part 2",
        "video_id": "NnPqFSc4asc",
        "video_url": "https://www.youtube.com/watch?v=NnPqFSc4asc",
        "length": "0:26",
        "transcript": "[Music]"
    }
]