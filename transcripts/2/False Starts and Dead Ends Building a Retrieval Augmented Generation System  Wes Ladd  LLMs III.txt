[Music] West are you with us yes I am good to see you man how are you good to see you doing well you got a beautiful background light and the colors man you got it uh well I'm stoked to see what you have for us today I see that you're also sharing your screen so let's see if this is working nice okay well yes awesome thanks Adam so today I'm going to talk about retrieval augmented generation uh our startup train GRC uh is focused on cloud security training and as part of our platform we decided to build stinkbait doio which is a application focused on cyber security research and Reporting uh in the cyber security domain in specific uh the knowledge base is very fra fragmented amongst different sub communities and even subub communities so that soled approach to information sharing has really created a scenario where in cyber security collecting a knowledge bases is fairly challenging uh furthermore you have this consideration that in relation to llm cyber security topics are often censored you'll potentially hear chat GPT say oh I'm sorry I can't do that for ethical safety reasons and so this creates a lot of challenges in uh sharing uh proper information in the cyber security space so we decided to build a retrieval augmented generation system that could help with this but there are some challenges that we've encountered that I think you and the audience may be interested in and you may experience similarly uh the first one we have is related to data quality so there are a variety of file tops that we try to handle as part part of our data processing pop line uh one of our fun in games to play on our team is is it really a PDF uh PDF's been around since 1993 and over the decade since as there's lots of extensions to PDFs uh various file formats that are called PDF may not actually be a valid PDF and PDF generator plugins that are associated with various websites they can often return invalid PDFs um furthermore you have PDFs that are often just poorly photocopied documents so these various file typs that all fall under sort of the spectrum of a PDF may not actually be a PDF so why does this matter well who cares if it's a PDF right well with optical character recognition you can extract the text from a PDF and we use Amazon textract for this particular particular use case um however even with a proven solution like textract you're often going to encounter an error saying that this isn't actually a PDF well how do you address that you can actually transform the PDF into a JPEG and then reprocesses that file as a JPEG so textract allows for us to have sort of this backup capability to extract data from the various PDFs we may need to J uh then you have web scraping content so this is one of the most common challenges that I see people face when trying to build their own data set for retrieval augmented generation is how do you actually get all the information off these various websites with the V variety of web Frameworks and wizzywig editors for creating websites HTML data is actually not that well structured so how do you get high quality data data well unfortunately a lot of this looks like manual inspection of various websites and understanding the HTML elements that are most relevant for that particular website but this is a critically important task because if you go in and look at some of the thirdparty or open- Source tools that'll help you with web scraping and you actually inspect the data that you're getting back you may find that the data isn't quite as well manicured as you anticipated so these are a couple of the things that we see in relation to data quality the next hurdle that we encountered was related to the search quality so there are different algorithms or indexes that you may use in relation to doing a search result now many people know that there's a difference between exact KNN versus approximate nearest neighor searches um and that often times the latency associated with exact k and is not what you're after in a real-time application so then you have to use indexing algorithms and we have for example IVF flat or hnsw and now a new cumber disn well all of these algorithms have different tradeoffs as far as recall or accuracy versus the latency associated with the request anecdotally we started out thinking hey we'll use postr SQL and PG VOR which at the time only supported ibf flat we thought that using an open-source database technology would have uh real benefits as we grew and and changed as an organization but when we started to see the latency results for obf flat it started to Dawn on us that maybe we had to go a different route now all of the different Vector databases that exist out there they may not support each of these different algorithms they may only sport one or a couple of these algorithms and you have to weigh the tradeoffs between do you want to pass off the responsibility for choosing your algorithm to a vector database company or do you want to have Fuller control of it and if you want to have Fuller control of it can you actually manage this technology in such a way as to return similar quality and similar latency of results you also have the consideration of picking a model so when you're picking a model most people just choose Ada embeddings off the shelf because they're already using the open AI API well good news for them is that the context length for Ada embeddings is very long you can get almost five pages worth a document uh under a single embedding and then furthermore Ada embeddings is fairly performant it's on the hugging face mte leaderboard however there are other uh embeddings models that may perform more uh positively for your particular use case uh the state-of-the-art uh embeddings models on the mte leader Bard is not Ada embedding it varies depending on what particular evaluation metric that you need to consider um then there's also the consideration of what types of data was the model trained on so with embeddings models you have uh sence Transformers that are called multi QA models meaning they've been uh trained on short question and answer responses then you have MS Marco passages which are more long form text maybe you need a multilingual model or a multimodal model that'll do image and text all of these considerations go into picking the appropriate model for doing your retrieval finally the last consideration that we kind of had to address is how do you chunk the context and how are we going to store this information longer term so when you're considering context chunking the ideal way to chunk your context would be along conceptual boundaries so all of the information associated with one particular topic would go in one embedding and all of the information associated with another topic would go in another embedding however what we've seen at scale is the only realistic way to chunk your embeddings other than just sentences or you know something similar to that is to use formatting so for example in marown formatting you have different heading values and you may be able to break up pieces of text based on the heading value well this is sort of a a proxy for contextual separation but it is not equivalent to contextual separation so when you're trying to re-inject your context in the gener genertion approach how do you ensure that the context that you're embedding is relevant and part of this is a challenge of how are you chunking your context so the last consideration we have here is what do you do when you need to change databases or embedding models so if you're experimenting with Vector databases and you think you may change your vector database maybe you should store your vectors on dis or in Blob storage because if you store those vector on disc or and blob storage you don't have to recompute them later on then if you're experimenting with embeddings models you know there are some considerations that you need to make up front here how many dimensions are the vectors how much space is that going to take up in your database uh and then also you know what source style is your data if you choose the wrong model you may not be able to to get the results you're looking for so thank you that's that's what I have today on retrieve wman a generation Wes thank you very much I mean there's so much wisdom packed into so few minutes uh this is It's fascinating just to see like I mean like the conceptual distinctions and the boundaries I mean it on one hand it sounds somewhat difficult when you first said it I was like well this is difficult to know how do you even draw those boundaries in the first place and then you're like oh yeah you probably already did that because of your markdown right like in some way so this is probably a useful proxy although I suspect that this is an area of just active research too right just like how do you because sure I've marked it down in a particular way but it may or may not be because it has a like a deep semantic stickiness it might just be because I thought it would be easier to digest for the kind of people that I was talking to at the time right so like yeah now there's lots of it manual inspection and you know just to throw my bid in for the mlops uh drinking game today garbage in garbage out right so like I mean I know it's a cliche and everybody says it but the reality is so much of the work that has to be done to make these models performant is done upfront in that reprocessing and processing of your data so bodily important find West on slack and and he's the mlops community organizer of Austin is that true that's true that's true Wes thank you very much for coming today [Music] Adam