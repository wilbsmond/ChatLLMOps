I'm going to just bring on our next performer where are you at Chris where are you at ah there he is as the others are yeah there he is dude I gotta see what you do what is this beard routine you have give me some of that magic I need some of this beard man wait can we bring this up full screen real fast I'm in Murray Utah Salt Lake City that is awesome that's that's what it is you just live you go there and the beard grows that's how it works that's right that's right dude oh epic man well I'm gonna let you get Kraken with your uh with your presentation I know that you've got all kinds of good stuff for us and uh I'll see you in a little bit maybe maybe I might not jump back on here but um thank you for that bro whatever works maybe I'll see you in 20 minutes see ya cool so uh awesome I'm trying to figure this out uh welcome so I'm Chris brousseau I'm a lead data scientist at MasterCard we're going to be talking about linguistically informed llms and um this is again not or I haven't even said this once yet this is meant to be a help to teams it's not meant to make anyone feel a detriment or anything like that um all of this Insight is coming out of a book that I'm co-authoring right now with uh Matthew Sharp he's a clean code for data scientist guy he was recently on the ml Ops Community podcast and I would really recommend if you haven't go and listen to that episode and yeah our book is about to hit the Manning Early Access program so look out for it if this stuff appeals to you we're going to be deep diving into all of it in the book and going even further into llm Ops and ml Ops and how they kind of mesh so let's get right into it um dimitrios mentioned the beard so I want to address that real quick um there are a lot of different lengths that my beard has been and there's a lot of different states of neatness that it has it has been in um basically everybody's got their own personal preference for their beard I'm talking about my beard because my hairline's receding a little bit but if you want to think about it as your hair you can as well the idea though is that whether it's long or short there's always a growth period in between where you are and where you want to be only somebody existed that could that has experience with hair that has experience with lots of different types of hair that would be able to help you not only set your goal to figure out what looks good on you with your type of hair and your color of hair and all of that but to help you get there in as painless a way as possible because as you can see in that top right photo I hate that photo of me I hate it so much but like I was just in I was in a growth stage and my beard wasn't neat and um so now we're gonna get into some fun stuff let's keep going so this is a quote that many of you are probably familiar with this is from IBM in 1988 from Fred yellinack um every time I fire a linguistic performance of our speech recognition system goes up I think this is true but considering they were working on shoebox at the time and I have never used shoebox and I don't know anyone else who uses shoe box for ASR on digits um I don't know if they I think they probably ran out of linguists to fire um the idea though is just like with a beard just like with hairstyle you can do however you'd like you can you can figure this out on your own um The Barber though is just gonna like they're gonna help you get there quicker and more painlessly um you can even trim your beard on your own you can fire your Barber at any point um they're just there to help you know uh one of the mistakes that I've seen when people are growing beards in this case llms when they're training llms is they don't have a goal in mind and so they end up solving for metrics right they're solving for precision they're solving for recall they're solving for an F1 score uh the problem that I see with that is you aren't hitting your goals when you're when you're measuring for kpis if your kpi is your goal you're either number one a late stage which that's fine that's really good or number two you don't know where you're going you're just growing out your beard and you're probably going to have to trim them back at some point and with llms as opposed to hairstyles this is a lot harder to do so uh let's talk about llms real quick then this is the focus of the conference right um what are llm solving for I'll just want to hear that in the chat so like give it a guess what do you think llms are solving for if there's a bit of a delay so I'm just going to keep going um llms are solving for language right they're large language models they're not large mathematical models they're not large statistical models I think they they are they're fundamentally mathematical they're fundamentally statistical but they're solving for language you know so let's talk about the features that we solved for to get to to our goals instead of trying to get hung up on metrics right we got our yeah exactly the next word chicken or egg first um features of language uh we have number one syntax we have number two morphology semantics pragmatics phonetics and just in case you're unfamiliar with these syntax is going to be your grammar that's your structure of language morphology is your structure of words you know how do you how do you get from nothing to a word it's your morphemes your smallest units of meaning uh semantics is your literal definitions of words uh for example just a quick example if I say I'm married to my ex-wife right married and ex-wife their literal definitions contradict each other but there could be pragmatic explanations as to how that could work I'm not really but um there could be pragmatic explanations for that pragmatics is the definitions surrounding the words so it's not the words themselves and then phonetics is the actual production of speeches your places of articulation it's assigning meaning to sounds it's the relationship between orthography and your actual sounds um yeah these are really complex problems Let's uh I don't want to pretend like I'm giving you something novel here that nobody's ever thought of before and in fact as soon as I go to the next slide you're going to see there's been a lot of thought put into this and so we're going to talk about some problems and we're especially going to talk about some solutions right so what have we already done to solve for these you know for syntax I'm of the opinion that llms solve it you know that when you're looking at transformational generative grammar a la Chomsky llms do it you know they they allow you to generate infinitely and they allow you to generate infinite combinations it's very very cool I think we might be done with syntax we're not done with the rest of them though for morphology we have tokenization if you're if you're looking at this from a computer vision perspective or any other classical ml perspective your morphology is vectorization it's making sure that the model has something that it it can view as opposed to trying to parse your words um and then on the other side of that related is embeddings where you take your vectors and you turn it into something that has inherent meaning to the model um I think that for morphology and embeddings we are like maybe 75 80 percent of the way solved there but we're going to talk about them a little bit more the bottom two are the ones that I think we've gotten the least solved but we're working on it and a lot of people are seeing that these two sections are the places where we can really complete our star you know with pragmatics being uh giving the model context whether that's in training or in inference we found that it works really really well and I'm I'm actually going to demo that for you a little bit whether that's through Chain of Thought or through instruct fine tuning or you know just giving the model rules that it can work with so that it has a way of interpreting the real world and all the stuff that's around the words around the semantics and the compositionality the the cognitiveness right morphemes can be part of words morphemes are part of words morphemes are exactly part of words um so yeah I'm sorry there's a huge delay here for me reading the chat but anyway um the last bit phonetics I love phonetic so much this is one of my favorite parts and so I'm saving that one for last but let's let's dive right into it we're going to talk about some problems and some solutions this is the hardest one the dictionary problem this is if you are familiar with dictionaries if you've ever seen or heard of a dictionary before a lot of people end up using dictionaries as some sort of an authority for semantics an authority on what is the literal definition of a word um I don't think that we need to use them that way because dictionaries are not intending themselves in that way they're more of snapshots in time of popular usage and this is this is helped by the actual dictionaries themselves you know you have a dictionary.com merriamwebster.com and the Oxford English Dictionary all of the all three of these dictionaries have weekly updates to their corpora to their vocabularies and I can think of something else that starts with an LL and ends with an M that has to manage vocabularies over time you know they have weekly updates they have monthly soft updates to their entire dictionary and they have yearly hard updates to make sure that they are representing language as it currently stands um beyond that basically I'm just going to stop there with this with this problem the idea here is when you consider your llm like a beard you need to think not just getting to the about not just getting to the length that you want in the style that you want but how do you stay there how long can you stay there is your llm going to just be a period piece for 2023 or is it going to go further can you make it last 20 30 and how do you do that um do you need it to last that long and maybe your llm is so specific maybe it's a financial one like the ones that we work with at MasterCard where we're working with language that doesn't really change all that fast and so we don't have to worry about that as much this is just a piece of consideration that can really level up your ability to create these with skill the next problem uh this is the Yeet problem and this is this has to do with morphology there are two things here and both of them have to do with predictability you know when we use popular methodologies for tokenization like by pair encoding or like sentence piece or even the chat GPT encoding we have uh some problems right uh there's a reason why goat 7 billion is able to outperform gpt4 which is 1.7 trillion token or 1.7 trillion parameters on every bit of arithmetic whether that's addition or division multiplication subtraction um the reason being that gpt4 used statistical uh tokenization methodologies that had to do with frequency we don't determine morphemes based on frequency and so they ran into problems with numbers where it's grouping commonly grouped numbers together and so when you ask it to do math problems sometimes it can't adequately see the problem the way that you need it to um this happened in 2014 with the word yeet seemingly came out of nowhere popularized on Vine uh the thing is though um English has sets of sounds and letters that are allowed to be together and it has other stuffs that are not allowed to be together and those allowances don't change as fast as the words themselves so edit is a predictable word for English in fact it's been in English before if you look at Old English when you yeeded someone it wasn't throwing them or anything like that it was um using the word Yi it was using that level of formality for them so piggybacking off of work that other people has have already done to show you what's predictable can really level up your ability to tokenize and your ability to utilize morphology to make your models better uh next portion this is the kimono problem this is basically the same thing just understanding deep down into it that when you're when your model is splitting words it's trying to find basic units of meaning mono is a unit of meaning right and so kimono to a model that is tokenizing English it looks at mono as a unit of meaning and kid is not a unit of meaning right it the kimono is a borrowed word in English and so um there are a lot of places where we end up using token tokenization and morphology and syntax as if our language is completely in a vacuum this is one of the reasons that multilingual models tend to outperform monolingual models on the exact same tasks you look at you you know it it keeps happening over and over again the more languages you're able to ingest the better your tokenization is which is the better your model can see what it is trying to work with let's go to the next one and this is the socratic problem I I'm kind of annoyed with the way that I named this one but it's it's dealing with pragmatics it has nothing to do with soccer juicer why it's just the entailment the pragmatics and in true ml Ops fashion we're going to talk about speed here uh this is for a company that um they they basically wanted a system that queries chat GPT and it queries it for 20 college level biology questions um when I did this vanilla which at GPT using the openai framework it got 7 out of 20 which is pretty remarkable but it's not where they wanted it to be and it took about four minutes to do it and admittedly that's unfair because the average amount of time running this several times it took about one minute to do once the API had warmed up and warmed up once it recognize my connection and everything I'm going to show you though doing it locally with Falcon 7B instruct how long it took on average using guidance which provided pragmatic instruction for Chain of Thought allowing it to prompt itself multiple times allowing it to do system prompts and basically coaxing the uh coaxing the knowledge right out of the model that I don't even know whether or not Falcon trained on biology textbooks but this is what ended up happening what was it gonna is it gonna play ah shoot it looks like it's not going to so um I'll just tell you it took about two seconds to play or to go through all 20 questions and it ended up getting 17 out of 20 instead of 7 out of 20. so it increased by by 10 which increased by 50 accuracy and increase in speed like almost 100 well just from using some pragmatic uh instruction on the inference side it's amazing this is one of the things that I'm really looking to explode in the next little while and if you're not using it you should be there are other tools like Lang chain there are other tools like vector databases that can add even more to this like document retrieval amazing you should be using this stuff um the last little bit which I'm really hoping is going to play oh geez well I'm really sorry this is this is not playing I would I would love to be able to show you these basically this is the I never said I loved you probably listen getting into the phonetics where um I never said I loved you is a sentence that's impossible to accurately um I never said I loved you as a sentence that's impossible to accurately understand through text because you can put emphasis on every single word in order and it will change the entire meaning of the sentence uh compare I never said I loved you two I never said I loved you right and that is information that is immediately lost when we reduce it to text um for this I basically I tried on the left we have two text-to-speech models which is um no less people using Falcon does not make it faster because Falcon is local it's downloadable so it's just downloading it makes it faster just giving you a little like wrap it up warning oh yeah sure well the videos aren't playing so we're good um Fortis and 11 labs are text to speech so they are completely reduced to text and I wish that I could show this to you I wish it was playing but tortoise and it basically um it it there's a lot of stuff wrong with it but it ruins Trump's accent makes him sound like he's from all over and he's speaking French so it's even worse 11 Labs has it pretty trumpy but it's not it's still very not good um and neither of them capture any of the melody neither of them capture any of the phonetic information that gets lost uh SVC is a speech to speech model so it's only phonetic and it does a pretty good job but it makes it sound like Trump has a severe cold and that he is French whereas the phonetic plus model which this ingests both text in the form of the International Phonetic Alphabet and the uh and phonetics it ingests an actual audio clip as a reference it sounds really really fantastic in fact I posted it on my LinkedIn so if you wanna if you want to get a view of what this sounds like um you can go there and I'll actually give you this here here are the three QR codes to connect with me the left one is LinkedIn the right one is my portfolio site and the bottom one is my YouTube thank you so much I'd love to connect with you all and talk more about this all right thank you so much Chris yeah please join folks in the chat I feel like they're gonna want links and questions and all that stuff thank you so much sure all right [Music]