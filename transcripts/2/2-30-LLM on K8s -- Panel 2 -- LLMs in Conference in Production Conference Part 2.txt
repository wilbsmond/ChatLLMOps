and now that is a perfect segue into because it feels like that was just like the warm-up that was a little bit of uh the sound check before we bring on our next guest what's up shree doing well Demetrius this looks like a great event Yeah man so I've got a shirt that I want to show you all but I'm going to get everyone on stage we've got Raul with his new microphone and I don't know how you sound me man yeah yeah it's right here can you get the ASMR field yeah maybe not that but a little too intimate we've also got my node I think it is very late where you are so I but it looks light out is it already tomorrow fortunately I'm traveling to uh so I'm in Seattle right now so it's the same time zone all right cool so that is awesome and then where is my man Patrick where there he is all right I why am I in big I am not the star of this show I'm going to as I make myself even bigger yeah Patrick what's up dude I love the Martin guitar in the back that is awesome I love seeing you all and treat I'm Gonna Leave it off to you this is kind of a uh what we would call a mix between a panel and a fireside chat we've got so many incredible kubernetes people in the same virtual space and while you kick it off I'm gonna get this shirt that is so good it is not the I hallucinate more than chat GPT shirt it is the I got it just for everyone here I don't know if you can see this I gotta make myself bigger not because ego needs it but because everyone should be able to see that says kubernetes is a gateway drug and that is what we are talking about I'm running llms on kubernetes I'm going to get off the stage now and I'll drop a link uh in case anyone wants to buy that shirt and support the community shree it's all yours my man thank you so much thank you so much Demetrius uh hey hello and welcome everyone thank you so much for joining this uh this panel panel discussion fireside chat about large language models and kubernetes uh you know one of the more well-known instances of you know kubernetes and llms coming together is open Ai and documenting a lot of how they've trained their models and actually serving chat GPT and Dali and other models on kubernetes so uh I asked chat CPT to write like a small humorous description about about this session and this is what it came up with in a world full of Errors may your code be bug free may your containers be slim and efficient and may they always sail smoothly like a rubber ducky in a giant data Lake is this even possible or am I hallucinating well well on that maybe not so funny not it's safe to say that there is a lot of work to do uh you know especially when it comes to llms and humor and maybe kubernetes has a role to play in that and that's just the perfect segue for us to kind of get deep into these conversations about how llms and kubernetes coexist what do they what do they how do they work and so on uh so before we get into the details like it might be you know a good time to just have quick introductions do you want to go next introduce yourself um hey I'm manjot I'm part of lightly right now and investing in a lot of these new age nlm companies are excited about that but before that uh life actually started uh as as an infrastructure engineer where I was building internal infrastructure libraries um and Google and from there I became I became a product manager in the kubernetes team this was you know far far into the kubernetes one not all ages when we were actually thinking about mesosphere and other such things uh so it was uh working primarily on networking and security and uh eventually you wanted to build something of my own so left and started up in machine learning infrastructure so um and uh it was lucky at stripe it's fantastic that's fantastic welcome uh Rahul why don't you introduce yourself yeah uh hello everybody I'm Rahul I'm the founder of AI hero um for me kubernetes is not just a great gateway drug it is a way to just stay high all the time right so um I I actively think about um how do you uh deploy the llm Ops stack apologies and uh to to use that phrase because it's not cool yet but um what exactly does um training your own foundational models on kubernetes look like what does fine tuning a large language model and kubernetes look like or even deploying applications like the Q and A's or search based applications agent applications on kubernetes and how that can help the increase the velocity of um what your company delivers or deploys is is is what I I uh what I like to work on excited to talk to everybody awesome awesome thank you Raul Patrick what about you hey yeah I'm uh I'm working as a text for a startup right now we're focused on LM planning which has been super fun to do historically I've uh worked a lot in kubernetes I worked for John Craig at a cup called heptio we get acquired by VMware I did a lot of stuff with some features in the kubernetes and whatnot and uh over the last several years gotten a you know traditional kind of machine learning 1.0 and then last you know year we've been focused on generative Ai and now really kind of heavily focused on uh Ellen planning awesome thank you so much and just for uh for completeness uh hi everyone my name is Shri I am currently an engineer at outer bounds and outer bonds we are building a you know large scale machine learning platform for fully managed platform for handling uh data science and machine learning workloads so yeah welcome everyone and uh maybe we can start off just a little bit about like what are your impressions of kind of you know first impressions of large language models on kubernetes uh especially maybe with respect to training where does kubernetes come in what does it offer what is good what's not so good what do you think sure maybe I can kick it off um so when kubernetes uh was uh I mean as you all might already be aware kubernetes is basically inspired by an internal uh Google uh product called Borg and uh it basically tried to achieve the same objective which is how do you make sure workloads you know are scalable or reliable uh you have containers and how do you have an orchestrator so that you don't have to manage that infrastructure and uh that's what kubernetes was promising to do you probably noticed in that last sentence I never machine anything related to machine learning so so just a historical perspective here um without keeping mlml uh core ml in mind and it was more about General workloads that traditional workloads that you might be running your apis your micro Services Etc so it was very micro service heavy and focusing on that architecture uh but for sure uh there are lots of in the past couple of years this is even before llms um became super popular uh in the last couple of years we've been seeing a lot of these uh you know uh products libraries everything come up that help you uh basically run your machine learning workloads on top of kubernetes because the principles remain the same what are the principles the principles are kubernetes gives you an abstraction layer an orchestration orchestration layer on top of your containers that abstract away Hardware right so effectively giving you more reliability scalable ability and and you know efficiency out of the box yeah sounds great yeah I think Rahul you had something to say yeah yeah um and and maybe I'll take a different uh perspective on this one so um a few like maybe a month ago um we were talking in in this community called tribea that I'm part of super intelligent people um and one of the recurring topics that came up was Enterprises are looking to deploy uh and train the models in-house because they really don't want data to leave um their VPC right and I think that is a business problem for which kubernetes provides a good solution obviously you have vendors um the cloud service providers or even startups who have kubernetes native solutions for you to deploy and train your machine learning models um but this is what you know hopefully what kubernetes allows you to do is have a cloud agnostic uh platform for you to train and deploy your machine learning or large language models foreign I would love to hear your thoughts on this like one of the perspectives that gets talked about specifically with kubernetes uh is that yes it has options to run a lot of workloads in the machine learning domain land up being batch workloads and there is yes kubernetes offers deep support for running batch workloads but oftentimes it seems like it's a little bit of an overlooked area or at least something that hasn't yet gotten the attention it was always about microservices and orchestration of services that are long running Services running servers whether it is web servers database servers what not but if you are talking about running bad jobs yes there is a kubernetes job go do something about it the community has come up with other things things like Argo um that have actually enabled some of this but still not kind of sort of being the first class citizens for batch workloads what's your take on kind of like given that a lot of machine learning workloads are batch and kubernetes offers some support but not a lot is that is that is there an impedance mismatch there somewhere is there something better that can happen I mean I'd say a lot of machine learning 1.0 workloads for batch and I would say more the 2.0 like generative AI most thoughts actually streaming so I see far I mean there still is some batch use cases but I think we're moving away from dags primarily and to kind of like chains and whatnot I think that's changed significantly I would say you know what kubernetes is built for which is microservices you can almost think of LMS you know as they grow as microservices where you have you know these different parts of the models these partitions that need to talk to each other so you need to schedule containers and network them together right it's really the same principles it's just kind of being thought of a different way so I think kubernetes actually fits that use case really well interesting interesting that's that's good use case that's a good point what about things like as we go deeper into kind of llms and you know one of the backbones almost so to speak of llms has been use of gpus pretty much every llm that at least from things that have you know generally been talked about always talk about how they've been trained on llm or sorry on gpus and then all these other conversations about GPU shortages and whatnot you know does do you think kubernetes helps there are there things that are better because of kubernetes for using gpus and being able to kind of train models there or are there there are lots of other companies nowadays which don't run on kubernetes but still provide you with GPU access what what does that happen what do you guys think um yeah I mean I think the biggest thing is cost right so I've worked a lot with startups you know who are trying to build generative AI applications and you know some of them don't want to use of an AI for you know the Privacy reasons or whatnot I and yeah the constitute views is horrible right I mean to run these models it's super expensive to keep these things running the start of time is super long ride so it's like you shut it off you can have you know several minutes to try and put a note back up and load the model weights into the GPU like that can all take a significant amount of time so kubernetes helps with us where you know obviously we can you know scale nodes up and down uh you know try to make that startup time is as fast as possible um but I think there's a lot more work that can be done here right now I'm kind of looking at you know it could be like hot swap Laura's or Q lauras in the models right so could we instead of like having to wait to pull up a whole node could we just run these baseballs and then hot swap foreign for special like fine tuning capabilities so it's kind of work where I'm heading with it right now yeah um uh yeah just adding uh on top of what Patrick said right uh spot on about cost being one of the bigger constraints and problems that needs to be solved with GPU access right so I think um in the first sort of wave um in the first set of set of changes and and these changes are all you know weeks apart but still uh just support for gpus uh was added so that you can run you can enable GPU node pools instead of your CPU node tools uh which enables at least some gpus to be used but obviously that's not enough then you know you have these newer things like spot instances being available um in in the various clouds that can really help with uh you know making your infrastructure run cheaper but even that's not enough because um just like with everything in infrastructure the devil is in the details so if you look at you know components of kubernetes and this is where going back to what I said previously right it was designed for traditional workloads not necessarily machine learning workloads so going back details of how kubernetes work something like scheduler which I think Patrick briefly adjustable how kubernetes actually decide to schedule and provision nodes balls and everything else is uh can there can be a lot of optimization over there uh because obviously there's costs and there's also things like you know networking uh how how much data is being transferred between different nodes and different pods and where they are actually located um so secondly I mean if you look at Auto scaler again uh in fact even funnily enough for traditional workloads as well we used to say Auto skater is a you know easiest way where you can shoot yourself in the foot uh I think it's even harder to make all those failures work with machine learning uh and llm workloads so I think there's a lot of work where Temporaries can be provided you know defaults can be configured well um and the right sort of parameters can be optimized when Auto scaling an llm based workload yeah just uh carrying that that thread of Auto scaling um a couple of weeks ago um we participated in a hackathon uh where we trained or fine-tuned a large language model on top of kubernetes with rlhf using the Deep speed library right and originally our aim was like Hey we're going to do this Auto scaling thing and enable you know uh make sure that that works the the real challenge right now is gpus are super uh rare to get Hands-On so even if you want to order sales you're not going to get it so um a smart way to think about this is negotiate with your cloud provider to get to reserve those nodes that you need in advance so that your workloads at least are are deploying um and then on the uh yeah basically that's that's the point with the auto scaling right I think the other side where uh the costing issue that Patrick mentioned and um we we talked about I think and this is speculative on my part so um we'll know in a couple of months whether this is true or not so as you know that h100s are going to be released isn't generally available or starting to get adopted and we're going to see The Horde of people start migrating to that and guess what that means a100s are going to be more available cheaper and so on right and so maybe the market will self-correct with availability but you know remains to be seen I think for now um either you can wait and watch for two months or you know plan out a strategy where you where you reserve your instances and instead of thinking about like over optimizing on auto scaling um try to get a repeatable platform in play which uh in place on which you can rapidly deploy and iterate on I think the iteration part of your model is where should you should be focused and not on the iterative like the how correct or how awesome your kubernetes platform looks like interesting so thinking about large language models so Rahul brought up this point a little earlier that if you think about large language models kind of sort of there are two or three kind of big domains of work one is companies or teams starting to train or doing training a large language model completely from the ground up like literally everything from the data to training the whole thing it can sometimes take months open AI for example has documented how it took I think what three and a half months or something for them to you know get it completely right and there is fine tuning of the models and then there is also prompt engineering so two or three different domains of work so Rahul how does kubernetes play a role in this does it make it better does it is any one type of work better suited for kubernetes versus the other does that make a difference and maybe even with gpus and and I think as as Architects and people who are you know designing these platform level Solutions there's no one right answer there's always trade-offs so the trade-off here is you know kubernetes is a great platform for you to be vendor agnostic move fast and and and you know be able to automate a lot of your workloads but it's not it comes with its own kind of challenges right number one being if you're a foundational model company your data is in the petabytes if you are um if you are trying to fine-tune your models you know obviously you can use um PFT and Laura to kind of you know fine tune only a small part but then you still have to have the whole model in memory guess what these models are really large um the containers that these would run on go in the tens of gigabytes and so you know Big Data bigger models and everything on kubernetes just becomes a you know a heavy lifting kind of effort and so it comes with its own problems just yesterday uh I had to like SSH into the node to look at the journal Kettle to see what's going on like hey why is this pod not pulling fast enough because some nodes are those five like 15 seconds some nodes it's like taking a minute and a half and so you know these kind of especially when you're talking about like 15 GB container sizes things start getting really really hard and kind of sort of everything then you know follows that larger containers can mean bigger startup times they also mean that it is potentially possible that you know pods may not come up because that much disk space is not available and then you know many many things that the cost can go up because you're pulling down especially if you're every part that starts is pulling down 15 GB of data for every time like you crashing that off they're just like adding on to cost complexity and everything so is it can you maybe quickly talk a little bit about what do you think is a is a good you know data scientist experience if they have to use kubernetes like what would be a good I'm I'm guessing data scientists are probably not very very enthusiastic about wanting to deal with you know deployments and Demon sets and uh DNS resolution and cool DNS and whatnot I I think I can maybe answer it in in one uh sentence on the best experience for data scientists to deal with kubernetes is not dealing with kubernetes um and even organizationally right if you look at this if you uh in Raul and Patrick feel for your chime in here I think organizationally what I've seen typically is there's a different deployment team versus a data science team that actually deploys these models and days with all these complexities uh in fact funny you know some some organizations were telling me that in the past at least with traditional machine learning workloads they would write their models in Python there would be a team converting that into like code and then finally deploy like this just sounds you know 100 steps for me uh that are not optimized um curious to hear your thoughts yeah I you know I think obviously traditionally it was used to try and make that work as good as possible I think what's interesting guys we're seeing it really stretch like we don't really need data scientists as much anymore right it's like these models are so smart that now I'm just regular old Engineers can pick them up and start building AI apps right so I think there's this like shift to where you know I think Python's slight deal but I think a lot of times it's going to be more actually like taking those models and like meeting Engineers wherever they are and one good point I think about all of this is that you know this is almost kind of on the other side of it which is on the one side there is complexity but the other side of that kubernetes gives you is is is composibility like there are so many different tools and services that we talked about literally in these what eight minutes that we've had this conversation about different libraries different tools and the amount of innovation happening there deep speed you know Laura theft so many of them uh coming up it's uh it's easier to get these up and running it's easier to containerize this and kind of sort of deal with them in within containers because with the isolation boundaries compared to let's say saying that here is here is one GPU instance four of you go and SSH to that box and run whatever you want I'm sure everyone will step on each other's toes saying I want python 3.9 someone else wants lead seed version 4.7 and then it's just like it'll be a chaos so there are benefits of kubernetes there is also the complexity of it but the composability and the extensibility that you get with it to some extent maybe even isolation that can be that is very that's very appealing but it probably comes a little later do you think we are there yet do you think there are teams who actually can benefit from all this you know large-scale platforms that can be built on kubernetes yeah AI platforms yeah so so one quick point that I think it's um obviously is switching into nodes kind of makes makes sense in in ways but if you look at um the non-foundational model and we've talked about most of the these foundational models and fine-tuning kind of approaches here uh so far the others which I think Patrick was alluding to was like let's say you wanted to spin up a q a uh bot that you've created with some Vector DB and some uh store you know data going in there answers maybe users um azure's uh openai bot are anthropics new stock to compliant uh but like creating these uh services that can help support these kind of applications can also also work with kubernetes right I think that's that's like the flexibility at the end of the day kubernetes is not the Silver Bullet like like we we you know everybody here agrees it's it's not but you know there are these challenges that if you were doing this Challenge on a regular node that you have set up the community support is just not there you try to Google search you try to ask Chad GPD to help you out it's not there I think kubernetes is just way more adoption and um you know the support that can allow you to debug and I think that is part of the developer experience as well and not just having a Dev box and sshing but also if you get stuck how do you unblock yourself is sorry yeah I think it's still best in the business of uh you know having multiple services and like Royal Raul just said you know you might be running a vector database you might have a web app you know and you might have your own lab and you need those all to coordinate to build your application so there's still nothing as good as kubernetes like run those all in one place orchestrate them together and have like one coherent API into that yeah I think you mentioned these three four areas as well right in the um in the previous uh question on uh prompt engineering versus fine tuning versus training and then these service oriented architectures I think the last bucket of that there is uh inferencing right um out of all these sort of workloads for sure uh um kubernetes right now is far more um uh you know scaled uh battle tested I would say for the training and the fine tuning paths versus the inferencing part but coming down to the inferencing part as well when you have and I think um at least the applications I've seen are still sort of figuring out exactly what that architecture would look like on having an external Vector maybe not just for training but also for um actual inferencing uh that sort of also fits into the service or Indian architecture for which kubernetes was basically developed interesting so so having said all of this I think like you know is there I'm sure there are lots of people who are maybe just starting out or who've always kind of sort of felt like yes we've been able to train models either locally or using some service but we want to kind of standardize on something and kubernetes seems like you know an industry standard almost what's a good place to start like what could be your recommendation for where to begin and how would they kind of you know how would they go about doing this I'm almost hesitant to say this but you know a lot of companies have come up with simpler um you know one line kind of commands that um you know data scientists can run to get started if they don't have a team that can help you set up the platform right so you just write your python code pytorch code or whatever you want for your training the last language model and then just have a one line command they will take care of containerizing that code and deploying it on on on their kubernetes thing right and so they are kind of taking care of the orchestration for you I think that's a good place to start if you um if you really want to get started and you don't have a team supporting you but I would almost be hesitant to say that that's the best way in which you'd do a repeatable kind of input so once you deploy like manjot's good point about inferences right once once that large language model is an inference um then you have this human feedback loop that you need to think about right so or people are uploading or downloading the reply that the agent is giving okay what do we need to iteratively improve on and now suddenly your data is again leaving your VPC to go into some other Cloud how do you connect all of that eventually you're going to have to have a robust platform on top of kubernetes to support all that and maybe it's not a bad idea to you know just have some help from from from somebody who's you know helping deploy this on top of your kubernetes platform or or you know learn it yourself I think there's enough resources available in upcoming that that would be super helpful so it's a fair point I think what you're saying is there is there is a layer that can be built on top that to some extent hides some of these complexities that people are talking about and it can provide that experience that people directly you know you write your code and like whatever run it with some special command or decorators or whatnot and it eventually goes to the back end and runs it on kubernetes as time goes by you might want to be interested in what is actually happening behind the scenes and that will be a good introduction to kind of start with something basic and then do this uh given that we like Dimitrius is going to tell us that in a few minutes I also want to touch up a little bit on a little bit of these non-technical aspects especially manjot like your experience I'm sure there are company people who have been thinking about you know building companies and products that kind of sort of Aid and assist data science and ml applications like what do you see on the non-technical side like kubernetes for llms is that does that seem like things that have you know that where products can be built that can you know be good viable businesses going forward yeah so uh I uh actually this is uh this is a great question and a great segue from the previous one as well where there was utter silence when you asked us best practices I think the main reason for that utter silence immunity was you know sweet there's no real answer right now I think this space is moving so fast and everything is so recent that uh people are still figuring out okay what are the you know best tech stacks for hosting an llm application in production uh what is the best way to use kubernetes or not like what are the Alternatives right uh so so every day you see like new things coming up like there's Kobe with the hosted model this replicate which is I completely like you don't have to care about anything which Rahul touched upon right there's so much happening uh and to answer your question on are there you know businesses possible uh I think for sure there is if there were ever a white face to create like you know a cloud offering from scratch uh this is a this is a very interesting white space uh there's such a dose of gpus right now uh there is also lack of knowledge on what is the best way to host manage uh and serve your applications right so there is a real white space right now in terms of a being the thought leader that hey you know what like you you care about what are the big problems like in serving in random application like abroad right so get a kubernetes hallucinations right that's one of the biggest core challenges for Enterprises second big problem is cost which Patrick also mentioned uh and last can be just reliability and latency so when someone thinks about creating the next you know uh infrastructure platform uh to serve llm applications these are the broad three four problems that they have to tackle and there is a good chance that the answer under the hood will be kubernetes but the I think the idea is to abstract these things away versus uh display all the moving paths interesting what do you think about the case I'd also say it really depends on the size of the organization right startups are going to really wrestle with the cost kubernetes and the cost of homes is already really high and then if you had kubernetes on top of that it gets even higher so you know if you compare that to running something like a Cloud Player worker using open AI API like it's a lot cheaper right so I think you have to kind of weigh like the size of the organization and you know the capital they have available interesting um I'll take one question from the audience here there is a question about can you describe the pros and cons of kubernetes versus the pros and cons for kubernetes for uh using kubernetes for llm inferencing so kubernetes for training versus kubernetes For llm inferencing What are the trade-offs uh I think Raul you've done both of these to some extent yeah and and this is this is a long question but uh long answer which I'll try to keep short I think when I'm I'm advising my clients um I'm telling them about like I'm I'm optimizing for velocity um and uh you know having a platform that is easily like making that's helping you deploy models faster and I think that is one thing that I think kubernetes is great on for training um for the inference side I'd I'd refer to Manchester she had raised that point yeah no I think the infant side right I um we've already discussed some of the challenges present uh in managing a model as well as challenges in kubernetes specific components on how they are designed today versus how they're supposed to work I think one more uh challenge I'll mentioned is um a lot of these uh uh you know a lot of the llm workloads might be using these you know specific libraries and divide uh drivers that work with accelerate or some specific Hardware component right and the whole point of kubernetes uh and containers is to abstract all of that away so I think the world is still sort of also uh uh struggling right now finding that sweet spot of okay I need to abstract away Hardware but like this world is not quite abstracted away 100 um so uh inferencing I think there's a lot of work to be done uh if someone were to start building an application today uh you are better off picking up some of these open source Solutions uh present that at least abstract some of that complexity away but I think we still see a lot of development potentially new products and services being built here got it and there is also a little bit of this uh inferencing at times can can lead you know real time uh online inferences versus training being kind of bad job so it goes back to one of the original earlier questions we talked about that kubernetes for bad jobs is I mean it's of course it's there it's useful versus kubernetes for inferencing can be like you know server-based processes and whatnot so there'll be some of that that comes about uh you mentioned about the abstractions like one of the abstractions that Avail that's available today is an open multiple open source projects out there one of them that at least I'm involved in is called meta flows that you can check that out um uh in case people are interested in kind of you know learning about abstractions that can deal with kubernetes or hide the complexity of kubernetes but uh but yeah I think this was this was great and you know we right on Queue we have we have someone we have a person with a red hat here I've raided the ship and I have come to steal the show back that was absolutely incredible thank you all and I will remind everyone well Raul I may need to make new shirts now like kubernetes is a gateway drug or I guess kubernetes is the way to stay high uh that is I you can't really see it because of the lighting there maybe you might be able to see it I'll throw up you know you know I I I wanted to make a joke about kind of sort of wearing your heart on your sleeve but you know your you know kubernetes is a gateway drug trumps me there yeah it's hard to follow that one man that is so true I did a little bit of uh I did a wardrobe change and I busted out the I hallucinate more than chat GPT shirt that you can get here in case anyone wants to grab a copy of that we've got it here folks on the panel I must say awesome and Raul I'm gonna give you a shout out right now because you are running the after party in San Francisco you got like a bus or what do you got that's right so if you're in San Francisco um you'd probably know Mission Bay has this uh awesome outdoor space called uh Spar social the sun outside it's a nice day for us to you know Mingo we have a bus a double decker bus that we'll be meeting up at there's 120 people already signed up so I I think we can get a couple of dozen more so just join us uh spark social is a big place this has been an awesome panel uh thank you everybody for your valuable a lot of times yes yes thank you so much everyone this was a lot of fun uh and a lot of interesting conversations over to your Demetrius yeah and any more questions for these kubernetes experts the kids as they like to call I think that's what the cool kids call it so I'm gonna pick that up go ahead and throw it in the chat throw it into slack wherever it may be I'll see you all later and hopefully thank you [Music] see you in a few weeks because most of you except for manage were in San Francisco ah yes Patrick not not really