foreign how's it going hey hi so nice to meet you it's going good how are you doing nice to meet you too let me take this giant QR code off the screen um let's see okay just changed it um all right let's see cool thank you so much for joining us um I think I have your slides up so we'll get started in just a moment um let's try it out yep oh these are oh these are slides oh yeah yeah awesome okay why don't you just let me know when you want me to go to the next one um so I I think I'm able to share um I think that's me sharing so what's up on the screen is what I'm sharing okay yeah okay I mean okay sounds good um here but I can try let me try yours real quick thank you everybody for being sharing with us through the technical difficulties it would not be a true virtual conference without them and thank you soham all right so let me try yours real quick see all right I think these are from your screen now yeah yeah awesome okay all right take it away okay thank you so much I'm really happy to be here and um yeah it's not easy giving a talk after mate and to speak uh but I you know I hope I can um uh you know say something that's uh that you still find useful um so you know my talk it's gonna be about um the challenges that I faced when I was building this llm product and and you know throughout these last um about six months of building this uh I've learned a lot of things about building the product and how we can actually um solve some of the challenges that that happened when deploying llm products so I'm hoping to you know like um tell you a bit about that and and hopefully um give you um a preface uh to some of the things that other people will be talking about in the conference later on so a bit about me um I'm I'm currently uh working as the lead um ml engineer at a fintech startup in Singapore called Sleek um apart from that I'm also an instructor at LinkedIn as well as Udacity so I've created a few courses um and uh and over the last few months I've been building this tool called speaker scribe that you can use to create um talks um uh talk proposals as well as like Workshop proposals and stuff like that and in the process like I'm hoping to learn a lot about deploying llms and um and um and what other standards like good standards and best practices for deploying llms um and also I'm I'm writing about what I'm learning throughout the process um at um at 10am.substack.com um so so I want to quickly cover like like three things basically one is like the challenges um that we are facing um with deploying llm applications uh the solutions that have worked for us so far um and also um End by talking a bit about the kind of products that I think are useful uh like the kind of useful products that you can build with llms uh so yeah so I want to start out by talking about um our biggest challenge which is um using llm apis uh I think one of the biggest challenge is uh is actually the lack of like slas or commitments on on endpoint uh up times and latencies uh from from API providers so for instance when we were building our application uh there were inconsistencies when we would get a result back from the API um this would cause uh uh and the problem with having like delays is um a lot of the tools that people are building with llms are for Creative applications um applications where you know if you don't give a response quickly uh you can't you kind of disrupt like a flow state that the user may have and um and having to wait you know even even a few extra seconds can uh can can be painful right um uh and and on top of that um the deprecation of apis actually kind of further add fuel to the fire um so when when openai um uh so so when we started building our application with um with the DaVinci zero zero two endpoint um it was working really well and um and we were able to um you know create a lot of grounds that were that worked and um and and we we had like this really good application uh but then they suddenly deprecated it and and when we tried moving to um to the new updated endpoint it took like a lot of time uh to adjust our application to make sure it was working at the same level um and so yeah so that's that's another issue that happens um and finally there's the issue of API costs um even even with the um reduction cost that open AI um announced yesterday um uh what happens is as your application kind of increases in complexity and in scope um you have to your prompts increase and and that causes an increase in your API costs which will kind of tend to um skydock it really quickly um and and you can't really rely on those API um uh uh you can really rely on these external apis because uh because of the costs uh so what you have to do instead is move on to other tasks like fine tuning um or maybe even actually training uh training your own custom model and then the second challenge that we had had to do with um how we structure prompts and how we parse and serve the results of the output to our clients um prompt engineering is not really an exact science and there's a lot of issues uh with uh with creating prompts because um so for instance issues we've had are like adding um uh apostrophe commas or you know like punctuations have uh usually like breaks a lot of prompts for us um and and so so it's it's really hard to uh make a prompt that um that works and then try to maintain it um hallucinations are also an issue and the problem with hallucinations especially now is that they're um they're really hard to spot as well um and and and so what we need to do to prevent that is have evaluation metrics for the outputs uh which which there aren't really um a lot of and um and it's also hard to evaluate a lot of your outputs as well um so uh so and and because it's hard to evaluate your outputs you can't really trust the output that the model is giving Which uh and and if you can't um trust it then it's very hard to serve it to a client because you don't know if it contains misinformation um another issue is especially with all the updates that um that open Ai and other llm providers are making with their models is um at least something that we've seen is that the outputs are not um creative enough sometimes um and and what that hap and when that happens um you don't want to serve like similar outputs to two people asking asking you to um also asking you to create like um similar proposals right um so so it's hard so so you have to engineer your prompts in such a way um that the outputs are creative you know play around with some of the parameters that open air provides um and finally um you have to deal with bias as well as incorrect data so um so what worked for us and some of the things that we are doing is uh with prompts what we've seen works really well is providing context to to the llm when creating prompts um so you know using few short prompting that that that helps a lot um so this is um a demo of speaker screen and so when you go and click the regenerate proposal um ideally you should also provide the context of of the previous output um so that helps the llm also be more creative and and make better outputs um there are some more complex um uh if uh prompt engineering techniques uh but what we've seen is anything like um anything after a Chain of Thought um kind of makes the prompts really huge as well as the outputs and and that increases your API costs as well um and regarding prompts prompts are now like your IP right um if you have a good problem you should save it um and you should um protect it because it's it's your IP and you should also version your prompts to see what what you know what changes how the changes are affecting the output and and you know maybe that might lead to something like Chrome tops or something later on as well um one way to provide context to llms and to also reduce like costs is to use Vector databases uh by Saving previous outputs like caching your outputs you can actually um save on like latent improved latency as well as you know by not having to um call the API you can save API costs as well um and then Vector databases also helped by finding context of uh finding context that you can provide to the llm to help improve its output as well as to reduce hallucinations um so for instance in our case for the speaker Skype product what we did was we uh we we created a database of past talks and past workshops that people submitted and and we use uh and whenever someone asks like um asks us to create a new a new proposal uh what we do is we provide um we use Vector databases to get the uh context of of the talks that we have in our database to to improve performance um and finally um you know like uh so so as we are starting to uh build more and more complex um products with llms um I think if you look at like the uh comic here um so I think right now we're kind of in like stage two or stage three of um of of of the uh complexity of of um building products with llms um and and the problem with using chains is that chains tend to fail um especially if the chains are like really long so so I think what you need to do is uh what worked for us is keeping change short um also uh by having changed long it increases the costs as well as latency and complexity which which is not great and finally um it helps to you not use agents agents right now they don't really work that well they're not um uh they're not reliable and reproducible um so in our in in our experience like agents don't really want that one um so bringing all that back together um how do you build like um a good and useful llm product right um and and what you need to do uh and and what you need and your llm product what it needs to do is uh make uh make sure that uh it um you need to make sure that people don't use transgivity and use your product instead right so how you do that is first of all you make it easier to access and that's something that for instance grammarly has done you know you can use grammarly go nowadays it's like it's similar to how we built this um this transgpt Chrome extension um so so just reduce the um effort it takes to go to chat GPT and then um and just provide it easier um also you can help make your application better than chat GPT by including domain knowledge so that's what we did with speakers screen and what we are doing at Sleek is we have like our own customer data so so we can we can use that as context as well to improve the output of our models right and finally another thing that that you should do is uh provide like some additional services on top of of the llm output that GPD provides so one interesting thing that I've seen is people using chat people using llms to create decks right for decks you need like images which sensitivity doesn't do right now um yeah so I think that's pretty much it thank you so much um you can check us check us out at speakerscribe.com and also um uh you know follow me at um thank you so much awesome thank you so much [Music]