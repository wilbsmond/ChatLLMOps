next one is about building red pajama um and I will bring the pool to the stage hello how's it going all right did you do any dancing on your end oh no I was quite inspired yeah quite talented uh I think you could see that he is a father of two with the unicorn and the bassinet in the background so right he's doing it all well here are the slides really pumped to hear about building red pajama thanks so much yep uh thanks for having me good afternoon everyone prakash uh uh co-founders here together we are focused on making AI more accessible and efficient and today I'm taught we'll be talking about building red pajama and open source llms um so we've you know seen this amazing progress in machine learning and AI over the uh the last few years we have these incredible models like stable diffusion and opening CPT that can generate you know coherent images and and text and it really feels like a step function has happened um but there's really been continuous progress uh in the field and we have principal ways of uh you know measuring this progress with benchmarks so if you look at the imagenet uh Benchmark we've achieved 30 points of accuracy in the last decade um if you look at the squad Benchmark which is uh NLP Benchmark we have achieved 25 points of accuracy in in the last seven years and this is pretty incredible we are hitting High 90s and accuracy on these systems and they're opening up a lot of applications for AI um and we have reasons to believe that this is a trend that is going to continue for a long time to come uh and what's behind this uh you know there are a lot of factors from research to experimentation um you know to to a capital that has gone into AI but uh really the two uh driving forces behind progress in AI are can be boiled down to data and compute and uh there's a couple of ways of thinking about uh the scale in this one of them is open AI scaling laws which were published in the paper a couple of years ago which showed that as the data size gets bigger the model gets better it's almost a linear relationship here and the counterpart of this on the right is the compute graph which shows you know as we are pumping more data through the training framework of these models we need more compute and we are now using prodigious amounts of compute to find these models but this is not you know this is not wasted compute a really every flop is producing um you know some proportional gain in quality of the models and this reflects in um data sets that kind of the sizes if you look at the largest data set we had 10 years ago which is available to the community in the open was the imagenet data set with three million images and today image models are trained on three billion images or more this is three hours of magnitude increase and the counterpart on the compute side to train a model like GP G3 um you know you need 1000 800 gpus running continuously for a month and this is not even the most expensive model that you can train today so it's really the combination of uh scaling data and scale and compute that is giving us the amazing AI models we have today uh and and this is this is the reason we think that this will continue to happen um from a practitioner and researcher's perspective this sort of manifests as a tension between uh three things and again this has been the case for for the last decade and likely will be the case for the next decade um uh and and you know one of the components is data as the data volume and complexity goes up invariably the data quality drops and the cost of cleaning and acquiring this data goes up uh and to sort of contend with a higher volume and higher complexity data uh we need models that are bigger and more complex and uh both of these things then put pressure on the infrastructure side where we need more flops uh to train these models and more memory to hold them for for inference and this has led to as we've seen uh specialization in GPU Hardware we have you know systolic arrays and tensor cores that that improves the speed of Matrix multiplications by four times and just as importantly scale out which is uh using many gpus in a distributed setting and and doing that effectively and it's you know kind of the uh in this sort of uh setup is where the Llama moment has happened if you look at the Llama paper they used uh a trillion tokens from a diverse set of data to train the Llama model it used tens of thousands of GPU hours to do this and you know produce this really sort of high quality model that we have and we were inspired by by this uh you know partly uh if you look at the data set this is an open data set this is not just available to you know one of the largest companies in the world this is available to everyone is available to everyone here and are objective with pajamas you know see if we can produce a you know create a fully reproducible open model uh with llama quality and more importantly go beyond beyond this quality um when we talk about open models you know open weights are obviously important but we think this openness on these four dimensions that's critical for monotonic progress in the quality of open models we need open weights so they can be used anywhere they can be fine-tuned they can be used in fully in fully private on-prem settings or on devices we also want open license so these models can be used in commercial applications and we want open data this really kind of gives us a way to think about what's gone into the model there is there is transparency associated with it and this data can then be refined and filtered and processed by the community to develop better models or fit specific applications and you know if there's concerns about some data that can be removed from from from the data set and just as importantly we think that recipe that creates the data should also be open because this lets us one reproduce this model from scratch but it also gives Community the tools to improve the data recipes create the same model in a different language and uh you know models that sort of fit these four pillars uh can really contribute to monotonic progress which is which is what we are looking for um so with detail about red pajama um on the red switch on my data set we followed the Llama paper uh it has seven different slices of data uh we also carefully followed the process described to filter this data to uh um uh you know transform it uh and uh all of that code is also available with with the data set uh and we we tuned the hyper parameters to select roughly the same number of tokens that allow a paper uh describe they use from different uh different slices and for the model you know we took a a trillion tokens from this data set we use the pithia architecture which is one of the most well documented or open architecture Transformer architectures available uh and we also instruct you and chat during these models to align them to Applications this is really very important because it brings out um you know uh quality uh uh in the latent space of of these models a lot of this was done uh in fact all of it was done with open source tools uh we used slurm and Spark for data processing we use a deeper speed for pretending and the gathers open chat gets fine tuning system for uh instruct tuning and chat tuning these models um use the helm Benchmark which is a comprehensive Benchmark around generative capabilities of llms as well as the LM harness model Benchmark published by a Luther which really focuses more on the log prop side and the model was built on the summit supercomputer at Oak Ridge National Labs with the generous Grant from the inside program and use 3300 gpus and everything that was uh you know from from the data to the final model checkpoints uh to intermediate checkpoints were published on hugging face and Apache to uh and the model works well it's uh uh you know one of the more uh surprising things about this was that we we built a three billion scale model and a seven million scale model in the first run and the three billion scale model uh is just six points behind the base model is just six points behind llama seven b and uh three billion instruction model is just one point behind uh and this is fantastic because you can quantize this model further we've worked with uh the llama.c P folks and the mlc folks to bring this to CPUs and um you know the iPhone you can uh download mlc chat app today to try out the chat version of this model uh and it's fantastic because you can use these you can use this model in few short mode uh to develop applications in context where this was just not possible to do before this model existed and the 7B model is also interesting uh you know a couple of things to note about this the one is that we have made uh progress over the last generation of open models it's uh the base model is 2.4 points ahead of gptj which is an excellent open model from a Luther um uh which will also note that open models including MPT Falcon and red pajama are lagging behind uh 3.3 to Four Points behind the Llama model um with instruction tuning uh the open models become the represent open model is two point side of llama so it really kind of provides a alternative for flu shot applications like sentiment analysis and classification um data extraction and for these applications is probably the best model that exists uh in open source today and this is super promising and you know and we have we still have a long way to go uh uh to get to Lama and to increase the quality of these models Beyond lava one of the interesting things about open data is that it really lets you do uh you know principled analysis on the quality of the model so one of the things that we spend a lot of time doing is contamination analysis especially on the instruct data set because it's a really important part of model quality and aggressibility contaminants it uh uh against benchmarks and Helm and this is important because uh uh if there's contamination in the data it can kind of give you a false sense of uh uh sort of performance in these models and uh uh you know this having the open data allows us to do this sort of effort and allows anyone else to who's using it and applying it to Downstream tasks to verify that the models have that it has been properly decontaminated but as I said we we still have a lot of work to do and I want to touch upon some of the Avenues for this you know a big part of uh monotonic progress is evaluation we need to know how well these models are performing uh hugging face has in for instance an open llm leaderboard which is uh really fun and um evaluates lots of models against a set of benchmarks which shows Falcon llama and red pajama are sort of roughly the same on The mmlu Benchmark but a few days ago there was a a technical report from a great set of researchers that try to reproduce mmlu and found very different results on the right where the open models are like behind llama uh the model does welder but this reference is really sort of uh problematic in uh you know we want these benchmarks to act as North Stars for model development and we have to find better ways of doing these evaluations making sure that reproducible they're unbiased um and probably is you know possibly other strategies like human evaluation which is uh uh you know which can be fairly hard to scale uh so we need to do a lot of work around this because once we have uh effective evaluation methods as a community uh you know we can really sort of Drive progress around model development another uh interesting area is data quality one of the things that we did for red pajama data set was to use the uh the mixture description in the Llama paper and um uh it's pretty clear that if you wait these these slices of data differently you get you know more quality from from the final artifact um and uh we also likely need to not just wait these next you know these slices also take sub slices of bigger uh sizes like common crawl and rate those and have to do this in a sort of principal Manner and there's a lot of value in this duplication is another interesting area um sorry Bruce just published uh data set called slim pajama which is uh you know more aggressively dedup version of the data set it's about half the size uh again we have to figure out what's the right strategy here how much what's the sort of sweet spot for the duplication and how that interacts with the data mixtures that we design and then there are problems of data transformation the biasing data you know detoxifying this data and we think this is really the going to be in many ways the Cornerstone of progress towards better better models in in over the next few years foreign another interesting and uh problem to solve is that of utilization uh as you are scaling out and using a larger number of gpus to train these models it becomes a challenge to use uh all those gpus effectively and because this graph shows an ideal run on the summit supercomputer and the real run for for red pajama and they see fairly close but you know once you get to 3000 GPU scale the difference means that 500 gpus are ideal you know idle which is yeah not ideal and uh in a property represents over a year like a few million dollars worth of computing resources uh there's a lot of work happening in this area around communication optimization together is particularly focused uh uh in the in this field and there is you know there are open problems of how to take these techniques and apply them into training harnesses so that uh you know you you can use them for training different kinds of models without sort of doing custom work for each type of training task hmm some of the work that we've done around this uh is uh now being used by US for instruct tuning models it's a algorithm called cocktail SGD that is able to do training on you know either over low Network conditions in distributed settings or get really sort of Maximum utilization in high performance Computing supercomputer type training setups and this this is a talk in its own right so I'm not going to dig into details it's a paper in icml this year as well as a source code release that's coming in as I mentioned we are now instruct tuning uh all the models and together with this approach and looking forward to uh pre-training models with coffee list releasing um so hopefully this gives you a little bit of a window into you know the process of training these models how we are using open data sets and open software uh and in case of red pajama open you know public infrastructure uh to build these uh but that said the uh models that we have today the artifacts that we have in hand are very powerful and can be used effectively for real world applications there are a few strategies that uh are particularly effective uh one of them is few short prompting it's just the most lightweight way to boost model quality where you provide one or two examples of your task along with your prompt uh the red pajama 3B and 7B instruct models are you know specifically optimized for this if you want to go further you can create your own instruct data start with a base model and instruct you in that model with your own data we've seen several examples where people have um instruct tuned with their own data and then apply fuchsia on top of that to get quality from 3B and 7B models that they can't get from gpd4 so this is a really ineffective way to get um uh you know high quality high accuracy for your tasks over these models and you can also if you have a large amount of data you can continue to pre-train these uh starting and you can mix the original data set which is an advantage with open data into your data mixture so you don't have problems of catastrophic forgetting Etc and find uh uh you know prepare a model that's really kind of designed for your tasks um there are many deployment options now you can quantize these models further uh to sort of fit your infrastructure application and performance needs uh and because the weights are open and the licenses are permissive the search really gives you a lot of different options and finally I uh want to mention that we have used uh taking a lot of the optimization work that we've done and uh created our AI specific cloud service which has thousands of gpus has official support for inference model hosting fine tuning training we're supporting many users in beta today and uh you know if you're focused on building your application you can use a service like together as a iCloud to manage all the production needs and I'm happy uh to announce that uh you know anyone watching this can go to this link and contact us for Early Access that's all I have awesome thank you so much and let's take a moment and see if anybody has some questions in the chat um otherwise um what's the best way for people to reach you follow your work is it LinkedIn or whatever yeah LinkedIn is great uh you can also um you know uh uh send me an email at Ripple together.xyz um you know and and write to us on using the contact form uh the store that and um will definitely contact anyone who writes there awesome that's great yeah it's been so interesting to kind of learn about like the proprietary models and the open source models and kind of the the different sort of pros and cons that people need to weigh when considering one or the other um you know I think I think the uh some of the sort of value that we see in open models is around you know higher higher ability to customize these models on your own data there is uh you can achieve better privacy if you are running them uh you know in your own environment or your Cloud instance that only you have access to and no one is able to see that data you can be sure that you did as private and you know also control uh what's happening with these models are often fairly important and strategic business processes that are being encoded into um you know into into prompts and there's customer data that's going uh against those problems and I think as uh it becomes a more important part of you know uh organizational sort of uh uh you know process having control of those models will become more important to more people yeah definitely and the explainability aspects of it as well transparency in explainability so uh I think important ideas that you can achieve with open better today than you can with closed yeah that's very cool that's awesome well thank you so much and I think we have to cut over to our next and final speaker um but it was awesome having you having you on today and all these talks are being recorded and we're going to share this with the community oh man okay of course now the questions are streaming in great um quick question Caesar he's asking why do you suggest mixing the data sets can't we just fine tune RP with our private data set you can and and uh I I think that's the great first thing to try um there are cases if your data set is large then um especially for smaller scale models you sometimes get this uh you know sort of catastrophic forgetting where they forget some of the uh things that they've learned from the original data set so it's a practice that's data mixing practice is often used in this context where you don't want the model to forget what uh it has learned from from the previous data and but it's not it's probably not a concern if it's you know a few thousand examples and you can get incredible amount of customization with a few thousand examples yeah that's great Caesar I hope that answered your question um we have another one from uh Manpreet he says these llama based models don't distribute the final weights but differentiate differential ones and the repos provide instructions to create the final model weights what is the rationale behind the that pattern what are the implications on license since the model weights do require the original weights yeah so the reason uh people are Distributing divs is that they don't have the rights to distribute the model um the model itself is under a restrictive research only license and I was also one of the reasons for us to create red pajama was that it would be a permissively licensed uh you know based model that does not have those restrictions and you could you know easily release uh models that are fine-tuned on top of it so it's it's really a way of sort of getting around the restrictive license that llama was under oh cool great question I'm pretty glad you brought that up um I'm not going to pronounce this name right and I apologize x i n k-u-n is asking does the AI cloud service work for private data or should companies with private data host their own open source models uh so we have designed REI Cloud to be you know uh very sort of aggressively uh one of the sort of big benefits of Open Source models is that you can run them in your private um you know in your private instances of Statute of control that said these models can really run anywhere they can run a new VPC they can run home together's Cloud they could even run on-prem and on devices great I hope that answers your question and if you guys have others maybe find people in the chat um or at some of the the places that he mentioned earlier so thank you so much um this was awesome thank you so much thanks for having me of course [Music]