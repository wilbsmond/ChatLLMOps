hello let's see we have the George Natalya Sahar and look at that thank you all so much for joining us I think this is everybody we're missing one person ah we're missing ah oh man Sam you're the most important here he is I'll let you all take it away thank you so much for joining us all right hey thanks so much Lily I am in Hardy agreement this is a very exciting topic and one that I uh get a lot of questions and comments around hey llms are super exciting but how do we actually build products around them and that is what we're going to dig into in this panel uh I will quickly introduce our panelists so that we can dig right into the heart of the discussion uh we've got a Smitha rothas A Smith is a machine learning engineer at prompt Ops where she's applying llms to Enterprise observability for devops engineers we've got George Matthew George is the managing director at Insight Partners where he's invested in companies like weights and biases and Fiddler we've got Natalia Barina a former AI product leader at meta where she focused on issues like transparency control and explainability and we've got Sahar Moore who's currently at stripe where he's leading the work around making Ella Lambs a thing at the company I am of course Sam charrington I Am host of the twiml AI podcast uh if you've not uh come across the podcast I encourage you to visit twimlai.com where your podcast platform of choice and check us out we've got 600 plus episodes on ML and AI including lots on llms and generative AI for you to dig into uh I asked our panelists for their spiciest takes on llm and uh we've got some really interesting responses there and we're going to start there uh just to get the we'll start it spicy uh Natalia why don't you kick us off with yours the hallucinations are a feature and not a bug um why people lie we can't verify even what people say so why are we expect that we'll be able to verify what machines are saying and how can we even differentiate uh truths from LIE there's a lot of gray area in this um my again the other part of it is I don't think we'll ever get rid of hallucinations um it's a Fool's errand to try instead the right thing to do is to use llns for The Right Use cases where they can shine and in particular that means around creativity and Inspirations and uh yeah that's my hot spicy take awesome Sahara why don't you get next yeah first I uh also be with Natalia that illumination is going to be stick around for some time for me to have great ways to mitigate those and so Maya and I actually think that um open source language models are the future and we got this recent paper from Berkeley about the false promises of Open Source synonyms but at the same time since meta's release in last February of llama we can think about our open source nlms can be quite powerful so first they can run at the edge and which is a great fit for everything that is privacy related use cases and also independent of internet connection we have like great work like moclm and also llama CPP it gets us really close to being able to run these amazing Technologies on at the edge developers can fine tune them for specific purposes so you can achieve better performance for Less compute and less latency and they also give you a lot of flexibility without predefined constraints or policies which is sometimes an issue for example when you use the open Ai and API and Licensing is no longer an issue we got like some really strong powerful llms and that are now commercially permissible like Falcon and Microsoft release um with this paper Orca a few weeks ago that basically shows how smaller LMS and open source LMS can also perform well and when you imitate the right things and not necessarily the style like maybe other open source of the lamps so an exciting features and looking forward to see what people will do with those um open sources yeah yeah my spicy take is that you know we're all here because of the generalized lens everyone's talking about it but I think the future is going to be very domain specific um specialized models where each company each organization's data their propriety data is going to be the key going forward everyone's going to have their own data have their own specialized models and they're going to release these as like a model as a service where the date the proprietary data is going to be the big thing awesome awesome and George I love how all of our commentary is like somewhat interrelated to each other mine is that we're going to run out of public Corpus to be able to train these large language models at scale and that's going to happen in the next half a decade when let's just say the GPT 5 style models start to emerge mainly because you know we're approximately training on 20 of the human Corpus and we've exponentialized every last run of generative pre-trained model on scale up to this moment so if there's another 10x happening in terms of where they extended the training data is going to come into training the next generation of model we're literally just going to run out of publicly available human Corpus and so coming back to a speed test Point as well as the hardest point a moment ago we are going to move to a world where private data particularly on smaller form factor large language models likely emergent around open source which is where some of these small form factor large actors models are most relevant is um where this sort of half a decade and Beyond is going to go for the future of building Transformer based architectures now are these spicy takes so spicy that uh there's some argument uh that anyone on the panel would like to pick on any of the takes we've heard that far or anyone in the audience for that matter anyone jumping out to uh disagree so I could go I'm not disagreeing but there is another point that models you know like companies where like right now we're seeing so many companies out there that they're training they're not training these models but they're using LMS into their products without any like without a huge ml team right there's no management of VR training your whole ml pipeline your whole ml infrastructure and with these open source like these apis that openai cohere have it's so easy for these companies that don't have an ml expertise to include llms into their product so I do see it coming from that side too there's a role for services yeah got it yeah I think if you were taking the other side of this argument you could make the case that hey the quality of these smaller form factor open source models might not be ever as good enough as the large research houses and what they're effectively producing and over time the research houses that are producing these large language models are going to have additional capabilities you know like you know the function libraries that emerge like the tokens that emerged about a few months ago where the there's just getting to be a general arms race where the open source community and the use of you know called the sort of public data sets is not going to be enough uh and you're going to see you know the the research habits take on a lot more workload when it comes to leveraging private data multimodal sets of data and uh there's likely a chance that like you know sort of the things that are kind of being predicted at least as we we indicated uh could have an alternative future so you could kind of argue very easily for the the countervailing point here that this is all going to stay um very much in the hands of three or four you know well-funded research shops um being open AI clicky or anthropic AI 21 as the the continued leaders in this space but um it doesn't seem that way right now it does seem after is there there's a prevalence towards small and four Factor models and open source in particular actually just maybe adding on that is if we look at timeline so I actually think that at some point it will be open source models will be good enough so now we have GPT 3.5 we already have models reaching this level of performance so maybe in one year from now we will have a GPT 4 or even more than that potentially I know it's kind of hard to think about it at this point but if we take history so far and we believe that some of those papers about open source models achieve this performance so I think it's a matter of time we get there and then the other question is potentially as we are building products in production what are the requirements for such products do we actually need a gpt4 like model to do question entering on documents or answer very specific questions so I think that as we build more Peaks and travels in this space we improve on evaluation we better understand what are the use cases we would like to serve we can get to better results even with smaller and open source models but this will be an iterative process for sure well we'll touch on all of those themes uh and in particular model selection a little bit later I'm I want to take us to one of the very first issues that a product builder needs to think about and that is the degree to which their use case is a good fit for llms right now llms are this shiny Magical Hammer and everything is a nail just waiting to be smacked with it um but Natalia you've got some really interesting experiences thinking about applicable use cases for llms and AI generally and I'd love to hear you time on what you've um you know Frameworks for thinking about this and in particular are there kind of recurring use cases that people get excited about that are ultimately doing a failure because they're not a good fit if you want to throw that spicy take in there yeah yeah absolutely so I think I would start off by thinking about what are the fundamental characteristics of llm that really make them shine and it's really about fluency so this is why llms are a big deal and when everyone's excited about them they have this uncanny ability to give answers that are surprisingly human-like this is why we have so much hype um and that's really what fluency is it's the ability to articulate very natural sounding answers that are human-like and that is their strength however on the other hand uh as we have already touched on we have also hallucinations or as other people like to call them confabulations where llms sound perfectly plausible however can be disastrously wrong um we can't trust them and so a good way to think about a framework for use cases around the is around two axes and you can visualize the accuracy axes kind of on the on X so it's your horizontal and I think about fluency as your vertical axes and then if you know you could you could kind of map your use cases along those two lines and if you do that you will see that places where you don't need accuracy but you might need you need fluency and you really benefit from llms or things like writing or greeting writing poem writing fiction writing children's stories so these are really creative uh use cases where you need lots of Inspirations then you could think about you know other on the other end you might have um use cases where you need a lot of accuracy but you don't need fluency so these are really about um being precise on the information and getting the right answer um so really to me those are kind of the two buckets the Creator and productivity use cases versus the second bucket which is more around decision making so I'd consider you know like where do your use cases fit along those two lines and once you map them I would recommend thinking about you know what is your business decision framework and there's really a few aspects of that and they are well our llm is going to make your business obsolete go all in and invest two can they boost your Revenue so this might be customer support call centers Etc then you have to think about do you build or buy third can you help can you get a competitive advantage and this is like that vertical scenario where that that is beautiful and I can't wait to see those come to fruition um and then finally you might be in a position where you just you know llms make no impact to your business and product and you could experiment but you don't really need to apply them so those are the two two kind of Frameworks one is around use cases and then the second one is around making that business decision and see what your situation is um before making an investment or before even beginning to experiment um which I I highly encourage everyone to experiment that's pretty and then that's where I'm going to conclude anyone want to uh jump in and elaborate yeah maybe I can add a few to my two cents so from my experience um that's right but also elsewhere I feel like the biggest Gap that will make us work more with the LMS or even feel more confident it actually again like evaluation and so in the world let's imagine a world where we every time we change the prompt we can immediately know how it well performs on like bias toxicity how often do they hallucinate and also how it correlates with our own performance and Benchmark then I will feel more confident deploying something like this into production so in a way it reminds me how like deep learning and machine learning what like a decade ago where it was really we were all like a bit scared to deploy something to production because my deal the results were not we don't expect but then evaluation monitoring like what we now call ml Ops really made it a lot more easy and and streamlined and therefore increase confidence in health deploying these kind of applications you know that fear is a real element that I see a lot um particularly on the part of you know product owners Executives they um you know it's easy to get enamored by a chat GPT demo but when you think about the brand risk uh that's possible when you're llm your customer facing llm starts um well hallucinating publicly it really uh you know put some pause uh and especially with the evaluation Frameworks not fully fully Baked has anyone come across any useful ways to kind of Drive team alignment and alleviate that kind of fear [Music] Natalia um I would recommend I'm going to plug for my own work but one of the things we developed um at meta was something called an AI System card and it's essentially a way to publicly report on the risks and to present the evaluation and actually this is what chat GPT did or open AI did with the latest version of GPT is they published their um AI System card and they literally went through and they said okay this is how we looked at all the safety risks this is how we evaluated talks City it's a very exhaustive document there's 40 pages um there's a lot in this space um and the other thing I would say is you know you have to think about evaluation is expensive evaluating your AR products is going to take time it's going to take resources so figure out like you know how much can you can you do but uh absolutely do it and I encourage everyone to go through and look at the gpt4 system card which is a really good guide for a very comprehensive evaluation of an llm and all sorts of issues that can go you know we mentioned privacy security um just robustness bias toxicity there's so much in here um yeah there's a lot to do yeah yeah uh prompting is obviously been raised quite a bit at this event so far um and it's core to llms and it's in a lot of ways a new way to create software and systems uh with a rule book that's being developed as we speak uh Smitha can you talk a little bit about your experience with uh building prompt based applications sure I would love to I think prompting is a whole topic on its own we could do like a whole panel just on that um I've let you know the past six months it's the whole like how we build the application it's changing so much with these prompts and you know with tools like Lang chain out there it's very easy to get a POC out over a weekend but now pushing that POC pushing your idea into production that's the hard part and there are a lot of things that you can actually do with prompts and let's take take for example like you know one use case that everyone's been talking about is like a document question answering system and you can provide a lot of context along with the prompt that you develop and this can be you know document one with its URL one document two with the with its significant URL but how the output comes out you can always structure this output you can prompt it say hey I want this output in a particular format I've seen that help a lot let's say like you want it in a Json format with XYZ and you know markdown tags all these help but another thing that also helps is adding examples lots of examples relevant context into your prompt has helped a lot for me at least it's like a weak rlhf kind of like a few shot examples that you can add within your context of your prompt but not just adding examples of how you want this prompt to complete but you can also get a lot of people say hey I add examples it overfits to these examples but another way to overcome is overfitting is by adding this relevant examples of the contextual retrieval everyone's talking about with like vector databases out there fetch the relevant context and you can provide a list of examples there and you can do Vector database search like semantics similarity search from what is the user's query get relevant context add it into your prompt and that that has I've seen do a great job there um another thing when it comes to um getting these lens into production is that I've faced is the latency right as you chain prompts you know there's so many agents chaining of prompts latency has been a huge issue and you can actually solve this by a lot of ux features depending on your use case you can you know streaming is a great thing that we've seen chat GPT do you don't feel the latency because chat GPT keeps throwing you throwing you these messages another thing is you could you could split your prompts into multiple things like for example like document searching first fetch the documents provide that to the user let the user look into the documents while you fetch the answer so splitting your prompts and getting intermediate messages out there is does wonders and hiding the latency is what I've seen and um yeah I think that's my overview like a really short overview of like prompting tips that I've learned awesome so how are you have a take on that yeah I mean one and maybe one thing I found really useful for avoiding hallucinations so first we know the normal tips actually like act as something when I think about GPT and nlms is this huge tree of many activations neurons and you always want to help it understand what we expect it to do so the more information you can provide within the problem and help it know what path to activate the easier it will be for it to help you in achieving your goal so specifying the format and stating what role to the ACT is always been useful one concrete tip for avoiding hallucinations within the prompt in many cases you can think I think about nlms as like this satisfyers so they always want to provide you with an answer so just by giving it an out they will actually follow so for example if you want to classify an Amazon review as positive or negative but maybe sometimes you're missing context or you're getting input that is not a review you can always ask it to provide maybe like say otherwise return an A or error and then by doing so it will actually avoid hallucinating an answer that is actually wrong and and return an N A or some other predictable form and talk more broadly about your experience combating hallucination you know what kinds of things if you run up against at stripe and and how have you approached it beyond the this prompt trick you mentioned yeah so I mean first um illumination is where LMS makes the makeup data that is incorrect and I feel like what's the main difference between other ML and deep learning um models before is that they do it really confidently so it's really hard to tell if it's actually wrong or not so that's one one of the biggest shortcomings um building up on Natalia's point is very use case dependent and so if you are doing working with a brainstorming partner it's one thing versus building maybe a behind the scenes classification model or pipeline that will then have substantial Downstream tasks so first about illumination understand what is your use case how what is the impact of a mistake then ways to mitigate hallucinations and what I've seen very useful is first giving the n11 out as I mentioned but also prompt chaining so there's like a few approaches like self-reflect and also like another video I've seen recently smarter gbt where you basically ask the you ask a question then you ask the llm to criticize the answer as an expert in this space and then provide even the resolution and by doing so it improves performance and substantially at the cost of latency and cost and so that's another idea like prompt training and kind of you get an answer and you check if the answer is actually correct and another option and mitigation path is actually citations so forcing the llm to quote the source for its answers um is really useful for using hallucinations and it will also allow your users or yourself to check if the generated response makes sense so you can actually go to that page in that PDF and see if this makes sense and lastly I've seen this is relatively innocent and recent there is research called um llm blender from Alan Institute of AI where they used they combine multiple llms so each llm kind of peaks in different areas so if you combine them them all and know which one to call in the relevant time and it might help you improve performance overall and avoid originations yeah we're seeing a bit of that in some of our companies that are you know doing some more advanced work here so Jasper for instance is a great example of someone who's actually using a blender style model to basically orchestrate across multiple LMS to just understand and produce the best results where there is effectively you know sort of links in the latent space of the model one of the other areas that we're also kind of noticing as a pretty significant amount of opportunity is where you can take call it visualizations of the latent space of a model and be able to understand where there's effectively holes in the latent space and the folks at uh GPT for all the Novac team was actually doing some pretty impressive work here in terms of visualizing the latent space of the model and then helping understand where to introduce additional retraining runs to improve the Fidelity of models over time so that's that was a pretty exciting area that we're seeing opportunity particularly when you're dealing with a lot more small form factor models that you're trying to train for domain specific purposes like retraining um based on where there's missing sort of elements of the data set to train the latent space properly is is something that's exciting uh George and one of the earlier sessions Josh open had this great point about the the companies that we really admire for doing a great job with uh AI Tesla was an example he gave didn't just kind of throw out this model and kind of declared Victory they built this continuous feedback and Improvement loop around their product and that's really the the differentiator and why we admire those companies what have you seen in terms of companies building these kinds of feedback and Improvement loops around uh llm based models in the wild yeah I'm glad you asked that question Sam I I think when you look at the best LMS and just Transformer based architectures that are going to continue to evolve in this space they're inherently all going to have some reimbursement learning associated with them and whether that be a human feedback loop a machine feedback loop a human and machine feedback loop I mean just think about the the fundamental difference between what we saw with chat GPT early on and more recently right it wasn't just the fact like okay they started to kind of evolve the model itself and you know why are there sort of so many less hallucinations around chat gbt today well it turns out that it is the feedback loop itself it's the rohf that improved the model effectively over time so I I think we're going to see quite a bit more um call it model Fidelity model Improvement by the fact that there's just good feedback loops just natural really part of any sort of you know call it model that's being built for the purpose of these domains uh as they are kind of improving the application experience around it I don't think this is a journey where you kind of just send the model off into production and then uh never call it and prove it Beyond just introducing the next generation of the model I think in most cases there's going to be some kind of feedback loop for reinforcement learning that's going to continuously improve models over time and when we think about that kind of continuous imp improvements um you know it's hopefully relevant related to some kind of evaluation metric we've touched on evaluation previously uh zahar dig into that a little bit deeper what have you seen working from an evaluation perspective for these kinds of models yeah first I will talk about saying that um I'm too surprised how we are quite behind on evaluation as a community like AI space it doesn't prevent us from running fast and deploying things into production but still there's something to be made there I know that many folks are working on making operation better like link chain and other smaller startups but you seal a gap and I feel like when it comes to evaluation there are a few ways um I we go about that so the first one is you know automated manner so some papers have used um llms like gpt4 to score the generations so because Generations are more like open-ended the text more subjective and nlms lend themselves really well one useful tip is actually use if you're using GPS for 3.5 you should use gpt4 to to judge um the inferior model so using Superior models to judge inferior models Works work actually surprisingly well and you also need to make really clear what are your kind of metrics and what your measuring for example you might want to have a benchmark or a set of examples for illumination so the father more examples about ElectroCity and what you define in your company as things that should not make their way into the user you might want to have more performance that products performance specific benchmark for your specific use case so hopefully we will get more and more tools that help us to streamline these processes and and I expect other players like Lane chain wasting biases also released a few tools to help us get it there and but still it's a lot more manual than we I have expected it to be at this point uh Smitha what have you seen yeah I think um I I chime in with um Sahar like what he said I think another thing to bring in is prompt versioning that's been a big thing prompt versioning like you change a bit of your prompt it can change like the output of something that worked before and it deviates so Benchmark questions that's been a big thing um at prom Dobbs we have like a bunch of Benchmark questions that we have that that we want to evaluate against and another thing is that like zahar mentioned you could have like GPT say hey do you think this answer is similar to the answer that I expect right and we could even do what I've done before is have like a scoring method where you have hey this is the question these are the keywords I expect and the output right it doesn't have to be the exact output but these are the the answer should be having certain keywords these are the significant keywords and you could check against those keywords and the other one is you could do like a quick semantic similarity do you think that this answer is similar to this answer how the score and you could have like a whole evaluation score like uh weighted against these and another thing to make sure is that the Benchmark questions that you have should span across multiple areas it shouldn't focus and as you find Benchmark questions just keep adding them in and it's it's not gonna you know you could keep running this maybe daily or at some frequency and see when it deviates actually I'm curious how do you um track all those feedback requests and responses from music something in production found a new carrier LM as expected what do you do yeah so I think at that point you have that what what is that question that deviated and I would think about like what is the answer that I expected at that point what did the user expect and then I could add that to the Benchmark or I could even add like as I mentioned earlier examples my whole um like I have like the semantic like the vector store database adds some examples there to improve the um improve like the output at that point and that's something that you know you need to have like um George said that you need to have this entire feedback loop and it's all there's there's no right way to do it and there's no nothing right now so it's going to keep evolving and are you finding that um you're able to put all that feedback back into the process through um through prompting or through rlhf are you also needing to apply heuristics regexes or whatever those might be to um you know to enforce constraints on the olm it's it's definitely a lot of um like you mentioned like regex matching I think that's been a big thing because it's not just going to be that the output like it's not going to be that the output always is going to follow the same format right I'm going to prompt it to it can go it can go some way so making sure that like the Jace maybe like a Json format or like tags things like that I think like open AI came up with the function calling yesterday that they released that's a huge thing that the model's been fine-tuned on so that's that's where that's where it's heading people know regex and and heuristics is the dirty secret of yeah exactly exactly so we've talked about uh chaining uh blunder pipelines all these things um one of the um you know the challenges that they introduce is uh these apis aren't free you know whether we're talking about open AI or inference calls aren't free so there's an economic component that's particularly important for folks that are trying to fill products uh Natalia can you talk a little bit about how uh product Builders should think about the economic of these kinds of models key thing to understand which I think everyone on this panel does is that the more explicit detail and examples you put into a prompt of course the better model performance as Sahara talked about you know one of the ways to assuage uh some of the hallucination issues but that means that you get the more expense of your inference will get with those longer more detailed prompts so I think that's the first key thing to understand um you know if you think about open AI they charge both for input and output tokens um depending on the task it you know it adds up but overall prompt engineering is still pretty cheap an easy way to experiment as opposed to building a whole ml product so I'd encourage people to do to really use prompt engineering um you know the next thing I'd say is set up your experiments and be very experiments are not free so be very thoughtful about how you set up your experiments make sure you're solving the right problems uh set clear expectations depending on like I've worked in large organizations which means a lot of alignment a lot of buy-in so you have to explain to leadership team partner teams what are what kind of expectations they should have from your experiments minimize the risks time box your experiments think about build versus buy and have a clear data story um so I would say you know all of those things experiment in order to figure out your costs uh and then I would say the second part is something we had already alluded to in the earlier conversations consider smaller models smaller models will be cheaper and there will be trade-off around accuracy but that might be a really good way to minimize your costs um and um yeah those are my recommendations I want I want to hear what what uh other people think yeah I was curious from George's perspective as an investor you know you're seeing a lot of these economic conversations uh you know what are how are founders kind of thinking about it and having those conversations with their investors yeah I'm glad you asked that I think we're seeing particularly some of the companies that are more call it earlier in their growth Journeys they're the ones that are experimenting faster and trying things in a way where they're bringing products to Market in Little League weeks right so I'll give you a good example a mid-stage for a company funny comb is in the observability market right and they build you know next Generation observability solutions they introduced a better query Builder so that instead of you know asking questions from a distributed tracing standpoint using a very esoteric query Builder to ask the question they introduce a natural language overlay now very very straightforward to introduce something like that using a finely tuned llm for you know introducing a product feature like that within weeks and interestingly that became I would say the most actively used feature inside of the honeycomb product within two days of it launching and so that ability to just kind of try something experiment iterate launch uh and move quickly is something we think is a pretty profound opportunity particularly for any startup that's going through this journey we're also by the way just quickly finding the fact that a number of larger scaled out incumbents have a pretty tremendous Advantage here which is the private data itself over time we don't think this is a moment where just incumbents will fall by the wayside because of just generative AI movement is happening it actually turns out they have one of the the most important weapons inside of inside of this sort of uh scale and capability that's going to occur in the space and that happens to be the private data itself and capital my capital uh so how are you wanted to jump in yeah just adding on the tennis point about how do we how do we make it make sense from a class perspective also find if you have longer context Windows fine tuning is also quite helpful we have found that so you can find in the model it's a one-time pass you have to pay but then future Generations will have way less problems and tokens or weightless token tokens and then another meta point is usually what what I've seen is we have two different type of companies right we have the Enterprise companies which so far I haven't heard from anyone working in this space Not only SRI but in general that having cost is an issue so given the impact it usually has on the bottom line business and for Enterprise companies it seemed like a blocker for startups I would encourage actually startups to generally kind of put this aside of course there will be a path for this becoming cheaper and cheaper if you look at the trend from the last 12 months or so nlms are becoming cheaper and more powerful so I wouldn't let anyone kind of be blocked by these profit margins unless it's mixed with no complete sense and then that's a different thing yeah I totally agree there I'm working at a startup like it's it's the cost isn't a huge concern right now because the value to the customer and the expectations of the customer is increasing they do always expect like hey I just want to ask it in normal English right that's always been like I don't want to remember this query language that's one thing and coming to Natalia's point about like the tokens you are charged for the input tokens and the output tokens so adding prompt engineering to constrain right constrain your output you could either say you know like if if your use case is like some summarization use case keep the length to like a short extent because we've seen that the completions can go on and on so constraining it having like the formatting output formatting expectation that can reduce it as well as semantic caching if that's something that for your use case it makes sense if you don't need the completion to happen every time if the similar question has been asked before just get that out but you don't need to do it apis open your API calls all the time just just one one quick observation I want to make we're talking about costs but in reality for Enterprise they're at the very beginning of the journey where they're scared of the risks and even waiting in and so once they get past that the I think costs uh might be a consideration but by that time I think the cost will be so reduced we're seeing every day we're seeing smaller and smaller models on edge I'm super excited about and open source I think increasingly like the story of cost is changing very quickly and costs are coming down very fast yeah yeah I think we're coming up on time but I don't see anyone here to kick us off so we'll we're past another question well uh I think we're doing a service until someone comes and says I'm ready for the next session uh we did say we wanted to come back to model selection uh feel free to do like one more question okay okay perfect um you know we've we've throughout this talked about how there are new models emerging all the time um we just talked about how the models you know there are smaller models bigger models uh you know still um you know their advantages advantages to to both of those um so our talk a little bit about your experiences from a model selection perspective how much time are you thinking about when you are approached with a new product opportunity you know what's the the right model to start with and and where do you start that conversation yeah so um we usually think about in two ways right we have the open source models and the commercial models on the commercial side and coming from more of an Enterprise perspective the number one factor to start experimenting is what do you have access to so like getting something off the ground working let's say with cloud from a Tropic requires other requirements agreement the same way that Open the eyes and Azure and all the other call providers so usually if I want to experiment I will go with what's most available at this point and but then when it comes to Performance it's still like if it all comes back to evaluation that's why I'm so excited about this space and there is a nlm leaderboard and there is also like the author leaderboard for um and back to databases and embeddings of all supports in the channel in the chat so what we usually do we start with something like gpt4 which is usually the most performant if it works creatively well we can always downgrade because GPT 3.5 is cheaper but more importantly faster and so depends on the use case but I would advise folks to start with the most the more powerful models if it works really well iterate The Prompt and then potentially downgrade if it makes sense and then when it comes to open source models I feel like the more we will build our own evaluation it's literally in the last four to eight weeks and this amazing progress is happening the easier it will be for us to also incorporate these problems and lastly I will say companies like AWS launching sagemaker like with working face and other llms but also and bedrock and and this kind of generative AI capabilities help us all experiment a lot faster so I would expect probably not in the next six months or so we will have a smarter engine slash layer to allow us to plug different language models and experiment a lot faster and then potentially using even more than one nlm for the same use case based on our latency requirements cost requirements and accuracy metrics awesome awesome Demetria what's up everybody this is uh angle two I'm trying to do both stages at once which you can imagine is a lot of fun and I've been listening in on this absolutely loved it thank you all for joining I wanted to just come on here for this moment because I uh have to say like George I've gotten to meet you in person it was incredible you blew my mind and I hope to meet the rest of you as samita Sahar Natalia and of course Sam in person but I wanted to just wrap it up with this that if anyone is not listening to Sam's podcast go and do that he has been absolutely inspirational for me and this whole community and so much help that I had to drop in here and tell everyone that and because I know you're too humble to say that so for the people that do not listen to twiml this week in machine learning and AI get out there listen to it we'll drop a link in the chat and I know that I'm pretty sure most of the panelists are huge fans of this and I'm pretty sure you probably got some fans out there listening right now so thank you all for doing this this was awesome thanks everyone thank you [Music]