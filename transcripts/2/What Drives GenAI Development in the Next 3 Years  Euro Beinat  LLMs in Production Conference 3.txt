now I've got to give a huge thank you to our sponsor process we're doing it up big this time around we're in a studio I can't believe it like a kid in a candy store there's wires Everywhere You all can't see it but there's wires literally everywhere I feel like a professional and it is way more than I should be getting but a man that does deserve this kind of professionality is my man who connects countries he connects so much he needs no introduction we've got Euro the head of AI from prois coming up onto the stage to join me and talk for a minute about the llms marketplace and the llms in production and how we fuse these two things together this time around so Euro come on up man I see you there waiting you're excited to come huh thank you very much what's going on phenomenal thank you thank you so it's great to have you here dude what is llm Marketplace uh it's our internal uh conference which we have run for several years where you've been guest you've been moderator for some of that work um and the reason is why we do it is because the community our communi about thousand data science machine learning experts across the globe it makes sense to bring them together yeah um why do we do it because prus is a very large consumer internet company yeah what is Press yeah and um um everything that we do has to do with platforms that connect buyers and sellers across the globe um we serve about a quarter of the global population so it's very large um you know some of our Brands because they are Global udemy or stack Overflow very I think very let's say known in this community particular but then also local like I food in Brazil or Swig in India and so because everything we do um is let's say this platforms we need to use machine learning at scale across the operations so we have hundred of models in production but also close to 1,000 uh scientists data science machine learning experts and so on it makes sense to bring them together we learn from each other we invite people from the community we invite people from outside some of our let's say uh colleagues are part of the ml community so why don't we do it together this year because we can do it in public and not in private well I'm so thankful that you did because up until this year it's been a little bit of a secret that I've had that I get to participate in the marketplace but nobody else gets to see it and this year we decided to open it up we decided to do this jointly and I'm very thankful that we did I also know that you spend a lot of time thinking about where things are going and what is coming next you all are doing you have a ton of use cases both traditional ML and these new llms what are some trends that you have been seeing where do you think the Puck's going can you share some of that with us I'm happy to do that and by the way more than Trends it is the dve so what we think um moves the plot for next few years so I have some observation love to share picked from some of the work we do internally hopefully that creates debate and some I agree or disagree what we see but in any case I mean I think there's this slide where people can ask questions if you yeah if you have a debate on what euro is saying there's the slider right now we're going to be throwing up the QR code right here there's also another one right here I think there's some we could put it here too we could put it over here even we'll put it everywhere so get your phone out join us in slido mention some things or put in the chat what you are thinking and we'll moderate that but Euro let's hear what you are thinking for the next couple years man thank you so this is part of the work that we do uh at all times which is mapping what um moves the plot of generative ai ai in general in the medium term and the reason why we do it is because we um are a global internet company but also a large investor therefore we need some guidance to understand the way we think about all this and where things are going so uh we like to um think of drivers in this domain in what we call gravitational they seem to have some sort of predefined path obviously there's going to be a lot of Divergence in terms of how well we can predict but in any case things seem to go in that direction patterns that emerg from the usage uh so how do we actually learn to use these tools how to deploy them in production so on but there are also a certain number of drivers that might accelerate or delay everything that we do um this is just a subset of what we um work on and I'm going to uh spend a little bit of time on these four that seem to be connected to many of the themes of the conference um today let me start with scaling lows so this is something which is very well known has been postulated a few years back by several organization open AI in anthropic uh in particular and it has to do with a couple of things the first one is that the model model capabilities increase with compute data and model size um it means that the loss function can be more or less predicted if you deploy more compute with have if you have more data if you have more parameters there are interlinks between these three so they're not independent obviously and to degree to which this can be generalized Beyond a certain amount of time is a subject of debate but everything that we have seen so far is that this trend is going to continue for some time maybe one maybe three years maybe longer but in any case for some time so there's one thing the second thing is that um we have seen it from practice but we also see from um research that the unit cost of quality so much it cost to achieve a certain quality tends to decrease over time estimates about half it goes uh 50% lower every 16 months be that 12 or 18 doesn't matter that's the trend then we can see that in practice as well so you have two Trends on the one hand you have the advantage of throwing let's say deploying Capital if you want to compute to parameters and data and so on and the other de Capital becomes more efficient so it's almost inevitable that we are going to have a stream of models in the next few years that are better significantly better than the models we had only last year the last six months and that continue for a while which is important for because you know a lot of what we do is predicated on this on this models getting better and multiple tasks it also makes it hard to decide where to invest because many the things that we do now which means using at best the tools that we have at this point in time may become obsolete in a year right before they become marketable because models can basically eliminate that need so that makes investing in this area particularly interesting but also challenging the fact that let's say you can sort of predict where the loss of these models can go has obviously a correlation with quality because that's what we care about here you can see well-known diagram of the relationship between GPT 3.5 and gp4 how much we gain in this particular case the in the quality of exams between 3.5 at four in a period of time which is relatively short I think is less than one year however what matters here it's also the nature of what we call the jacked Frontier so if you look at every human task there is a certain type of task where humans are good at right and then if you make it a little bit difficult maybe we continue being good after a certain point it becomes too hard and we fail so for every task we have a sort of idea of what's the sort of human Frontier if you stay to the left we know that we can do it stay to the right we know that we don't when you deploy these models that Frontier becomes much different it's Jagged in the sense that it has a strange shape which is different for every model but what makes what why that's important is because on certain tasks that Frontier is much much far away from the human one so these models just do much much better than any average human but in other places which you know there's not obvious which ones they are they are much less capable so when you deploy these models you have to have an idea and ideally you would need to predict where this Frontier is and we're not very good at I don't think anybody around in the in the community but in any case in the science in general is good at making this prediction so the way that the way that we sort of compensate for that we use multiple models which sort of create a uh a frontier which is the total of those or we do trial and error so one of the areas where hopefully today we hear some ideas or uh maybe somebody has better Solutions how can we predict this Jack Frontier where it is it will make a big difference in terms of deployments the next area where we pay attention and we think it is um fundamental for what is going to happen next is Agents so for everybody that's been in computer science for you know some time you know that agents has always been one of these areas where people spend a lot of time in research and test but it has always been very very hard to deploy agents at scale now we have models llms that can serve for that purpose because they can break down um the type of task that they want an agent to do and for each task then Define how to do it use perhaps external tools and use memory and action they can also reflect about how this um models perform iterate until you find a solution so it is promising and is actually one of probably the most promising thing we have seen in a while because agents are a step change what we find interesting it's also how much research has been activated by llms in this area you can see here the number of papers before 23 and and those after 23 so there a literal explosion so research is going fast progress is fast and there's a clear demand we see demand for this tools everywhere across the entire group of companies that we have but across the entire industry however it's also very early days right current agents are experimental it's easy to create a demo it's really hard to create reliable applications at scale so we we tend to believe and here you may or may not agree that in the next let's say months or years couple of years you will see first agents that try to get better as specific niches so it can be for instance where you have tools combined with llms in a very specific domain or um agents that do the same type of work that we do so data analyst so they create code execute the code and then give you the result because there's an element of being more controllable once we have this kind of boundary conditions of course by doing this there's an array of reliability security and ethical concerns that we need to address in general if you look at agents and everything that um uh llms make possible for agents we start thinking that perhaps we should think about this one as an operating system rather than models and so on it looks like this is a paradigm of computing which is very early stage but it is a new paradigm of computing so so instead of thinking of models we might want to think about as operating systems for reasoning and that obviously opens a certain number of additional um discussions the next one uh which is an area where we pay a lot of attention is open source so um in the last year I may two years and so on we see a stream of Open Source models that have been released for many reasons but the reason why we care about is because some use cases really need models you can deploy in premise you need all control and so on it's also true that if you map the performances of these models against a certain number of benchmarks then proprietary models like gp4 uh anthropic models and so on they're still better almost everywhere right while the open source models are improving but they're not there yet so you have a situation in which for the time being propriety large models are still better than almost anything which is out there however there's also an increasing amount of practice and research that use demonstrates that if you narrow down the task and if you have the proper data for training then you can have open source model which are fine-tuned and exceed uh the standard model so the proprietary models does it matter all this well the first one matters that propriety is generally better than open source now because in some situations where you really need the top performance of a model think about Healthcare you will will never take a model which is 1% less you just take the best one but in other situations where we have 100 models all give you a plausible answer summarization for instance it doesn't matter then many other let's say tradeoffs become important so we believe that any solution going forward for some time will have a portfolio of models that use some Proprietors some open source some are going to Pure open source as they are some are fine-tuned and so on and this is going to be more or less the uh um the way that we see the relationship between products and open source going forward the last one that I want to pay a little bit of attention on is this idea that in general what you really want is personalized augmented models to uh deploy for your applications they incur costs and if you plot all the options that you have starting from an existing model to a model that really does what you want while you start with prompting then you can chain models put more than one to do the same task and then combine it you might want plugins then of course there's the entire domain of augmented um generation and then you get to the point of fine-tuning then you can align and then if nothing works you train your own models now obviously with all this these drivers of making the models larger better over time a lot of the things can be done on the left side of this diagram just prompting might be sufficient for what to do but it's also true that in many cases you have to go to the right hand side when you get to the right hand side train ground up then compute costs become important and it may be be a threshold you know you cannot cannot afford it might be possible or not many more people will use fine tuning so we have done a lot of work on fine tuning we have fine tuned many many models including the llamas that we have seen in the introduction here and we talk about a lot during the day and the fine tuning is an interesting thing because I think we Overlook and we underestimate the data cost of fine tuning in all the experiments that we've done most of experience that we have done the Computing costs are a portion but many times is a small portion of the total cost you need to create datab bases curate data sets this curation has to be done in many different directions because you might need to F tune in multiple ways to achieve what you get and this cannot something that you actually Outsource it needs it requires competent expert competent people that know how the data Works they know the domain where they work and they also know how data is used for models so I think if there is one area where we we sort of don't spend sufficient time we don't have enough let's say tools here is how do we manage the data preparation for fine-tuning because in practice it makes a lot of difference I'm not sure if you agree or disagree with all this but in any case um hopefully you have comments and you can um put them in slider would be great if some of these teams are discussed today and we can learn from the collective experience of this group with that Demetrios all right Euro I wasn't expecting you to be such a Visionary and was a bit of a surprise I'm going to say that you are going to be my co- Shaman we're going to be Visionaries together talking through this journey all guide us you see the Visions you project it to the Future now there is one thing because I'm waiting for the stuff to come through on slido right now but I've got a question for you after that talk and I I absolutely love the slide where you go through all the different pieces that is so cool to see and this data prep is huge but when it comes to you being a Visionary man like why is this so important why do you spend so many cycles thinking about it well listen I mean uh we have been using machine learning let's say call it pre gen right uh for for several years and at the scale where we operate um you couldn't really run these businesses without machine learning so so it is not something that you is nice to have you need to have for the operation scale where we are so we know the importance of all this number of use cases could not be possible before so language enables those so you really want to find out when that's the case but fundamentally we believe this is a new Computing Paradigm and Computing Paradigm that is going to change technology companies we are one so we pay a lot of attention because we want to be in the position of taking advantage of that shift happening but also being ahead of that well exactly on that theme there's a question that came through from Remco and he's asking about your views on the GPU shortage and the dominance of Nvidia in generative AI development well there's a lot of discussion about this and that goes beyond let's say My Views I think right I think can tell you what our experience it is yes it is something that um impacts what organizations can do but we haven't found the yet that has limited in any way what we do right so if that shortage might be let's say applicable at some scale for what we do we don't see that I do believe that because of let's say the interest and the demand there are going to be multiple suppliers so it's it's hard to believe that Nvidia is going to be the only one in the next two to three years well I'm getting the time I got to keep time very clean right now but there is another great question that came through so rapid fire real quick what do you think do you see AMD playing a big role with the recent announcements that have come out from lini yes I think the CEO of lemini or CTO of lamb is going to be speaking later so you can ask all those questions but AMD yes yes there we go that's it you want the short one right that's the rapid fire man I appreciate you coming on here [Music] thank