I'm not in ASMR mode anymore you there what's up Waleed hey how's it going dimitrios are you doing all right I'm doing great man I'm blissing out now after that one oh yeah there we go there we go stretch it out if you need to and wow awesome so uh everyone is very clear-headed now I believe for your talk and they are ready but we need to just School them on the fact that there is an any scale Ray Workshop happening in person once we finish this conference today online and it is happening in San Francisco so we actually filled up the full conference sorry the full Workshop the full conference the conference is pretty full too I mean there's like 2 000 people online right now so that's pretty full I would say but we filled up the workshop there's like 50 people on the wait list I'm not gonna be the guy that says it but I bet if you just show up and you slip the doorman 100 you'll get in all right that's all I got to say well you take it over man it's all yours hey thank you very much D um it's really great to be presenting it um at mlops community and mlms in production um I have an update you know obviously things have moved very fast in the industry uh but the last time I talked uh an ml Ops event was a few weeks ago two weeks ago and we were just giving people a preview of Aviary and now what I hope to share today is our first two weeks of the painful lessons we've learned building it deploying it and getting it running so what I'll be talking about today uh is is really about the lessons we learned and this is one of my favorite quotes which is that experience is a good school but the fees are very high so my goal today is to avoid you paying some of the fees that we did so if you learn some of the things about what it takes to really deploy lens in production or at least our experiences from it then I would have done my job so we're going to start off with something very simple which is you know there's lots of different ways to focus you know how you deploy llm in Productions but the ones what we're talking today about are self-hosted models in other words you as an organization run and maintain the model uh usually based on a foundation an open source foundational model that you've fine-tuned or maybe you're just using it as this but first let's dig into the question of why you would want to use like your why would you want a self-host anyway second is we'll talk a little bit about exploration and what we built something called Avo Explorer which has been out now for two weeks um and uh then we'll share some lessons that we learned the hard way right the first one is model selection is tricky serving llms turns out to be pretty hard in many ways be prepared for like deploying more than just a single llm understanding the importance of GPU RAM and finally how do you keep costs down and what we'll be doing is we're giving be giving you a little bit of a behind the scenes look of how it works we'll be showing you our dashboards our tooling of our live Production Service just to give you an idea of like what we learned uh during this process so the first question I wanted to discuss was really the oh and one of the things we'll be doing lots of demos so hopefully the demos don't die so when you try to deploy an llm in production you really have three different choices or four choices but I think the fourth choice is kind of very rare so you can either go to a commercial company like open AI anthropic um you can go to a hosted open source solution like hugging face Mosaic octo Ai and again these slides are Super Fresh because the October I just launched yesterday um or on we'll talk a bit about every coming forward and you know how you can do that um but um any scale is the company behind an open source project called drain and as well as a managed hosted service called uh any scale and what we'll talk about today can work both ways so everything I'll be showing you except for one small thing is completely open source and you can have a play with it um the third option is is really self-hosted so usually what you'll do is you'll take an open source language model like the the language model du jour is of course Falcon with this 40 billion parameters and the smaller seven billion parameter model and then you'll you'll either deploy it as is or you'll do your own fine tuning on top of it there's a small fraction of companies that are building their own llms from scratch but I don't think that represents the majority for those companies they often end up using Ray with both open Ai and cohere who are building those commercial applications using Ray the open source toolkit that we've made available for distributed computing to train their models but that's not going to be the focus today so why would you want to use self-hosted llms and the way we think about it is we're big fans of self-hosted llms at any scale but you know we're not blind fans to it we use gpt4 for things some things and you know every you just want to deploy the right tool for the job but in particular I think the advantages of OSS llms is you have control over your data you know you might have regulatory restrictions and you might just decide that you know what llm's are a competitive advantage and I'm not really I don't really want to share it with a potential competitor who knows what open AI is going to do next second reason is cost and you know gpt4 at the high end can be very expensive at 10 cents per thousand tokens so we worked with a company called Kappa and they help us with document search for Ray documentation it works really great the only problem is that they charge us 25 cents per query which is just ridiculously expensive but they don't have a choice because the back end is GPT form so we want to be careful about costs you won't you never want to be in a situation where your vendor has locked you in because that removes your negotiating position when it comes to cost and finally customizability so um you want to do things like fine tune your models but if you go to the commercial companies like open AI they will charge you six times as much for serving a fine-tuned model as a non-functioning model so the one area that I think opens self-hosted llms are not quite as good is um Quality um and and this is something that's changing you know there was a really interesting Google paper that said we have no note and they actually plotted the graph of how quickly open source quality is improving um testing is starting to show that The Gap is narrowing so that's from places like lmcs and so on and finally um we've been running some experiments we're not ready to share the results yet because they're a little bit preliminary but what we're seeing is open source is perfectly good enough for things like summarization and retrieval assistant generation so it's not that they're plug-in replacements for the full power of gpt4 but if you think carefully about your use case you may be able to find a good fit so just trying to summarize these options here you know this is a chart that I hope you find you find useful as you're trying to choose when do I use commercial apis when do I do self-hosted and what the pros and cons are um clearly for example the hosted approaches are easier to use self-hosted has some risks obviously what I'm trying to do and what we're trying to do with Avery is turn this sad face into at least a neutral face if not a smiley face to make it easier for people to build self-hosted models so now let's dive into the lessons the first lesson is uh wait a second it's great that you have a self-hosted model but how do I how do I choose the right OSS model what size do I need how much does each option cost um where are the quality characteristics some models come with up to four Tunes which of the four Tunes do you use so the first thing that we did is that we built some tools and I'll I'll walk you through those tools right now so the first is Avery Explorer and that's uh you can try this out yourself at avery.anyscale.com um and so I'm just going to give you a quick overview of Aviary the tool that we built and then we can um I'll also show you some of the command line tools that we've built right so this is Aviary um and as you can see here it just it allows you to choose different llm models you can choose different characteristics we have certain options like you can choose the fast option which uses the fastest seller lens we have and then you can just quickly type on like whatever you want your prompt to be um and you can click uh and then very quickly you can compare these three llms and see which ones uh you find personally more acceptable which ones work better the latency is going up I can see uh probably all of you are hitting the website soon we'll see if we see the spikes but you can you can see that the outputs of each of them cost and the tokens per you know the you know how much each of these models costs um and so that gives you a quick overview if you just want to kind of see on the surface of course it's also possible for you to load your own models so what you could do is you could also have fine-tuned versions of your own models that you put into this list which we'll talk about later um the other thing we're able to do is very easily to kind of we've allowed people here to vote so you know I might come here and say you know what uh well this one doesn't have five books in it so maybe we'll just choose this answer and this one's two of those so I'm going to choose this answer now imagine what happens if if we have pants people around the world uh filling that in and what we can build is we can build a leaderboard of like what are the best seller lens for this particular use case um not only can we build a leader leaderboard um and there are other websites that have leaderboards but I think one of the unique um characteristics for us is that we've tried to characterize things like the cost the latency and so on so that you can build a model now there are some simplifying assumptions here particularly that it doesn't fully include batching and batching can have a 10x impact so think of these as an upper bound but what I can do for example is I can sort by the fastest which also means the cheapest in terms of generating outputs and you can see that Amazon like GPT is pretty fast but unfortunately it doesn't rank very highly on performance whereas I think the Superstar and some of the models that we're seeing people want to use are things like uh MB like the chat model from Mosaic ml or open assistant so let's have a look at those you know what it looks like um you know in terms of the cost and we see not surprisingly like that those are some of the most expensive ones Mosaic chat is not too bad um but you can also see that part of the reason that something like story writer or open Assistant are so expensive is not because it's the cost per token is high but it's things like open Assistant likes to generate like long answers and the same thing for story writer which is that was what it was fine-tuned for so we hope that you will find a tool like this useful in your selection of the appropriate model let's go to a live demo this is that so that's you know just the UI version but what we can also do is we can um we can uh also we also have some command line tools uh so uh for example if you you what you can do is we've created this um Avery command and then every backend can list all of the different models that it has available to it this is the same model that you folks are hitting really hard right now um and you can get a list of models but you what you can also do is you can um let's say that I have a bunch of prompts so I'm I'm going to look at something like examples slash QA products right I'm just trying to what I might be doing is I might be working on a trivia bot and I want to work out you know is are the answers good or not for these different um questions so what I can do is I can actually now uh run a query on this um and what I can do is just um see compared two outputs so we said that Mosaic was a good one we've also added a capability to compare with open Ai and it takes that input and it's now just going to sit there and process it um sending the the Mosaic MLS to our back end and at the same time uh just sending those same queries to open AI so that will take a few seconds um but very soon we'll be able to get the results there and so we've asked it a few trivia questions um and now it's going to hit open AI to just show us what the results possible there are but you can see this tool is is kind of making it much easier for us to evaluate what the open source options are and uh yeah so that gives you an idea of like how we approach the problem of how can we help people understand what the right llm model for them to use is so I'm going to continue now hopefully if we have time we might see what the results are there but then the funny we were trying to build this application and that is that we found that deploying lolens is harder than it looks you know it's one thing to say oh that's a really great app and then you know that's very fun but really think about what it took it took you know what we found the hardware was managing this Aviary so to speak of 12 models was giving us headaches um and the initial assumption might be that what you just got to do is I can just download the model from hugging face and run that and what we found is that it's much harder than that um you have to think about different definitions of roles different markers all that kind of stuff different stop tokens different accelerators that work for different models some need more gpus some need more gpus as well as support for things like um matching and streaming so um that's the challenge we faced and as we thought about it you know we thought it you know maybe there's a way we can define a configuration um that allows us to specify what we want and to prepare each model uh for production um and you know we were really excited about this because once we added this kind of model config abstraction Demetrius is everything okay you know I am the bearer of bad news yeah uh you were just sharing you've been sharing Avery this whole time and you didn't share the CLI so I'm realizing now that there was a few things that people missed out on okay so there we go there it is let me rewind sorry for that thanks that's all good man um so um so uh yeah as I said you know we uh where we were was we ran this model we tested uh open AI versus MPT and it generated the output for us so you know it's put that model in a file called uh qa.json and it has you know all of the statistics of each of the models what was the difference between one and you know we've also added a command called Avery evaluate and what we can do is again this is the time to use gpt4 is send those input to gpt4 and ask it which of the inputs which of the answers were better and very soon that should be able to produce the result for us and print a table that shows us what it thought the best results are so hopefully that has given you an idea of some of the capabilities and I apologize for the for the issue that we saw earlier and I will remember to hit the the share the tab button in future cool so just in a moment it will print a table and you can see that it's basically scored each of these inputs um and compared them and in this particular case this evaluation thing is still something that we're fine tuning but it gives you like it you can see looking at these results that actually the open source results are not all like are not bad at all like they're actually very good for this particular type of application all right let's go back to the slides and keep the share tab instead so so as I was saying um really as a result of thinking about things in terms of model configuration we were able to add support for a new llm that came out in about five minutes so let's have a look at those uh those models and what one of those model configs looks like so what we can see here is that we have a little config file for each of the different models um and you'll see that you know in some ways it's very similar but you'll notice things like this they'll use different strings to denote you know what's the start of a string in the end of a string if I compare it to the one for Mosaic ml there's all this fine tuning you have to do there's all of these settings that you have to make about you know that use it uh slightly differently so you can see that there's differences in what we are trying to do and the the tweaks that we have to make to get particular models to produce reasonable values um but what we've done is effectively there's a little model config for each of the different models that we want to run and ideally and what happens a lot of the time is you just modify the model configure a bit and it's able to run menu models or say you have a fine tuning model you can you can do something like that and it's just a change of the direction cool so the third lesson that we learned was that you're probably going to deploy deploy many models more models than you think um so we thought at the beginning of this that we might have like three models uh we might have like a small medium large but very quickly we found that we wanted to add more you know maybe there's different sizes maybe there's different fine tunes maybe you want to do a b testing and the other thing that we're seeing on the horizon is that there's a router design pattern where you don't have a single llm but you might have like three llms fine-tuned for different things and then you kind of have like a router llm that decides which of the three yellow lens to call now just that simple configuration already we're talking about four llms cool so very very quickly we started to realize that managing multiple models was very very difficult um H has different requirements and everything uh we needed um and could we make it so that deploying a new llm was no harder than any other microservice um and that was really the Genesis of the Aviary back end and as you folks know this is an llm you know there's kind of a very important devops principle which is that you treat the things you're working with like cattle not like pets you know you don't you don't have you know cattle you know that you still care for them but you care for them as a group you don't ask a cow to come sit on your couch the way that you would ask a pet to sit on your couch right so this principle of treating llms like cattle and not pattern pets really um simplifies the management and the declarative approach that we took of the model configs also simplifies that so our model guys it's okay you can just load it from the the model config again it dies Ray serve and Ray which is what the system is built on has really good um um fault tolerance such one of the service dies it'll just bring up another one um and because of the features built into Ray and racer that's super easy so what I'd like to do now is just show you a little bit about the real back end that's actually serving every.netfl.com so uh I'm going to click on share this tab instead this is showing you um the um any scale and um right now the back end that you're seeing is actually this guy um and you can see that there's actually one service or one deployment for each of the different models and then there's one kind of glue model called router deployment that kind of holds it all together and redirects traffic so this particular model as we said that's running in production we're hosting 12 llns together and we can click on any of them and get their statistics but the thing that I wanted to show was just like the serve dashboard so very quickly you can come over here and you can look at you know what are the statistics for like this service what's happening to our traffic um and it looks like you guys are being busy in the last five minutes or so you folks have started to hit and you know we've gone from like 15 replicas to like 20 replicas very very quickly so that gives you an idea of like um you know how our back end works we also monitor it in terms of like understanding how many users are using it so I'm going to share this tab now and this is um let me just do a quick Refresh on this this is um this is our live traffic showing us you know how many queries people have put in um what the token distribution is like you know we realized that people were using us a lot for summarization um you know but also we know you notice that we're actually getting you can see here just how spiky the traffic is and that spiky traffic does not play well with llms unfortunately so this kind of gives you an idea of like uh the you know how it works um how we monitor it um how easy it is to um switch and you know you can very easily I'm switching tabs now um you can very easily let me see if I can create this you can very easily kind of deploy new models uh with this woman so I might you know uh let me try this and and we'll run locally this will probably take a while but I can just spotted uh um specify the model config file models and let's say Amazon like gpp and very quickly this model will load it up so again it's this kind of character of um kind of making it very easy to to kind of deploy these models and handle all of that particular issues cool so let's get back to the slides Lesson Four it's all about GPU Ram not about compute um so we had this um page you can check out called llmnumbers.ra.io numbers every llm developer should know um and what you realize is that gpus um how we use them when we're building nlms is we put we use about half of it for model parameters and then half of it for working memory for batching and batching is very important for performance reasons you know processing if you didn't do batching for example you might see uh nobody else sees me don't let me distract you Ellie that was for the people that are waiting for the next talk okay you keep cruising man you're doing great all right great um so it's all about the jpu ram um and really managing that so you need this memory uh for both purposes and that means that you have to think uh very carefully about like how you lay things out in memory um so we often um how we dealt with that is we used um bigger instances to allow for more efficient batching um so basically now we rarely use 16 gig gpus um the um the atmgs tend to be kind of our go-to GPU and the a100s for the really big models like Falcon and um um open Assistant that are 30 billion or 40 billion parameters often we'll need two of them because the parameters are so many they don't fit on a single GPU and will run two gpus in parallel but the thing to up we're optimizing for is making sure there's enough GPU memory that we can do batting and batching is where you send you might take five requests and send them at once to the GPU which gets you much much greater uh throughput um at a small cost in latency um and so even it doesn't really matter which GPU is faster but the key thing to focus on is really which GPU has more memory and generally you know the a100s can be a bit pricey but even the atgs going from a normal 16 gig GPU which might be slightly faster to a 24 gig GPU will probably give you a better performance Improvement and final lesson uh cost management so um I want to tell you that you know every you know we have uh we we like to keep one replica around we can actually do scale to zero but that introduces you know a minute or two every time someone uses a new llm so we keep at least one replica of each llm around just running so that they can get good results and if you have 12 models that's essentially 12 gpus we have to get running um it I know it might sound high but it's actually after optimization um you just have to be very very careful when you provision so you really need to be able to Auto scale up and up and down but that's not really the the key problem because if you're Auto scaling time you know it's 15 minutes and you have a 20 minute Peak uh that's not going to be fun so what we've done is we've really done a lot to kind of optimize fast loading and startup time uh we we locally cached the hugging face models and we might use um nvme drives these are very fast local drives to help accelerate that um you don't realize it but hugging face you know like any service has variations in how long it's taking so if you can run a local S3 and manage a local cache um then you can absolutely accelerate how long it takes for a node to come up if you're Auto scaling and then the second thing is uh this is the one part of this that's not open source on our proprietary any scale service we've optimized the crap out of starting up notes and we can start up nodes in like under a minute so um that's kind of within your system you can optimize your fast startup time or you can use any scale to do that cool um the other thing is that we um very carefully uh fine-tune the auto scaling parameters and we took advantage of Ray surf capabilities which is a very flexible system so you'll see if I go and we have a look at one of those files again and as we saw earlier you know I just started a service with nothing more than an Aviary run and now if I do as the A3 models it will tell me the list of models but now I'm going to tell it that I want to use the local Aviary post and sure enough we should see our local model and our local model running on this machine only has live GPT running on it um so it makes it very easy to load things up but just looking at the dashboard over here and looking at the peakiness of the traffic you can see that there's lots of variations so if I look at the last seven days instead you can see that there's a lot of spikiness there and we we have to tell it for it um and so the way that we've done that is we've made it so that you can specify you know what are the minimum number of replicas that you want how do you how often do you check to see the the look back is so how frequently you're looking at those statistics um and also bias towards down scaling slightly slower which costs a bit more but it really um prevents those types of Peaks that you can't handle so let's draw the conclusions now um what we've seen is that self hosted Solutions of llms have some advantages if those advantages align with your particular use case then it's a really good idea to use self-hosted options but there are some things you you need to pay attention to you need to choose the right OSS model you need to really think about what it takes to serve a model you got to expect that you're going to be serving more than just a few models and really think about GPU and keeping cost Downs means you have to have things like Auto scaling you need to find tuna now as I mentioned everything that's part of Avery except for that one little thing which is fast node startup time um is open source the UI is open source the CLI is open source the back end is open source and so you can help us to build for the mlops community a tool that allows them to more easily do some hosted models and so what's next we're adding features like streaming continuous batching um and more ammo lens so the key follow-ups here you can try it out yourself uh there's a open source for the whole thing and if you want managed Aviary just drop us an email and if you need some numbers to help guide your process from putting things in production have a look at mlmnumbers.radio.com thanks dude there's number that numbers is so cool I love that you all thought of that and I would expect nothing less from you Mr Ali that that is just quality right there man and uh like one question I had the dashboard that you showed that was basically you had to set that up everyone is going to have to set up their own of those or that comes right out of the box with Avery um so we have a public one that anybody can use at any time if you want to run your own and choose your own models you can do that too we expect people to to run their own Aviary um um you know we've made it very easy we can just give you a quick account and you can set it up on Amazon AWS very quickly it just requires array cluster to run dude so there are a few questions that are coming through here and we are horribly late on time as usual so the panel that's coming up they're being very patient with us and because there's just too many too many good questions in here for me not to ask them uh that was a great presentation man an awesome demo can you uh okay does it make sense to predict peak times and scale the cluster in advance is that possible yeah if you can do it um you know what we found is the Peaks are very very random you don't know what time zone someone's going to be be you know is interested in your system we had an article come out yesterday and all of a sudden the traffic quadrupled to the website um so the dynamic approach is probably the the safer approach um but yes if you want to you can definitely just kind of say I'm gonna you know between four o'clock and six o'clock I want you to have two replicas instead of one yeah yeah exactly if you can predict the future get on it there you go that's basically the understanding that I got from your answer yeah and if you if you can advise me on stock trades that would be even more amazing awesome so last one for you Wale and then I'm going to ask you to jump in the chat and also Waleed is on slack so if anyone wants to ask in the community conferences Channel just tag Walid and uh he's in there but glad was asking what would be the metric that auto scaling is configured on traffic how would you differentiate High memory consumption due to lots of small requests from a few large ones great question yeah um I think that's something that we're still working on fine tuning um one of the tricks with llms that's very different is you know a two-word and you know maybe a very short prompt has a very long answer and that's where things like continuous batching become very useful because if you have continuous batching and you have streaming it kind of evens that out right it's just you know what rate of tokens am I getting in and what rate of tokens am I getting at and if you can model that you can do that to more accurately predict things so good well everyone I am going to also before I kick you off while leaving I want to say that any scale is an incredible sponsor of our llmn production report that we just put out so go get your hands on that and thank you any scale and we'll lead you uh I give you a whole lot of when you came on the podcast for not sponsoring and you pulled through you made it happen you threw some budget around so in case the uh in case Avery goes out it's because they probably spent too much money on this damn report sponsoring the community so we need to help them uh we need to help make it as reliable as possible Wally dude thank you so much man has been awesome and you know I'm really eager to answer your questions uh just drop me an email or just uh I'll be on online on slack to answer your Christmas list there we go there we go all right [Music]