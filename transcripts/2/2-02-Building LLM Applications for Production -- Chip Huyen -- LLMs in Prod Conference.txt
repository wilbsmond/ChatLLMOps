[Music] I just wanted to get us all on the same uh screen for a moment because we're all Gonna Be At This llm Avalanche event in two weeks so if you're in San Francisco I'm going to say it again I hope to see you there at the llm Avalanche event I'm flying to San Francisco specifically for that it's going to be a blast and I'm gonna get to meet both of you in person face to face for the first time after we've been hanging out virtually for the last like three years yeah should be fun yeah yeah I can't wait to put the picture with your button here we go that's it thank you so much for talking to us about DSP and all this great knowledge I look forward to seeing you in a few weeks man yep thanks a lot see ya so Chip hey oh my God I'm still very nervous wow I uh I know that it's gonna be incredible do not feel nervous at all I just want to say that every time I like try to write something about something that's happening in ml Ops whether that's like like real-time streaming or now this report it feels like you beat me to the punch and then I watch I see what you write and then I go oh how am I supposed to even do anything now this is this is so good so if people do not follow Chip's blog we're gonna leave a link to that in the chat because I imagine there's only like two people on here that do not know who you are and do not follow your blog it is so good and so refreshing whenever you drop a so like in-depth research really is what you do and the way you can put it all together and explain it I love it so that is my uh I really like you you don't have to say so many nice things about me so that's that's what I'm gonna do right now I'm gonna leave it at that and I'm gonna let you jump on and give your full talk and then all the questions how do I see the chat from the audience so all it's um it is on the I'll post the link for you on the behind the scenes stuff so that you can just jump in and then people can uh talk to talk to you through that and also I think so I did not do your slido thing yet but I'm going to and I'll let you know when that's ready I will try and do that right now as you're talking so Chip's got a little interactivity with her talk and if I can do my part then it's gonna be cool and so yeah don't worry it's not a big part I usually I love like stuff uh okay but cool um don't worry about the Slidell thank you so much Demetrios yeah I'll text you when it's ready don't worry I'm gonna make it happen this is this is all me I'll try I'll see you in like 20 minutes 25 minutes however long it takes yeah thank you see ya okay so can you all see me foreign cool wow uh that's an interesting experience not being able to see the audience but hi my name is Chip thank you to make sure for this very warm welcome and I love the community you guys are amazing uh so much more organized in our community so we do running a little bit of like mL of Discord community so here's a link if you want to check it out um so so the talk today is a little bit different from the talk the original title so I thought that would be more fun to just go over like open challenges because the things everything is changing so fast so I just worried it's like everything I talk uh if I show people like okay building applications it's gonna go outdated uh so I'm just trying to like focus on some of the few challenges that I see today so today ranks by like how I perceive as a important I would love to hear from everyone's like how you guys are thinking about the space so challenge one is like consistency uh so whenever we use an applications users expect some certain level of consistency and the second aspect consistency is like how to ensure that your applications can run without breaking because like a lot of time when we when we use LM we don't just use LM and get the response back but we might want to process the response for the ship applications so I think it's like pretty well known problem at this point so like for LM uh for because of its look at this nature sometimes you can have the same input and different outputs and you can enforce um enforce deterministic uh determinate determinism by saying something I said temperature is equal to zero however it doesn't really fix the cases uh when you can just have a small changes in the input and it can lead to a very big output changes for example here is just I'm just using chatibility very simple prompt you like get some review score from like one to five and you can see it's just like it's changed so I put a little exchange input a little bit and the output is like completely different and it's just like part of the soccer text nature of LM and of course there's a part about like like how we might with Downstream applications on top of LM and I was using this and I want to use om to generate a score and they want to use an application to parse the score but then I realize that this is really hard because there's no way to enforce the output schema of LMS um the Second Challenge is hallucinate hallucinations and I would say I see that as the biggest one of the biggest challenges that prevent companies from adopting um LM so this is very interesting sometimes there is a teacher who use charity BT so like see he used LGBT to write essay and then assigned his student to correct the essay and they found out that like every single one of these essays have had hallucinated informations so this is this is very dangerous for on the task way factuality like is very very important so one example is that um so like one example is of course like on the tasks that require legal age or a lot of contract uh processing but also like on the tasks that involve some kind of like code writing so for example today LM actually performed very poorly on text to SQL Generations so here's a birth Circle leaderboard and you can see that's the best model achieved like under 50 accuracy which is like very very bad um so there are multiple reasons like why um they are we we don't really know I don't I haven't met anyone who knows for sure what LMS hallucinate but they are hypothesis so one thing one by deepmeister said that like it's due to the models like the understanding of the cause and effect of their actions and another hypothesis that uh pushed by several people from open AI is that this uses a mismatch between LM internet knowledge and label label label sorry uh this is like early event for me so so I have had enough coffee and I'm just my turn Twisted all the time okay so between the LM internal knowledge and the labelers internal knowledge so imagine in the face of like when we try to fight tune um fighting the model and you wouldn't want to ask a leveler to annotate uh the answer first of all question answer and celebrities might know things that the model doesn't know so that like so if the laborer if we teach as a model to like generate responses based on information that the laborer knows but the model doesn't know so blue Essence when you're teaching the mono monologue hallucinate and this is like really hard to correct um and like we can go actually go March in in my blog post but uh we probably do not have time to unpack this in this talk but feel free to reach out we want to chat more about this uh another challenge is a privacy so it's it's a it's um it's challenging in both of my whether you build or buy so say if you want to build your own uh chat or say like if you want to let users talk to your data how to ensure that this chatbot doesn't accidentally reveal sensitive informations um so so here's the example I didn't um I I didn't update it to them to the new one uh but it's uh there's a there is like it's a sport to jailbreak jgbt and you can see a lot of examples like that so companies like companies that provide apis are like open AI or other companies but a lot of effort into making sure that like the answers do not accidentally reveal informations uh but like if you build a chatbot in-house that is your responsibility you want me to make sure that you don't just accidentally reveal some uh pii and then if you buy um then you have the whole challenge of like you have to depend on the AI providers compliance uh so right now open AI like retains the data for 30 days and it really depends on the industry whether it's accessible to you or not um another big big challenge is context uh context length um so I think we have a lot of discussions on like whether prom engineering is gonna go away so I would say as always is it is that like I don't think the context learning when uh when uh will ever go away but there will certainly be um just certain hacks around from engineering that might not last so the reason why context learning will never go away uh is because it's like a lot of conversations or questions as it's like very context dependent so it's a very interesting study on the paper like situated in QA and they found us like roughly like 16.5 percent of on the questions I require context so first of all it might be asking is like who what is the best restaurant Chinese restaurant so it's the context here might be very much dependent on like the locations like where you're asking is it in the wall in the US or in China and like or if if there's no context provided what is the exumed context like what could be there for context for for them for the questions so this is very uh common in like use cases like document processing um summarizations any use cases that involves like genes or proteins so um here's a you can see on the table there's some um study always a 1998 percentile input link and you can see that like for some use cases the context length uh can be like up to like hundreds of thousands of tokens and I notice they have been like a lot of studies to like um making the model can work with long context length however there's still like questions about like okay a model might be able to take in a hundred thousand tokens as input length but how efficiently can that model use on this token I think this is like uh still open question um one thing that's nice generative AI is that's um I'm really glad about is this like chat GPT made everyone realize the importance of data drift because I will be asking questions it was like oh because our knowledge cut off is like blah blah blah that's why I cannot answer the questions so um so also the same paper for uh I think it's a really really good paper um and so it was showing that like for the models I trained on data collected in the past um this failed to zero like to answering questions asked in the presence even when you provide the evidence in the context so like the performance drop is about like 15 and this is quite significant and here I think we have a pretty interesting breakdown of like what kind of um like how quickly things go outdated so like from like um one week six months two years 50 years here's some example um so so this leads us to the next challenge which is like so say like the model has drifted uh the data have driven because the world changes right and now we might have like the same model new data or another case when like there might be a new model architectures a new sharing technique so Transformer architecture has been incredibly sticky but how long is this Gonna Last so in many cases we're gonna have like new models so imagine that you have a company that you are using a lot of prompts and the problems have been tests that you work with the current model so like if you swap out the model underneath then how often like how well can the can the problem still work on the new model and I think there's very little study around this area right now and actually it's pretty funny um as I saw I think Hacker News a couple days on the front page so we're discussing this um so another very very big challenge is uh also like LM on the edge um so I see this like as I say there are like a lot of use cases when I just don't want to send data outside of the locations for example like uh in a lot of um we will talk to a lot of healthcare companies who just have like a lot of different like medical centers and you just like either because of the data is sensitive you don't want to send a patient data or just because of the many of these medical centers is located in in the regions where the internet is not reliable so like in the Life in that situation you just don't want to like make some about somebody on someone's life depend on whether the international uh goes through or not I also like autonomous vehicles there could be a lot of cars on the road you know each Karma have its owns like compute um compute engine distribution like um run their own model and so I drive through voice bot and I do things it's like there's a very big dreams I see a lot of uh startups are trying to do is like or build everyone a personal judgibility so it can be trained on their own data and can run out their own MacBook and the dream here is that if it's if the LM runs on their own MacBooks and you're not worried about like sharing this or sensitive informations and it can be more open and it can be more helpful to you however for this there will be a lot of like challenges as well so um so first thing is like this model one has to be able to like run um do inference on device so the device has to be powerful enough to support the model's inference and then maybe you have some like a lot of techniques to like quantize or make the model runs inference faster on the device so another challenge is also like on term of training so it's one thing to do inference on device but like how would you like update the model because we have established that the world that changes so uh so say like maybe this is a newer model or texturing technique comes out and it and it can improve the model performance and how would we update uh each model on device so on device trainings is like today's bottleneck but it really depends on on the model uh of the model size and model architecture but a lot of it is like still bottlenecked by compute and memory and Technology like for continue learning and evaluations and then if you train on servers and you go into if if you don't want you to do inference on server do all due to like sensitivity then how would you um then you put it on into the same problem if you want to send data back to a server somewhere to train as a model um and another thing is just like maybe like a heavy user model I have phytune it on a lot of my like data uh on device um so like if I somehow like update the model um the base model how how can if I have a new base model how can I still like um update how high do you still like continue um fine-tuning this new model on on that I have shared in the past so I think this is a lot of uh I was a a panel recently and there was a questions on um how is there a way to do like Reddit uh training so learning so that you can update a model on data on device without having to send the device back and it's a very big challenge um so so there's a question like choosing a model size um so I think it's related to your own device because it's a dream it should eventually run a model on device so we probably need to think carefully about whether how big the model should be and I think I say a lot of factors to consider as well when considering a model size so I've seen this like there's usually it's a sweet spot between like a model performance and cost and of course I would say this nice changes um over time a lot and also is going to bring up a point about um about counting model parameters so today we just like assume it's like five billion parameter 10 billion parameter and it's just like said as in um as in like the number of parameters is the only things that matters but I think it's like highly variants so far small it also depends on whether a model is sparse or not so like say like um 100 billion uh 100 billion parameter sparse model is going to be very different from a 100 million billion parameter dense model so I haven't seen a lot of research about sparse and dense model and LM but I would be very interested in learning more about that um so another things I haven't seen about talking around about as much is the LMS for non-english languages so you can probably guess by now uh through so much of my tongue twisting in this talk is that I'm not a native speaker so I'm from Vietnam and one of the first thing I try with Chachi PT was she used it in for Vietnamese and it didn't perform well at all so one thing is that like when I try to translate AI into Vietnamese um it's actually translated into reproductive AI so there are many many examples of like it's not doing well so so here I think it's very interesting paper actually have been several papers on the study of like lm's uh performance on uh standardized tasks but in different languages and you can see that like it doesn't perform well for for a lot of non-english languages especially for low resource languages another big challenge is um is a tokenizations process so here's a very interesting study by Jenny to show that um the median token length for different languages and gbd4 and you can see that like it's pretty bad for languages like uh for for low resource languages like Burmese or America and um so say like it's the same uh for for a given input if the tokenizations that produce a lot of tokens that can affect both the latency and cost um so yeah so um I would be interested in seeing like how much of more cost um and and latency increase for different languages um yeah so uh so a lot of apis today charge people by uh the output token length so like if a language just like have a lot of tokens and the course is clearly you can maybe a lot higher um another very interesting questions I have seen a lot about discussings is the efficiency of chat as a as an an interface or more generally like a chat as a universal interface um I was meant for this to be more of a discussions but it can't see you guys so yeah so so maybe answer in your head uh which you prefer search interface or chat interface so um I see that um people have been complaining telling me this like oh the file the chat is not very efficient right so like if you want something you have to like oh make a small talk no maybe not make small talk but like you have to give it context and and you have to like go back and forth you get exactly what you're looking for whether it's on search you like you have learned the the DSL of search and you can just like fight uh like what you're looking for so um I think it's like this is um I don't agree that I do things that I chat is not efficient but I do think that it's a very very robust API um and the reason is just like for chat you can just like type in anything anything and you get back a response it might not be very helpful response but you get back something um so and um okay this is probably going out of tangent here sorry I think the stream might have stopped I don't know if people missed out on it I'm gonna see uh oh so yeah uh where we last saw you was on Challenge number five data drift and I am going to start the stream up again just to make sure that we're all good but sorry about that don't worry I'm now on my phone so we may have to do some in a minute wait is it my internet or what happened no it's all on my side don't worry it's it's my internet went out all right it works we can see we can see we can see and people are back they're back online oh my God people I just said so many intelligent things on the spot you just totally missed it um don't worry I think everybody said it was fine and they saw it all okay it was just me that went out oh I I'm overreacting and so we're all good the chat yeah intelligent things yeah so they heard it they appreciate it I'm gonna get off the stage before I uh continue to look like an idiot I will say though that the slido is not going to happen because I need you to grant me access to that um but we can do that during q a and then we can do this later all right see you later go ahead sorry to break the float yeah no worries no worries okay so I think I'll just talk about like chat is done when is the output is not that helpful um so yeah so uh sorry I see some slack messages like you'll see everything that's great um cool so yeah so we're thinking about like uh the tension I was going into is the evolutions and um if you look back it's like it's not so we have been saying it's like it's a survival of the of the fitness um but I don't think like humans are actually optimize for being efficient humans are actually optimized to be uh to be robust in I think like survival is not of the fittest okay um I feel like I pretty do not have time to go into this in detail or so so but basically like there's a lot of discussion about like this it's not um what was very important is you have an interface that is robust and I have seen actually um so so the discussion of chat as an as a universal interface is not new it has been around like you can see like this Dan Grover has great discussions back in 2015 um because like um in China and a lot of countries in Asia people are very very familiar with chat uh as the universe interface and they do everything in chat um and also like there's a few studies that have seen I can't name them because they are confidential but basically um people have studied like um the how people prefer the search of the chat interface really depends on how much the the users have been exposed to each interface so for example for our country where a lot of populations have not been exposed to like internet for a long time they actually may prefer the chat interface because this is like feel more natural and easier to use um okay so the last challenge is a big is a little bottleneck um so we have seen just like a lot of models use a tongue tongue of data and it seems like there have been some study first of all by demise that shows that a lot of models like a lot of data is just still under utilized so like the models still has a capacity to even learn from like even more data so um and we seems to be running out of like internet data so this is very interesting study showing that the rate of training data set size grow is much faster than the rate of new data being generated so at some point I think his Mark is like 20 26 that we will actually run out of publicly available data and on top of that like the internet is being rapidly populated with AI Jr attacks so then so if so for the future if you chronic data actually train a new LMS it wouldn't likely be trained on like text General by the existing um LMS so um yes that's something like uh definitely a thing about so if anything that we have seen through jet of AI is that data is very essential to uh to any companies that want to leverage Ai and we have since like uh over the last um since AI came around uh what 70 years ago there have been many happy Cycles so maybe today is like Jeremiah is a high but we do not know like in the future what is going to be the next new hype so once things are consistent through owners is data my data is just very important so we see that a lot of companies in the face of General AI the first thing they do is to like figure out the data story like console it exiting data across departments and sources but I update like the terms of use you can see like it's like overflow and Reddit responded pretty fast but like not fast enough like you maybe they could have done it like years ago um all I put guard rails that I did a quality and governance so yes that is pretty much it and here is a summary of all these challenges and to reach out if you have any questions I'm on LinkedIn Twitter I'm also on Discord all the time and I have a book but yeah that is uh for me awesome a lot of people were saying that they had your book and they loved it and I wholeheartedly agree I think it is amazing and I always wonder how you're able to write so much and also run a company it is amazing to me it's super cool so I think there's going to be some questions coming through in the chat I don't want to cut you off because of my little um My Little interlude and I want to let people do it luckily I learned the first time that we had this conference there is a little bit of cushion now on the brakes so we can have more flexibility when it comes to the timing all right so I can ask you some questions and also in the meantime do you want to give me access to that slido and I uh while you're answering the question no no sorry you give me access to the uh document yeah the slides and also people were asking about the slides and if the slides are going to be anywhere so yes we are most definitely going to have the slides yeah everyone to check out and we will throw those in the recording right in the um in the description of the recording once it's out so while there are questions coming through who's got questions in the chat feel free to oh somebody's asking if there's going to be a next edition of the book perhaps [Laughter] oh my God I was so sorry I think this is a new talk um I got so nervous about going after mate that I I was I couldn't sleep last night I was like I need to sorry for and I was like what happens when I don't get no sleep it's just do not do not speak very well somehow like own this like immigrant immigrant side images came out when I do not sleep so perfect idea of drinking coffee beforehand the other thing is I mean you did mention in the talk in the part I um that I was on for because I got kicked off and thought everyone got kicked off for a little bit and was afraid like oh the conference stopped but I saw that uh you were talking about um how there's not what was it there's not models that are useful in other languages and especially like the languages that are less spoken or that have less speakers of the languages it's much harder to find llms that are useful with them I I want to correct that so yeah so I think it's not it's not uh proportional so the other languages does this have a lot of speakers but they're not represented equally in the Corpus of like training data so for example their language is by hundreds of millions of speakers but they are careful like what under one percent of like uh or like sorry um 0.0 some percent and I say common crawl couples so yeah I see I see okay good well uh should we get into some questions Jim you want to hear what the the chat has to say um so I just want to follow on the on the like non-english um models yes I noticed like uh so it's just like I think it's like I like to go on attention I see this like the evolutions of my language model right so you start going from like what uh both architecture wise uh like um metal Squad so we went from accounting method to like neural network so we went from a single task to like multiple task right like with like single task before we have one model which you might do machine translations and now let's do Cinema analysis and now we have like models General like multiple tasks and that's the center another access it should like go from like a model for each language to like a model for like multiple languages so so I think it's very funny to say like a language model and not languages model uh it's like language is singular uh as he was trying to write about a language modeling it's really hard it's like okay this is a model to model language or like the languages um because it's somehow it's like mid everything is like one single language is that LM can understand so yeah but but it doesn't understand some language um as well as other and I see a lot of countries actually working really hard to build language models that work very well for that uh for that language for their language super somebody was asking if you know any projects that are going on in that um that field oh I think Japan is very big on that I think Japan is like make is like a government priority uh Vietnam uh so I'm from Vietnam so I'm more aware of the effort there I think they're like if you're Vietnamese you should still like check out um ping ping ping me um they have been a very very big push both by companies and like open source community in Vietnam hmm and have you tried llms for annotating articles what do you think about that if you have entertaining article what does it mean yeah like I think it's annotating data basically from articles like uh newspaper articles or stuff like that like to summarize uh articles or fact check I think it's for the actual like data labeling and that's what I understand from the question uh but John John feel free to uh put more context around that if I'm not saying it correctly [Music] yeah so you mean like July decided is this is yeah I think so I guess from what I gather from the question so this is this is the question word for word and then I'll tell you how I interpret it and then you can go ahead and interpret it however you want to uh have you tried llm for annotating articles what do you think about that and the way that I interpret it is that it's like you're using llms to data label for your data labeling for training later another model installation does a whole premise behind it alpaca right like you use a large uh better model to like generate output and then you make a smaller model which is like copy that behavior um so I think see that's a lot um so now we've got no I think it's funny because I was talking to a friend about I was asking him like how do you think about the future of like prom engineering and he was like from engineering matters as long as human to human communication matters right because it's just like for human communication sometimes we still need to put a lot of contacts for like the other person to understand so the same thing with prime engineering is like a matter of putting in things so that like LM can understand awesome so we have a panel coming up but there are some incredible questions that are coming through the chat now it generally happens like that right it gets a little bit delayed and then people are typing and so then it's a little bit even more delayed but I want to ask these questions because they are awesome do you think that llm developers are cooperating adequately with experts from other disciplines such as Linguistics sociology and ethics so how do you like a very big questions uh so so how can we measure that I mean like I know I know example of like how different disciplines came together right like Linguistics and computational uh sign computation and computational scientists so if you look at the history of like NLP it is not uh it's definitely like already like the Merit of like statistics uh computation uh computer scientist and Linguistics so uh it's a very incredible fascinating field and I could see that like these adoptions in like other fields as well so I think I'm very excited for like Gene sequencings right about shooting it like um language model um and I can see that's like if we can bring a lot of like life science um scientists it could be pretty cool hmm yeah I completely agree so there's all kinds of other incredible questions that came through the chat and Chip you have the link to the chat I think so you can jump over there also chip is in slack in the mlops community slack and if anyone wants to ask any questions to or just tag her in the community conference Channel if you're in there and you want to start a thread and I think that's it chip I'll see you in a few weeks for the first time oh my God yes for it I don't know I don't know why but like when ever [Music] yep I got lots of expectations I don't like to keep it up here a little lower the bar please yeah yeah all right see you chip thank you again