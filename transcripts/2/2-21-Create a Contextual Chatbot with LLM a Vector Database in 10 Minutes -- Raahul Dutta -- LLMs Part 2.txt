awesome so hi guys my name is Rahul and I am talking from the envelops community in Amsterdam so in next 10 minutes I will try to provide the information how you can create a contextual chatbot with 811 Vector DB I'll try to cover it in 10 minutes so a little bit of information about the use case what is the ultimate use case we are trying to solve and little bit of information about us so uh my name is Rahul and I am working in elsevier and in elsewhere we are a research publishing organization one of the largest research publishing organization and we have lots of research articles so sometimes we need to we have some clients who are asking that can you please provide us a information about a topic in last year so for example uh only EU and that type of policy maker come to come to us and they ask that what is happening in lithium in last one years or two years so sometimes we need to we need to build a team and that team will read all the lithium related documents in elsevier and then they will try to build a policy paper and they will forward this policy paper to EU and even so with this LM we are trying to reduce this manual effort expensive manual effort uh with Lang chain Vector DB and llm model so that um this tool can read all the lithium related documents and it can provide the it can generate the policy paper and at the at the end of the slides we will show you our generated one of our generated policy papers so um that was the use case and uh we are team of three peoples more years Boaz is our intern and I was working on that part so about implementation this is the uh well I just want to interact with the basic function architecture but we have changed little bit uh in couple of places to increase the quality of the prompt and to reduce the latency so normally in the 19 architecture what happens that if you are a user then you will talk with a y b y that everybody will call the Langston orchestrator so whenever you are prompting something that prompt will first go to the embedding model it will provide you the embedding and with the embedding it will go to the vector database we are using quadrant and this Vector database will provide you more most similar three or four documents according to your parameter and when you will get the result of the vectors from the vector database it will search you're asking to go in presentation modes okay this one better cool cool sorry uh so um these vectors will send to llm model to generate the prompts so little bit of modification we have done uh where we have done this modification we will come back to you later so the first part is embedding so embedding normally use two parts the first part is that when you are uh when you are building this Vector database you need to provide all the vectors of the documents so we have had around 500k half million documents related lithium and we have found that if we use the built-in solution for Quadrant and Langston it will be like really slow for us so we have a pipeline of our key flow bytours pipeline K surface pipeline which is mainly based in tensority quantization which will provide us these result of thousands of documents in couple of minutes so that pipeline we have used to grab the embedding vector and we stored this embedding factor in a numpy um this is just like a list and then we dumped somewhere and after that we are using this quadrant database where we are uploading these numpy vectors to this quadrant database why we have used quadrant because we have found that the documentation is really cool and it will support it it supports multi-language so sometimes you can write your code in Rust you can write your code in Python um there are a couple of ways where you can upload your vector database to upload your numpy vectors to a quadrant but we used rust because in Rust we have just uploaded our 500k Vector database in some minutes we have little bit fine-tuned as index index Optimizer and Main map threshold values to to get better latency time um for when you are searching any prompt and we have deployed this quadrant and Docker in our cluster so it will provide the uh when there are like lots of loads it will be Auto scaled scale down to zero everything that can be accommodated in the create cluster then the third part is the LM fine tuning model so we have tried couple of models we have tried vikron 13 billion we got good result of 6.7 good results Bloom not a good results and rate pajama we are trying to uh account it now uh we have also fine-tuned uh Falcon 7 million instruct and with our data set and we have got really good results so for the fine tuning uh we did couple of um coding for that um so um we built our own framework where you can pass any type of model uh with PFT so this is this model architecture it support Bloom it's about Falcon it's about many type of models so for PFT you need to just Target some of the layers of the model so this is the configuration and uh and when you're doing this when when you are generating these uh PFT model you can debug anything from this pipeline you can see that your training will match attainable parameters you can load your models from here and then um we can show you actually where we have successfully fine-tuned our uh one of our model with this half million data set and it takes around 30 minutes to to fine tune the model we did it for this Falcon 7B and we did really we got really good results and uh for some of the models like Bloom uh we are trying to encapsulate with Onyx Optimizer and write an inference server but we for this demo we're just using a first API cool and for the web app we just use a gradual to to show it so I think now it's the demo time so uh before that meeting I just uh built a Jupiter notebook to show you everything so this is this blanken thing where we just embedding that parts and this quadrant local client where we just initializing the vector database is really simple what Collision name you want to Target and what content payload really simple it will take a couple of seconds to load and then the language in init method that launching init method will be called uh when you will start a start this conversation this endpoint URL is where your llm models are hosted you can you can pass these type of parameters and this is little bit of code of The Graduate interface and when you will do it uh you will see this page uh this thing and here you can find in all of your parameters so um we have some prompt dictionary that from dictionary we are using to generate the total document um Boaz actually has written these prompts uh to get the output so I'm facing the first one it will take a couple of seconds to generate the first output and uh why it is taking some time I don't know yeah it is done at least so this is this uh output and uh so in that way we in this in this type of problems we have uh tried different prompting and we got we we find that these problems are really good for some of the model so for different different model we have a different different prompt uh for the Falcon one we are using these prompts and we got this output so it was a disclaimer it is a title it is executive summary everything has been written by these earlier models recommendation introduction and the quality is really good conclusion also we have appended the source document from the from the from from where we got this type of article and uh now we are good we can provide this document to our product owners to review that so this is about the Langston chatbot that we have built in-house using our own infrastructure I think I am done that was great thank you how'd it feel s good list all right well make sure to jump over to the chat people might have questions for you see you later thank you yes see you bye [Music]