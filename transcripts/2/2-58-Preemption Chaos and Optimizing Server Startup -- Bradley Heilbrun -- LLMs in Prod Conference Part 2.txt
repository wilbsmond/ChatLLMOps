so we're going to keep it moving and next up I've got the best fun fact ever about our next guest Bradley where are you at Bradley we're currently working for replit but I've read in your bio that you are also known as YouTube's engineer number seven uh yeah I think employees have an SRE one oh my God man that's so wild back that's like uh history that is so cool yeah you can see you see all these gray hairs those are outages that have uh that's when you you remember every one of them huh exactly there's a lot a lot of lost cat videos that is awesome well dude I've been chatting so much I'm gonna let you chat know and I'll be back in 10 minutes no thank you thank you for keeping this honest all right uh I'm gonna get started here so welcome to my uh lightning talk try and keep it to 10 minutes preemption chaos and server startup is kind of a nuts and bolts um uh system talk for those uh serving their their own llms so what does a replit I should go back Bradley Halloran I work at replit what is replit um it's a online uh web-based development environment among other things um this is a shot of our IDE experience uh we use lens for a number of different things um for code completion or transform code explain code there's a debugging experience where we try and debug with the llm and we've uh be self-trained as well as hosted our own models for a long time now um well what feels like a long time this is a slide of a couple of the models that we've built it's a tweet from Mom John our founder um and one of these is open source you can find it on on hugging face but we're um I'll pitch a replica we're always looking for people so if you're interested in joining uh let us know so the abstract of this talk um using preemptable gpus in the cloud to to cut your cost by two-thirds while maintaining your uptime for your users so um large language models are large um and serving them at low latency requires the best available gpus just kind of as a general statement you know for the best user experience we want to drive down those latencies and as we push models you know as they get bigger we need better gpus but those are expensive here's a slide here's a shot from Google's pricing page um you can see that for an a100 best available GPU right now although we're testing H1 h100s is about three thousand dollars per month um no discounts by contrast the spot price is a thousand dollars um so you can cut your cost by two-thirds if you were able to use um the spot prices um what is spot wellspot is the charitable term for preemptable and best effort unfortunately practical matters um so preemptions are are real unfortunately um if you if you have any workloads on preemptable node pools you you can go to your logs and just see a steady stream of of node preemptions and stockouts are real what's the stock out well it's when an entire Zone has no resources available um and and these are these these are real events that happen every day in fact if you go to the Google's if you go to Google's documentation page for spot nodes they say Don't run highly available services on this but we have and we will so another thing that's that's sort of like a little harder to figure out is that there are guarantees are overall just worse there's only 15 seconds of notification when a node is gonna um when a node is going to go down and for example podges Russian budgets don't work which is a which is a kubernetes feature so in general we kind of tackle this from three ways um we spread across as many availability zones as possible spread the risk their techniques for falling back to expensive nodes or also having sort of a using commitments to drive down costs maybe for your Baseline capacity this talk is about number three we have only 10 minutes so focus on number three here and that's speeding up the server startup you need to be really Dynamic and reactive if the nodes are sort of like popping in and out of existence so just there's a quick story about how we sped up our server startup real quick windows can disappear in 15 seconds uh you don't want boot times to take 15 minutes which is what happened to us so here's an example I pulled from from pre-optimization there's about two minutes to get the node online this is like installing the drivers then there was about 11 minutes of our application containers starting 11 minutes and then about five minutes for the model to to load the weights and become healthy for serving in total about 18 minutes if I did my math right so the first thing we did was just let's try and make the container smaller and we were able to shave about 10 gigabytes from the from the compressed size overall and here's some examples it turns out that uh pip by default has a cache we don't need that in production we're not going to be reinstalling packages we also had some dependencies that were Dev and test only like Pi torch turns out installing pytorch also in store installed Cuda the Cuda libraries are huge not two gigabytes there switching to a slim base image pretty pretty standard stuff we use Triton inference server from Nvidia to serve our models the self-hosted models and by default it includes support for multiple Frameworks tensorflow pytorch Onyx and if you're starting one model you probably only need one framework support so we're able to shave off a lot of dependencies there and then finally it turns out our build process was leaving a bunch of artifacts in the container and and so just kind of like General housekeeping this should a little bit of time off the 18 minutes but but sort of like not not enough it was like a minute or something like that minute or two enter gke image streaming um Google has this feature called image streaming something equivalent exists in Amazon um where to use the blog to use a quote from their blog reduces image pull time from minutes to seconds and this is this is what happened for us um honestly our our the container portion of the startup went from minutes to seconds and it does this by in the background streaming the actual file contents as you read them so this works great if you don't need every file in a container um which was the case for us that might not be the case for for everyone's container but it was the case for us so it was a great help not mentioned as as sort of like um obviously is enabling this feature is actually on the entire node is node wide so the system containers the kubernetes containers um those actually started to boot faster as well this this shaved many minutes off of the overall start time next we came to loading the actual model as I said large models are large um and so you know like a three gigabyte a three billion parameter model might be like 12 gigabytes on debt on disk time we were fetching our models from GCS Google Cloud Storage we realized that we were fetching onto a remotely attached Spinning Disk um it's like the slowest thing you could imagine and so we're like aha let's use let's use the fastest thing available which is a locally attached uh nvme SSD oh my God this is gonna be much better it didn't help it turns out there was no improvement uh which was which was frustrating and surprising so we started to dig in we use the tool gsutil from Google um which is basically rsync for GCS and um we're only getting 50 megabytes a second um of transfer and and which was upsetting um because there's at least a gig Nick we should be able to buffer into RAM the the disk should be faster than that so like you know what's going on took a long time to figure out but switching to a different container image quintupled the performance so the the transfer speed went up 5x over 5x um just by switching the container image uh it's still a Google Cloud SDK but we moved from Alpine to slim and it got wildly better it turns out multi-processing is hard so I dug into the code for gsutil and there was this comment that said basically uh gsutil would hang on Alpine when multi-process was enabled we couldn't figure it out so we disabled multi-process um and of course nowhere is this documented and it's it's only in the repository um so that that was all quite upsetting but um this allowed the model loading portion to change from you know let's say like four minutes for that model down to less than 30 seconds um which is a huge win and allows them to come online much faster in total we got our our pod startup our server startup time from 18 minutes down to two minutes and I've seen it be even well under that um the way we did that was trimming the containers lots of um craft enabling gke image streaming moving to ephemeral local ssds which was useful once we fixed the tooling and then finally that that tooling fix where we uh were able to find that rather nasty uh bug uh or feature a mystery classify that um and so overall we got the Pod to start up much quicker and this was part of an overall strategy of moving on to preemptable nodes which cut our cost by two-thirds um and we were able to maintain our uptime so in total thank you for joining the stock I think we were able to keep it under 10 minutes so uh here we go you're making my job easy man you're making it too easy wow that was so awesome thank you a pleasure was mine I I look forward to uh more guitar uh uh later oh wait are you in San Francisco I got an inkling that you may be yeah we're based in San Francisco I'm uh uh east of Berkeley right now all right so in two weeks I'm gonna be there and I'm gonna be hosting the llm Avalanche Meetup we're expecting around a thousand people and it would be my honor if you came maybe you got other things that you have to do in real life and it's not quite as easy as just hopping on a zoom call but I'm putting it out there I would be I would be honored you would I gotta commit I gotta come in so you'll be able to see some music playing in person and you uh I I feel like you play music too I've been known to to riff a little bit yeah I I think yeah that sounds like you're being very very uh what is it called what is it when the you I mean my brain stops working at this time of night because I'm in Europe uh and uh false false modesties I think what it was modest is the word I was looking for without the false the Modest part so I'm gonna try and just get a bunch of instruments that we bring to this Meetup and we can have like a little Jam room I can't promise anything but hopefully we can jam in person and make some songs about llm infrastructure and spot instances and all that good stuff I laughed so hard when you're like yeah so we saw this comment and multi uh threading is hard so we just got rid of multi-threading that's how that goes the [Music] oh man it's so good it is so good so thank you Bradley I really appreciate this and I'll probably hit you up to bring you on the podcast if you're open for it uh soon thanks for having me [Music]