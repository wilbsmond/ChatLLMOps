and I am going to bring on a good friend Mr rasa from Human Loop where you at there he is a pleasure to be here how you doing I'm good man it is great to have you here I know that the human Loop team is helping coordinate the live Meetup that's happening in London right now so shout out to everybody that is watching from London we've got uh a London local given a talk here huh and Raza and human Loop is doing some incredible stuff I want to let everyone know that in case they want to go deeper into human Lube remember we've got the solutions tab on the left hand side of your screen you can click on that and find all kinds of good details about human Loop and speaking of live in-person events we are tuning in right now to Amsterdam that's a live view of people having some drinks and feeds by the bar and watching us live 20 seconds later so they're not going to realize it but we are putting them on screen and they're going to see it in just a moment and by the time they see it they're not on screen anymore so Raza man I'm gonna hand it over to you feel free to uh yeah so you got your screen shared there it is all right cool now I'm giving it to you and I'll be back in 20 minutes 25 minutes man talk to you soon thanks very much Matt and uh and nice to meet everyone who's virtually out there joining us what I wanted to chat about today were some of the pitfalls and best practices that we've seen from putting llms in production or helping others do that um at human Loop so maybe just to start off with you know like why are we sort of in a position to talk about this so human Loop is a developer platform that makes it easier for people to build reliable applications on top of large language models we've been doing this for over a year now uh so you know not just post chat TPT and we've seen you know several hundred projects both succeed and not succeed in production and through that we've gotten a sense of like what are some of the things that really work and also what are some of the things that seem like they might work we feel like good ideas but actually end up being pitfalls or uh are kind of things to avoid so that's what I'm to try and talk about today which is what are some of those lessons that we've seen and how can you apply them to try and get to uh more reliable or best LM applications in production so before I kind of dive into the pitfalls best practices Etc I think it's helpful for all of us to kind of be on the same page about what we're talking about what are the components of an LM app how do we how do I think about this and so the way I think about it is that llm apps are composed of traditional software with blocks you know what I call an llm block which is a combination of some kind of base model could be anthropic could be open AI it could be a custom fine-tuned uh open source model and then some sort of prompt template which is a set of instructions or a template into which data is going to be fed and some selection strategy for getting that data so it's becoming increasingly common to do things like retrieval augmentation followed by generation where our user query comes in you search for something that gets pulled into the context GitHub co-pilot grabs code from around your code base and puts that into a prompt template you know in chat GPT it's the history of the conversation that's there but in either case you've got you've always got these three components and then you know if we have agents like we just discussed in the talk previously then you chaining these together in some way or putting this in a while loop but fundamentally in order to get something to work well you've got to get all three of these pieces right and so we're thinking about like what is the right base model um how do I come up with my prompt template what data goes into it what's the selection strategy whereas where is that coming from and the challenges to building strong LM apps are that you know getting each of these three parts to be good is still difficult prompt engineering which is the art of coming up with that template or set of instructions to the model um has a big impact on the performance of the model so small changes can have quite large differences in performance but it's difficult to know what those changes are unless you do quite a lot of experimentation and then you know hallucinations or the problem as I call it is something that comes up quite a lot how do you stop llms confidently answering things incorrectly evaluation is harder than traditional software because it's often very subjective and then if you're using the largest models and they can quickly become very expensive or they may not be appropriate for situations where you need very low latency so how can you go beyond some of those things and and you know related to these challenges is just a host of questions that you have to start thinking about once you get into the weeds of building this you know you start off with like okay what model should I use should I be going open or closed how do I find good prompts should I'm using chains or agents do I use tools do I stick to prompting should I find you should I not find you you know it goes on and on and actually there's a lot of small decisions you've got to get right in order to get good performance and so what I want to talk about across this is sort of you know how can you get the right structure in place to get those decisions right um and what are the what are the differences between the people who succeed and the ones who don't so that kind of naturally leads me into talking about the pitfalls and best practices for building LM applications and there's really five or sorry four that I'm going to talk about here there's a fifth one I'll sneak in later but for that I want to talk about um and I'll go to I'll go into detail on each of these across the next slide so I won't you know dwell here for too long but um I've lived I've left this slide in here so that you guys will have a summary at the end and we can come back to this but the first one I want to talk about is objective evaluation and the fact you know the necessity for objective evaluation um so one of the the pitfalls that we see is that for some reason when people come to build LM applications especially because prompting feels different to traditional software um they don't put into place as systematic a process for measuring things as they might otherwise so they start off in the open AI playground and they're eyeballing a couple of examples they're trying things out if they do have a retrieval augmented system then you know even if they do have an evaluation system in place for the whole thing they may be not look at evaluating the quality of the retrievals versus things that are a result of the prompt or versus things that are the embedding and the mindset for some reason seems to be a little bit different to how you might go about doing traditional software I think it's very natural to end up in this because it feels like you're just programming a natural language and then related to this people don't plan ahead for when things are in production what are the feedback signals they're going to need to be able to monitor things or measure performance and they ultimately end up trying to shoehorn this into traditional analytics tools like mixpanel or they dump logs to a database with the intention of looking at those letters for driving performance improvements and they end up not looking at them or not really having a sense for how things are working and the consequence of this is that either people give up they try something and they think oh it doesn't work so we've seen a few companies you know different companies running in parallel some of them achieving it on others like mistakenly reaching the conclusion that it's not possible with llms or you see people just trying a lot of different things one after another changing the retrieval system trying chains trying agents doing different prompting strategies and they don't really have a sense of whether or not they're making progress and so that's the kind of like Risk here um and I also wanted to talk about okay what are examples of getting it right um and so oh yeah and before I do that actually it's just bearing in mind that evaluation matters like at various different stages of building these applications and so some people have it in some of these but not in all so the place where it's often missing the most is actually during prompt engineering and iteration um you know you're tweaking things very quickly it's very easy to try a lot of different changes without versioning them or seeing the history um but it's also actually important to have good evaluations once you're in Productions you can monitor what's happening and whether or not your users are actually having a good outcome and then finally if you make any changes um it's hard to avoid regressions you know these are the same problems you have with traditional software but now if you're going in and changing a prompt or you're you know upgrading from one model to the next or something like that how do you know that you aren't causing problems or you know causing regressions it's not as easy as it is for traditional software um so maybe to look at one example of you know an application that really gets this right is I think GitHub and it's an interesting one to look at because they also took evaluation like really really seriously when they did this so um GitHub co-pilot you know most people are familiar with coding assistant one of the most successful applications of llms out there well over a million users and serving a quite critical audience software engineers and one of the things that they do for evaluation is they rely very heavily on end user feedback in production so they're looking at um when you get a suggestion from GitHub co-pilot was that suggestion accepted But Not only was it accepted but does it stay in your code at various different time intervals later so they're really getting a very strong signal of was the code they generated actually useful to people and that allows them both to monitor the application well but also take steps to improve it later um and just you know the amount of thought on engineering that's gone into this I think like speaks to the importance of having appropriate evaluation Tools in place and it's not just GitHub co-pilot that does this you know I think the best apps that we've seen out there and I just put a handle here chat GPT find suit or write others try to capture both explicit and implicit sources of user feedback in production in addition to human feedback during development and use that for monitoring Improvement and development and so you know one thing that the pattern that we've seen to work out really well is trying to capture these three types of feedback uh votes are like the simplest one that we you know you'll see in applications things like thumbs up thumbs down but also actions so implicit signals of what is and isn't working and then finally uh Corrections uh you know if users are able to edit a generation then that's a very useful thing to be able to capture so if you take the case of GitHub co-pilot being able to capture any edited text is extremely is extremely helpful um and then okay so yeah so that's the first sort of Pitfall but also best practice right which is the importance of having objective evaluation if you can't measure things then you can't improve them and kind of everything else in terms of all the effort you pour into your applications can go to waste the next one that I think is underappreciated is actually the importance of having good infrastructure around prompt management we have all of these new artifacts floating around um that affect the performance of our applications but it's easy to not give them the same seriousness as you would to you know your normal code and so it's it's very common to see people starting off in something like an open AI playground and then using Two fighter to kind of push the experimentation a little further maybe prompt templates end up being stored in Excel or if they're collaborating with non-technical teammates they maybe put them on that in Google Docs as well and the problem with this is it's really easy to lose histories of experimentation so you're trying lots of stuff you're implicitly learning a lot in that process that your teammates then don't have access to so you're not accumulating learnings we've seen companies like actually run the same set of evaluations with external annotators on the same prompts multiple times because they hadn't realized that across teams they basically tried the same thing and so a lot of effort gets spent trying to solve this and then I think people also underestimate how hard it is to get the right Tooling in place for this so it feels like maybe you can hook together a stream that app and do better notebooks with spreadsheets and get something that works quite well but the challenges that they become a main the maintenance burden becomes really high and it's difficult to have the right level of access controls to who can deploy things to production or not so you you often either have a lot of friction with getting things from that system into code and into production or you end up in a situation where if you do do what might feel like the most natural solution which is let's just keep everything in in code and use git for versioning and management right it's these are prompt templates are fundamentally just text artifacts they're like code the problem that we've seen with that is that unlike traditional software there's typically a lot more collaboration going on between domain experts who might have an ability to contribute to prompt engineering and software engineers and so if you put everything into git and you should definitely do that but you don't also have a way for the non-technical experts to contribute to that or change it you end up creating a lot of friction in sort of how well the team can uh can sort of work together and so you know the solutions here you can there are many options you know we human works on this you don't need to use human Loop but I think there are three things that you want from a system like this whatever system you choose to have that makes it work well which is one you want it to record a history of everything you're trying throughout experimentation right the way through from when you're playing in the playground when you're doing more quantitative evaluation and you should be storing what was the history alongside the model config and making it genuinely easily accessible so you can draw conclusions from that and it needs to be accessible to both non-technical and Technical team members because if you separate this into code then you end up alienating a really key stakeholder in this process so the next so that's kind of where I want to leave that one the next one I want to talk about um is actually avoiding premature optimizations and this relates really strongly to the previous talk that we just listened to so I think the phrase was used you know my friends on Twitter are gaslighting me with how good these AI agents work and actually in practice um that's what we've seen happening um with people who've tried to put this in production as well so we've had a number of customers who have tried to cross a wide range of use cases to try and build with agents or chains and sometimes moving really quickly to quite complicated sets of chains or agents um to try and get things to work and later on having to rip these things out um and the you know the problem with doing that too soon and I'm not saying that that chains and models don't work or agents don't work in certain circumstances they definitely do and we've seen positive examples of it but if you do it too soon it makes it harder to evaluate what isn't isn't working because you have multiple different places that you're changing things and so you have this combinatorial complexity or I you know I have five different prompts in a chain so which one of those is affecting outcomes it becomes harder to maintain things over time because as much as you might try to modularize this changes in one place often affect things down the line and it's also harder to evaluate so the kind of advice on this one is actually to start with the best model that you can get your hands on so typically this is one of the the largest LM gpd4 cloud from anthropic whatever it might be and don't over optimize costs and latency don't over optimize the complexity of you know stuff early on focus on the limits of what you can do with prompt engineering initially push that as far as you humanly can and then once you've done that the next thing that I might consider would be fine-tuning smaller models to try and improve performance or latency and I would only go to agents or chains if you're in a situation more complex chains if you're in a situation where reasoning is really important and where you've like you have explored these other Alternatives and found them to be wanting first with one caveat to that which is there is like a very common change you know there are two caveats I guess one is chat you know you can think of as chat as the most primitive agent and that obviously works well and the other is retrieval augmented generation where your first sort of calling from a database of some kind putting that into a prompt template and then calling the model those chains work but but more complicated ones I would discourage people from starting there unless they've already explored Alternatives and and seen how well they work and and so you know when people start off with prompt engineering they hit the limits the default today feels like it's to go to something like chaining or agents but actually I think this is another kind of common mistake which is to underestimate the power of fine-tuning so people have gotten so used to the power of these very large models that there's a tendency to assume that fine-tuning smaller models will either require more data that can get the Hands-On or that small models you know it's anything smaller than GP 3.5 won't be effective so it's not worth trying and actually I think this is a really common misconception like we quite regularly see customers successfully fine-tuning smaller models often not relying that much on that much annotated data so sometimes hundreds of data points is enough for them to get started and thousands of data points can get you really good performance in terms of annotated data for fine tuning and when you've got a smaller model you're getting the benefits of increased cost and you know well lower cost yeah the consequences of not doing this that's what my slide says our increased cost and latency Reliance on models are larger than you need and the fact that you don't benefit from proprietary data and by that I mean that if you are fine-tuning and you're capturing feedback data in production then actually this can give you a bit of a data flywheel where you're able to improve the performance of your applications quicker than your competitors because you can free your specific use case get get better at that task right the very largest models are very general in their capabilities but if you're trying to generate a sales email or you're trying to answer questions about a legal document you don't need dpd4 to be able to do all of the other things that can do like write poetry or answer questions about sports whatever it might be those are not relevant tasks so fine-tuning can be very effective and we see this kind of often underestimated and people are very surprised when they try this we had a customer who spent I think three weeks like doing different forms of prompt engineering they tried retrieval augmentation embedding their kind of history of data and we ask them like if you have all this historical data to embed and it's not changing very quickly have you tried fine-tuning they hadn't and when they did it outperforms gpd4 a much smaller model for their tasks quite quickly I think they did three and a half thousand data points so um another kind of concrete example of this in practice is uh is one of my favorite startups I use this pretty frequently which is a company called find which is a search engine for developers so it's llm based search they do like we've discussed retrieval first they put that into a template and then they generate an answer to your question but they're very much focused on questions that are relevant to developers and software Engineers so their model needs to be much better at code and those things but it doesn't need to be good in general and they started with gpd4 they gathered a lot of different you know user feedback in production and you can see that on the right hand side I just grabbed a snippet of all of their kind of feedback buttons that they have in the app and as a result they've been able to fine-tune a custom open source model and that model is now or in my in the case I've written here open source model I'll fix that after and that is now better for performance in their Niche than anything that you could get from the closed model providers if not better in general but it is better for their specific use case and I think it's a really you know telling case study and as a result they have lower costs lower latency and better performance and I asked them kind of if they had any recommendations on what the best performing open source models are and they recommended the flan T5 and flannel 2 sort of family of models from Google they also said they're exploring Falcon right now but they've had a lot of success with those models um and you know I kind of mentioned this in passing but fine-tuning builds you a level of defensibility that it's difficult to maybe otherwise get and a very common pattern for doing this that we've seen in terms of the best practice that works well is people will generate data from their existing model they'll filter that data based on some success criteria and that criteria might be explicit user feedback it might actually be another llm scoring those data points for how good they are and then they'll refine team and often they can run a few cycles of this while still getting improved performance over time and so you know I I come I come to the end of my my talk and really want to like leave you guys with a message which is that like we need to draw the right lessons from traditional software with respect to building applications for large language models so it's not that we want you know in the in the beginning I talked about people having maybe a casual or less rigorous attitude toward things like prompt engineering um then they might otherwise have uh compared to normal traditional software because it feels like it's in natural language and maybe isn't quite as um it doesn't feel the same as code I'm not advocating that we should copy all of the same things from traditional software though I think that actually we need in this case our own set of you know tools that are appropriate for the job that have been designed from the bottom up from first principles with LMS in mind and I kind of jotted down like some of the traditional software principles that you might have but the thought about like why they're different in the case of llms right so the first point I talked spoke about having prompt management and having a good way of handling that um you know we have solved this problem for traditional software we have git we have Version Control so why do we need something different um and in this case the big difference with LM applications is there tends to be a lot more collaboration between Technical and non-technical team members prompt engineering is something that actually you often want a domain expert to be involved in and also the speed of experimentation and updating is just a lot higher so for traditional software you write a spec you can write tests you get something going and then you deploy it and yeah you're going to change and update it over time but not at the frequency that you're going to be doing this for LM based models whether that's fine-tuning or updating prompts um and then when it comes to evaluation and testing like obviously we do evaluation and testing for traditional software but now we're in a situation where the outcomes are non-deterministic there's a lot of different criteria for Success because everything is so much more subjective so you it's much harder to write down an objective metric like accuracy or even just write a unit test that it would be compared to um you know even traditional machine learning right at least in you know you're doing classification or ner or something like that there is a correct answer here it's not so clear-cut um and then when it comes to something like Ci CD we obviously want something similar for llms right we want to avoid regressions we want to be able to make changes without worrying about it but we need a different type of tool partly because we want a much faster iteration cycle than we might for traditional software right like you can go in and make changes in natural language to a prompt and have an impact from that almost immediately we want to be learning from data and so having the ability to kind of co-locate feedback data and be able to use that as part of this process is different and it's just much harder to write unit tests you can maybe use llms as part of this because they're quite good at evaluating their own outputs but it's you know there isn't a correct answer for a lot of the things we want llms to do which is why we need to have something that's slightly different so I would be encouraging people to think critically about being rigorous in their development of llm applications especially if they want to see success in production but also to kind of acknowledge the differences in what they're doing now compared to traditional software and that's really you know you know what we've been thinking about as we've been developing applications at human loop we're trying to build the right tooling for this Paradigm from the ground up so thanks very much and if there are any questions I'd be very happy to answer them oh you saved me your timing is impeccable and I just have to say that you make my job much easier so I'm very glad to meet you guys I had a little stopwatch up in front of me I was like if I can get the same stuff in then I'll uh I'll accelerate in the places where I can yes so now we have time for questions there were some awesome questions that came through in the chat and I get to ask you to them directly first things first how do you manage these so-called data leaks that you spoke of so data leaks I'm not gonna or leaks I think is what you talked you said maybe I threw in the data there I missed did I did I mention leaks to someone mind just like clarifying what they what the drum yeah in a minute tell us what exactly you mean by leaks and I'll go on to the next question which is a bit of an ethical one it's a beefy one so excellent if you're on your ethicist hat what are your views on the ethics of GitHub co-pilot sending Telemetry data back to Microsoft is this an example of opting into alignment or something more sinister I think as long as people know up front what they're signing up for and you always get the permissions from users to capture Telemetry then then I think it's fine right people have to be willing to give that people have to give it willingly um and as long as it is willingly given um and everyone's like clear on what's going on that actually it improves the service for everybody and everyone benefits together but I think like where it's a bit Sinister is if they were capturing these things without permission so for me that's the that's the ethical line that I wouldn't want to cross um but as long as you make it clear to your users what you're doing I actually think this benefits them so basically if you say it it's cool if not not cool no bueno no yeah let people choose that makes sense all right so do you have examples of tools we can leverage for prompt management aside from General Solutions like Google Docs Etc I think you you may know a tool that helps with that is there yeah this case like part of the reason I've seen this so much and thought about it so much is because this is part of the human Loop platform right so part of what human Loop does is it gives you a place where your team can iterate on prompts find out what works evaluate performance manage the ICD like the reason that I'm able to talk so deeply about some of these problems is we've been building tooling specifically to solve them so I'd encourage you to come check it out um you can you can sign up and try it for free uh and yeah that's my that's my one that's my main recommendation few questions that came through about fine tuning uh is there a way so basically there was one that said I have a ah I'm trying to every time somebody asks a question then it pulls me back down so I have a can I use the 16 gigabyte laptop with GPU without gpus to fine-tune Falcon and I guess they're talking about I mean the small Falcon I would yeah I assume so um honestly I would assume not that I don't know as in like I'd be very surprised if it's the case like we've been getting Falcon up and running you know to try and include it as one of the models in the human Loop playground and and uh working with a company called Mystic AI I'll give them a quick shout out to because they've been super helpful to us um and it's been hard work to actually get those models to be served and performant and you know have low latency and fine-tune them so and that's operating on a lot of gpus so I'd be surprised if you can get the Falcon model the you know even the medium-sized one there's two right the smaller and larger running on a laptop without gpus at 16 gigs but you know people are very creative and maybe you can quantize it or do something smart so I don't want to I don't say can't be done but I'd be I'd be surprised just yeah maybe next week someone is going to come out with something cool like runner on your phone it's all good so going to the uh going to the question about and I'm just checking time this might be our last one because I wanted to get in a quick meditation during the break before we have walids um while Leeds talk coming up there is an awesome question here from Matt specific version of the data leak question how do you and human Loop think about the model exposing private information that it's been pre-trained or fine-tuned on if you fine-tune on docs that not everyone at the company has access to and someone prompt injects it to get the output yeah so this this is a really interesting question and maybe I can expand a little bit on just explaining what Matt means for for people out there because I don't know if everyone's going to come across this issue before but the the situation that you would be in is you go and you gather a bunch of data and you fine-tune a model and some of that data you gather may be private to you know maybe a subsection of your company but now it's stored in the weights of your model somehow can you know could other people at the company uh exfiltrate that that data that they shouldn't have access to and how can you kind of mitigate or have you have that control and the honest answer is I think Beyond fine-tuning separate models or not using instruction tuned models I don't think there is a very clear-cut solution to this yet I think this is like something that people are still working on so what we've seen I haven't actually seen a situation where people have shared a fine-tuned model across people who wouldn't all have had access to the find to the to the training data um so it's either been in cases where the training data was like publicly available like find where they were fine-tuning on you know search results or it's been within a company department where everyone has data access if you were trying to control that my suspicion is you might want to have something like adapters um so these are like lightweight things like low rank basically changes to the model that you can fine tune and swap out so that you could have multiple different versions of a fine-tuned model uh of which you know you could control access in certain subsets of your company but but I think it's an unsolved problem dude awesome uh there are a few more incredible questions coming through in the chat and so I am going to direct you there if anyone wants to talk with Raza keep the questions coming and chat and I'll answer people there if people want to uh there were some awesome ones so Raza my man thank you so much and if anybody wants to know more about human Loop you just gotta click on the solutions tab on your left hand sidebar and you can go down and uh explore the booth talk with some people and also if you're in London and you're at the watch party that's happening what's happening and thank you Raza for making that happen thank you for sponsoring this event it is so cool down here and of course thank you for the wisdom and thank you thanks so much for having me and thanks for organizing an awesome conference there we go all right man I'll see you later