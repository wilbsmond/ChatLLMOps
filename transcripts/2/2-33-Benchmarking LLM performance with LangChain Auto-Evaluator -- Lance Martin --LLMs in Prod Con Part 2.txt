all right next on the agenda I think we're one half behind is Lance Lance thank you so much for joining us thank you for your patience hello I'm here that's right you share my screen yep here you go all righty thank you well thanks for having me and really enjoyed the talks today I'll talk a bit about Lang chain which is a framework uh for building language model applications um and one of the popular paradigms here is retrieval augmented llms so this is where we want to retrieve documents from some source put them in the working memory of an llm and produce answers so Lang chain has Integrations kind of cross this kind of flow here over 80 Integrations for document loading over 30 Integrations for storage different Vector stores over 30 for retrieval and over 40 different llms and I'll walk you through kind of how you can use it to build an application and evaluate it with some of our tools so let's take an application where we want to build a chat app from a set of YouTube URLs um now we can actually get the documents or text from a set of URLs in really only um three or four lines of code using one of our document loaders I show this here um and I'll provide slides and links later so you can dig into this yourself but it's very easy to go from any list of YouTube URLs or playlists to set up documents when you have this you're kind of confronted with this challenge like how do you set these parameters they're split size or split overlaps split method retrievers different language models and so we actually have an application for this called Auto evaluator which is free to use hosted application and it's also open source where you can input your documents of choice and you can input your settings and it'll evaluate it for you against a test that you provide or an auto-generated test set of question answered pairs do you expect remember documents so this kind of shows you what it looks like I share the link down there but it has user settings on this side so it's kind of a playground environment you can select all sorts of different settings um you know different chunk sizes different models different retriever methods um different numbers of docks to retrieve again you can see you can actually just upload your documents of interest you can upload an email set run a bunch of experiments and in this nice UI you can see down here logs or experiments you probably can't see the details of this particular run but that's okay it's showing you the big idea this is a nice little playground where you can upload any documents and quickly evaluate different chain parameters and behind the scenes Lane chain we'll build a chain for you and evaluate it and so this is we this is for our example application we took all the lectures from carpathy's recent uh llm course uh we input it into our app we test a bunch of different settings so here I test let's say I want to try different chunk sizes different llms to synthesize answers and you can kind of see in the visualization it shows you you know which setting gave you better quality versus latency so you can kind of you can see here in this particular case smaller chunks were a little bit better you can see open source models like the kuna did lag gbt35 and 4 a little bit in this case also in this case gpd435 performed about the same but indicate this is just showing generally how to use this application for really any um any question answer application you'd want to build um so again use Auto evaluator to kind of select your settings in a no code manner very easy and then we provide some nice templates uh Langston works very nicely with for example fast API you can set up a streaming back end connect that to a new front-end framework you like for example for cell the app is here shown and this is kind of it running and so it will stream answers so Lang chain is powering all this um and again we kind of use the auto evaluator itself to kind of choose our best settings before we went and implemented them since building your vector DB is kind of costly it's a nice adjustment to check things before you go to all the effort of building default Vector DB um and I'll share a few other learnings that we've had with this Auto evaluator app so this slide's a little bit busy but the main thing I'm trying to communicate is like there's lots of different ways to approach a retrieval question in these retrieval augmented generation applications of course there's kind of lexical statistical search of course there's semantic search which many are very familiar with there's also some kind of newer methods that use semantic search with metadata filtering um the sub Queen retriever is one very interesting one in line chain and I'll talk about that in a little bit um and the thing one other thing I'll highlight is some newer models with very large contact sizes like anthropic's 100k model you can actually stuff documents in together um also I want to highlight post-processing coheres re-rank is a very interesting way of doing that and so we've integrated a lot of these options with auto evaluator and I'll just share a few like little insights one thing I found very interesting is that anthropic's 100K model is actually really good um this is actually inputting the entire gpt3 paper it's a 75 page PDF I have a test set of 15 or I'm sorry in this case five questions and it performs as well as three five and gpd4 with a independent Vector store retriever so again this is taking that whole 75 page PDF putting into the working memory of the model and just asking the model questions about it was very impressive of course it's slower but it's an interesting thing to highlight if you have like a single document use case maybe you can get away with using one of these larger models putting the entire document in and don't worry about like building an independent Vector DB um another thing I'll highlight open source models are very much worth considering um mosaics MPT 7B for example is quite fast um kudos to them on the inference work there I've seen very good results from fukuna as well it's we host on replicate you can play with both of these in Auto evaluator and it's very much worth considering these particularly for retrieval augmented generation you may not need a massive model some of these smaller open source models uh could can be sufficient um and the final thing I'll wrap with metadata filtering is like a very interesting topic um we have an application it's linked here it's part of Auto Valley where you can test different meta beta filtering schemes it can be a little bit tricky like this is a case where I had an app built for the entire podcast um of Lex Friedman and if you want to ask a question say uh what did Elon say in episode 252 I want to do a metadata filter for that in some cases um your metadata is actually stored using a different format which is non-obvious so in that case something like so Queen retriever might fail and things like other libraries like core may be more useful I provide a bunch of examples in the notes here you can go go into that in my slides um but that provides an alternative and we provide a nice way to kind of evaluate different metadata filtering schemes which can be quite tricky actually because it's often harder for the metadata from the natural language query um and maybe so in summary Lang chain is kind of framework for building and evaluating llm applications with many Integrations over 80 Integrations for loading 30 for storage retrieval four different llms we have tools for evaluation and prototyping um we have app templates like carbathy GPT that highlights how to stream applications with playing chain uh integrate with different front ends like Purcell we have support for Python and JavaScript so uh everything is free to use everything is open source so um yep that's probably about it please reach out to me with any questions and thanks for the opportunity to speak awesome thank you so much Lance I really appreciate it and we'll send out the slides and the recordings to everybody who's tuned in um a little bit later thank you so much thank you all right see you later [Music] all right