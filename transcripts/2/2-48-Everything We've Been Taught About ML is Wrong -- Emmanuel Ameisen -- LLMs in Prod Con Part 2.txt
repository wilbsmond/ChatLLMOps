okay next up this is gonna be awesome we have a manual from anthropic with a very spicy title for his talk uh it is everything we've been taught about ml is wrong so let's see let's see what he has to to teach us about what's new in the world of ml hey Emmanuel hello um thanks for the intro uh I think yeah I think everybody can see my slides so I think I'll just get started you look good take it away perfect thank you um yeah so you know spicy title Justice clickbait uh the takes are much more nuanced but but I think there's a lot of truth to this and so uh basically the inspiration for this talk is that you know I've been working in ml for almost 10 years now and I found that like a lot of my fundamental intuitions now that we've moved in the world of albums are wrong and so I'll tell you a bit about uh why I think that's the case so you know who am I uh why should you maybe listen to me uh so you know I'm Emmanuel I work uh currently at anthropic uh although I just started very recently um and I work as a research engineer so I can improve the elements here before that I was at stripe for a long time um where I worked I like building models improving models and a lot of ml opsy things like automatically evaluating and deploying models and that sort of stuff um and then before that actually I worked in ml education where I developed a lot of these kind of like um teachings about ml that I would teach people transitioning into the field you know I mentored over a 100 data scientists that would like transition and become data scientists in Industry um and so I I had a lot of strong opinions about what are good and bad ideas when you're doing an ml project and over the last couple years I found that a lot of these strong opinions uh are outdated so what are we going to talk about so here's some of the ml fundamental fundamentals that I used to teach um you know the first thing is like start with a simple model um this is again something that kind of like makes sense it's like if you are going to do any ml task you should probably take the simplest possible approach uh and try that and you know if that works good move on if it doesn't uh you know maybe it's a very hard task but you should definitely not start with like a super complicated model so that might be like logistic regression if you're doing kind of like um um tabular classification you know kind of like fraud prediction like what we did a stripe uh that's a great Baseline uh bag of words if you're doing NLP or maybe you know transfer learning or resonant if you're doing uh anything in computer vision what else would you do you should probably use relevant training data that seems like a truism but it's like if you know if you want to um to take again describe example like uh predict fraud well you should take a data set of like credit card transactions good and bad and then train on that um if you instead took a data set like I don't know like a patient care information from a hospital you wouldn't expect to have a good fraud classifier um you know spam classifiers can't be trained uh on like a review data set uh don't use synthetic data this may be like a more spicy take I had but I found that uh in practice it was very hard to get synthetic data working so like uh specifically model generated data I feel like in the recent history of ml has um not often done whales like people have often proposed like using Gans to like make your ml models better um but in practice that seems to always be hard infinity and your model is very good at usually detecting that you know this is kind of fake data you're passing it and overfitting onto some like synthetic feature um and finally you know like I guess like beware of automated metrics uh it's easy to fool yourself into thinking you have a great model um without looking into Data yourself you know the most naive example of this is like people reporting uh 99 accuracy when they're working on a problem where uh you know there's point one percent uh of positives and the rest of negatives you would get 99.9 accuracy by just always saying no um and so looking at the data is always useful and finally this is kind of like the you know Converse of use relevant training data it's user model only in the task it was trained on that kind of makes sense you're training like a fraud classifier again you're not going to use it to like predict you know uh if a review is positive or not cool so I'm going to go through each of those and explain to you why I think they're categorically wrong now so start with a simple model um I was so convinced by this that I like wrote a blog post about it um and you know my recognition is always like yeah just try like the simplest stupidest thing you can think of a logistic regression usually is great for anything that's not NLP of computer or computer vision um and that's just because it's the fastest way to kind of like get an initial result in the fastest way to decide whether it's worth investing more time and what it's worth investing in um but now uh you know if I want to write a regular expression uh I don't I just use like an llm instead um if I want to like do something as simple as wrapping every element of a list in parentheses I use nlm um if I want to like reformat something I use an llm and so we've gone from like use a simple model to do a couple complex tasks like use an incredibly complex model to do a simple task and so you know why and why has has that happened well I think the meaning of Simplicity in the context of this expression has changed Simplicity used to mean like you know uh a simple model in terms of like how hard it is to build but really the goal is to increase the duration speed and so you should just take the shortest path to results and that used to be correlated with taking the simplest model but it's not anymore um and why is that well um let's take an example so like what's the best way to estimate whether you can like classify some tasks some text which was something we would like commonly do um and inside my education job well in 2015 you do like uh logistic regression and bag of words it takes you half a day and you have a pretty good Baseline uh and you can tell you know whether it's possible or not whether the data is good or not Etc in 2019 uh you know kind of large large-ish models were starting to be released and so you just like fine-tuned Bert uh that would take you a couple hours and again would give you like a really good Baseline faster than literacy regression slightly more complex model today you know just using llm just like ask it to do the things if it does the thing and then like you're you're much better off and that takes you two minutes and you know as astute observers will note and these models are getting increasingly more complex right like a loser regression might have a thousand parameters depending on the um dimension of your word to Evac embeddings Bert has about 110 million you know llms commonly have over 100 billion um although of course that depends um so start with a simple model no start with a competent model I think is uh really the new version of this and you know as with all these hot takes there's going to be a bunch of exceptions like if you're doing something that's not NLP related uh you know maybe that doesn't apply or something that you cannot easily ask an llm to do of course you might still want to start with kind of a simple bass line cool so use relevant data um you know if this was live I would do a poll like a show of hands but you know if I asked you uh which data set should you use if you wanted to like for example generate realistic looking Yelp reviews um and then I should point out that there is a Yelp open data set that has a bunch of Yelp reviews in it um and I should of course also point out that this is a trick question so I'll let you think about it you can tell me what it is that you want to use see if there's like uh the answer to this uh is just use all of the internet of course why would you only use the other data sets um if you use the whole internet you will do much better than if you just train on Yelp data training on yelpstata only gets you so far the rest of the internet will give you so many more uh linguistic abilities and reasoning abilities that you'll be able to um just perform much better on that task and this is sort of this is a chart from like the original gbt3 paper for example that shows that you know um kind of like if you take a big model and you uh prompt it even zero shot actually it will do on certain tasks better than a fine tune state-of-the-art model um and you know certainly it's that that large model is trained on all the data that I started the art model was trained on probably it's seen it in this training set and the only difference between them is the model is much larger and has been trained on all the other data um and so now it's not true that you should just train on relevant data if you have a model that's large enough you should just train on all the data because it'll make you better at all the things which is also like a ridiculous proposition I would think uh you know like would be a ridiculous proposition like five years ago again if you're training like some spam classifier and you told me like well I'm also going to train you know on sentiment analysis I'll tell you that that's a terrible idea and that you know you probably aren't fit for machine learning but is this surprising well um if you think about it this is kind of like an extension of transfer learning if you've been doing a machine learning for a while um you know uh kind of transferring learning on imagenet was the first similar example where you could train you know a computer vision model on your own data or you could train one that was trained on a massive data set imagenet and then fine tune it a little bit on your data and you'd get much better results um if you think about it the kind of like llm 0 or one shot in context learning is just an extension of that where you know in 2015 you would like uh train on your tasks data in 2019 you like train on a huge data set and then fine-tune on your data and then now you know we can just train on an absolutely staggeringly large data set and maybe we don't need to fine-tune like a little asterisk here because I think you know fine-tuning llms still actually provides benefits but uh but certainly they give you much much better performance than any model before without any fine tuning so use relevant data no just use all the data it's happening at all um you know again caveats you want to use all the good data now use bad data and only use the data that you have uh actually like um the ability to to use cool so the next one is I think like use human-made training data uh by default this is what you do in every machine learning project you know if you're doing again like fraud classification which I was doing recently you're using like credit card transactions that humans made um to like train your model if you're doing like um classifying reviews uh in terms like sentiment analysis you're using Real reviews of humans wrote um and model generated data just has a history of working poorly against the generate raw data or approach to me like every couple years and to my knowledge they have never worked on any practical application um smote was a similar technique that's just I've found in a very brittle in practice um distillation is another approach which works sometimes but is also challenging to get to work in practice and so model generate data used to be a bad idea if you were on a time crunch and certainly it could work but would require a lot of effort to get it to work um and certainly you would use a different model not the same model because you know having a model generate its own training data would be absurd in the world of ml you know you'd be told like well there's no additional information that you're entering into the system and so this is kind of like a snake eating its own tail if the model you train is generating some data and then you train them all you train on the data it generated then you're just going to mode collapse and it's not going to be interesting or is it um and Robin famously released constitutional AI which is essentially a version of this where you know we ask a trained model to read a constitution and then we ask it to use the Constitution to rate whether answers are good or not and then we train a model based on that and then we train the original model based on this this preference and the original model is better and you know that whole process is just like train model generates more data train on that data and then gets better um which you know I think again would have been absolutely wild to think of before but now it's like a very reasonable thing that gives you a parity level Improvement in terms of like how good the model is so you know maybe humans don't need to to make training data for long maybe we can use model made training data for a lot of things in the future that's really not everything but the fact that we can use it at all is a huge change all right well certainly if you know we're using models to generate data uh we still need to kind of have humans look at the outputs in order to you know verify that the outputs are actually good we can't really trust the models to judge themselves um that would also be kind of wild uh the thing is like evaluating um generative outputs is hard in fact you know if you're evaluating like summarization in Translation having a human is kind of like the only thing that we've been able to do for years because any metrics like common metrics are called you know like and blur and there's other ones um they're often very pessimistic because they punish you aggressively for deviating from like whatever label there is so you know if you're translating a sentence in from one language to another you have this kind of like gold translation that you're trying your model's trying to to match but if like your model let's say like perfectly paraphrases that translation it'll get like in many of these metrics the score zero even though that translation is correct so the gold standard is you know like ask humans to to read outputs um as I said you know having a model uh write its own output would be absurd like obviously you know it would just like I assume like just say oh like my own position I love my output right it's been uh trained to Output that output so according to it it's the maximum likelihood output it could produce um it turns out somehow that this not as insane as we thought um this is from the second paper in the sources below the Machiavelli paper um which talks about kind of like ethical behavior on Amazon is interesting um but also talks about this fascinating pattern where uh they have the researchers write a bunch of labels and then they measure uh you know crop human crowd workers um versus uh using in this case gpd4 or I think like a little like set of heuristics on top of gpd4 and you find a gpt4 is better than the human graders in fact it agrees more with their golden uh ratings than the human Raiders so it turns out that if he wants to like really evaluate a model thoroughly uh you can just use the model and in some cases not all in some cases it'll do a better job than humans which I think is also something that that would have gotten you like laughed out of any ml101 course a few years ago by the way if you're not convinced by this and you say well you know I'd still like to use human Raiders there's another recent paper that estimated that about 33 to 46 of crowd workers I believe it was on Mechanical Turk use llms when completing a task um and so it might be the case that even when you think you're getting human ratings you're actually getting llm ratings which maybe also explains some of the reasons why the differences between human and unrators has been shrinking cool um so you see Mr judge results now just use models to judge results you know use models to train your models use models to evaluate your models just models all the way down um cool next one I think is like also interesting is like in the history of machine learning you mostly had a narrow models you know models that are trained for a purpose you train a frog classifier to do fraud classification you train you know a Spam classifier just to to classify spam emails uh Etc they don't do anything else um to my knowledge no one at stripe you know ever tried to use our fraud classifiers to write poems that's just not a thing that these models do they're not built to do them they don't even have the capacity to try right the architecture does not offer it um and yeah people find new use cases for large language models that go beyond token prediction you know every day uh it seems like you know yes you can use them to like write emails but you can also use them to like read emails you can use them to do math you can use them to write code you can use them to like simulate code execution uh you can use them to help you summarize various documents uh etc etc you can use them to generate training data or evaluate model data it seems like their abilities um are just much more General than kind of their intended training objective um and so ml models now have overhangs which is not a thing they used to have meaning that they have a set of capabilities that is not necessarily immediately apparent those capabilities can be unexpected there are many surprises um I think in like uh the abilities of you know post let's say post gpt3 models um that weren't immediately apparent when those models were being trained it's you know when they were evaluated by external parties that or internal parties and folks found a bunch of of interesting um emergent abilities they also have unexplored capabilities I think this is like slightly different um and so it's like one there's they can do things that you might not expect but two even if you spend a while trying to tease out these capabilities there's no existing at the moment way to like list out all of the capabilities of such a model and so you know once you release it there will be um potentially unexplored capabilities that somebody might discover uh some of that might be great and fun and creative and some of that of course uh might cause safety risks the kind of risks that companies like anthropic are concerned about um and and that's again not something that you ever had you know if you kind of open sourced your um I don't know yeah let's take fraud classification since that's that's the example I've been using you know it's like yeah people can use it to classify fraud but there's no risk that they'll use it to like you know try to ask how to make a bomb or something um and so the fact that these these models have these overhangs is completely new and I think a little bit mind-bending as well so you know it used to be that we use a tool only for its purpose and now we have this General tool and we can teach it as purpose um by prompting all right so this is kind of like a whirlwind tour um of you know everything that that I think has has just changed quite a bit in the last three to four years luckily for us you know uh and for our sanity some things have remained the same um and I'll end the talk with just a few of those which is that you know um it is still the case that the most important thing you can do as somebody that uses machine learning whether you're building those language models or using them in your product is to just you know focus on solving a useful problem not playing with Cool Tools uh I think that's the like number one thing that I know people uh end up doing that is maybe not the best of their time is like trying to build a fancy model to do something or getting excited about new architectures a lot of the time you know spending more time uh thinking about like how you can solve something is more useful I think that's even more true in kind of like llm worlds where you know you might get a lot more value from cleaning out the data uh as has always been the case um you know that that you pass into your your prompt has zero shot examples right like better data here has has huge um effects might be better value from like prompting better as opposed to trying something much more complex models aren't magic uh yet you know um and and I say Yeah in a cheeky manner I don't think they actually ever will be but you know some of their behavior Can Can Feel magic to like magic to us um but they still hallucinate uh as the panelists were just talking about you know they still make mistakes uh they they still have biases and so uh kind of like I think any good and responsible um ml practitioner you know builds around that and kind of like almost treats the model adverse very early in in some cases where it's like okay well I know that sometimes this model will be wrong so I need to build my product around that assumption you know if um you know you're doing something anything that has to do for example with like providing advice to a user you know you might want to like um verify that your model actually is outputting something that's true uh before you know uh returning the response and that's just as true now as it was true before engineering skills are always key certainly they're key if you're building these models you know these models are kind of like just basically uh using super computers at this point uh and so like that's just a very hard engineering problem but also um in kind of like deploying these models um you know it used to be that yeah you were deploying the the like model itself and now maybe you're deploying like your prompting server but we again in the panel we're just hearing about how much work goes into like you know carefully like um crafting prompts and getting results and like post-processing them and detecting latency uh spikes and kind of addressing them in various ways and so engineering skills I feel like uh in my ml career has become more and more important every year and I think that that remains true you know still as big of a thing as it used to be um people have tried to call it llm Ops now I think we can just stay with the ml apps name uh not to revive the flame War but you know um I mean that's a lot of the challenge of deploying uh these models uh and that certainly hasn't changed with with llms if anything like handling breaking version changes in llms is I think something that's going to be uh thought about quite a bit uh and there's a new envelops challenge monitoring monitoring you always need to do it um manual projects have failed uh silently and then you know uh three or six months later somebody realizes that the model is doing something completely wrong this is also true with with nlms and so it's also useful cool that's about it um Whirlwind tour um I'm glad we got to cover it all in just just a little over the a lot of 20 minutes um yeah thanks for listening you can reach out to me you know I'm at Emma powered on Twitter uh you can reach out to me on LinkedIn uh as well if you have questions I'm also at uh Emmanuel at anthropic.com so you can email me there uh but yeah that's that's most of it awesome that was so amazing Emmanuel I thought that was like really insightful um and yeah just like so many things blew my mind in terms of like how we thought about things in the past and how to think about things differently especially like the one about um models judging models I think that sort of blew my mind a little bit not what I expected um yeah so well there's a little bit of a delay so I want to give people an opportunity to ask questions um if they have them um a couple that I'm already seeing in the chat um one from marwa uh she asked how would you take a generalized model and let it evaluate itself on a specialized task not sure if you want to tackle that yeah yeah I think that's a good question so I mean for what it's worth um we can maybe I can actually go back to that slide um because that's essentially what they do so in this slide they take a pre-general model GT4 kind of like definition of a general model in my opinion and and the tasks they're actually running in this is they are asking various models to like if I remember well to be deceptive uh in various ways or or harmful in various ways and then they're asking crowd workers and this model labeler like hey you know this was the question this was the answer uh was this deceptive was this you know related to like manipulation betrayal physical harm all that stuff and then the model just outputs the answer and so that's how they do it in this case and I think that's just generally how you can do it in in any case which is that the kind of in context learning ability of these models mean that you can take a general model and prompt it and be like hey I'm going to show you you know an example of somebody trying to let's say like have a conversation and you need to tell me whether you know that response is blah deceptive uh you know positive whatever your task is um and then you know uh have that as your prompt there's a lot of tricks to prompt engineering but of course you could like make your model even more specific by adding examples saying like you know if the person said for example oh I you know uh I I swear I'm not lying to you like that might be a little sketchy uh if the person said you know oh I want to like um build a bomb that is uh like violence related whatever like these examples um and then with that with that ability of these large models to learn in context you can take a general model and have it do your specific task um very well that's awesome marwa I hope that gave you a little bit of clarity let's see any other questions coming in there's one question from Manpreet he asked I mean this is like a philosophical question or at least it starts with it how can we trust humans for specialized domains or languages that need expertise to even understand the context how would even QA work um yeah so I think like that's funny it's kind of like a um the chicken of the egg question uh so so like I think you can trust humans uh well can you trust humans as a broader question but I'll answer the like maybe a more localized version which is um for many many tasks where there's um specific knowledge that's required let's say like I don't know like a medical task or coding um you know the answer isn't going to surprise you that like a lot of uh labeling Services let you uh select for people that have that expertise right so you might say well you know I I want to like grade these like uh yeah like these these samples they're like medical advice uh I need doctors to look over them or you know or lawyers or Engineers or whatever and so in that sense you can trust humans um although I will say that if you haven't spent much time labeling data yourself like I'd consider myself you know an expert in let's say like engineering or ml um I've labeled you know examples just for my own identification of kind of like LM outputs and it's it's extremely hard because it's like you have to like a context switch you know and like read some random like requests and then some completion and try to think carefully through bugs um even if you're good at it uh you'll at least I feel I'm pretty pretty good at the Domain and I'll make mistakes just because you know uh you especially if you're trying to label many examples and you're giving yourself let's say like a minute or two minutes for example um it's very hard especially the task is complex and so I think that's actually where um that's my theory for why these models are sort of like upperforming humans in some cases is that they're slightly worse than the human experts at it uh but there's so much more robust they don't get tired you know they can get through uh as as many model as many examples as you want and so I think that that's that's likely what is happening currently is that you know just a human Raiders even an expert is going to have a hard time writing many examples uh it takes a lot of mental Toil and it's actually quite challenging whereas a model just cranks away yeah I think I think sense cool I think those are the questions I'm seeing so far um definitely make sure to what's the best way for people to kind of ask you questions or check in on your material in the future I think you had something in the last yeah um yeah Twitter if you like it I'm a powered LinkedIn is here and I should put it on the slides but I am just my name's Emmanuel from the manual.com any of those is completely fine awesome thank you so much Manuel this is awesome have a great rest of the day yeah thanks so much [Music]