[Music] yeah there he is what's up dude there he is he's looking very focused I don't know if he can hear us I know angry yeah I think he's trying to share his screen can you hear us I don't think he he must have muted yeah he can hear us all right there we go hi what's up man good how are you dimitrios hey hi David nice to meet you how are you very good very good dude so I'm guessing you're trying to share your screen right now because I don't see it on yes I am trying to share my screen um and there we go I see it and uh I may have to give you the winning talk title anytime you start to throw around big numbers in a talk title you got my attention so 360 000 question that is where we're at right now I love that nice dude well I'm gonna kick it off to you I'll be back in like 20 25 minutes when you're done and then I'll ask you some questions does that sound all right sure it should be uh like so um am I um like what time do we like start presenting start presenting right now maybe you're on stage you're up and running all good yeah right tell me a minute I could play us a song no no I think I don't need a minute if just let me know when you're putting me live okay you already are live we've been live perfect you're fully live people are loving this but the good thing is that uh I don't know if you know anything about me and my technical Follies over the past couple months but whatever technical difficulties you have if you have any then I can one-up you in Spades so don't worry about anything you're you're all good right now I see your screen I see the slides on the left too and I'm gonna get off the stage man perfect thanks a lot dimitrios yeah all right um thanks a lot uh dimitrios for getting me in and uh nicely putting me on a spot right now um welcome everyone I am nikunj Bajaj uh co-founder and CEO at true Foundry and today as Demetrius mentioned we're going to talk about this large number 360 000 question and the goal for this talk um basically is twofold we are going to talk about what is this 360 000 number what the hell does this mean um where is this number coming from and what does it mean in the context of llms and secondly we are going to put this number in perspective um the idea is like you know the the overall overarching goal here is we all of us have been starting to use have started to use llms and there's a lot of interesting type of pricing that we see in terms of GPU usage in terms of API calls in terms of how many tokens you're invoking and while in my conversations with customers I realized that this pricing is not as clear in people's heads so I just decided that I'm gonna take take this time in the talk and just demystify and put all the pricing of llms in perspective okay and I do want to call out in the beginning that this is actually not going to be a presentation on llms you will notice why I mentioned that this is going to be practically a math presentation and we are going to be dealing with a lot of numbers we are going to be doing a lot of additions and multiplications so so bear with me on that um cool so before we get started let's actually Define um a sample problem statement that we are going to solve throughout this presentation okay and given that we are talking about NLP llms Wikipedia is our great friend so so the problem statement that we want to take up is we want to summarize the entire Wikipedia to half its size it's a very dummy problem statement please don't try this at home um but but it will hopefully help put things in perspective so we're looking at about 6 million articles in Wikipedia where each article has on an average 750 words I have taken some artistic Liberties when I came in with this numbers because I have approximated them so please uh don't hold me accountable to the last decimal point here um now 750 words roughly translates to a thousand tokens and token is essentially the currency of pricing when it comes to llams right and our goal is that we want to reduce this to uh 50 less words so same 6 million articles but now we want to have only 500 tokens per article so that's the problem that we're trying to solve and we're going to use our favorite llms to solve these problems now just a quick disclaimer that while I'm focusing only on the cost aspect of running llms in this presentation which element to decide uh like that that works best for you is completely for you to decide because there are so many other factors like the quality of response the latency uh the Privacy angle to it um so so you you want to figure out like you know all those like way in all those other factors and decide but in this case we are going to just focus on the economics around it all right so what's our favorite model what's our favorite model our favorite model is GPT fold um gpt4 with 8K context length let's talk about how are we going to use gpt4 and summarize our Wikipedia to half its size so there are two main levels of pricing when we talk about GPT food number one that you pay for the number of tokens that you send in your prompt okay and number two you pay for the number of tokens that it sends you in its response okay as simple as that now they charge about 30 dollars for every million token that you send in the prompt and sixty dollars for every million token that it gives you in the response so in our case we talked about Wikipedia articles 1000 tokens per article right so basically one million tokens would be one thousand such articles okay so if you summarize 1000 articles from one million tokens to roughly 500k tokens what's happening let's let's look at that and overall we have six thousand such blocks of Articles right so effectively here's the pricing you have 6 000 articles you're paying 30 uh dollars for the prompt so roughly that's that's come that comes out to 180 000 and for your response we have again like you know six thousand articles and we multiply it by a factor of half because we are reducing the size to half um and then sixty dollars for the pricing so that's another 180 000 so that's your number three hundred sixty thousand dollars to take Wikipedia and reduce it to half the size okay that's that's what I meant by the 360 000 question and now let's go ahead and try to put this number in perspective first off if we choose the bigger brother of gpt4 the one with the 32k context length they actually have the prompt pricing and the response pricing just 2x of the 8K context length which would effectively bring down bring up our cost to 720 000 okay and if you look at the neighbor anthropic like if you pick up one of the best models of anthropic um we see that there is some disparity in terms of how the price their prompt tokens compared to their response tokens but roughly the the pricing model is similar eleven dollars for a million prompts 32 dollars for a million response and the cost comes out to be 162 000 okay now let's let's come back to let's come back to open Ai and talk about one of the instruction tuned models so like from the GPT three threes right so they have like four models at our Babbage Curie and DaVinci and we will talk about the two uh the two of the higher end models right the da Vinci and the Fury in this in this talk now the way DaVinci is priced very similar except they they charge the same for both processing your prompts and your response so you're paying about twenty dollars per million tokens okay and that would bring uh our pricing to about 180 000 if you were to use DaVinci and the cost of running query is significantly lower actually in order of magnitude lower and you're paying basically just two dollars per million tokens uh for your prompt and response and your cost becomes eighteen thousand dollars so roughly here's the math like we started with 360k for gpt4 720k for GPT 4 with 32k context window then we talked about anthropic Da Vinci and Curie okay I think so far this this part is clear now let's make this math this this entire arithmetic a bit more complex um and talk about some of the self-hosted models right now the way you price self-hosted models is actually very very different compared to how you would price the API API calls right because now the unit of your pricing becomes the cost of your gpus right the the shovel of the of the new Gold Rush basically as they call it right um and in this case uh I'm taking a node uh with 800 gpus 8 800 gpus I'm sure most people in this talk are very familiar with this kind of node I have put together a pricing of the spot instance but realistically you might be using um the on-demand instance and like this very variable pricing across different GPU vendors so again like you know just for ease of calculation I've taken the pricing of the spot instance here which is 10 dollars per hour for this eight node uh eight GPU machine okay now how do we price how do we figure out that how much would it cost to summarize our Wikipedia uh to half its size right so the way this works is how fast a model which is roughly a 7 billion parameter model um is able to process its inputs and its outputs so the unit is called number of tokens per second or number of tokens per hour that it's able to process and you're basically going to multiply with the cost of gpus for those many number of hours right so in this example we uh this this number comes comes out to process the basically when you process the input you are able to send out all the inputs in parallel this process is essentially what we call as flop bound process okay and you're able to process the inputs a lot lot faster than you are able to produce the outputs uh which which happens in a sequence okay so the cost of processing your prompts is going to be 350 dollars so you can basically feed the entire Wikipedia to this model in about 350 dollars and you can get half of Wikipedia out of this model in about 1750 so that brings our cost to about twenty one hundred dollars okay and I'm not happy with this much amount of math I want to throw some more math into the equation so what I'm going to do next is bring fine tuning to the mix of what we're talking about so far right let's take a couple of examples of how fine tuning changes the pricing of summarizing our Wikipedia okay so let's start the the high highest end uh model available that's open to fine tuning from openai right which is DaVinci now you would notice that in this case the pricing changes quite a bit so the cost of processing your prompt tokens now is 120 dollars per million tokens this is in contrast to twenty dollars in per million tokens that we saw in the previous slide for the same DaVinci model and the same price for your producing the responses 120 dollars okay now I have also put together the pricing for the actual fine tuning using Wikipedia so like if you were taking like a Wikipedia science data set and you were fine-tuning the model with that you would roughly be paying this much amount of money okay so now let's let's get back to some some more calculations so cost of processing your input from a fine-tuned DaVinci model basically feeding the entire Wikipedia to a fine-tuned DaVinci model is 720 000 and the cost of processing the output is going to be three hundred sixty thousand dollars if you add in the cost of fine tuning that goes over 1.25 million dollars okay so 1.25 million dollar to fine tune on Wikipedia and summarize it to half its size that's the API cost that you would be paying okay but DaVinci is a very high-end model model so the default if you go by the default fine tune model with openai that's query okay and as I mentioned query is one tenth the cost of DaVinci so on an average every single number that you see on the slide is slashed by a factor of 10 so 12 dollars twelve dollars eighteen thousand dollars and this number finally becomes 126 000 dollars all right and now let's compare this pricing of fine tuning a query model compared to what we had of ourselves hosted model right the 7 billion parameter model that we were hosting now the good news about this model is whether you are using a fine-tuned version of it or you are using a trained version of it the cost of processing the inputs and producing the outputs actually does not change so this remains at 350 this remains at 1750 but you will be paying for the cost of fine tuning and the speed of training is actually different from processing the inputs and processing the outputs so roughly this comes out to be about fourteen hundred dollars which brings our net pricing to about thirty five hundred dollars here okay um now let's put all of this these numbers that we have seen in perspective if there is one table that you want to capture from this presentation it's probably this that um that kind of summarizes all the uh pricing together there are a few key takeaways that I want us to focus on so when you're invoking your open AI models right there are roughly 7x more expensive when you come when you use the fine-tuned versions of them compared to only 1.6 X more expensive when you're fine-tuning an open source model right so the way you want to think about like at least one factor that you want to consider in your choice of large language models is would you need to fine tune or are you okay with let's say a few short learning or a zero shot learning type of technique if you want to fine tune maybe it would make sense that you consider using some of the open source models again there are a lot of considerations but at least one to keep in mind okay secondly the cost increases to X if you are increasing the length of your context window so 32k context is uh charging like you're paying uh 2x more for compared to the 8K context window and lastly as all of us can imagine like the much larger models number of parameters actually you end up paying a lot more compared to like when you work with smaller models right so if you have a use case that you can work off of smaller model that's that's a lot nicer now this one slide is the only one in the presentation where I'm going to be deviating from my focus on just talking about cost because I wanted to put some things in perspective now here I'm not presenting our work actually I'm presenting um like you know one of the other uh works from the community which is done by a company called move works and what they did was not only do they talk about the pricing aspect of it they actually talk about the quality aspect of it okay so they ended up training a 7 billion parameter model internally okay they ended up fine-tuning this model and compared the performance of this model across a bunch of different use cases uh they had 14 use cases across which they evaluated I only put a sample up here okay and then they realized that you are able to get even better performance or almost equally equal equally almost a comparable performance to gpd4 few short learning so you notice that the performance here is 0.93 compared to 0.95 but then in three other tasks it's actually beating gpt4 and in this case also the performance is kind of comparable right so the the message that they give in this in this blog and I encourage everyone to read out this blog to read this blog is that lower number of parameters if you're fine tuning you could actually get a decent performance on a few tasks and that's what we believe at true Foundry that eventually um open source llms and the commercial llms are to coexist we actually don't think that one will completely Trump over the other and they are going to coexist probably even within an application like within an application you will have multiple stages and some of these problems would be would make more sense to be solved via the open source llms and some others we are the commercial elements and one simple mental model to think about this is you could offload easier or more specific tasks where you can have some more fine-tuning data the task is simpler this does not require a lot of complex reasoning you can offload that to your cheaper open source large language models and be more ambiguous the more abstract reasoning type of task that's picking up context from uh word number one to word number 39 000 from your context you probably want the mega llms to take up those tasks and pay for that basically right right and this is where true Foundry come in so like either you could be using your openai apis or your commercial apis and we help a little bit when you are when you are doing that part and the second thing is you might want to focus on open source llms and we help a little bit in this in this side of your journey as well so let's talk about how we do a very simple thing uh when we talk about the open AI apis right we we basically reduce the number of tokens that you are sending to open Ai apis and thereby saving you real dollars let's see how like what's the intuition behind this right so in one of our previous slides we noticed that you are essentially paying more than half your cost in just processing your context or prompt tokens let me take you back to this slide so in this case if you look at the total cost you would notice that in almost every single row half or even more than half of your cost is coming from your input cost or your prompt token processing cost right now the the key to the kingdom here is that not all the words that you are sending as part of your context are actually necessary for your llm llms are actually great working with some incomplete sentences reduce the length of the word remove some words that are not necessary retrace these sentences do whatever but you could actually reduce a lot of context and still get practically the same response right so call it a lossless compression and that's one of the apis that we're building so you basically invoke our compression API and roughly save 30 of the cost that you're invoking with open AI models that's one now let's say you want to work with your open source models we have actually a lot more elaborate offering in that where we are building a model catalog of a bunch of these open source models that are highly optimized for inferencing and fine tuning so it it and like you know the way we present this is a drop in replacement for your hugging face or open AI apis so if you are already using let's say one of the open AI models and you wanted to try out that how will my dolly 7 billion or MPT perform for a particular task it's it's like probably changing one word in your in your code base and things will just work out of the box and lastly the way we are orchestrating this entire setup from an infrastructure standpoint is we're running this on kubernetes clusters running on top of spot instances you are able to completely leverage which which runs within your own infra right so you can leverage your Cloud credits your Cloud budget anything um that comes from your cloud cloud directly as opposed to kind of like you know trying to figure out a whole new budget for for running your llms basically let's dive a little bit deeper into the compression API so I'm giving a very quick example here where we have a context and this context is about Beyonce okay and I'm asking a question that what was the name of Beyonce's first solo album we get an answer Dangerously in Love I don't know if this is the correct answer but this is what we get out of the context and the number of tokens consumed for this task is 649 okay and the humongous cost that you're paying for this operation is 0.001 dollars okay however if you were to use our compression API for the same task you basically provide the exact same context you provide the exact same question you get the exact same answer but the number of tokens that is getting consumed is 465 which is roughly 30 percent lower than what you were passing in the previous one and when you are running your open source models there are a few things that that we are working and we would love to build the platform together if some folks are actually using these open source llms uh we would love to build up build this thing together with you all the idea is that we have a model catalog of all these open source models that are ready to be deployed or ready to be fine-tuned you can very quickly build and deploy your no code apps and one of the other realizations in this case is when you're working with llms it's actually not just the llms themselves but in whole with the whole ecosystem around it because you yes you want to deploy your models but you may want to run some of your data processing jobs you may want to have some other python functions which are like your pre-processing functions you may want to deploy a vector database you you probably want to have your data to be read really really fast to improve on the performance right so there's a lot of things that go around to actually build a full full-blown llm app and the the the part that we are trying to solve is kind of build the whole ecosystem around it so that you as a developer can have a lot of these problems solved in one place if any of this sounds interesting to you all I encourage you all to sign up using this link and I promise to provide a 200 GPU credit on our platform and love to work with you all thank you so much for listening 200 dollars oh man all right that's the one way to do it I like it I like it a lot so now I gotta ask you a few questions I mean there's some incredible questions coming through in the chat and I love it I think this is so valuable man it's one of those things if you look at the report that we just put out and I'll throw that on screen in case uh there's people that have not downloaded this report yet go check it out we just did it warning I am not Gardener and I am not at all to be seen as someone that writes these reports on a regular basis but I did spend the last three months doing it and coming back to your talk the biggest thing that people were mentioning as a challenge or a reason that they're not using llms was cost and how they could not figure out what the ROI was they could not figure out how much cost it would actually take to do this and so I think that is so valuable man thank you for putting this together appreciated appreciate it dimitrios and I don't know if I might have scared people away from using llms by giving this talk if cost was the biggest concern for them oh man yeah so actually there is a there is something else that I was going to ask about when it comes to the um the algorithms you're using or basically I want to know have you played around with Frugal GPT have you seen that one I have I have I have seen The Frugal GPD I went through some of the optimizations that that you know they have proposed in the paper and uh like some of the things that we are incorporating where we are incorporating our compression API are actually like you know we noticed that there were a lot of overlap that we actually put in there yeah yeah that's cool I mean looks like great minds think alike so let's get to some of these questions um in the chat and so guillem Bosh is saying that they want you to be their personal finance advisor and that is uh yeah I'll second that one that's awesome if you could do this with my finances that would be great llms work as well on text that you remove stop words and um and the limitizing words or oh man that's a big thing right now yeah your words so like is the is the llm going to perform as well so I'll share what what experiments with it the short answer to this question is no they would not okay uh if you just directly like went ahead and removed stop words and like limitized the words uh without any context the performance actually drops so we actually ran some extensive experiments on Q a data sets summarization data sets to figure out how can we ensure the performance does not drop and when we did this naive approach the performance actually did end up dropping we tried a bunch of other things like you know we tried some few short learning approaches to kind of um actually give human labeled um some summaries which were a lot lower in the number of tokens and that performed much better but still not quite there yet so eventually where we got to like you know much better performance was actually fine tuning one of the llms with this specific task that you learn to not lose context but still reduce the number of tokens basically so that's that's kind of how we we had to get to solving this problem and to be completely honest this this solution even with all the stuff that we that I'm talking about is still not General applicable to all the llm use cases for example if you were passing a code as a context and you try to summarize your code well it's gonna suck at it so there are some specific use cases where this performs quite well in some other use cases it is still not performing very well that makes complete sense and you mentioned fine-tuning in your talk it is that fine-tuning with Q Laura oh so this fine tuning that we are talking about is actually a full-blown uh fine tuning to be honest uh but if you were to fine tune it with uh low rank adaptations and stuff like this would be a lot cheaper a lot faster uh than what the the numbers that I put together on the slide yeah yeah I feel you all right um do you feel like the 7B self-hosted model is capable enough um the answer to that question is is always on the task that we are talking about I don't think that the 7 billion or the 13 billion models themselves are genetically capable and like nowhere close to being capable uh to the the larger commercial counterparts right but what we have seen is sometimes because commercial Lambs are so generic they work well in all the use cases we actually try to throw these nlms at every problem that we're trying to solve in this context and what I've realized is that for simpler problems it probably makes more sense to have like these smaller models fine tune to that specific use case for example if you were to just figure out one of the four classes uh that that you want to get you would still probably in your workflow today throw a prompt at it and say that okay figure out which class does this thing belong to but problems like those you could actually get practically the same performance using the smaller llms dude awesome thank you so much for this and it is such a big question I mean it is something that everyone is thinking about and wondering and I know that one person in the report mentioned how just because you add AI capabilities to your product doesn't necessarily mean that you get to charge more for those capabilities and so the cost of you running those that these new AI capabilities you have to basically take out of your margin and so yeah that that's something to think about you really want to know how much you're going to be losing off the margin if it is something that is big and I actually I just noticed uh the other day on notion the free notion now you can't do the AI uh stuff it doesn't have the AI capabilities and so I realized that that was probably because it cost them quite a lot of money to be running that AI stuff for sure yeah absolutely so dude nikoons thank you so much and if anyone wants to check out more of what you are doing I am going to just show them that they can go to the solutions tab right here and find out all kinds of cool stuff about what you're doing at true Foundry check out that virtual Booth man that's looking good and you've got some cool things for people to go off of so feel free to look at that have a peek and now I'm seeing myself inside of a picture myself this is gonna get a little bit confusing in a minute so all right man I'll let you go and for any more questions uh Manoj is asking you a question so I think you'll you're going to go into the chat right now otherwise you're also in the ml Ops Community slack so just tag the coons and let them know that uh that you got questions all right so thanks man all right thank you so much dimitrios thank you so much everyone for listening take care bye [Music]