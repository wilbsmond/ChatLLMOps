I suspect that it should have recommended to everybody to listen to you guys e and Phillip uh I think you guys are coming up next nice where are you guys located again San Francisco you're in sou right I think we did a big event with you guys uh not too long ago uh which I heard was a massive success and well I'm stoked to see what you guys are up to today the Spotlight is on you guys thanks for the intro ad I'm really glad to be here so today in this session we're going to talk about Iraq and just a quick introduction my name is eong I'm at em at any scale and I'm philli I'm dis n skill first just a quick overview of what we're going to be talking about we'll be walk through walk you through our journey building a retrial augmented generation application so we'll talk about the architecture we used and how we evaluate our system as well as some of the experiments we ran and the result we had and after that we'll also talk about the challenges we ran into in each of the step in building this rack application and the solution that we had so the rack application want to build is this question answer assistant for Ray so what is Ray Ray is this open source Library um the goal of it is really to help scale in your a and and python application extremely easy fast and cost efficient um so you can focus on kind of building your AI and P application instead of dealing with how to like scale up your system so it come with a suet of application uh library that handles different part of your life cycle in developing your AI Pyon application so for data processing we have red data and retrain for course train your model as well as reserve for deploy your model so R kind of has become this de facto go to tools for scaling up your you know ml workloads as you can see here that we've seen um it being adapted across different companies on all sorts of application we're really excited to see that ins the of the community and we really want to build something to help uh everybody in the community to learn about Ray and you know adopt Ray in their application so that's why we really want to build this question and answer assistant so you might ask why don't we just use as simple they seem to be pretty powerful nowadays in terms of you know doing question answer system so there are a few problem with this approach first is these LM are kind of trained on all the data and in our case we're continuously updating our documentation as you know new fees are released second general purpose LM They Don't Really include your private data so for example if you have us a data or intellectual property you you like to include in your answer they will not have access to those and third is H nation is a common issue of this General person LM and in our case we definitely don't want our assistant to come up with some random Cod nuts okay so what's our solution we can provide these models with context so in our case a lot of information for the answer of this question already in our documentation but we really need to stitch all these different pieces together so here I want to give an example uh this question is a real question from one of our user the user ask I want to use radar weight what happen if the task fail okay just two different some context here uh radar weight is what you call when you want to block on um remote asynchronous call until you get the result of it before you proceed Witham so to really answer this question in need two part um from our documentation one is to understand how does rate. weight work and the other is to understand how does Ray handle F tolerance so this is a good example of how such system can find these two different piece of information put it together in a reasonable way and present that to the user so uh I'm gonna pass Mike to philli to talk about how we build this track application so let's now go over um how we build this application and um this is the architecture of the retrievable augmented Generation Um it's both a very simple Paradigm but also incredibly effective at um putting more knowledge into llms um so the query um comes in and then we look up the most similar or relevant documents to the query from some database and then we get um the top results and we put them into the context of the LM and then the TR to response to the user in practice um often what is um used here for most relevant is a similarity um Vector similarity so um we take all the documents we have and we um use an embedding model to um calculate vectors and then we store those into a vector database and then when the query comes in we embed the query with vectors and then we look up the most similar vectors from the vector database and then use those documents as context um and in practice the embedding Dimension is often 512 to um 1,500 um um um Dimensions so let's now dive a little bit more into how we can create the vector database so everything starts with the data sources in our case that is um either the right documentation or the right blog or the right book and then we load all those documents and then we have to chunk them up into smaller chunks that is because the embedding models are typically most effective at smaller chunks like a single paragraph um that only contains a single topic um and then we run the embedding model to convert all those chunks into vectors and then we index those into a vector database and the vector database can efficiently represent those vectors and can efficiently support queries over those vectors so we can get the most relevant documents the documents if as you see below are often represented as a a tuple of text and then the source so we can link the user back to um the information and then the embedding and that gets started into the vector database so um let me dive a little bit more into the chunking because it's actually very crucial and it always depends on your data on how you should do the chunking so um oftentimes you have to deal with HTML documents like the right documentation or the right block and so um we basically pass the HTML and then we um split it up into sections for each section we keep a link to the section um with the HL anchor and then um if the section is long we will um split it further up into paragraphs or um codes snippits and also try to keep those um paragraphs together and try to not split up the individual code Snippets so the LM can be most effective at answering the questions um and then after we have created the vector database we have to do the retrieval so for the retrieval um the query comes in and then we embed the query with the embedding model we get the query embed embedding that is a vector um and then we um pass that over to the um Vector database index that we built and then we get the top K documents from that the K is of course a parameter that we can choose and then um we feed those into the LM so that's the next step um we take those topk documents feed them into the LM together with the query and then we generate the response for the user so um there's a bunch of parameters that um I've been talking about and we need to select those parameters so these include what concrete embedding model am I going to use which LM am I going to use um how large um are my chunks going to be what chunking strategy am I going to use um and then how many topk chunks to I put into them so we need a robust way to evaluate which of these is going to be the best um often times we start off with just trying trying it out and trying a few queries but then um as you um get more serious and iterate on the application um you also want to have some way to check that the EV that the application still works after you do more changes so you should start building a data set of those queries um what we did is we built um from the community questions we built um a data set of about 200 um questions and then we hand annotated them with the sources of the chunks in the documentation that answer those questions and then there is basically two ways how you can use this data set um to do the evaluation one is component wise this is very similar to unit tests in traditional software engineering where you take a single component of the system and then you test that for example you could test the retrieval mechanism by just um uh running the retrieval and then checking if the underlying um documents that you annotated your query with are found by the retrieval or you could test the llm part by fixing the content text the um ground truth context you put into the llm and then checking how good the answer is um you can also um do um endtoend evaluation this is kind of like an end to end test in software engineering and make sure that the overall system works end to end so you feed the query into the system and then you run it and then you get the response out and then you do the evaluation on the response so now the question is how do I do um the valuation um because um okay you can do human evaluation where you um check the response by hand but this is not really scalable so there are actually good techniques how you can do it um in a most scalable manner by using llms as a judge for the evaluation so let me talk a little bit more about that so you can basically put the question and the context into the LM and then say um I have just proposed answer by the system how would you rank that on a score between one and five and also give you reasoning and then you get um and you can get answers like and this one where um in this case it's a very good answer and the LM gave it a score five and then the reasoning tells us um what the um LM likes about it and this works actually it while it does not um uh replace um human evaluation it actually works pretty well in practice and you can use that to automatically run um for example if you make modification to the system you just run your evaluation suit on your data set and then you get a good idea of whether things are working so let's now dive a little bit more into the experiments that we run to get the best parameters and of course the best parameters will depend on your own application um but we're going to share um what we found so what um the experiments we ran were um first of all um we tested the number of chunks from one um to seven and then also the chunk sizes um from um 100 wordss to 700 RS and then we also tested the embedding model um starting with some um widely used embeddings maybe you're aware of this the hugging Force hugging phase leader leaderboard where there's a bunch of closing embedding models evaluated against standardized um data sets and so we took some of those and we also took the openi embeddings and then we also um evaluated against um a bunch of llms like gbt series from OPI and then the Lama models from Facebook and also f80b so let me share some of the results with you um the first one is about chunking um so here you see um at the top we did the experiment um both without context and then with context so that shows and um as you would expect with context is a decent one better and that shows that it's actually um important to put in the context and to build the whole R application then we also did experiments with chunking and there for the overall quality the end quality score typically better chunk uh larger chunks are better because there's more information in there but you also see on the retrieval score that the performance actually goes down as you increase the size of the chunks that is because the embedding models actually get less effective at representing information in the chunk if the chunks are too big and then we also did experiments against the number of chunks that we feed into the LM and there um it's often times more is better but there's a limit because um the LMS have a limited context size so we cannot exceed that size and then at the end when we took the best results from everything it got a pretty decent score of 3.6 we also tested this against a bunch of different models so first of all against embedding models and there actually it was a little bit surprising for us because um the best model in the hugging F leader board is actually this model BG large and but that was actually for our application not the best model um the best model instead was another open source model called GT Bas um we also tested against the open embeddings but they actually performed slightly than the best open source model and then we also tested different llms and um as you would probably expect gbd4 is still the Undisputed winner here with a score of 3.8 um but um gbd 3.5 turbo which is much cheaper it's actually um also doing pretty well and then the Llama 70b model is actually pretty close in performance to GPD 3.5 T so I'm actually very um excited about all the progress in open source LMS and I'm looking forward to future um open source LMS that are going to do even better and then maybe a little bit surprisingly um the larger model falcon 1 ATB was actually slightly worse than the 70 um P Falcon model okay now that we talked about the application let me talk um about some of the challenges in building and deploying the application into production so the first one um is about scaling up this actually hits you right when you start working on the application often times you have a a large number of data and then also um you need to compute all these embeddings and that is pretty computation expensive and need gpus and also you need to run all these evaluations which um often also um um takes a long time and is good to paralyze so we chose um to use right data for scaling up the data pipeline right data actually makes it very um easy to integrate all these different data sources it supports um pet files Chason files CSV text files like um HTML images tensorflow records SQL and you can load those from different sources like a shared file system or blob storage or a database um Ray data was built from the ground up to support um um both CPU and GPU compute and it can effectively um use both of those together so some of the commutation often happens on the CPU and then um embeddings for example utilize gpus and right data will make sure that um um the right resources are um utilized and we SC up CPUs and gpus independently it also has been built um with first class support for tensors and it's actually the fastest and cheapest solution for this kind of offline inference on unstructured data and if you want to learn more about it um um I have a link to blog post here where we do evaluation of different systems and it has more information about R data so R data was really helpful um for implementing this data Pipeline and then scaling up um those embedding calculations the second um very important challenge is in deployment so we have actually many different components that we need to stitch together like our embedding model and then um the llm and then also um we do actually some more things like ranking um um and and then there's a bunch of operation challenges around this in terms of how you do logging um how you keep um the metrics um about request latencies and so on um and improving those so we chose um rerf um to deploy the model rarf is a production and gr deployment model model um system built on top of Ray and um it supports some of the unique challenges that M LM applications have one of them is being able to compose multiple models like for example embedding models um oftentimes you have um need some sort of personalization in our case that was not the case but many applications need that and then you want to rank the results um of your retrieval and then run the LMS another very important challenge is low latency often times these LM applications are assistant to users so if it takes 30 seconds to get the answer then um that really makes it much less productive so it's crucial that we get the response to the user as fast as possible and rerf um can help you with that by both um efficiently and in a low latency way evaluating the prior model um before the LM is called and then also for the LM rayar supports end to end streaming so we can take the tokens as they appear and then form the response to the user and immediately start um pulling the answer all the way to the user through websockets or other streaming ways in addition to these llm specific challenges there are some um ml Ops challenges that you have when um building and deploying um such an application one is being able to upgrade um the application with zero downtime as you evolve the model and the chunking logic and the index and the evaluation I'm also being able to do Canary roll outs to serve only some of your users seeing how it performs and then I'm rolling back um if there's a problem also um being able to do observability like logging to see what are respon even having feedback from users thumbs up thumbs down is the um result good and seeing what are the metrics how's the performance doing um is very important and then last but not least race surf also supports efficient Autos scaling and flexible retour allocation that can really help you bring down your costs race surface it's very convenient to develop um in because um it's just native python development it's very um simple and flexible you can develop on your laptop um you can iterate quickly and then you can robustly deploy the application on for example fores um if you want to learn more about rerf um we have a blog post that we talk about how RAR can help you um serve your AI application in production so now EA is going to tell you a little bit more about how you we can continuously improve the application um and yep cool so now you have deployed your application and the next thing you want to do is maybe hey how do we incop all these feedback we get from user and improve it so okay first you can do as Philip mentioned is you can collect some of these feedback so for example collect things like okay what kind question um our user are asking our system and what are the chunk of contexts the vector databas returned back and you know whether user find this response helpful or not there are few things we can do here here one thing is we go back to our data source in uh in this context our uh documentation and see hey if you just need to improve our documentation uh we might also want to like retweet our existing system or some of the parameter that Philip mentioned earlier for example re chunk you know our documentation and then we might want to re our data pipeline again to regenerate the vectors for the vector database and another thing we can do is to really make our model more intricate here so for example we can find in our ining model these after shelf ining models they don't really have domain knowledge of um you know since St specific to uh Ray for example so we can introduce some of that by fine tuning the embeding models and another really popular thing to do is to just rerank retrieve the result based on you really know about the curry so basically what we're doing here is we are introducing a lot of new models here and there our system to improve the quality of the entire system and we'll need to keep iterate and retrain these models again again as we're getting new data from our user so how do we make this process really easy and really uh iterative and really fast the toway choose to use here is R Trin uh one great thing about R train is that it integrates integrate really well with all your existing um deep learning framework so you can choose whichever tools that you are already familiar with are really optimized on the hardware that you want to run and mix match all of this across all of these different models and to configure your scaling it's also really really easy with just one off code and you can scale it across CPU GPU even your customized Hardware as well and the r what R really shines is also its performance um it can really help you speed up this iterative process so just to give an example uh instant car is one of the retrain user and they are using AI across the board for fulfillments for recommendation um pretty much all sorts of different application and they were able to run training 12 times faster on retrain and train on 100 times more data which is very impressive result okay so now we iterated a bunch improved our model A Lot using ret Trin and our system is doing really well our R user are flocking to you know this system and we're getting usages and now we really want to think about how to keep our cost down so the LM part is really going to be uh the most Compu intensive and costly part here so what we did is we did a cost performance analysis across all sorts of L and as Philip mentioned earlier it's not surprising chp4 did better than the other models but it also comes at 47 times the cost of say llama 70b models and and in a lot of situation we might not need a really powerful model like gb4 just to give a few example if user is asking a pretty straightforward question and the answer can be fine just somewhere copy pasting from the documentation you probably don't need a model that's that can reason a lot and some small open source model might be good enough in this case um or if you know users asking hey can you give me a code example you can use use Kama instead of gb4 and that's probably good enough so what you really want to do here it's something like introducing a classifier to do hybrid D routing depends on the question folks are asking here so uh what we did is we used Reserve to implement this hybrid routing Reserve really make this kind of model composition extremely easy uh to implement another really good thing that Reserve can do is it can Autos scale these different models separately so you only really pay the compute cost when you need need the load okay so another way to reduce cost it's uh really uh switch to our open source model and maybe you don't want to host this model yourself and the what we used is we used any scale endpoint which is a open API open API compatible uh platform that opens with L inter application so it's a pretty easy dropping replacement and uh also inference for L is really a full stack um problem here so at any scale we work really hard to integrate optimization across the board um this is how we can offer the lowest cost for Lama 70b in the market right now uh so at $1 per million token so if that's something that um you wanted to reduce your cost that's something also great uh to try out so I think that's everything we have today uh we still there are a lot of really great question about the details of how we build the rout application we're of course happy to answer all this question at the end of the presentation but if you're interested in a lot more details I also encourage you to check out the blog codes here on the left and it also comes with the GitHub source code for everything that we developed so if you want to go check that out as well maybe take that and adapt it to your R application um please to do so as well and on the right side we also have a link for any scale endpoint and we uh including not just endpoint API as well at fine tuning and if you have sensitive data you want to just deploy everything in your own environment uh we also offer something like a private Eno if you're interested in checking out thanks again for being us happy to take any questions thank you very much guys uh and yeah we did the workshop on Ray I think so which was very Hands-On uh and that was excellent so if anybody's in sf' be great to do this again we're flooded with questions let's see whim says which Vector DB is used here so we used um um postgress with PG vectors that has been a great solution um it um we can leverage existing postc knowledge and um using existing databases RDS and other Cloud databases for example support PG vectors um so um um it won't work for like huge scale data but for for many use cases I think it's a great um solution from manit how do you select the embedding model especially if your use case has multilingual content uh and how important is the choice of the vector database for rag so um we the embedding is um chosen by this evaluation technique that um was talking about earlier so we created data set um with um ground truth um um sections and then and then we used um both theal score as well as the end to end score and that's much more detail in the blog post um if you're interested and more into that and then how important is the vector database um um I would say uh if the uh if you have an existing database um like post that supports um the use case then that actually supports all the different um indices that are widely used so that will that will work if you have very large scale um use case then there's also dedicated Vector databases um so it depends on your use case this is San asks I'm curious if you know examples where elastic is used instead of vector DB and what are the trade-offs compared to more traditional um so I personally haven't used elastic um but um I know elastic is definitely working a lot on making it a great database for Vector DB stuff I would say if you already have atic database and your documents are already in there and then it's it's a very good way to check it out um um it also allows you I think to um combine Vector search with um keyboard um based search with the metadata and so on so if you have an existing database and where your documents are in um I would encourage you to try out if that also supports vect wedings then that would be a good solution otherwise there's a decent number of um dedicated Vector databases yeah one more thread here is about not using vctor databases but instead to use sort of like more traditional knowledge graphs well in so far as they already have the data that you're trying to index I mean it might this is consistent with your answer you might as well just and also if you already have an existing information retrieval system um it it totally makes sense to try to plug that into your application and see how how well that performs yeah um have you experimented with different retrieval algorithms how do you think about retrieval in cases where semantic similari is insufficient to identify the reference chunks best suited to your purpose so we mostly used um um embeddings and but what we also did is some amount of three ranking so um after your um um query comes in you can take the top maybe 100 um documents and then you can based on the query maybe based on the user and based on some other context um you can select the ones that are the most relevant um for example one thing we did is train our model on um the rid discour then mapping the query to the topic like the library that the user is using and then ranking those um libraries higher that can actually help um and yeah it depends on your application I would say but there's a decent amount of Al the traditional ml um techniques that you can incorporate here yeah we have a few questions here both on quality and evaluation on one hand and on the other hand on chunking so maybe we could take the chunking ones first very quickly the chunk sizes whether you're chunking words or characters um think was anything about how could you explain how you broke down information into chunks for the with chunking so um what we do first is um because we have um um structure from the HTML documents we first chunk things up by section that make sure that there's no mixing in between sections and then what we do is we try to find good breaking points like things like paragraph um we try to keep the Cod together for example um it depends very much on the semantics of your document the chunking is actually very important like if you chunk if you cut a a code segment in the middle the Alum will get confused and the answer won't be as good so um I would say this is one uh place where I would spend a decent amount of time trying out different things trying out your domain knowledge of your specific application um and then yeah um anything else that you could say about the evaluation so benstein is asking how did you run the tests what is a good what is a quality score how do you define that why is 3.6 good oh yeah quality score retrieval scores many more people are asking how you got the retrieval scores so we used um scale from one to five and that works pretty well I've seen other people use like binary scores um between zero and one um or or zero or one um I think all of these can work pretty well um to fine gr um doesn't make sense like um one to 100 probably doesn't make sense um and then uh gbd4 I would say is still the probably um best LM to do the evaluation um and um yeah um um yes much more information in our blog post so check it out he fan philli thank you very much for joining us today this was an incredible talk thank you so much [Music] bye