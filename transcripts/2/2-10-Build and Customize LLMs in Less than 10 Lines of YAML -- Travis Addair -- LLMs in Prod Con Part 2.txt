a lot of pressure from some marketing teams not Anton's and not Travis's where are you at Travis hey hey how's it going I think we uh great man how are you doing good good good dude so let's talk for a minute before we jump into your talk we had an awesome panel like a week ago it was me you and a few others at the snorkel event that was great and now you're gonna it's that was like the warm-up that was the sound check or what Now You're Gonna Keep It cruising yeah yeah so I mean uh definitely we had some some hints in that panel talk at some of the things that we're thinking about at predabase and some of the things that we're building in the open source with with Ludwig and so yeah I'm really excited to now get to share some of that uh with some slides and a little bit of demo at the end as well excellent dude so if anyone wants to check out pretty base you have a whole a lot of information on the um where is it right here on if you go to the solutions tab on the left of your screen where you're watching this throw that on there boom click the solutions Tab and you will see pretty base is on there you can enter the virtual Booth but if you're just looking to watch this conversation Travis I'm gonna hand it over to you and before I do I'm going to mention it so I'll let you share your screen while you're doing that I'm going to mention to everyone that we actually had your good old co-founder on the ml Ops Community podcast uh probably like actually now probably like a year ago I think time flies when you're having fun but if anyone wants to check out the ml Ops Community podcast and hear more from pretty base and how Pierro and the team Travis's co-founder and uh the CEO of priority base thinks about machine learning ml Ops and open source also we got into like the idea of how you want things to be flexible you want it to run out of the box and you want it to be simple but you want people to be able to turn the knobs if they know what they're doing and that resonated with me obviously one year later I still remember that theory and that idea and so I love the ethos there I'll let people scan this QR code if you want to join us in the ml Ops Community podcast we drop them once a week now it is your turn Travis I see that you shared your screen I'm going to get this off of the screen and I am going to leave it all to you man I'll be back in about I think it's uh what are we at 10 minutes no we've got 30 that we've got there all right all right awesome thank you dimitrios and yeah thanks everyone for coming out today uh so I'm Travis uh from predabase I'm the CTO and co-founder uh we're a company that came out of Uber's AI group um working on the machine learning platform known as Michelangelo and also the applied research teams we worked on a number of Open Source projects there including Horvat a distributed training project and Ludwig which is a declarative low code framework for building deep learning models including large language models and so today I'm going to talk about what we've done in the open source with Ludwig for large language models as well as what we're doing in predabase which is all about building and customizing these llms using our declarative interfaces and so the promise there is that you can get to something that is customized for your particular task with llms in less than 10 lines of configuration using a yaml-like language and so in this um well talk not webinar and this talk we're going to be uh starting with a little bit of framing on um Emma loves versus lmops you know like I think at this talk were held um you know a year ago we'd be framing it as in all ops in some way but now everything is very much oriented towards larger large language models and and what these things can do for us we're also going to talk about what it actually means in practice to build and host a large language model and then I'll go into you know different options that you have in terms of choosing the right type of customization for your task whether that's you know zero shot learning you know prompting based strategy is indexing which was what the previous talk was was covering or even fine-tuning uh so actually training models yourself um you know using a base large language model and then I'll also give you a live demo um knock on wood that everything works covering some of the features um in predabase that we talk about in the slides so first I want to go back to the origins of you know this kind of type of problem that we're trying to solve which is you know traditionally been based on using machine learning and so with machine learning you know we have this whole uh now domain of ml Ops which is all about productionizing these systems so trying to get to production keep things live and fresh in production um where the artifacts they're putting in production are these systems that enable us to derive insights from our data for performing useful tasks for a business so like driving insights about which customers are likely to turn and being able to then take an action like prevent them from churning in some way like giving them a promotion or something like that or recommending products to people uh to buy when they're browsing our apps things like that and so traditionally the ml Ops Journey kind of looks something like this you have a user that wants to transform some data from a very large data set into a snapshot they do some exploratory data analysis on that they engineer some features from that data then they create a model uh on top of the training data evaluated on top of the test data and iterate on this until they're generally happy with the performance and they deploy it and then the model is available for real time and batch prediction and so the important thing I want to call out here is that um the value comes at the very end right the value is once you have something live in production that you can actually start generating predictions from and so all of this part which is typically like uh days weeks months maybe even years long process is all just costs that you're sinking into it until you can finally actually use something for for solving a problem and so when I think about what LMS really do to change the game is that they basically take this entire upfront cost and kind of move it move it away in some ways and so you know you see this in practice with you know systems like chat EVT that I'm sure everyone here has used is that you know you have a system that performs a lot of these machine learning type tasks that is just out there to views and you've never had to do any work to get it to produce useful output right because companies like open Ai and other companies that build LMS like Google and Facebook or meta Etc have already done all that heavy lifting for you right and so when we think about the new llm life cycle um it really starts with building these foundational large language models which has a self-supervised training component over unstructured data like the whole internet has an instruction fine-tuning step over a labeled data set with like curated examples that you want the the model to emulate and then there's also a reinforcement learning through human feedback stuff where you try to get the model to refine its output to be tailored towards human preferences so like not saying things that are offensive or illegal and making sure that's generally the responses match what the company hosting the model once once the outputs look like and then once this is done you know you have the model living in external registry like something like uh the hugging face Hub right and then you can deploy it as this large language model box here and then all these sorts of tasks open up for you you can do prompting on top of that model you might want to do indexing where you have an embedding model that you know looks up relevant examples from an index and uses it to enhance the prompt for what we would call like retrieval augmented language modeling or in context learning and then you also have a fine tuning step that you can do where you take your label data and you you know refine it and then the output becomes another model that now lives in an internal registry that you can similarly host and then this becomes a flywheel right of continuous improvements in theory and the other nice attribute of this is that you don't have to start anymore with the training you don't have to start a fine tuning like you can you know start by prompting you can do some indexing and then as you go you can build up a labeled data set by you know providing feedback or providing annotations on the results from the model and slowly work your way up to a data set without having to pay that upfront cost but one thing that I want to call out that's very important here is that this whole part in the bottom left here is really the kind of general intelligence part it's like you're building this Foundation model that knows how to do all sorts of things like which might be your task but might also be things that you don't care about like generating poetry or whatever um and in most cases you don't need this general intelligence even though this is the part that's most expensive what you really need is this solution oriented flow here which is all about getting the model to do something specific for for your organization right and so when we then think about what the llm Ops life cycle looks like in practice it's really something more like this which is just that previous diagram with all the pre-training work cut out and really the journey is all about choosing which of these refinement steps you care about in order to get good results now certainly some people actually are interested in building a large language model from scratch today that tip is a very costly Endeavor that can cost tens of thousands hundreds of thousands billions of dollars but the nice thing is that you know because these systems are general purpose most of us don't need to do that most of us can take open source models like the Falcon model that came out and fine-tune them to our specific tasks without needing to spend all that upfront cost again but in practice building and hosting uh these open source llms um is still quite a bit of a challenge even after the model's been initially trained so you know on the surface you think about hosting the llm for low latency zero shot inference but as you start to go deeper you can think about building the information retrieval layer for doing few shot inference and and doing chunking of documents and document retrieval you can think about doing distributed training to do the fine tuning over larger data sets of label data and then you can go all the way down to uh you know building the models from scratch with the pre-training and the rohf and all that if you if your use case really requires it so there's lots of complexity to this problem that sits beneath the surface and so what we're trying to do with Ludwig and predabase by extension is make this whole process a lot easier to work with through declarative interfaces so declarative is all about you know if you think of languages like SQL it's all about saying what you want kind of describing your problem describing your desired State and then having a compiler system and an execution engine that sits underneath that translates that into some actionable model training process or model inference process to solve the task right so for example in Ludwig today coming out in our new version 0.8 you can specify a zero shot configuration using a model like the Llama 7 billion model you can describe what kinds of inputs you want what kinds of outputs you want what your task description is and you can start getting value without having to do any training you just basically can throw it at a data set and start getting predictions about like what was the intent from the user based on the inputs uh the info text from there you can expand to indexing in few shots with just a few additional lines of configuration so you can say I want to do semantic retrieval I want to take the top three most relevant examples and insert them into the prompt and then you can also transition over to full fine tuning by just adding an additional training section where you say I want to do fine tuning and you know all of your normal supervised machine learning parameters then become available like what Optimizer I want to use what learning rate Etc and so normally this would require quite a lot of infrastructure to kind of transition between these different points but with predabase we want to make it as simple as just you know okay I'm going to change a little bit of my configuration and now I have a very different type of task configuration and model configuration and so this is all Ludwig but which provides this uh low code interface no Reinventing the wheel while also providing quite a lot of depth to what you can do and then what protobase layers on top is an Enterprise uh platform that manages infrastructure data deployment things like that and so we have data connectors that can connect to snowflake data bricks uh S3 all sorts of different data lakes and data warehouses we have a model repository that tracks all of your experiments and all the trained weights from fine-tuning and other processes and then we also have an interface for querying these models that allows you to run interactive analyzes like uh do um a zero shot prompt or a batch prompt over a subset of your data or do index-based querying in a UI and then we also have the whole managed infrastructure layer that sits on top of all this that provides the serverless training and prediction capabilities of the platform and so what this looks like in practice you know we have multiple different entry points there's a UI there's a python SDK there's also a command line tool and so from the command line tool if you want to deploy a model like let's say Falcon 40 billion it's very simple you just say pbase deploy that model and then you can start prompting you know what are some popular machine learning Frameworks from there batch prediction is similarly as simple as you know being able to uh take your prompts and then now apply them over a data set so for example given this input provide a rating and then you have a bunch of unscored examples which lives as a data set like a CSV file or something like that and you want to do batch prediction over them that's as simple as running a query over a data set and then if you want to enhance those prompts with an index of some sort that's also similarly as simple as just providing an index that exists in predabase to your command so that you know instead of just using only the context of the of the input and the prompt you also now have the ability to augment that prompt with other relevant examples to help it improve its responses and you know just to kind of make this a little bit more concrete for example you might have you know a sample input that's like let's say we're trying to predict hotel reviews something like that the hotel was nice and quiet that could blah blah blah like that could be the input the task is you know given the sample input provide a rating and then what protobase is going to do under the hood here is we're going to go to our index and fetch the most relevant examples uh to help you complete the task and then from there we'll feed that to an LM and make the prediction and so if you saw the last talk I'm sure you're very familiar by now with you know this kind of retrieval-based approach but one thing that I think is very interesting about what we're doing at protabase is that we're applying this specifically to doing task-based workflows like supervised ml or classification or predictive tasks and providing an interface that's very similar to what you would do if you want to do just traditional supervised machine learning as well and so being able to swap between those different strategies to trade off between cost and time and compute resources and performance and so when you're ready to formalize this we represent these sorts of configurations as models in predabase so you can create a zero shot model definition from your configuration and then similarly you can create a few shot model definition um as well with just a very simple you know command line interface from there we also support fine-tuning in the platform natively and so for folks you know very quick primer uh if you haven't done fine tuning before is essentially taking a pre-trained model providing some examples of inputs and outputs that you want and then from there the model weights will be adjusted to be better at outputting the type of thing that you're interested in and translating this into a configuration lubricant predabase is just as simple as you know giving a specific trainer type and then specifying some hyper parameters optional hyper parameters like how many ad blocks you want to use what your optimizers Etc and so the promise of fine-tuning is really about being able to achieve better performance on your tasks and also lower latency in some cases whereas if you got you know good performance with Falcon 40 billion without fine tuning you know and zero shot or few shot prompting that might be good enough but ideally you might want to increase the throughput or decrease the latency by using a smaller model and oftentimes fine-tuning a smaller model will give you good if not better performance at a much lower latency cost and there are different ways to fine-tune I think most people often imagine that when they talk about fine-tuning LM they're thinking about fine-tuning the full text to text model so you have an encoder that I'll put some embeddings and then you have a language model head decoder that sits on top that generates sequence output certainly for some cases like generative cases that's very useful but oftentimes what you're trying to do is something like classification right where you have a fixed number of categories that you want to classify into and in those cases you might actually be better off doing something like chopping off the language model head that generates text and instead just taking the hidden state of the model and then attaching a classification head on top in which case you have a few different options you might want to fine tune the whole thing together or you might want to fine-tune just the classification head and leave the original encoder Frozen and so thinking about like how to do these different things with pytorch even like framework that make this very easy that sit on top of pytorch it can often be a little bit of a of a problem right but the nice thing about again with Ludwig is that we make it very easy to transition between these different things again through the declarative interface so if you want to do this sort of full text generation fine tuning it's as simple as a config that looks like this whereas if you want to train you know a model that just uses the large language models encoder you just specify that as an encoder in your input feature and then have an output feature that's specified as category which attaches the category classification head and then similarly if you want to freeze the LM parameters which will you know speed up training uh you just have to specify that the trainable uh the weights are trainable equals false which will freeze the weights and therefore give you better uh better throughput and one thing I want to call out is that you know setting trainable equals false you might think why would you want to do that would it always be better to adjust the weights and from a strictly performance oriented view like most of the time training the weights is going to give you better model performance but I want to point out that is much much faster and cheaper to not train the weights of the model if you if you don't have to and so I always recommend starting when doing experiment with not training the weights of the llm and so just to give you a comparison points like here's a training duration for one test this was on I think an IMDb reviews data set it took about 60 Minutes to train with full fine tuning and then going all the way to some of our optimizations that we provide including and predabase which is automatic mix Precision Fresh Plus cached encoder embeddings we got that down all the way to just in order of less than a minute actually to do the fine tuning which is pretty crazy because at that point you're just you know doing one forward pass to generate the embeddings and then you're just fine-tuning a bunch of fully connected layers on top on the bottom in addition to that we have other state-of-the-art techniques and Ludwig available with single parameters like parameter efficient fine tuning methods like Laura which is essentially a way to augment the mall architecture with certain trainable parameters but you know doing this is you know their Frameworks that make this easier but still like a non-trivial thing in most cases but with Ludwig again it's just one parameter and we also provide distributed training with model parallelism and data parallelism out of the box so for example using deep speed that's as simple as just specifying a back-end configuration Ludwig or if you're using predabase there's no need to specify this at all because we do all the right sizing of compute for you based on the description of the model that you provide and so we'll do the selection of deep speed and the selection of compute resources for you and with that I want to get into a quick demo that shows you how some of these features work in practice so let me switch over to our product-based environment for a sec and so this is our where you land in product base we have the llm query editor and here I've selected a model of ikuna 13 billion not important which model you have lots of different ones available this one's just for demo purposes here but here you see that I previously queried the model you know what is machine learning it gave me a response now again going back to the hotel reviews data set I might want to ask a question like what are some popular hotel chains and then from there I'll go ahead and clear the model [Music] and okay so generate this response which was uh there you know some include Marriott Hilton Hyatt Etc so it looks pretty good now what if I want to ask something a little bit more specific that might be something about a data set in particular so for example what are some hotels with poor Wi-Fi now the previous response was something that the model already knew because it's seen it in its training data when it was originally trained right but in this case we're asking about something that doesn't know and so its responses you know as an AI language model I don't know anything about what hotels have for Wi-Fi and then here are some things that might help you with your answer but sorry I can't help you so at this point you know you might think well now I gotta fine tune this model but uh actually it's not the best place to start in those cases like what you probably want to do is instead just augment uh your model with some domain-specific data without having to do any fine tuning so for that you know we provide this parameter here called data set to index and then once I specify that um I can go ahead and send this query off to um the large language ball and then the first thing it's going to do is index the data if it hasn't been indexed before and then go ahead and do the uh in context learning and retrieval process on the Fly and so here now you see that says based on the given context you know we see that uh you know the American n had poor Wi-Fi uh Best Westerns uh Carmel townhouse Lodge Etc and if you look at the you know raw prompt that was submitted to the model um this one's a little bit not formatted particularly great but you can see that you know in there was all the attribution needed to kind of give you a sense for why the model ultimately uh came to the conclusion that it did now once I do this um this is I think like a very good way to kind of get to know your data and do some qualitative type queries every data but oftentimes what you'll want to do is transition to something a little bit more quantitative as well like a prediction and so what I can do is show you an example of trying to predict uh specific um specific Hotel review score so for example you could say like given a review which in this case I'm gonna know is in my data called reviews full uh predictor rating on a scale from one to five and we'll go ahead and send that query off and see what happens and just to give you a little bit of a sense for um what exactly is uh what data this what the data looks like under the hood what we can see is that you know I upload this data set here is that we have a data set of reviews on a scale from one to five with about half of them being five and then a bunch of review text that go along with them with the you know median sequence length being you know or mean sequencing being about 112 and then you know all the the content of the review is sitting you know here are various forms right and so if you look at the responses um we see that uh you know it predicts like oh I think it'd be like a four out of five I think it might be also a four out of five um two out of five Etc but the you'll notice that the responses are not really in a format that we're particularly happy with is the model is very verbose um it's you know adding a lot of filler words that we don't really care about though you know one thing you might want to do is iterate on the prompt to try to get uh to something that more accurately uh reflects the format that you're interested in and so here I just have a prompt that I uh spend a little bit of time iterating on myself that shows okay you know here I've managed to get them all to output something that looks a lot closer to um what I would ultimately want so which is something like you know a particular format and so now instead of the model kind of spewing a bunch of you know verbose stuff it just gets straight to the point the review is four the review is three Etc and now at this point you can see that if you compare to the ground truth it's in most cases not too far off from the actual review score so at this point what I might want to do is formalize this as a task definition in predabase and so that's where you get into the model repos that exists in protobase so here we have a model repo for this particular task that shows a zero shot a few shot config with random retrieval and a few shot config with semantic retrieval and in the interest of time since we're running a little bit low on time it won't get too big much in the weeds here but you can see how I took my same prompt and then specified as the prompt for this model and then told it that I want my output to be a category that has this vocabulary in additionally I can provide some validators that kind of validate that the output you know matches what was in the out what was in the output of the response in a particular format and if I want to then go to doing fuchsia shot with a semantic retreat with random retrieval it's a simple saying I want to do retrieval is random k equals three and if I want to do then semantic retrieval is as simple as saying you know I want to do semantic retrieval and the important Point here that I want to emphasize is that you can see also the progression and performance as you go from models that are doing just basic zero shock with 37 accuracy all the way up to the semantic retrieval which gets you to 66 accuracy and then if I want to go further we can start to look into fine tuning techniques or I can say create a new model from latest and it draws me into this UI which is our model builder UI so I select a large language model that's the type of model I want to build and then from there there are all these parameters I can select the model name the prompt template that I want to use retrieval strategies and then my various adaptation methods like Laura or adalora and and all of the different parameters which come with reasonable defaults but then I can configure beneath that and then all this is also fully modifiable in uh yaml through the config editor which is this here so um yeah so all the options available um from zero shot few shot fine tuning all in one platform and then all the ability to deploy this model back into production so you can start querying in um from the you know original editor uh completes the loop so that you have it all integrated in one place and I'll go ahead and stop there and open it up to any questions uh from folks so thank you right on very cool thank you for showing us this and your prayers were answered because the demo gods were very nice to you this time around now yep no uh no crazy errors from what I saw so it worked out well that is awesome so it takes like 20 seconds for the what we're saying right now to be seen by the people in the uh in the chat so I'm gonna go ahead and ask a question that came through and uh Michael actually already answered it in the chat but it might be good to talk a little bit more about like fine-tuning different models and it doesn't matter if there's different Market model architecture or size or any of that like what do you need to keep in mind when it comes to the fine tuning piece yeah that's a good question um so I'd say that with fine-tuning the most important part is definitely the data um I would honestly say that most of the parameters um you don't really need to play around with them too much in most cases like with fine-tuning your goal is really to avoid kind of screwing up all the background knowledge that the model already has so that catastrophic forgetting problem and so when you use like these parameter efficient techniques like Laura because you're only adjusting a minimal set of Weights it really helps with that part of the problem I would say that the much more important thing is to make sure your data is in the right form and then after that I would say it's important to make sure that you use the right type of fine-tuning technique for the problem as well so again I think a lot of people often starve like I have a text classification problem let's fine tune large language models to Output text that looks more like the type of thing I want to predict in most cases you'd much rather just get rid of the large language model output and then add that classification head on top rather than try to stick to a text to text model right so choosing the right technique for the task is is also the the most important thing to consider but then from there in terms of like specific model architectures and things like that I would say that the important thing is to try to start with something that's small enough to get the job done because you know in terms of like the iteration velocity trying to start with like fine-tuning a 175 billion parameter model or something for a classification task it's going to cost you a lot and probably you know won't do any better than a smaller model if the task complexity is is you know bounded enough right yeah be pragmatic that is a great call and also you don't really know what it is until it comes out so it's kind of like a crap shoot uh you you can if you do it like what you were talking about with these gigantic models then it may cost you a ton of money and it might not even be that good yeah so absolutely aware of that so I think oftentimes a good strategy is to you know a lot of these models come in different uh sizes right so like Falcon 40 billion Falcon 7 billion I think a good strategy in a lot of cases is um like use Falcon 40 billion for doing prompt engineering find like a prompt that gives you pretty good results then try fine-tuning Falcon 7 billion with uh for your particular task using that prompt as the input prompt um and you'll find that it's in some ways you're kind of distilling the model at that point um into like this smaller model to predict similar to how the original wall predicted but you'll find that actually it probably does quite well at the task at that point compared to the 40 billion one excellent all right last one for you before we keep it cruising does pretty base work with self-hosted open source llms if the data is sensitive does the data have to go to the pretty based server great question yeah so we have two different ways of deploying protobase we have a SAS version which runs in our Cloud so you don't have to manage any infrastructure yourself but for the particular type of use case that that person asked about we also have a VPC so virtual private Cloud version where uh all of the compute runs in your environment and so no data ever leaves your environment so if you have sensitive data in AWS for example you can actually run the part of the product base that processes the model for training in your data plane in your in your AWS environment and none of that data is ever crosses the boundary into our environment so good yeah airtight make it that airtight Gap and then uh get all those stock twos and those stock threes and all that fun stuff complied with you're golden so dude Travis this has been awesome man I appreciate you coming on here and I appreciate the demo if anyone wants to continue the conversation with Travis of course you you've got the chat but you also are in slack I believe so feel free to tag Travis in the community conference Channel and we're gonna keep it moving right now also I I mentioned it before but I'll mention it again hit up the tab on the left called Solutions and you can find out more about predibase and that's all thanks Travis man I'll talk to you soon excuse me Trails bye foreign