our next speaker I'm so pumped about Claire welcome to the stage thank you for your um for your patience and for joining us I know you're a rock star at a rise and also in the Denver Community you've been really active there so very excited to hear your uh talks today yeah great to meet everyone how's my audio everything coming through okay yeah sounds good and here are your slides Perfect all right hello everyone I'm Claire Longo I'm currently head of ml Solutions engineering for customer success at arise um this is an absolute amazing job it's one of my favorites that I've had um and it's because I have this opportunity to collaborate with all these different customers we're really working with like some of the best machine learning teams in the world and through this role I have a window into how our customers are thinking about um monitoring machine learning models and I also start to see how people are starting to think about monitoring llms um I consult with customers to really help them integrate arise as an ml observability solution and my background is a little bit like data science um I used to be a data scientist I train models Hands-On I got frustrated that I couldn't get them into production so I went and learned to be an engineer and I became passionate about ml Ops so that's really my focus I've been an ml Ops person for a while and now I'm hearing this new term um ml or llm Ops here let me get to my slides yeah so this is my favorite meme right now it is it really resonated with me because I've heard a lot of different terms here like being an Emma lops person really passionate about applying devops principles to scaling machine learning models in production I've heard a lot of different terms I've heard AI Ops I heard DL Ops which is like deep learning Ops and now I'm hearing LL amlops and I'm almost honestly it's just like having a lot of trouble saying this word it is not easy to pronounce I've heard some other people articulate it very well but to me this is a tongue twister um so I'd love it if we could just stick with Al uh with L with ML Ops but as I really dive into like llms and think about how to put these models in production I do think there are some nuances with um productionalizing the generative AI um that really differentiated enough from this old school kind of tabular ml it's funny to really think of machine learning as a little bit old school here but um there's nuances in productionalizing the LMS that I think are worth digging into I don't know if it's really worth a whole new term here that's this hard to articulate but um it's definitely worth diving into the differences in the concepts here and really looking at monitoring specifically through the lens of llms and what that might look like so what I'm going to talk through today is um taking my learnings from monitoring machine learning models in production and really think through how we might apply that to monitoring these llms in production systems so um I think the audience here has probably heard this already a few times but I talked through how to choose the right Tech stack um for deploying an LM and the reason I'm going to talk to that is that there are like kind of different monitoring opportunities and so I think it's worth kind of highlighting that so when deciding to implement an llm into production there are two things to consider these are very simple things but I do think it's worth calling out here because it really will prepare you with the right data and information to choose your best tech stack so number one is proprietary data does your system require proprietary data this is going to help you decide if you should really be building things in-house or if you're comfortable sending that data through a open API and then number two is just what is the value of applying generative AI here I kind of laugh at this one because I feel like um I think it was about 10 years ago there was like the big data craze and I was asking myself the same question in terms of what is the value of applying data science here more traditional machine learning models and I would always try to compare it to a baseline like machine learning is exciting it's the next big thing we could totally use it to solve this problem but with a simple rules-based heuristic do what is the value of layering in the more complexities and I think it's so important to answer the same questions with the generative AI um and then also thinking about the value I like to think of it in terms of the cost of building the system as well as the ROI to the business so what business problem are we solving how much do we think that we're going to impact the business in terms of um Roi and they'll help us choose the right text tag so I think this is very similar to the previous talk um but I will talk through it quickly just as a recap so how do we choose the right Tech stack for deploying our LM model so starting with the first row the first option is kind of the MVP option this is the out of the box uh just a single endpoint prototype this is made something like using the open AI API um there's nothing else here there's no prompt engineering or anything else so although this is really easy to get off the ground just kind of test out an idea it's great for an MVP or proof of concept it's not going to be personalized to solving your business use case in any way so that's where we go to the next row here we're looking at prompt engineering I think there's been a lot of analysis to show that there's a ton of value here figuring out the right prompt so when we start using the prompt template we will need to provide a light framework here to really integrate that template into the system so there's a little bit of more coding and you're building out a real kind of Software System here but you are also um kind of automating and um starting to tune things a little bit more to your business use case the third line here is going to be custom Vector database so the way I like to think about this is actually through a bit of an example let's say you were building a chat bot that answers questions about a specific software tool and if you send those questions just to chat GPT or some llm API it might not give you anything specific but if you provide with that query um specific context about the software such as the user documentation you can start to get some really good responses that are very um tuned to the use case that you're trying to create this system for so that's an example of context data context data can be a user's document it can be any really kind of set of text it gets decomposed into vectors and stored in a database and then these get integrated with your prompt so it's a way to really kind of greatly enrich your prompt with a proper context data to really now we're tuning this even more to our business problem that we're solving and we have a lot of control over what goes into that context data so there is a lot to optimize here lots of opportunity the fourth line here is fine tuning so this one this is this one is expensive um I would go through these in order and we're kind of like increasing complexity as we go but we're also probably increasing value so I think that's worth considering um fine-tuning you take your data and you're actually fine-tuning these public LMS your use case but the reason I say it's expensive is like these are large models there's a lot of Weights here so this is um obviously going to have some cost to it um the last one is going to be your foundation model serving on your own infrastructure so this is like the most secure option your data doesn't have to leave your own infrastructure you have full control over creating the llm as well as the data comes into it um obviously I think the benefits here are clear you just have control over your system you don't have to send any data out of your infrastructure but it is going to be the most complex to build because you're building from the ground up so with that in mind I want to talk through kind of what a general LM system here might look like so in this example we have a user query and once we receive the query we're pulling data out of a vector store that's your context data Maybe user documentation or something like that and then we're using a prompt template to construct a excellent prompt with that user query that context data the best prompt that we can and we send that into the llm LM is going to give us a response and then if we're really fancy we can start collecting user feedback and I think there's a lot of value in this user feedback because of the kind of um feedback loop it can create as you improve things so I definitely recommend layering that in if you can but let's look at the system in the context of monitoring so what I'm showing here is um on the left of the screen you see these two little areas where the embeddings are generated the reason I highlight these is because this is the data that you're going to want to start to monitor to see how well the system is doing um really quickly what is an embedding I think this group is pretty technical so I'm not going to go deep into it I think we're all kind of working with embeddings at this point which is really fantastic I used to have to explain what an embedding is all the time but essentially what this is is user query it's a document of text it's just text data and we need to translate that data into numbers to be able to work with it in a meaningful way in mathematical models so the embedding is really just a mathematical representation of the text Data it preserves all these interesting patterns in the text data and then we can use it to work with in mathematical models which is what an llm is so it really just takes text turns it into a meaningful Vector of numbers and it's really just a vector of numbers here so now we're generating all kinds of vectors of numbers we have embeddings from user query our context store um our Vector store I'm sorry with the context data is embedding is embeddings already and then also there can be embeddings generated out of the response as well so when we look at this from the lens of monitoring there's all that data flowing through the system that's going to be very valuable to collect and analyze and then there's also kind of like these points where these systems can kind of start to break there's situations where even with a really good prompt if you don't have good context data involved in that prompt or the LM hasn't been fine-tuned to really answer the the question that's coming in through the query we all know we've probably seen it these models are just hallucinate they just make stuff up and they will vary confidently um respond so it's a little bit hard to spot these hallucinations but that is a pain point to look for and it's usually because there is a gap in the context data or the model hasn't been properly fine-tuned the other area here is just evaluating the accuracy of this kind of system um I think that it's hard to really say how accurate are these models because we are not this is not tabular machine learning anymore where it's like right or wrong it's either kind of like relevant not relevant but even that's not quite the right way to think about it um there's just a challenge here in evaluating them there are a ton of metrics here that we can look at and um there's also looking at how the system is impacting your business so let's say I deploy a chat bot that does something meaningful for my business if I click that user feedback I can start to really get kind of that proxy for how accurate these systems are and so that's kind of a good way to go here so really high level on the slide what I'm trying to really show here is like what an LM llm system looks like and where the opportunities are and the data that should be collected for monitoring so I've monitored quite a few tabular machine learning models um a lot of recommended systems models and these are really easy to think about in terms of monitoring and I want to talk through what that looks like and then we'll kind of translate that to LMS because there are new complexities here so for an ml inference like you're just making like a prediction out of a machine learning model let's say it's a recommender system that uses tabular data the things you want to log so you can do proper monitoring of that system and make this entire system auditable are going to be your features and your predictions and then you're also going to have a feedback loop collecting those um truth labels so um maybe an example here with a recommender system is the prediction would be like a probability that customer might purchase an item and the features might be their past purchase history and then the truth label is very straightforward they either purchased it or they didn't this is all pretty easy when we look at an llm system and think about like what might an inference really be in this situation the data looks more complex so the data that we want to log are no longer features and predictions we're looking at queries and responses we also have that context data whatever context was retrieved for a specific query and then we also want to collect human feedback data but it may or may not be as easily generated as some of these tabular machine learning use cases so the data just came became a little bit more complex but I think the Core Concepts and kind of best practices around monitoring really kind of remain the same here so what we want to do is we want to monitor those embeddings and what does that even mean and how do you even get embedding out of this system so as I mentioned the embeddings are just very long vectors of numbers they're super meaningful and I'll show you how we can get meaning out of them but the first step is to generate them so if we have a system that looks like this we have to generate and save these embeddings in the um out of the production system so the way to do this is there's kind of two options to consider here one is to extract them directly from your neural network neural networks will naturally create embeddings as they are trained so if you went with option where you're creating your own model here you can just pull embeddings out of that and the second option would be to generate them through an API such as like open AI hugging face have really good options for this so there's a lot of ways to get the embeddings out of here but if you have that raw text Data like let's say you have that query you can translate that into embedding using these tools um or if that's flowing through a model already um like your own llm that you can you can pull it out of that just a quick one minute warning play okay thank you I can wrap this up very quickly um that's actually perfect timing so once you have the embeddings you start monitoring them and it's a lot it's a different way than kind of monitoring the the tabular data where it's very easy to see like an immediate change in tabular data you're just really looking at um one dimension there but in beddings we're talking about n Dimensions latent space so what we want to do is visualize those embeddings in latent space and calculate distances between them so in the situation where I talked about there might be a gap in your contacts data and your queries maybe your user starts to query your system for data that is not well represented in your contacts data Vector store and you'd love to kind of add some data to provide that context your system starts performing better you can visualize this you could look at your embeddings for your queries you can look at your embeddings for your contract store you can measure different distances between that and if there's a gap it'll actually pop out really quickly um so visualizing it is one thing but monitoring it is another how do we actually proactively monitor that you can use distance metrics um to calculate kind of those distances between those clusters and then monitor on that so in this example we're looking at euclidean distance between two different clusters so if you had an automated Mantra on this this would tell you that your um queries and your context have a significant difference and that is all I had here's a very beautiful graph of what that might look like this is an example from arise Phoenix so if you're interested check it out um and if you want to Deep dive more into like monitoring machine Learning Systems albums anything like that feel free to connect with me I'm happy to meet more people in the community thanks everyone awesome thanks Claire beautiful last slide I love it