so now we're going to keep it cruising we're gonna bring on my man Eugene where you at where you at here he is oh dude all I can say is people love freaking Vector databases so you are going to hit this out of the park it's gonna be too easy can you hear me all right I can hear you can you hear me yep I hear you loud and clear so dude I'm gonna take it I'm gonna let you uh take it from here and I will be back in another 30 minutes I want to mention to everyone if they have not checked out the Liz oh no not messages Solutions tab right here you've got the virtual Booth Booth from zeliz and these are the people that bring you milvis so you know what it is all right I'm Gonna Leave You and I'll be back in like 20 25 minutes man looking forward to this chat all right okay uh I think everyone can probably see the screen now um yeah so welcome to this talk my name is yujin I'm going to be talking to you about the key to scaling llm applications so everybody's kind of building llms now and one of the major things that we are concerned with is how do you scale these applications and how do you productionize these applications right some of the problems with llms come with uh mostly data and costs so they don't have the domain knowledge that you need they don't have up-to-date data right for example gpt3 was trained up until September of 2021 um or it's just it can be incredibly expensive to rerun many many queries especially if you're going to be running them frequently so a little bit about me my name is yujin Tang I'm a developer Advocate at zillus um these are my contact links but you know listed below the QR code will take you to my um my LinkedIn so if you've got you know you've got your phone out you can scan that and connect with me and message me a little bit about zillus zealous is the company that is behind milvis which is an open source Vector database we are the world's most popular open source Factor database and mainly our really really good at scale and I'll tell you about why Melvis has such good performance at scale later on um but for now that's kind of what you need to know right uh and these are some links to go find us either um you know join us on slack come talk to us about batch databases unstructured data AI agents you know we've got channels for all this stuff and also check us out on GitHub Novus is a project that is written in go so the content of this talk is pretty much going to be talking about large language models I'm going to walk you through neural networks in general kind of like you know starting from way back not way back but like you know it's about 10 years ago up until now talk about some of the challenges the llms face right the data challenges um then we're going to talk about the CBP framework which is a framework that we here at zilla's uh abide by and advocate for in terms of building out production level um llm apps and the CBP framework stands for chat gbt Vector databases impromptus code then we'll go over you know what is a vector database because we're going to be talking about them but we're never really going to cover them until the end we're going to talk about what is a vector database and we're going to talk about novice uh and if at the end we have time I will go over a quick demo of um how how uh you can build a scale by online application or I'll go over a quick demo of a scalable application so part one um unless you've been living under a rock you know about Chad gbt you know about Clyde you know from anthropic you know about Bard from Google uh they've been in the news like crazy and there's been a ton of development in this space so let's go back to 10 years ago um when convolutional neural networks got pretty big and the reason we're going to talk about this is because convolutional networks give you this thing called local context so you can see from this kind of you know very simple image uh Melvis is the world's most popular open source Vector block and you can see that these arrows are pointing to the neurons that are right next to them and that is basically what local context is in the sense of a convolution the next kind of thing that came about was self-attention or Global context I didn't finish the sentence this one in the slide obviously just says Melvis is the world's and then you know I should say most popular open source Vector database um but you can see that the way that the self-attention architecture is different is that we are pointing uh our connections to every neuron in the next layer if I only have causal attention which is kind of the way that these statistical stochastic generational models work right you have every token after the first one has access to the first one every token after the second one has access the second one so in this case milvis is the world's most popular vectors database the world the word world has access to you know Melvis but milvis doesn't have access to the word world and this is directional Global context right so this brings us to uh kind of today how llms are made obviously llms are not this simple looking um you know they have Transformers and encoder tea Cutters whatever however you want to kind of Imagine That architecture but at their core LM still are stochastic models they fit this neural network um the underlying you know uh principle of neural networks is that they are stochastic and so what happens really is if you are an out if you're working with an llm an LM is going to generate the sentence milvis is the world's most popular vector and the way it finishes the sentence is it asks the data basically you know given all of these former tokens what is the most likely next word and so in this example we say like we say that database 0.86 most likely next word um obviously the downside to this is hallucination which is kind of what the what the these these slides are kind of focused around oriented around is solving the problem of Hallucination is one of the examples of effective database usage and why Factor databases are such an important part of the llm stack so I'm gonna just show some some math here for those of you who uh are into the math Behind these things right so basically the way that you formulate this problem that lens are solving the way they're getting these statistics the statistically most likely next token is that given some set of tokens t0 T1 all the way to TN predict TN plus one and the way that we formulate that in a probability perspective is the probability of database given that milbus is the ball blog very similar to Bayesian statistics if you kind of remember that you can think of the way that these generative models generate their tokens in the form of these statistical models uh so challenges of llms um you know these are very very popular models but they still have some of their own issues and like I was saying earlier the one of the biggest issues is hallucination and so for example if we query chat gbt uh how do I perform a query using Novus it's going to give me some code and the code is actually going to look like it could be right because you know this looks like this this looks like python code right but what we'll notice if we understand like we know how milvis works so if we've read the uh you know the documentation this is an open source software so the documentation is is uh out there right we'll know that this is actually not how this is actually not how you use Melvis it's not how the connection is done so uh later on we'll see how this is actually done um but for now I just want to show this example of what is a hallucination so the solution to uh these hallucinations is basically to inject domain knowledge uh into large language models really to inject domain knowledge on top of them on top of large language models uh but into llm applications and how do we do that we use the CBP framework so the key idea behind the CVP framework is that we can view llm apps as a general purpose computer kind of like how computers work there's a processor there's memory and there's code and in this CBP framework you will use chat gbt or any other llm so you can actually replace letter C with L but we've just decided to go C but this can be interpreted as the CPU the processor the you know the the main compute power Behind these llm applications V is the vector database this is the storage um you know your your ROM kind of your hard drive in a sense uh and so in this example like you know milvis could be a vector database and then promptus code is basically your interface so you know like the the UI in a sense the the code that makes the UI the OS whatever the way that you interface between the processor the storage it's a type of code right and an example of an application that uses this CBP framework is an application called OSS chat which if you use will actually give you the correct way to use Melvis um in this chat this is a this is a chat bot that talks to open source software and we've got a bunch of different open source software on there now it's not just novice uh there's like Pi torch YOLO um there's many other you know Auto GPT there's many other open source softwares that we've put behind OSS chat and the way this works is basically we generate we go into we get the documents we generate some set of embeddings from the documents and the answers so we'll ask chadbc to generate some questions um that would make sense based on the documents as well so you're working with the question space and the answer space and then we use those Cloud to host these uh sorry to hold these embeddings and when a user comes and asks a question such as how do I perform a query using novice they should get a correct response back like this because we've scraped the actual code the actual documents that are related to the um related to the software and so we also use this thing called gbt cache which was built which is another open source library and this was built originally as part of OSS chat because we saw that there were some questions that were asked many times and it is ineffective uh inefficient um and it's costly to run the same questions multiple times through an llm when you could actually just say like hey maybe a user maybe many users come and say how do I use pytorch to resize my images you know if you have 10 users coming and ask that you probably want to Cache that and just query your vector database and say okay here it is here here's your answer we're just going to return this we don't need to um you know query the llm so recreate the answer for us or to decompose the query if you're using something like llama index um okay so how does it solve hallucinations why does this work right so I've I've told you that this is supposed to work this way but why does it work so what vectors databases do is they give us an access to domain knowledge and then they allow us to perform semantics at search on domain knowledge via the vector embeddings and Vector embeddings are these numbers that represent some sort of object uh in this example that I've shown here the object is a word and this example kind of shows you that if you take the word Queen and you subtract the word woman and you add the word man you get the word King uh and this is this is a very um is not that old but like this is a seminal paper in the space and the advances in technology have gone far beyond what is shown here this is a very simple example of vector there's no such like nobody's operating on two-dimensional vectors anymore uh the standard size is like so if you're using open AI I think it's like 15 36 if you're using cohere it's like 768 um there's something like 384. so the the standard size of these vectors are hundreds um so in practice the way this works the way that you can operate on these Vector embeddings you know do math on objects like images or words is you take your knowledge base you run it through a deep learning model and then you get the factors out from that you get the factory embeddings from your deep learning model and the way that works is that you actually take the outputs from the second to last layer of your model in order to get the vector embeddings so the a deep learning model will typically do something such as classification uh or maybe like NC recognition part of speech tagging something like that and instead of asking the model to give us the you know the classify the classification we just we just pull the the values from the same to last layer and that is a representation of the object that we put into the um into the network and then we put that into a vector database such as Melvis okay so what is the vector database why do you need one Vector database here at zillis we like to say that a inspector database is a database purpose built to store index and query large quantities of vector embeddings uh and you know with with milva's Focus really is that is on billion scale right is on these large quantities of vector embeddings so this is a bunch of bunch of text dump um but the basic idea that you want to get uh get from this slide is there are actually other solutions to performing Vector search or to working with Vector embeddings you don't need a vector database you can use a high performance Vector search Library such as face um you know Facebook's Facebook AI semantics or maybe similarity search faiss or you can use you know hierarchical navigable small worlds hnsw um you can use annoy which is approximate nearest neighbors oh yeah which is built by Spotify um but if you want to you know have a more production ready Factor uh search Vector similarity application out of the box then you probably need a vector database so Vector databases allow you to do things like filtering on your vectors so if perhaps you only want you want to filter on some of your metadata um it allows you to do hybrid search so for example um you know you can search your text and your a dense Vector maybe a sparse Vector um it provides you with backups right your your data is backed up you're not going to lose it really easily um High availability you don't have to worry about scaling um maybe sharding if you're doing a lot of streaming data aggregation search parallel search lifecycle management multi-tenancy working on a GPU accelerator for example we have a GPU accelerated uh what's it called integration with Nvidia and one of the other things we do really well is the billion scale storage so let's walk through some Factor indexes so you can understand kind of how we're comparing these vectors and how that works so this is the one I was talking about earlier called noi which is approximately nearest neighbor search and this builds a binary tree and the way it does it is it just you take two separate um we take two points in your space and then you split the space in half and then you do it again and again and again and again until you have like maybe like three vectors four vectors five vectors this is a hyper parameter that you can control that is um in one section and this produces like I said like a tree kind of index and the way that you search is you say like okay find me you know the closest space and then the closest Vector in that space then there's IVF which you know this looks like a boronoid diagram um and basically the way this works is some sort of k-means type clustering algorithm and you come up with say some number of centroids so you throw your data in there and then you cluster all your data and then you get these like you know areas uh that are clustered around a certain centroid and when you search what you do is you say let's say I want to search five centroids maybe your your dot is like somewhere in here right you search these five centroids and you see which one of the actual vectors is closest to your new data point then hnsw this is a really really popular one um and it's also a somewhat complex algorithm basically you're starting you're creating a graph index of all of your data points and then when you insert them into the graph what's actually happening is you're also assigning each data point a uniform random variable and that uniform random variable is just determines what layer they're going to be in so for example let's say you know we're building something and we're assigning uniform random variable if you get between 0 and 0.9 maybe you're on layer 0. if you go between 0.9 and 0.99 you're in layer one if you're between 0.99 and 0.99 you're in Layer Two and so on and so on and so on so as you can see the layers get sparser and sparser as you go up um just because of this uniform random variable now that does mean that it's possible that you have four million data points and everything is in layer 0 but it's unlikely and the way this search works is you start at the top layer and then you just sparsely search your way down and it's really fast because it's Graph Search right your graph is already built you just have to find the next point so let's talk a little bit about you know architectures why is Nova's fast why is Novus good right um milvis uses a distributed system native back end and uses um these so this in this diagram you probably don't need to pay attention to most of this you just need to know okay how do I interact with it SDK sends it to a load balancer load balancers bounces all the you know the requests and then the important part is to know the stuff about like the query nodes the data nodes and the index nodes typically um these nodes can actually be really the same node right they're like kind of like a kind of like a pod in kubernetes but we have dedicated nodes that do different things because it makes better performance basically so the query nodes query they query the database and they return the data that you need the data nodes hold the data that you need to kind of work with in memory and the index nodes do your indexing when you are well when you're indexing and I'll just address this here with Melvis you should almost never need to re-index because of the way that the architecture is is none of this actually saves all your data in 512 megabyte chunks instead of one whole you know block and that makes querying much much faster because then what you do is you query these 512 megabyte chunks in um in parallel instead of you know running through an entire uh 1 10 30 100 gigabyte block um and then you know when when your data is in there we save it in task three or Min IO Azure block you know some sort of uh permanent storage so yeah that's Vector databases um that's milvis do we have questions at the end of this or um yeah we can definitely get some questions going there are a few that are coming through in the chat but it takes probably about like 20 seconds for the stream to come through and you're good with the screen I can take it off share on your screen um Let me let me do it let me do the demo you see this yes I can do you want me to ask a question now or at the end of this so we'll we'll prompt people to ask their questions now and by the end of the demo hopefully it'll be full in the chat yeah that'll work so I'll just show a quick demo here there we go um so we'll say something like what can I build with Lane chain is on here and if you try this at home I would go to Chachi BT and ask chat gbt what you can build with Lane chain and see what it says because it will not it will not tell you this but this one you know tells us you know Lane chain can help us build a variety of tools with llms uh you know we can say like show me how to use conversation chain and it should show you know something about conversation chain which I think is a one of the tools of Lane chain um but yes I suggest you go try this with a llm and without like any layer on top of it and just see what happens oh see fills out the code Etc et cetera yeah that's uh pretty much pretty much it for my My Demo here there we go there we go Dude Sweet all right so what made you use go instead of rust uh that's a good question um I don't know actually that's that's not what you're supposed to say mix some up we're hallucinating like chat GPT man come on I hallucinate more than Chad gbt wow um yeah I I I really don't know though I don't know like how popular rust was back when Millis was written in like 2017 I think goes like coming out and it was like a pretty popular language and people were like oh this is uh you know really performant and um awesome safe whereas you know rust is is actually pretty good for a lot of ml type stuff I would say that um well Russ was around in 2016 2017. yeah I know that actually um I'm not really sure though all right well we got a few more that came through here um how does milvis compare to Vespa and quadrant how does Mills compare to Vespa and quadrant well um actually oh I have something for you for this uh there's this thing called uh we just released this thing called the vector DB bench I will is there somewhere for me to like paste this link yeah just throw it in the chat right here and I'll put it into the big chat there we go that is awesome I think people are gonna love it yes this is a Richard uh database benchmarking tool that includes um milvis zealous quadrants and weaviate and uh with the results for these four and as well actually elastics coach rules for these five and then it allows you to perform your own uh benchmarking as well but if you download it you can see all of the results for these five existing ones and I will just tell you that um novice really outperforms quadrant when it comes to load testing like the number of effects you can put in there and uh there's spoiler alert hold on uh and uh also queries per second and for vectors at large sizes and small sizes all right I like it question coming through with Mateo okay how can you cash semantically identical questions that are worded differently oh great question I'd like you that is a really good question um you would save them and with the vector uh with their Vector representation and if effective representation is close enough we'll just return the closer one it's it's actually it's just straight up just just look at the back to representation um no other no other real tricks there nice all right all right so for more people so a few people are saying in the chat that they really appreciate your honesty and they love to talk it was very helpful and uh dude I'm gonna direct you over to the chat for people that are asking more questions feel free to keep it going and we just recorded a podcast episode that is gonna drop real soon hopefully next week if I can get everything my ducks in a row as they say so I look forward to that happening and Bradley just or Brady sorry that Brady just asked another question in the chat so I'm gonna kick you off go over there because I've got a fireside chat to get to and I'm gonna bring on the next the next guests as I mentioned earlier and I will mention again you can call me Ringo today because I'm keeping time like a clock baby I'll see you later dude it was great having you all right cool [Music]