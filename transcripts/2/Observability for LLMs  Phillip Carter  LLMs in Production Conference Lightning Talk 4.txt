I was just mentioning that we have a podcast right and you have been on that podcast and it was quite a conversation I will say so if anyone wants to go check it out I think it's like episode number 183 I will throw in the chat which episode exactly it was but we talked about all of the stuff that you went through in the two blog posts that you've written and now you're going to give us a talk about the llm observability and honeycomb and how you all are doing that if anyone doesn't know I'm just going to say this you got pretty popular out there because you shared all of the hard stuff nobody's talking about when it comes to using llms and I think that struck a nerve with people yeah that's definitely true it is guilty as charged all right so dude I think you got a presentation I would imagine you're going to share some slides all that fun stuff I'm going to let you get to it this is a 10-minute lightning talk and so so I'm going to jump in in 10 minutes and get back to it all right Phillip I'll talk to you soon sounds good all right uh well hello everyone my name's Phillip I work for as Demetrius said honeycomb uh we are an observability company uh we've been building with LMS for the better part of the year and uh we have learned quite a few things and so I want to talk about that and in particular I want to talk about observability for llms because I believe this is not just because I work for honeycomb and observability Company but I believe that observability is part of the toolkit that you need to make llms reliable for you um over time so let's uh let's get into this so the key thing I think is that anyone who has built with llms uh over the past year has probably understood that it's actually not too hard to take something to market right like yeah there's a lot of work you got to do stuff but if you give a product engineering team uh about a month's time and API keys to open AI they can probably get something out there that's going to solve some real problems for people and create a good marketing moment and actually be pretty good uh but the problem is pretty good is not production ready it's like well it could be production ready but it's not necessarily like high quality production stuff over time and the thing is like after that initial Splash your users are going to expect a lot more out of what you're able to do and so you're going to have to be able to iterate the thing that you shipped initially is not going to be perfect and you're going to want to fix things that are not working right without regressing things that are already working now the problem is that typical tool belt that you might have as a software engineer to make software more reliable like unit tests or debugging in your IDE or writing regression tests or something like that those are exceptionally difficult with LMS if not impossible depending on what you're doing uh the other thing is that as anyone who's been doing prompt engineering uh to solve like a hard task knows that very very subtle changes in your prompt can massively influence the way that the llm is going to behave uh so it's really really easy to introduce a regression often for things that you may not have even understood were working in the first place um and then on top of that the more that you build on top of them like with a rag pipeline or pulling in other bits of data's context where like each request that a user makes has like all this unique data that's like associated with the request to an l um all of those things if you tweak any one of those things again massively influences the way that the LM is going to behave and that's kind of hard to deal with when you want to make software reliable over time so what do you do well I would say observability is a part of that toolkit and so I want to talk to you about how we did this at honeycomb uh dive into it a little bit and uh hopefully have this be like an illustrative example for you so that you can go and do this today and hopefully get something to be more than just like a beta marketing moment have like real Reliable Software that you're shipping so um earlier this year at about May 3rd I like to say we around and we found out uh the idea being that we released a natural language querying interface for our product so honeycomb being an observability tool is about querying data that you send and a lot of people wanted to do with do that with natural language so we built that uh it was great we solved about 80% of our use cases in the first release which I think was pretty awesome uh it was a great marketing moment all that and then we iterated and the iteration is where it was actually hard um yeah there was hard stuff leading up to the release but afterwards is where it actually got hard because I said we solved about 80% of our use cases well there's a 20% that was really really a massive long tale of things and every time we would we would fix one thing another thing could potentially break and so on and critically that 20% was also use case cases that our paying customers cared about so it was really important that we actually got this thing done um the problem is there were no best practices there were no tools it's kind of still true today although we think we've kind of stumbled into some uh and uh we eventually made it work um some of the results that we saw were we got quite a bit higher product retention overall especially for new teams who come in and don't know how to use our interface we got higher conversion rates to paid tiers within our product and our sales team in particular really really loved the feature because uh part of the motion in our Enterprise sales thing that we do is people need to come in and learn how to use our UI to query their data and the sales team doesn't have to teach them that they can just say yeah go to the text box type in what you want hit the button that says get query and then you'll get a query and it'll kind of fill out the UI for you and you'll be able to see it and that's great so but how did we get there that's that that this is this is where I think there's an illustrative example so it's kind of simple we practiced observability to the Max and so being an observability company it's like okay yeah sure Phillip you're an observability company of course you did observability for your feature uh but we really really did this to an extent that we've not done with any other product feature before uh we captured literally everything like not just latency and token usage and errors and that kind of stuff like what you'll kind of see with some stuff if you kind of Google it around today we capture user IDs team IDs the full text that is sent to every single request to open AI every single decision that we make in our rag pipeline which is about 40 or so steps uh the full response to the llm when we take that data we actually parse it into some data structures then we validate those and those go through a series of steps every possible way those can those can fail we also capture those and we capture user feedback at the end so there's actually a whole lot that goes on from when somebody clicks a button called get query and a query is actually executed against our querying engine and somebody sees the result and so what it allows us to do is it allows us to very cleanly isolate a specific problem to go and solve right so what I'm showing here is Honeycombs UI where we're observing the honeycomb feature itself that we built we have an instance of honeycomb that it's called our dog food instance uh but the idea is I have narrowed this down to every single time we failed to produce a query for someone and I'm grouping by user input and the response of the large language model and so the idea there is that we can see okay these are inputs that people gave and these are the outputs where like we clearly couldn't do anything we failed to do our job in in terms of our user experience um and you know you can kind of see here okay somebody was asking what are my errors but the LM didn't respond with anything so okay well maybe there's something wrong with like that full request so how do we dig into that and understand what potentially went wrong well we can view a trace and so when I said we capture everything I said we're not just capturing everything like you know um abstractly right we're using distributed tracing because what this is is Honeycomb this is open AI That's a separate system and our quering engine which is another separate system this is actually technically a distributed system and so the best way to understand distributed systems is to build a distributed trace and so this Trace critically is connected to our entire application so that we can track the full user experience rather than just the call to the llm itself the slice of that Trace that deals with the llm and deals with this stuff is 48 spans in length and everything is captured along the way um the full rag pipeline in particular makes up the majority of it and so to give you an example of this you can see there there's a collaps one thing that's over 20 spans in length there's a whole lot of decisions that we're making before we ever make a request to open Ai and that's really important because this is telling us okay this is literally what's happening before we ever make something I can get a full picture view of okay this thing did not do its job what exactly happened throughout the entire process so I can deeply understand this problem take that into my prompt engineering efforts or fine-tuning efforts if if I'm fine-tuning a model and then say okay we now know exactly what we need to do and then critically we know what to look for when we want to validate that this is working and so what you do with that is you monitor it and you monitor the end user experience right we're not monitoring just latency and errors we're monitoring the full end to end that full like 48 span thing that entire thing every one of those counts as effectively a bucket of events that we Monitor and so what we can do is we can then say okay yes we're compliant blah blah blah but like all right for the kinds that we're failing how do we Splat out all of the dimensions in the data and the values of those Dimensions the data so we can isolate specifically okay there's an error ml response does not contain valid Json I can Group by that field I can start querying for that specific instance of things that went on dig into a request and so on and so forth what's critical here is this allows you to establish a flywheel where you can fix a problem in prompt engineering you deploy that fix and then you look at the past 24 hours after you've deployed it and you say okay did our rate of success or failure for the entire user experience go up or down as a result of the work that we did yes or no um that you repeat that again and again you deploy daily you look at what you did over the previous day and over time this is how you get your llms to be uh reliable and so what's important is you can do this today you can use open Telemetry which is supported for already 11 uh um languages right now you can instrument your application like we did you can use an observability tool doesn't have to be honeycom it can be any one of the like dozen or so observability tools that people use today and you track real World behavior and you use that to influence how you do your prompt engineering this is your data source for fine-tuning it's your data source for any kind of evaluations that you're doing it's critical and I think lastly this is something that you can expect to get easier pretty soon probably within about the next six months I'm a maintainer in the open Telemetry project and we're taking uh this very very seriously we think that Ai and Vector databases and all that are critical components of applications that people need to write these days and so we developing semantic conventions that describe how that data gets captured and we're building automatic instrumentation libraries that capture a lot of that instrumentation that you need already uh so you can just load up an agent or a single line of code and boom you're good to go uh and we're also going to be developing best practices in the open Telemetry community so that you can know how to manually instrument your applications and capture application specific context in your stuff so I think this is something you can get started with today I think that if you hold off for about 6 months it'll be even easier for you to get started critically I think this is the path to making llms more reliable for you in the long run and thank you dude well done that was just in time and you make my life so much easier thanks [Music] again