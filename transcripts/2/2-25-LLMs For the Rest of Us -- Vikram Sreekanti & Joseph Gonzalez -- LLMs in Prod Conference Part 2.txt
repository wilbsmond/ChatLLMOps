hello hi how's it going good well thank you guys so much for joining us where are you joining us from [Music] I'm from Berkeley so joining from the East Bay California I'm in Oakland so right by right by Berkeley nice nice I'm in uh Washington DC got flooded with the smoke this past week but I feel like you guys are probably used to that sadly yeah sadly we got lucky this year this year wasn't uh wasn't too bad but yeah a couple years ago it was pretty pretty painful man hopefully it is not the new Norm but who knows what the future holds um well on a brighter note I will give you guys the floor um and thanks so much for joining us thank you thank you for having us so thank you for you know joining in um hi I'm Joey Gonzalez I'm joined here with vikramshakanti um and today I want to talk about the stuff we're doing with LMS and what it really means for the rest of us let me start with a bit of introduction so as I said I'm Jerry Gonzalez I'm a professor at UC Berkeley I'm one of the co-directors of the UC Berkeley rise in Sky competing Labs I'm a co-founder and vpa product at aqueduct so I've been doing research for a long time at the intersection machine learning and systems I've worked on Parts Apache spark I built Graphics did a lot of work in computer vision natural language processing I did a lot of the early work industry Training Systems then some of the kind of similar work in prediction serving systems um and then I've also been doing a fair amount of work in autonomous driving and Robotics so do a lot of work in the whole Space machine learning uh and I'm joined by Vikram who is one of my four former students and now my boss and CEO at Aqueduct and vikram's work has been studying how to simplify Cloud infrastructure and he's been building on Cutting Edge serverless technology with applications distributed computing databases and machine learning and at Aqueduct he's really focused on how to commercialize this Cutting Edge research so I come here wearing two hats I don't have the hats on me right now but I I my hat of the Berkeley perspective you know where I get to see where LMS are headed kind of The Cutting Edge research and then from my Aqueduct perspective what it means for for everyone thinking about kind of how these LMS are being Incorporated in products today um and so in today's talk I want to kind of combine these two perspectives and say what what they mean for the the field so starting from the Berkeley perspective at Berkeley I'm lucky to be part of a leading academic research center on genitive AI we've built some of the core technology behind everything from chat GPT to staple diffusion the reinforcement learning the the algorithms even the underlying systems are being developed by colleagues and my group at Berkeley so there are a lot of innovation across the model architectures evaluation techniques inference optimization distribute training techniques I'll try to highlight a few of those in my talk today one of the things that's important to keep in mind is that at Berkeley our goal is to push boundaries on what's possible to think about the next set of problems that people face sort of the one percent problems that the the frontier of Technology well Aqueduct is a little different it Aqueduct our goal is to really understand what is happening today what's easy what's hard what's missing what do people need to do to make llms real now um and this is really driven by real world use cases talking to ml practitioners understanding their challenges the limitations they face with basic machine learning all the way to LM technology so our gold Aqueduct is to enable every software team to be able to build LM powered applications so those are my two hats you know and I've been doing this for a while uh and I have to say you know for the first time these two trajectories are colliding what we're doing in research is you know supposed to be the distant future is you know affecting what happens kind of next week and we're seeing this intersection and LMS have kind of supercharged this interaction so I want to highlight that I'm going to start with the the perspective from Berkeley so I said uh Berkeley is an innovation Center for LM so I'm going to actually now just focus on some of the stuff in my group so we're doing a lot in this design of systems both for training and serving these models for a couple years now we've been thinking about how to make it easier for anyone to train very large models using whole clusters of gpus or we have a whole set of projects around compilers optimization techniques is part of the bigger Alpha effort and we've been transitioning that technology not just for training large models but now fine-tuning and serving them there's also thinking a lot about how we use memory more intelligently to be able to take these models and put them on edge devices to train them on edge devices and also even in the cloud to make them you know 10x 30X Faster by being more intelligent on how to use memory so building on these Technologies we've also been thinking a lot about what that means for the models themselves and my group launched an effort to build what we what is now I guess today one of the best open llms the vicunia model and as part of that effort we started to rethink how we Benchmark llms and so I thought for today's talk it'd actually be more fun to discuss what's happening on the right here what's going on the space of open llms and the kind of implications they have on on you know where industry is headed so we have to go way back to the beginning of 2023 uh the foundation model on which a lot of the the recent open work in my group and others in Academia um is built on this llama model uh and this is a second iteration of one of the big models developed by by meta um what are the Innovations in the Islam model that really made it a turning point for for research is it's actually built on much better data than some of the earlier Foundation models uh trained better and in fact smaller models train better on better data results in significantly better performance than the much larger models that preceded it now the problem with the lava model is it's not very good instruction following it just kind of finishes the sentence and so I have some colleagues at another University in the South Bay um in fact my four advisor uh I've been playing around with how to take that llama model and actually make it fall instructions you know chat like a human like chat GPT would to be more human aligned and they develop these techniques using self-instruct uh to create an instruction data set to fine-tune the Llama model and they produce the alpaca model which was a pretty big step forward it was an open model that we in a research Community could start to evaluate these conversational AIS but it was developed at Stanford uh and we're at Berkeley and we can do better and if we refer back to this this underlying truth in the design of AI broadly and then you know critically of LMS as well is it's really all about the data and there's a better source of data that we can build a better model on and that source of data is share GPT it's a website that Aggregates people's entertaining uh fun hilarious uh insightful conversations with AI technology like chat GPT and this website is good because these are really high quality conversations multi-round conversations that people found engaging engaging enough that they wanted to share with someone else so these are essentially hand annotated 70 000 conversations you're able to pull from the website before the apis the public apis are turned off um and so there's about 800 megabytes it's not large by by large language model standard it's actually a tiny amount of data but it's really high quality data so with the vikunya project our first key Innovation was to download this data and remove HTML tags which is kind of embarrassing but the reality is that was enough to get a really high quality data set we then basically followed what our colleagues in the South Bay had done at Stanford um to fine-tune the Llama model uh and then we did something a little different because we had a model that that's we believed was better instruction following and most NLP benchmarks don't really capture that we created a new Benchmark using gpt4 to evaluate these models and so the idea here is that we would give each and give this model and other models uh open-ended questions like compose an engaging travel blog uh post about a recent trip to Hawaii highlight cultural experiences and must-see attractions and something like alpaca goes I have composed a travel blog post about most recent my most recent trip to Hawaii whereas facuna goes Aloha fellow Travelers if you're looking for a Tropical Paradise with a rich cultural and breathtaking scenery look no further than Hawaii so you have these you know two different responses and then we asked gpt4 to be the judge assess them on engaging this insightfulness factful uh factfulness and we actually scored on multiple criteria and built a a scoring system to rank each of these models so we did this and it was pretty exciting so here's our vicunia model at 13 billion parameters uh here's my colleagues at Stanford so we were better than the Stanford model uh we were comparable to a model being developed by Google bard uh and you know this is gbt35 so we're you know the far-right model here uh is arguably one of the state of the art models that's not gpd4 and we calibrate that at 100 uh and so we're getting close to what GPT is is able to produce and this was an open effort done by a few students over a week you know if a few hundred dollars to do a fine-tuning training run so very exciting results for us somewhat of an existential crisis for some of our colleagues in big Tech you know again a few students a couple hundred dollars a 13 billion per hour model and it's pretty good colleagues at another big tech company you know spend 10 million dollars on a 540 billion parameter model over several years and it's also about the same um this this blog got a lot of press I want to First say that it's wrong uh they're not the same those models can do a lot more and the the people and teams behind them will actually be able to make them do much more we're already seeing uh in newer incarnations in a much broader set of applications but in these basic open-ended tasks uh the cunha was pretty comparable um so this is an exciting step forward it was also a little concerning for us like how are we actually that good maybe our Benchmark is wrong and so we decided to create a new Benchmark which would use open conversations with humans in the wild uh it's a living Benchmark where you you can go right now to arena.lmsys.org and talk to any pair of AIS chosen at random um and we're adding new AIS as quickly as we can or as quickly as people give us gpus and then you ask them a question you can ask them logic questions coding questions and then you evaluate the response and this gives us really rich signal about how people interpret these models in the wild so we took these these paralyzed competitions we call them battles um and we use the chess rating system the ELO rating system to then score each of the models based on all these pairwise battles uh and what's kind of neat is we see a similar result gpd4 ranks pretty high in that list uh and then Claude a new entrant uh you know ranks just below gpd4 and just above gpt35 and then here's our model the cunha our open model is actually doing pretty well here's Google's latest model Palm 2 and it's performing comparably to the cunya and then here's kind of the open source models and down here is the you know the Stanford model um so it's a pretty cool leaderboard uh there are some flaws in this approach and we actually just launched another paper that discussed some of the flaws in this approach one thing to keep in mind is that Palm for example will abstain and weird questions whereas a lot of these models will give an answer and humans like an answer not uh abstention and so when we start to think about like how we evaluate these we need to have a richer Benchmark um and so we've just developed a new Benchmark that goes much further than what we did in these earlier experiments based on the data we collected with with this Arena so what did we learn from this experience uh well maybe the first signal I'll come back to this again is that things are happening fast uh I'd like to tell you you should stop training llms you should use rlms uh unless you're my grad students and you should keep training LMS but you know the rest of the world should just train you know should just use the LMS that were created unfortunately we're starting to see some signal that you might still need to fine-tune llms taking open source models and extending them maybe what I think we're seeing more and I'm going to come back to one more project is that the Innovation is is going to move more and more from the LMS themselves to how we use the LMS how they will drive the the apps that we create and maybe the best example of this is a brand new project for my group which is thinking about how we use LMS to solve a problem we have in my lab and that is grad students need access to gpus and we're asking them to to use multiple Cloud providers and new startups and they all have different apis so we've been using LMS to make it so you can say go launch for this is Gorilla go launch four VMS with eight a100s uh in US West one and then the idea is that we have a fine-tuned LM that's designed to take these kinds of instructions it goes and looks at documentation according to the questions you were asked and then uses that documentation to directly launch these services to make it easier for humans to interact with the cloud and this is a big step forward for us because now we can use more more compute and this generated a lot of interest uh and and uh in in the public in fact before we even had a chance to promote the paper or land an archive and it was getting thousands of stars and it was just in the top of Hacker News uh last night uh so it's a project that's pretty exciting and it's really maybe pointing to where the world is heading and it's you know heading away from just building these LMS to really using to solve interesting problems so what does this really mean for the rest of us so I can't discuss this enough things are happening fast what's happening in Academia is transforming what's happening in industry and it means that we need to start to to react more quickly uh and my colleague Vikram here has been thinking a lot about what it takes to enable people to react so I'm going to hand it over to him thanks Joey so when I want to spend the rest of the talk focused on is how Joey and I have been thinking about all this Innovation that's happening on the left side of the screen here the new models the new tools that are coming out and how that Innovation actually translates into llm-powered applications for those of us who don't have the opportunity to work at you know one of the top research institutions we've been talking with a lot of folks who've been building these applications and trying to do some of this ourselves and as we've been doing this we found that this translation has something of a missing link this Missing Link is the tooling that lives around how you actually take a model um whether it's an open source model or a hosted one package it up in a way that will connect back to an application and make sure that the application does what you would expect this is really one of uh if not the next critical challenge that we are focused on here is how we can take these LMS and build real applications and as Joey said this space has been evolving really fast and the good news is that um there's a ton of tools open source especially tools that have been popping up in the last five six months that help us make this transition more so as we've been thinking about this Missing Link we've started to notice some patterns in the pitfalls the the traps that people fall into as they're building these applications the first one the most obvious one is probably building or running your own model um Emmanuel who's a who's another speaker at the at the conference here had this tweet the other day that I really appreciated his old advice used to be uh to always start with the simplest model but now his new advice to start with the largest model you can easily and those last two words are doing I think a lot of work here the the models that you can easily try are most often the hosted models of course and you know uh Healthcare or Finance or some more uh sensitive Industries you're gonna not necessarily be able to use these models in production but to get started to prove that you can build an application that again takes the Innovation happening in research and applies it into an industry application the place to start is probably by picking a hosted model maybe using cognitive face for as an API for some of your once you have a model the next Pitfall uh is really around data not using any data trying to construct a very clever prompt uh that that encodes a bunch of information but turns out to be very brittle or on the flip side using the wrong data oftentimes stuffing all of your data into a prompt which can both be very expensive and can also lead to some kind of jumbled up answers at the end of it the solution here is is uh perhaps obvious to many of you to pick up one of the many very powerful Vector databases that have popped up um recently uh and to use them to retrieve the right data at the right time contextualize your uh prompts and get better answers once you have the code and the data the next challenge comes around how you actually build these applications chaining together prompts uh experimenting with different models different techniques and at this point you all are probably uh yelling the the answer at your screens um I think everyone here is probably familiar with tools like link chain and llama index that allow you to chain together multiple prompts try out different models different techniques plug things in and experiment very quickly now all of this is probably very familiar to most of the folks here but something that we found in our conversations is that the space of people who are interested in llms is growing really really fast and there's always new Engineers new people coming in trying to build these applications and and running into many of the same challenges not knowing necessarily where to start with the tools but even for the kind of the initiated the folks who know a lot about llms we started to notice some some Trends in the kind of things that uh the kind of pitfalls that that these folks are running into and one that's perhaps a little counter-intuitive is relying on the model to do everything that you need these models are obviously very powerful they can process very complex pieces of information generate really interesting answers but at the end of the day to use these models just like with any machine learning model you're going to want to take some code that retrieves some data from an API cleans it featurizes it passes it into the model takes the result validates it just little bits of code that live around the bottom at least now now today most of the time this just looks like python functions that live around your your model indication now at this point we can put a box around this and we can call this roughly the ideal llm stack the tools here allow you to build a prototype put together an application that proves the point on on real data with llm indications but the next pitfall maybe the most painful one here is actually forgetting about everything else that comes after what do I mean by everything else well the list is probably pretty long but I have a few examples here the first one is cloud infrastructure management thinking about where all of these systems run where does your code run how does it link up with the rest of the application that you have essentially connecting the wires so that this llm-powered application you've built can connect back to your team's application or your company's product once you have it running you have to think about the data that's coming into it not just how you process the data and put it into a vector database but how you get access to the right systems at the right time have the right Apple support so that you're not leaking data unintentionally and kind of just have General governance around around after that um you know your application is running uh and you want to make sure that you're tracking what's happening the inputs and the outputs in case something goes wrong or you get a complaint you actually have the the Telemetry and the data that you need to to debug and then maybe most painfully at least for me there's things like budgeting and cost management where you know you can run up very large bills with many of these hosted apis or even running your own open source model very quickly and so having a sense of how much money you're spending what's actually happening under the hood and whether you're kind of staying in like I said this list was not meant to be comprehensive there's probably a ton of other things that are supporting pieces of functionality infrastructure that's required to make these applications real but these are some of the things that Jeremy and I have been thinking about and have heard from the folks that we've been talking to in this space and so that's really influenced our our thinking and looking ahead we're really focused on what's required to enable that next generation of Apple we only have a couple minutes left so we'll leave you with a couple hypotheses that we've been forming that have influenced how we're approaching this the first one which is probably uncontroversial to everyone here which is that the pace of innovation um is incredible at every layer of the stack the models themselves the supporting infrastructure like the databases the python libraries the number of incredibly smart people who are focused on every one of these problems is is a little mind-boggling and there's going to constantly be new stuff coming out at every layer of this as a result of that The Cutting Edge technology here is being adopted faster than ever by by top teams Joey mentioned this earlier about how the gap between what's happening in research and in Academia is smaller than ever um and that's that plays out when you see that people are taking models like vikunya that are just a couple months old um or even when they were a couple days old and really starting to experiment and build applications and combining both of those the last hypothesis is that teams are going to need the infrastructure all of these surrounding components these boxes in blue they're going to pick the tools that allow them to move fast and keep up with the with the times we don't really have the space for opinionated infrastructure anymore you can't really be uh in a place where you can say hey I support this tool but not that tool there's going to be so much going on in the space that we're going to need to keep up both from an infrastructure and from an application so the goal here is really to start a conversation around uh some of these hypotheses and what this infrastructure looks like if you are um interested in the applications here or you're building some yourselves we'd absolutely love to to connect please reach out at any of the um any of the communication media here and thank you all for for taking the time to listen I feel like because it's not an in-person conference conference the mass of them we haven't killed just as much thank you um we had one question actually Jay thanks so much and also Jay um you should message me after because we'll send you some swag um from Jay he asked how to reduce latency associated with any hosted models and can you suggest a strategy to increase context length of llms yeah uh I can talk a lot about that so we've been a lot of our research and prediction serving has been around how to reduce latency um it's often an interesting trade-off uh we've been looking at how to play with different memory bandwidth tricks uh to improve throughput allow for better batching often uh it doesn't actually increase latency but it gets better Hardware utilization oh sorry it doesn't decrease late and see if it gets better harder Equalization um we've been looking at a lot of work in quantization to get better memory bandwidth which could translate to reductions in latency um the on the topic of context uh there's a really cool new research from again colleagues at Stanford and other places about how to deal with these larger contacts um it's still pretty new and it's unclear if it's going to translate to you know models that could be used in practice um what I've seen as a better solution for people thinking about context is to really focus on retrieval methods and being more intelligent about what context you provide in part because models get distracted uh so you stuff too much in the context that's often actually a bad sign that's awesome and maybe we can do one more um from Aaron Hoffman do you see any other languages than python becoming relevant in the near future it's a great question uh and I think the short answer is yes absolutely uh one of the cool things about um about just the explosion of interest in olms is that I think it's attracted in much broader audience of people than were previously interested in well you know the the old world of machine learning and you know if you just kind of Google around or go on GitHub you'll see the people who've built the equivalents of things like link chain in tool and languages like go or rust or even you know Lang chain itself I think has JavaScript typescript bindings and I think we're going to see a lot of the stuff is going to really expand in the scope and we're going to start with python because machine learning has historically been done in Python at least for the last 10 years but I think absolutely you're going to want to see findings in other languages and really allowing people to kind of more deeply connect llms back to the application Stacks that they're awesome thank you guys so much well definitely um head over to the chat I think people might have some other questions but unfortunately we are gonna have to kick you out for the next folks on the agenda thank you so much for having us so um foreign