where is he at where are you at there he is what's up hello how's it going it is going good thank you guys for for having me I am a little bit sick you might be able to hear it in my voice but I am super pumped for uh that is dedication he could not let us [Music] dedicate it no that's right it's right not today that's awesome well dude we are David and I are going to jump off we'll be back with questions in 20-25 minutes so talk to you soon sounds good um will you be putting my slides okay there there it is cool all right hello hello everyone um my name is mandari I am um a co-founder and the architect at the Microsoft deep speed team um many of you might already know what deep speed is many of you might have for train your models um using deep speed um today I'll give um a brief overview of deep speed and walk you through some of uh um our insights along the way as we developed uh deep speed uh just some some some fun stories along the the way um and also tell you about the key features of deep speed so at a high level deep speed is a a high yeah [Laughter] so we're improvising like a jazz band basically uh do you mind resizing the window that you're presenting with for the slides because it's a little bit vertical right now and there we go boom all right all right all right cool um so deep speed is a library for training compression and in uh inference of deep speed um we started by primarily focusing on model scale and speed so let me tell you a little bit about what that means um we all know large language models have been blowing up in the last two years the size of the Transformer models that were trained increased from a few hundred million parameters to hundreds of billions of parameters about a 240 times increase in model size during the same time if you look at the AI Hardware that these models are trained on the memory capacity of these Hardware only increased by two and a half times so how did we enable this massive increase in model size of these large language models and that boils down to system capabilities we build systems that allowed us to scale the model from us single device to hundreds and thousands of devices and if you look at the trajectory of deep speed in enabling large models we enabled a 4000 increase in model size in the same two years and where the Transformer or the large language model size increased by 240x because of that um deep speed has been used to train a lot of large language models that you see including the Microsoft touring 17 building the bloom um big science model Megatron touring 530b model and and so many more using some of the large model training technologies that I will share with you in in just a moment now in addition to model scale deep speed is super fast at training these large models if we go back to the days where vert was considered a large model deep speed at the world fastest bird training time where we were able to train the bird in 44 minutes on a cluster of 1024 Nvidia V100 gpus uh fast forward now we're able to train trillion parameter models on on large GPU clusters we can achieve near perfect linear scalability what that means is as we increase the number of CPUs your throughput or your training speed increases proportionally there is very little efficiency loss and we are able to do that on both older generation of Hardware like the Nvidia V100 gpus or the newer generation of Hardware like the the Nvidia a100 gpus all the way to thousands of gpus one of the challenges of large language model training is it's generally inaccessible because of just the size of the model and amount of resources required or at least that used to be the case in the in the early days if you look at the largest model you can train on a single device something that a lot of data scientists have access to you are limited to a couple of building parameters you cannot fine tune something like llama 65 billion um so in order to make it more accessible we develop Technologies like zero Infinity which allows you to leverage heterogeneous memory things like CPU memory nvme to increase the model size that you can train and by using this you're able to fine-tune models like lava 65b or even a trillion parameter model with a single GPU if you have enough nvme device and that makes large language models a lot more accessible than if you had if you needed a massive cluster to be able to do the same thing we also develop technologies that reduces communication between devices when training large models this is important because if the communication is slow most of the time you're just spending doing the communication rather than any useful work and that means you can only train these large language models one very powerful very expensive super Computing clusters by reducing the communication volume you make it more accessible where you can train it with much less bandwidth um so the model scale speed and democratization are kind of the three main emphasis points for for deep speed so let me take you guys a little bit deeper into uh how some of these features were were developed uh some of the uh the moments of insights where we were like yeah oh this kind of works and we built around that uh so let me just take you back a little bit and share some of that so early 2019 um a couple of months before um bird had come out and we were trying to train bird at Microsoft um at that time um we already knew about distributed data parallel training and Nvidia Apex provided the infrastructure to do this we didn't have deep speed back then uh we had access to 64 V100 gpus pretty decent gpus at the time but the problem was that this was a cloud cluster the network on this machine was really really slow four gigabits per second ethernet to give you a reference on today's super Computing clusters where a lot of these large language models are trained that bandwidth is about 1600 gigabits per second so roughly 400 times slower and the memory wasn't too too big either so you could do a batch size of maybe four per GPU and when we're training this model it was incredibly slow training it on 64 gpus was slower than training it on a single GPU and we were just scratching our head what is going on turns out we were spending all of our time doing the communication that I referred to earlier here's a here's a here's a here's a simple Loop of what uh that captures essentially what Apex was doing we were using what is called gradient accumulation to increase the batch size where you run with a small batch size do a forward backward and you keep accumulating over the gradients and then right before you accumulate you do this gradient averaging across all the gpus that you have all 64 gpus every step after every forward and backward and that was just killing time we're just spending all our time just average ingredients average ingredients we were not making any profits but once we saw this we realized we could actually just move out the averaging part outside of the greeting accumulation you let the forward and backward run for a couple of steps just keep averaging on the local gradients on the same GPU and once you're ready to update your model that's when you can do the averaging across all the gpus it reduced the communication Time by 16 x we were starting to see some scalability now the 64 GPU performance was no longer slower than a single GPU performance and with that we were able to train bird in in eight days no longer a super Speedy bird but we managed to train it in a in a reasonable time frame and that's where deep scale was was born we used this to train a few early models inside Microsoft um and we were we were quite quite excited to get into this distributed training landscape then a couple of months later new model started coming out for example open AI released a 1.5 billion parameter gpt2 model Megatron released 8.3 billion parameter Megatron model and we were wondering what can we do next can we train a larger more powerful model and what would it take to to do something like that the issue was that at that time there were two forms of parallelism technologies that were used to train the model data parallelism where you replicate all of your data across all of your gpus and you're limited by the single GPU memory you can again scale to maybe a couple of billion parameters and then model parallelism which is how the Megatron models were trained where you partition the model and you communicate all your activations throughout the the training and it anchors super high communication volume the only way you can get around it is by having very fast communication which was present inside a single node uh what you call a dgx2 node from Nvidia but as soon as you go outside the bandwidth wasn't there and and you can't really scale it further so you were limited by the capacity of a single node which was roughly about a 20 billion parameters if you didn't care too much about running it efficiently but we soon realized that the way data parallelism was trained we were replicating everything but we could actually not replicate everything we could just have each GPU hold just a part of the model and we can communicate them as they are needed and if you did that you significantly reduced the amount of memory required but you didn't really incur a much larger communication overhead not nothing compared to the model parallelism the tensor slicing that was done within the node so that's where the the whole zero line of idea was born zero redundancy Optimizer we don't redundantly store any Optimizer model States across gpus so with that um it opened up the possibility to train models with hundreds of billions or even trillions of parameters without losing compute efficiency and it was really easy to use and so using xero and the the model parallelism we at that time trained Megatron Turing 17 billion it was the largest language model um at the time and we were really excited about this zero technology we wanted to bring this out to the community and see what the community did with it um so we wanted to open source deep scale but uh interesting story deep scale was already taken as a name so after brainstorming for for quite a bit um we decided to change the name from deep scale to deep speed and we open sourced zero and deep speed in 2020 and since then zero has been used as part of deep speed library to train several large language models it has been adopted to several Frameworks including pipe torch itself and we were at the time um a little bit upset that we had to change the name but a couple of uh weeks down the line one of my colleagues daughter um she noticed that deep speed is the same when you spell it forward or backward and that was kind of cool and we all started to enjoy the name a little bit more over time and and right now um I feel like it captures the essence of what we do uh a lot better than we can deep scale so so we actually really like the name and it's about all right so then we were starting to think what is next around June 2020 open AI released 175 billion parameter model and uh we started to wonder um what would it take to train a trolling parameter model we knew there were two possible paths one is through zero and zero has uh several different stages won't go into the details there but we didn't have the full implementation of zero at that point but the other line of work that we had started to look into is 3D practicalism excuse me um 3D parallelism combines three different forms of parallelism pipeline model and uh zero and the key here is the pipeline parallelism pipeline parallelism incurs very very little communication overhead so by using pipeline parallelism to scale across nodes you can scale to really large models and it is really really efficient as well the only kicker is that it is very complicated to both develop and use but since we were planning to try to train a trillion parameter model at a very large scale it made it worth it to try to develop a system where we are trying to get the best efficiency out of the system so um after a couple of months of work we had our 3D parallelism ready um and we wanted to try it out and luckily we had I remember this one weekend where um one of one of the large GPU clusters at this time we already started getting super Computing clusters inside Microsoft and one of these clusters was going through an upgrade um and we had an agreement that after the upgrade they would lend us the entire GPU clusters with over a thousand gpus for about two days to test scalability so we had to if we were going to test a trillion parameter this was our window and we had to make it work um long story short we managed to doing this weekend um schedule a job with over a trillion parameters we submitted it we waited for 30 minutes and we got the first iteration time meaning it was running and during that weekend we were able to produce this graph which shows how the 3D parallelism technology can scale to a trillion parameter model was perfectly near scalability on 800 V100 gpus so with this technology we collaborated with Nvidia and we used it to train Megatron during 530 building parameter models this was done in about two months using 2 000 a100 gpus and then we use the same technology to also collaborate with the community to train boom 176 billion parameter model okay so it wasn't quite a trillion parameters but we know we could train trillion and we were training uh models that were half a trillion parameters so that was quite exciting but then the question is what next can we continue scaling the the size of the model even further well the problem is that the Megatron touring 530b it took two months to train on 2000 gpus and we only trained it on 300 under 300 billion tokens from today's standards that's heavily under trained today the Lama 65 billion a model that's almost 10 times smaller is trained with chip with with over 1.2 trillion tokens and even the MPT 7 billion is trained with about a trillion tokens if you use the same kind of scaling laws laughs and try to train a 500 billion or a trillion parameter model using a trillion token it would take six months to a year on 2000 gpus and if we did the scaling properly and trained it with 10 trillion tokens which is probably about the number of tokens that would actually be needed to train this model to its full potential it will take 10 years it is no longer feasible to train these massive dense models so what do we do then we can actually now scale with sparsity using mixture of expert models what does that mean an analogy that that is that helps me think through this is when you're going to a hospital for a diagnosis or to a clinic you don't go see every single doctor and combine what they had to say to get your diagnosis you go to an expert based on your symptoms make sure if expert is kind of similar to that you might have a really large model but you will only use a subset of the parameters in the model based on the the input token that is going through the model and so here's a plot that shows um a 1.3 billion parameter dense model um loss curve during the training compared to a 1.3 billion with 128 experts and then we compare that with the 6.7 billion parameter against model excuse me [Music] here we're seeing that the 1.3 billion with 128 experts has a similar um accuracy as a 6.7 billion parameter dense model now if you were to compare the throughput the 6.7 billion intense model is roughly five times slower than the expert model and if you look at the total number of parameters on the expert model it's actually 52 billion parameters because you have all those experts so now you can scale to a really large model but also run it fairly cheaply so in order to enable this we released deep speed Moe for training which allows you to train multi-training parameter Moe models with excellent training efficiency and the cost of training a much smaller dense model so so far we've been talking about just pushing the um pushing the size of the the models right but as we start increasing the size of the models you would need larger and larger GPU clusters to fit the the model so for example if we had to fine tune a 175 billion parameter gpt3 model um that's going to take 256 V100 gpus it is a large GPU cluster that very few people have access to but if we actually look at the Modern uh Hardware the limitation is because we're trying to fit the entire model in GPU memory but if you look at the CPU and nvme storage there's actually a lot more the GPU memory is less than 50 times the overall storage in a system and so if we could Leverage The GPU CPU and nvme memory you could easily fit a trillion parameter model for fine tuning on a single gtx2 node and that would make large models a lot more accessible the catch here is that those forms of memories are much slower so ndme for example is 60 times slower than the GPU memory and even worse in order to use these forms of memory you would need to bring the data from these slower memories to GPU memory through the pcie link and that becomes your bottling and that link is over 100 times slower than than GPU memory so what do we do so I actually remember when we had the aha moment on on how we can fix this um this was back in in in 2020 um I was uh we had we had rented a cabin with a bunch of friends uh to celebrate my wife's uh birthday and I remember I couldn't sleep because I couldn't stop this thought of data moving back and forth between nvme and ntp memory and there was this moment where I realized well the ndme and pcie links are really slow by itself but if you could partition each parameters across all your gpus and try to bring the data in from the slow memory you actually have lots of these running in parallel so you can get the aggregate bandwidth across all of these and so we uh we can now increase the bandwidth from something like 16 gigabytes or 30 gigabytes per second to hundreds of gigabytes per second and um this is this was the the fundamental idea behind zero Infinity uh we build upon it um and we release serial Infinity um as part of deep speed and with it we can fine-tune models with hundreds of billions of parameters on a single GPU Illuminating the barrier to entry for a lot of data scientists for uh fine-tuning these large models all right so we mostly talked about training but there are a few other things that we can do with uh deep speed for example we can do compressed training it allows you to train with really long sequence length using sparse attentions or we have mechanisms like the progressive layer Dropout where instead of training through every single Transformer layer of the model you can selectively choose just kind of like the export of tokens uh expert mixture of experts using your token and a gating function to figure out which layers you want to go through and which you don't and that can help you speed up your training and of course inference once you've trained these storage models you want to be able to inference them effectively at a low cost and productionize them the challenge here is that the large model landscape is quite diverse you have models that range from a few hundred million parameters to hundreds of billions of parameters you have dense models you have sparse models and different scenarios have different requirements for inference you might be doing a throughput-based scenario where you just care about cost you don't care about latency it's all offline and you want to maximize your throughput or you might be doing a scenario where you're facing customers where your latency is super critical and you want to minimize latency so at Deep speed we developed a systematic composition of different optimizations such that you can get the best latency and throughput depending on the scenario across this entire model stack however the challenge is that there are lots of optimizations that goes into speeding up large language models and from a data scientist perspective it's not always clear what you want to do even though some of the tools to do it might already be available for example deep speed inference so in order to address that we developed a new library called deepspeed mi2 Deep speed model implementations for inference where we've taken the popular open source model and applied deep speed inference optimizations to them so that you can simply just with the click of a button run this model with the best latency and throughput so here is a small code snippet on how you can deploy a model using deep speed mi2 you can pick your hugging face model that you want to deploy you should say mi2.deploy specify a bunch of things and that's it at the back end uh deep speed will apply all its optimizations and you can get a handle on which you can submit your queries and you can get really fast latencies and throughput deep speed mi2 supports over 24 000 different models that are open sourced at the time when we released it it reduced the cost of inferencing Boom 176 billion model by up to 40x and and it can also create models like stable diffusion hello am I running late really good sorry it was my internet and I wanted to make sure I was still working keep going man my bad foreign okay no no no um something else that is tied to deep speed inference that we are really excited about that we release pretty recently is deep speed chat I'm sorry guys um my my throat is acting up a little bit um so deep speed tat is kind of this this interesting project because training a chat model requires rlsf training um the and the RLS training pipeline has both training components and influence components in it and it is equally important to accelerate both training and inference and this was the first time where we used all the techniques that we've developed on the training side combined it with all the techniques we've developed on the imprint side to accelerate rlsf training um and it allows you to train really large models for for chat really fast if that's what you want or really cheaply if you don't care about the latency and all you care about is is is cost Okay so we've kind of covered uh several features inside deep speed several deep speed offerings like the Deep speed mi2 and the Deep speed set however um all of these features do not mean much if they are very hard to to use and so at Deep speed we greatly care about usability um the zero line of Technology makes it easy to scale your model with virtually no code change you can go from 1.4 billion parameter to trillions of parameters by using deep Speed without any change to your code and because of the of how easy it is to use deep speech has also been integrated into hugging phase and pytors and you can just enable the Deep speed backhand by just changing your configs or using a different launcher without having to change anything in in your code deep speed is also infrastructure agnostic it supports agile ml Azure VMS or if you want to bring your own hardware and run it there it works there as well um let me kind of share um how you would apply deep speed uh to your to your models so normally in pytors you would create a model all you have to do is wrap your model inside the Deep speed engine and you're ready to go you specify a deep speed configuration file where you specify all different kind of optimizations that you want for for deep speed to enable and that is it um and we are very grateful and we are we're very excited that the community uh has been growing and we've seen a massive adoption of deep speed if you're using a large language model there's a good chance that it was either trained with deep speed or it was trained using one of the technologies that was developed by Deep speed we had over 4 million plus installs since we released it we have tons of unique contributors and a lot of third parties depend on on on deep speed for their uh for training their workload and inference so it's it's been a great journey and I am super grateful uh to to be part of the of deep speed and the Deep speed open source community and that is all I have for you guys um we welcome contributions and if you enjoy this talk please do like uh our repo um and thank you wow that was a great presentation thank you so much samyam you really had a a great pace so I I appreciate you coming here even though you were sick uh you did a great job uh there are some questions uh but before we get into the questions that were there I had one question that as I was thinking about a lot of this stuff deep speed has had a lot of impact in the industry so much to the point that you see Zero or zero like uh optimizations being implemented in other optimization libraries can you just tell us a little bit about how zero differs from um fully sharded data parallelism in pi torch or maybe the implementation in colossal uh just anything that you can say about that uh yeah first it's to to me it's it's really exciting to see something that was developed at Deep speed now being adopted at uh at different places at the core of the fundamental technology is is that it's the same you you partition your parameters of the United States gradients and then you do the the collectives there um there are there are different there are differences related to usability um I think last time I checked on on pytorch side the fully distributed data parallelism it is not for the automatic um you have to apply that um some somewhat recursively um I understood okay there I don't know if it has changed over over time we've also seen uh zero being adopted at um at Colossal AI um initially since since deep speed is is an open source uh most of the implementation came from Deep speed but over time I feel like they've added their um enhancements to it um one of the things that I I recall um not for zero but for zero offload and infinity they have this thing called Gemini which is a a sports yeah memory management um to allow for for partial uh offloading which helps in some Corner cases um as well um but I think fundamentally all of all of these are are the same um zero technology um I might be biased but so far I I haven't seen an implementation which uh outperforms the one that we have in in deep speed um but it's still exciting to to see the the adoption and a little competition is great yeah yeah I think it's healthy to push the balance of the space um and one thing I also just want to say before against the questions is I really loved how you you thought about some of these things and came up with insights and it like progressively built on top of each other I think that's really awesome and it gets encouraging for other people uh some of these Innovations don't come out of the blue right like it comes through Progressive iterations on some of these things so I really appreciate that there are some questions I'd like to get to people are very uh happy with this talk um okay so yeah I'm so sorry I'm very bad at this I see a few different ones I see someone asking a question about uh dealing with uh failures so is there any cape abilities like or any fault tolerance baked into the deep speed training mechanism or is that something you have to do on your own um we've we've we've talked about um elasticity and and fault tolerance um and it's been been featured on our our wish list but it's not something that is uh fully baked into deep sky at the moment however it is definitely a pain Point especially when you're uh scaling to tens hundreds or thousands of of CPUs it is something that we we incur regularly especially with like node failures and yeah yeah yeah BCC errors it's hard to tell which piece is is broken and so on yeah so one suggestion I have though um and that we use a lot is just a simple all reduced test before you start the training you would just do an all-reduced test to ensure that we're getting the the bandwidth that we we expect experience yeah something is wrong with your cluster and then you start doing a binary search on which part of the cluster it's something that is definitely economy that can be automated okay that's good to know but I I remember we were talking before you were telling me about what it was like being on call as you were developing some of these really large models and how painful it was right to have to wake up in the middle of the night and be like oh my God there's this failure and the I guess the pressure behind it because of how expensive these resources are like one thing I was really uh I was thinking about is how expensive all this research is because you know it's it's very hard to you know do some of these experiments um also making it very hard to you know you want to make sure what you're doing works or that you expect it to work because if not you're just wasting time and money so yeah it was really great um sorry I want to keep keep it moving make sure I get to other people's questions uh someone asked something if you could shed some light on speeding up the inference of an llm like let's say the 7 billion or 40 billion Falcon without considering quantization so maybe that's a deep speed M2 question ah sure um so it how you accelerate it really depends on whether you're trying to push for latency or throughput if you're trying to push for latency then then you are usually running at a very small batch size um and at that point you're limited in your performance by how fast you can read the parameters from the GPU memory so all the optimization centers around getting the best possible memory uh bandwidth saturating all of your SMS so that you're pulling from the HPM memory as quickly as as possible so that's at a high level what we will do if you're trying to maximize for throughput then you would try to use a really large batch size and get the compute course to use as efficiently as possible keep it compute round love that thank you so much Dimitri sir go ahead no man I'm just coming on here because sadly we got to keep it moving I told you all my name is Ringo today I'm keeping time like a clock just keeping us cruising throughout the day some you know this has been awesome I really appreciate you coming on here I know it's no small feat to get a talk pushed through the PR department at these gigantic companies so I think you probably worked harder on that than you did on the actual Talk itself which we all appreciate there's a ton of people that said how excited they were for your talk and that's awesome you did not disappoint man you did not at all you came sick and everything and we all appreciate that so much yeah thank you so much thanks for having me thank you yeah [Music]