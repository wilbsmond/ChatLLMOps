let's get the real deal on stage and talk with somebody that's actually doing this for real and where is he I'm gonna bring matte to the stage paging mate hey there is how's it going dude it is great to have you here man I am so excited this is like something that of course it is it's always a pleasure talking to you and as I mentioned when you came on the podcast uh for I think the ml world and the mlops world you're like the DJ Khaled of our industry because you just keep dropping hits and hits I mean before let's just give it in case for like the two people that do not know you you created spark you were co-creators of spark you also had your hand in ml flow that everyone knows and loves and is starting to play around ml flow now supports llms which maybe you're going to talk about maybe not but we should talk about that later and then last since we talked last man you dropped Dolly so I mean wow uh and and then you've got your name over all over all kinds of cool papers Frugal GPT if anyone has not read that paper that is incredible I really love that you're still keeping your roots in the Academia and putting out papers and so man last thing I want to announce to everyone before we get started and I'm gonna let you take over and drop some wisdom on us in two weeks on June 26th in San Francisco you and I we're gonna meet face to face we're gonna be in person at the llm Avalanche Meetup that we are both doing and that is uh it's like the sound check or it's gearing up for the data and AI Summit that you all are putting on and so I'm super excited for that if anyone is in San Francisco before June 26th let us know I'm gonna drop a link to that in the chat and I'm gonna hand it over to you matte man it's great to have you here excited to be here uh let me share oh now the fun part starts no no this requires this requires AGI to figure out this is uh this is not your regular Zoom call man yeah all right okay you see my slides I do I see it yep okay perfect all right so yeah so I'm going to um you know talk about this emerging ups and some of the things uh that I'm seeing um in industry and databricks you know both building are internal a lot of them applications which we have you know quite a few of and um uh you know helping uh you know our customers with them um and I'll also talk about some research uh that I've been doing at Stanford um uh even kind of before Chad GPD became really popular on using llms more um reliably that is also I think relevant to this field so it will be sort of a tour of a bunch of things you know not I'm also going to list a bunch of stuff that like you know I'm not working on at all but then I think it's cool um and hopefully it gives you you know a little bit of a perspective of how to approach this problem okay so large language models are super amazing they can you know generate text as very fluent often like very grounded in reality you can do a lot of cool applications with them all the way from uh the traditional um you know uh few shots uh classification type of stuff to new things like chat Bots or you can ask questions um you know they know about all kinds of like research research and stuff like that uh take it even do programming so very cool but if you look at these answers a little bit more closely um there are actually you know a bunch of problems so for example this first one we asked John GPD when was Stanford University founded and it says 1891 but that's actually wrong that's just when the first class came in but the University was founded earlier um called bear uh this is a model that my research group developed so we were super excited that called that you know Chad GPD like knew something about it but it says it was developed by Salesforce research which it wasn't it was developed by us at Stanford and finally this code here you can't see you know I'm it's not showing all the code but it's saying to parallelize this code you should use the edible executor but the problem with the code was actually that it's some kind of Serial algorithm so you you need to change the algorithm so you just you can't just throw in third pull executor so uh there are definitely challenges with the quality of uh responses in llms especially if you want the production grade app and on top of that there are also other operational challenges with cost performance piracy and just the need to update your model and your application uh so here are like some examples of things that can go wrong um you know of course these models are expensive to run early on you know they used to go down some of the time because of the load um another interesting issue that people have been talking about is Farm drift which is the models are changing um you know over time as they're being trained and they might get worse at like some tasks they could do before now now they're worse at it and then there are also issues with timeliness and with privacy that affect of your lapse so even today if you ask you know all these open AI models about um you know events past 2021 they don't know so and obviously it's expensive to update them otherwise they would have updated them by now um and if you have an application with privacy you know with the emerging sort of rules around privacy you might also run into issues there where a user wants to remove their data so for example chat GPD seems to know something about where Bill Gates lives but if Bill Gates was a European or California and he could ask them to remove that data about him and then it would have to forget this information so the question is how can we make llm apps reliable uh this is this area that people are calling llam Ops and I'm going to talk about like three uh you know things in in this area uh the first one is how do we extend what we currently do in ml apps to work well for a generative AI um and I'll talk about what we're doing in ml flow specifically based on you know our experience the second is new programming models for these kind of applications which is one of the things I've been doing at Stanford and then I'll talk a little bit about other emerging tools too okay so let's start with uh with the basics so I really think that for you know any anything you're doing with language models even though it's a you know really cool new kind of application if you want it to work very reliably you want a way to improve it you need to start with the basics and these are the same as in ml apps uh the way it normally was so one of the the most basic things you need is a way to swap and compare different models uh or even pipelines or models because a lot of this is done using shading now um for an application and just say how are they doing you know at different stages how do they compare on different metrics so you should set up your development you know infrastructure to support uh this simple kind of comparison task my second thing you need is to track and evaluate outputs you know and and have this historical database of what happens so that you can kind of search um you need a way to deploy pipelines um reliably um and and see what's happening and you need a way to monitor these deployed applications and analyze the data once they're actually out to see what's happening with them over time so we're going you know we're basically implementing all of these in uh the ml flow open source project and then the part that ties with data will also integrating into just uh databricks as a platform if you you know if you want to have um kind of a consistent platform across data engineering and ml engineering we're designing it so it's very easy to do both of these things so let's start with the first one stopping and comparing language models so in ml flow if you you know if you if you're not super familiar with that one of the key Concepts there is this model abstraction that can wrap a model so the rest of your code doesn't need to know how it's implemented you don't need to know you know what framework is it using is it a local model or something I'm calling over the network like open AI um is that you know a single thing or is it a pipeline actually of multiple steps uh you just basically have this function for example you know a function from string to string could be a call to open AI or a local model or some kind of Pipeline and we've um created uh Integrations with a lot of the popular llm tools um uh to let you manage them as models in there and the same thing with pipeline specifically we've been focusing on Lang J for that so you can easily wrap all these things into the these into these model objects and then in your application you can swap them in your evaluation code you can compare different ones um you know you could imagine like routing some percent of traffic to one versus the other and so on um so then you can just call these General functions on each one so that's very basic but important and we're seeing this come up a lot as people want to try you know different uh you know different options in this space um a second thing that we're doing for the experiment tracking is extending the what we call Auto logging in ml flow so in ml flow the idea is you know you develop your machine learning code and we have some lightweight calls you can make that cause ml flow to record what's happening rapid into these kind of um generic formats like the model I talked about and make it easy to evaluate and compare um and track results over time so one of these is you know you can just do ml flow.star to run and then you can you can train models and log them and so on uh so we've we've extended this Auto logging for open AI so you can you can you know remember basically you're pumped and other parameters you used in open Ai and wrap that up and call it later with with model.predict um and we've also extended it with Lang change so you can set up a chain with local models or remote models and save it and again uh you know load it later and call stuff over about it and once you've run these the the other thing we've done is in the UI we've created uh you know a new uh view in the UI that works well for evaluating uh long form like generative outputs like these texts from these models so basically you can run all these models on you know some evaluation data sets some questions or whatever some input text and then you can see for each one side by side you can see what the age uh producing um and um we're also going to extend this so you can actually add ratings like right here or import ratings from a third-party labeling service so you can compare all of them and of course you once you've saved these models you can also evaluate them programmatically we have this function ml flow that evaluate with a lot of common metrics people want to run and we've put in a lot of the metrics you want for llm applications like the huge metrics for uh summarization and various you know various other ones some metrics some toxicity various things like that you can also easily extend this to have your own so that's uh you know that's basically like working with the models themselves um and then of course you also want to deploy the models and check what's happening after and um the way ml flow works you can deploy a model to a variety of different serving layers we want to make it portable across them so you can pick you know your favorite one here and once deployed you can get these predictions out of it and this is like one of the areas where on database specifically we've integrated this very closely with what you do for data engineering like basically as your model runs you just get a table that's automatically created and you can run alerts on the table you can basically have a SQL statement and say you know alert me if whatever the SQL statement ever returns you know to or something like that um so um so it's easy to do that but you know you could use whatever deployment tool you want with this stuff um just a little bit more on some of the things we're doing with this at databricks we um we've also released um uh some reference applications like for example how to do a customer support bot that includes a vector database uh language model and the ability to say you know it doesn't know about certain topics and so on so some kind of filtering um and we've also released this um free course on edx um on llm's application to production that covers you know what we've been seeing at different stages of this workflow not just llms themselves but all the peripheral things like monitoring uh searching you know Vector databases chains and stuff like that so that's kind of the basics of ml apps and uh they will definitely help you like get far more demo which is usually easy to make with uh with these llm tools to something that like Works reliably you know 80 90 hopefully eventually 99 of the time or more um but it's still you know it's still a lot of work to make these these happen so I also wanted to talk a little bit about what the uh future of programming with these uh models could look like and this is one of the projects that my my grad students have been doing at Stanford it's called DSP or demonstrate search predict um so today if you look at not just LMS but Foundation models in general these pre-trained models that on large data sets there are so many different tools and techniques you can use you know just for language models there are different ways to use them there's instruction prompting Chain of Thought agents um fine tuning and so on um uh and in addition to language models there are different to retrieval models you know your favorite Vector index and and database that specific models and maybe models for other things as well like vision and speech that you want to combine and most people want to build these into pipelines so we're seeing that um you know you've seen it if you've used Lang chain or you probably it's just a very natural way for you to work with these so the idea is you break down your problems in and delegate smaller subtests to the models and I'm showing a couple pipelines from the research literature on the right that are you know that kind of did this by by hand um so in principle it sounds great that you can just make a pipeline but in practice it's actually pretty hard to connect all these pieces and then to optimize the whole pipeline you know if you want to get from like whatever default accuracy it has as you hook everything up to increase your accuracy and uh you know say eliminate like the the you know some type of bad output it's producing or something like that you've just got all these models sitting there maybe with bombs or with like some python code to glue them together and it's quite hard to you know to fix that um and every time every change you make to the pipeline affects everything in it all the other models and for example if you want to um if you were training one of the models or fine-tuning it to deal with like whatever input format you've got to give it from above you'll have to heat drain it if you change what you're doing above so we want developers to be able to step away from this and just think about their system design and components not about gluing and optimizing all these stages together so that's what we're trying to do in this uh in this programming model DSP or demonstrate search predict um so DSP is a is a declarative uh programming model for these pipelines uh basically think about a little bit like pytorch how invite which you can hook together layers that are different uh you know operations um and um and fast you know data in the form of tensors between them in in DSP the layers are actually different Foundation models like language models or retrievers or potentially other things you know a calculator maybe some some other you know string to string function and then uh the stuff you're passing between them is uh text objects it's not tensors um so there are three types of uh functions in DSP um or parameters you can use on your program there's demonstrate which is how you tell it what to do you know this can be basically providing some trading data or other ways to specify constraints uh their search which is breaking down problems and looking up useful information this is worth something like a vector database would come in and then there's predict which is different strategies for using the information you pulled out checking the quality you know asking is this really the right answer and so on and you set up your program uh but uh once you set up this different steps in it you delegated a DSP on time to figure out how to implement each component and it sees the whole Pipeline and it tries all these techniques ahead on the previous slide you know the fuel shot bomb thing Chain of Thought different ways of selecting data for each step to automatically tune the whole pipeline so you don't have to do that yourself so let's just show a little bit of an example here this is from this research project we did called baleen which is a system that can answer complicated questions using multiple searches over text documents so for example over Wikipedia so one of the one example of the the kind of question you could ask was uh when was the creator of Hadoop given an award uh so Hadoop is a piece of software that was designed by Doug cutting and then Doug cutting got this O'Reilly award in 2015. But to answer this question if I gave you this question as a person you'd have to do a bunch of research you know a bunch of searches over Wikipedia or whatever uh uh to figure this all out so this is how you hide that in DSP and hopefully you see it's very simple um so basically you have this example the example is what holds our state you can have different fields on it and you can say hey um for you can you can have a maximum number of searches or hops it's doing and what you can do is you can say hey first generate a question from this example like some question some query that I want to run on my search engine uh for example who was the creator of Hadoop and then search for that and and take the top three examples and stick them in this array called passages and next generate a summary of everything I've read so far in light of the question and append that to the context so that's the strategy just keep doing these searches and finally once you've done this many hops um generally don't answer so it seems a little bit crazy by the way these things the question to answer and so on think of them a little bit like bombs so it seems a little bit um crazy that this would just work out of the box but if you set up the prompts it can work sort of okay not all the time but a bunch of the time um just an aside on how to set this up uh we actually make the bonds uh somewhat declarative so basically you just say you know for each step I'm gonna have these inputs like a context and a question here there's a text description and then I'm looking for an answer which is a short sentence but the DSP framework can figure out how to actually watch the bombs to make and give you this and this is one of the cool things that lets you uh you know evolve your your program and optimize it over time so if you just ran this program uh DSP would um would generate a pump that looks like this uh you know as it's going along so let's imagine it did the searches correctly and it found some stuff it would have these passages here what is Hadoop you know um who was it created by whatever um and it would it uses these items like it says hey I'm looking for a short sentence so it says answer with a short sentence right um and it even automatically does Chain of Thought it says Hey fill in this stuff so this is kind of the pump that does and the python green is what the model fills in and you know in this case you get the right answer but this is again this is like few basically zero shot I pumped at everything but it probably won't work all the time let's say it only works like 30 of the time or something how do I make this program better so this is where you can feed DSP additional data and it will automatically tune the program to make it work so one of the things you can give it and this is this function called DSP compile you can give it a bunch of like labeled examples of a question like this and the final answer you don't even have to show it the search as it should do and DSP is going to like try running each of them multiple times and figure out if it can create a path like get a pass that actually you know basically do some searches and actually give you the right answer because it just knows the right answer for each one and for the ones where it does it will use them as few shot examples in your Bot so for example to to teach uh your model like let's say the final call here is to something like open AI to teach us how to use this kind of context and question and rationale um you know we gave it this one but maybe we have an example for from the training data where we found that we could answer something you know there was this question about which award did the first book of guys you could have received and uh you know we did these searches and we found some stuff so we automatically found an example that we can put in there or maybe multiple examples and we've just gone from Instructions alone to few shock bombing which works a lot better and the other thing that it can do here if you've got enough examples it can actually fine tune a supervised model for each part so instead of trying to do it with Chain of Thought and so on it you know you can train a model that's good at answering these kinds of like trivia questions or at generating the searches for them you know which which works in a similar way so that's the idea here we want to separate the um you know your specification of your pipeline from the optimization and to work on them separately so what this gives you is a great way to do iterative development uh it's super easy to get started and it does something out of the box with instructions which is probably what you would have done on yourself um it's modular because it unties the system design from farming and fine tuning you you can you know as as we update the DSP software those parts get better and you don't have to rewrite your whole program um and um it also enables optimization because as we make new releases of of the DSP compiler for example it benefits all the programs and another really cool thing is that it actually works really well in practice we've been evaluating this on a lot of um research desks especially these knowledge intensive tests that require looking up information and compared to other fuel shock methods you know what you don't tune on like millions of examples uh this is basically competitive with all of them and it's this very simple interface so in this plot it's so it's showing a couple of different data sets and all of these things here are different implementations with engines and you can see the scores here on different data sets and the score with DSP was the same number of examples I think it's like 10 or 20 examples um is better than all of them in this case because it automatically searches for how to optimize this um so this is this is the SB itself um I'll skip this one but this this another thing we can do is even feed an unlabeled examples and basically use those to train a cheap model that does the same thing or expensive model does and again basically uh even better performance after you deploy um so yeah so these are you know these are a couple of you know things but there's quite a bit more in the llm up space that you should look at um uh one cool thing is constrained generation uh there's this project lmql and there's also guard rails and Json former that Force the model to Output stuff in a specific syntax that really helps in some applications um dimitrios mentioned uh Frugal GPT which is uh something we've been developing that can combine multiple models and how to waste them or combine the results from several models to get better cost performance like the same you know better performance than gpd4 uh and even lower cost while achieving that performance um and then there are all kinds of ideas for reasoning post-processing filtering stuff that further improve quality and definitely if you want to learn more about these I do encourage you to check out uh the data AI Summit which is like free to view online and to check out the organizing as well cool all right so I can't hear you for some reason but let's see yeah that was on my side and okay there we go for uh anyone who came to the first one of these knows that I spent the first like 10 minutes talking on mute to myself oh no yeah and yeah having a blast and so I um I'm waiting for some questions to come through in the chat because there is a bit of a lag between when we talk right now yeah it makes sense ask questions but one has already come through and it's about the latency and it's asking uh do these searches take a lot of time from Joel Alexander yeah great question so so it depends a lot on what you use the search so in DSP we actually used um we used our own search index which is called called bear over Wikipedia and so the search is um is very fast it's like basically like maybe tens of milliseconds for each search um it's uh it's comparable to like the cost of calling the model um so it doesn't have to be it depends on your setting like if you called out to like Bing or Google search you know it would be slower that makes sense that makes sense and while we're waiting for more questions to come through the chat I just want to bring up that there is a whole Meetup going on in Amsterdam right now and we've got a live feed of them and so I'm just gonna let somebody jump on screen look at that so they don't know they're on live yet but now they're going to get it about 20 seconds later so we're going to take them off before they even realize that they're on they get to do that yeah oh man this is this is great so uh we've got chip coming up next but before I bring her on there is a question that's coming through so DSP is better than uh is DSP better than gpt4 is this uh a correct assumption so it so it depends on the task but for these the tasks we were doing which are these complicated like knowledge tests like this Wikipedia one it's definitely better yeah like out of the box gpd4 won't won't do that if you just ask it the question uh of course you can use gpd4 as the language model in DSP so you can kind of benefit from everything it's doing if you used a better language model you know the whole program will do better but the combining like search plus you know some strategy I didn't show it but it can also do things like uh you know take a majority vote across answers and stuff like that uh does do better than just the language model Ah that's awesome so the stream just hit the chat because the chat blew off now there's so many questions man I gotta actually like point to them and figure out what the best one is uh so are there any downsides to using DSP versus alternatives yeah I mean so definitely it depends what alternative but definitely this idea of breaking down your application into many steps will add cost and latency as you run each step so again in DSP the hope is that there you can optimize it or you know the most important thing is to get the right quality but um yeah you can it will add some cost um although so when we started DSP like no one was using chaining at all so everyone was worried like wait at some cost but now if you use something like Lang chain or agents it's very similar similar cost so you can think of DSP as a way to like take that programming model but try to figure out how to auto optimize stuff in the background all right I got one last one for you and then we're going to bring chip on uh and we'll all have a little chat before she gives her presentation question for mate how does DSP evaluate its optimizations does it do them before deployment or even during execution again yeah great question yeah the the stuff I showed the dsp.compile stuff is all before deployment so there's nothing like online um and you can give it you know you can give it some examples where you know the right answer um we're also working on other ways of doing it where you can give it just other metrics or things you know you tell it avoid this you know this this uh this situation um so it's all offline and then you deploy and you can collect more data pass that packet killer man so uh I knew it was gonna be great I did not realize it was going to be so good you do not disappoint Ed [Music]