next up is uh speaking of which we've I mean this is almost as interesting of a topic as the Gandalf game but we've got some Auto GPT in the house is silent around where you at and I'm going to find the microphone while you're while you're coming back on hey there he is what's up dude I'm good good how are you guys I'm really good so agents are all the rage right now and you sir are doing a lot with agents Auto GPT blew up and it is super cool to see where things are going I am excited for your talk right now about what uh where the agents where the pucks going with agents and some best practices and I'm going to let you kick it off and I'll be back in uh I think it is 10 minutes that no 30 minutes I'll be back in 20 minutes 25 to ask you some questions in the chat wonderful uh you guys can see my screen can yeah okay amazing all right very happy to be here um love the love the energy in the mlops community uh and let's talk about agents man agents are all the rage um you know I want to I want to talk a little bit today about what an agent actually is and how they came about um what led to this like recent um you know this recent agent Revolution uh why do they suck right now because let's be honest um they're not the best at the moment um and you know how do we how do we improve how do we get to commercial use cases um and uh what does the future look like so yeah let's start with what an agent is um I think a lot of you guys have a good idea but I'm GNA I'm going to take a bit of a technical approach here so um hopefully some of you guys are familiar with deep mind and familiar with this image here so uh if you mentioned agents two years ago to anyone in the AI field uh they would assume you were talking about reinforcement learning if you ask chbt what an agent is you know it's cut off as is at the end of 2021 um it will tell you about reinforcement learning it will not tell you about autog GPT it won't tell you about language models it'll say reinforcement learning agents right and how do reinforcement learning agents work uh it's very similar IL to to agents these days right you have um you have your environment I have this covering my slides there you go um yeah you have your uh environment and an agent in an environment right and the agent performs actions in the environment uh and then you know gets us the state of the environment meaning it does something and now it's like okay what changed in my environment and did it change in a way that I wanted it to change right and that's how you would train these models and so um you know the world state would come in like you know I picked up a block a block is now to my right um and then you know there's there the there's this giant inscrutable Matrix that no human can understand um and there's some action that comes out like grab or move to the right um or whatever else it may right but what if there could be a different processing engine right uh and I know when when chbt all of came out everyone here at least me uh speaking I was very astonished at at how far language models had come um and I know some people don't have an internal monologue but for those that do I'm sure one of your first thoughts was like hey isn't this just thoughts like how sentient is this thing um and we're not going to get into philosophy here we're not going to talk about how San it is but we do know that it's good at generating uh words and it's good at talking right uh and so you know gp4 came out uh and this is when the real Revolution took off this is when we saw um you know what's really possible it thought at levels that we didn't think were at least I didn't think was going to be possible for years uh and it unlocked things um that I didn't you know that that that that are extremely Advanced for for you know what we thought we were able to do uh four or five months ago um and now what you were able to do is you were able to talk to this giant and scrutable Matrix um and you were able to have some sort of output that you could understand it wasn't just a grab function right it wasn't just grab with the world State coming in it was you know hey can you do this for me then it said uh here is why you should do this here's what I think right and this is essentially a brain um you can think of it you know this this this brain is essentially doing processing this is an internal monologue that's happening that's asking for actions in the real world and then reacting to those actions so the same as a reinforcement learning agent uh but now we're using a language model for the brain instead of a um you know a giant incable Matrix um and you know we can zoom in on any of these arrows right this flow goes you have you have a input that comes in there's some processing that happens then the language model is like all right let's let's make an action happen um and then there's some reflection on the action and it either continues looping or it says okay like would you like me to confirm this purchase and so not only are you able to uh have you know see see that these see see what actions are being performed and why they're being performed you're also able to talk to this agent right and ask it and nudge it in different directions this is very valuable um and this led to uh you know a big revolution in in AI where we now have agents where if you connect everything together uh you have something something that loops and can perform actions so autog GPT blew up this was back in April um there was a few different videos there was this one where you know it could it could write its own code and then run that code um and it could like retrieve and write up-to-date information like you ask it to you know on the right here this demo is asking it to retrieve the information um of Auto GPT GitHub which you know chat GPT definitely does not have that information that would require a Google search or two and uh one of the questions that I get quite often is what can autog GPT do uh you know I've seen it make a chess game that you can play in the console uh you know it's created a PIP plan it's organized files for me it's gotten the revenue of a company from last year but the truth is um is that agents suck right now um they're they're not that great you know these applications only happen once in a while and with nudging you know the nice thing is is that you can talk to it but if you were to leave it up to its own devices often if you if you've played with auto gbt or another agent you see that it falls into um endless Loops or it gets confused or doesn't actually complete the task in the way that you want it to complete right which is one the beauty of being able to prompt it but two this reduces the commercial applications outside of chat um but regardless the Sparks are there there's a reason why I blew up we recently hit 150,000 stars on GitHub um you know it's because you can see the Sparks you can see wow like once in a while with prompting you can make this happen you can really make a chess game you can really just say uh please make a chess game for me and it does it for you you can really just say give me the revenue of this company and it does it for you right and that's beautiful like that so many jobs are made up of um or so many parts of jobs are able to be augmented by this right everyone in the future will be able to have a personal assistant and we can see Sparks of that that's why everyone's so excited by this yeah I mean back to the um back to the back to the cold heart truth is that agents aren't there right now um and you know there's a few reasons for that um there's two main ones I would say and they're kind of linked and um you know they that that they're yeah they're linked uh and and the first one is reliability um you know agents are very robust so you have a very large amount of tasks that you can do and a lot of different interpretations uh with natural language right when you're talking in the language of code for example it's much more logical there's a certain flow of data um and there's something that you know an if statement means an if statement but when you're getting to you know for example um natural language right when you talk to someone uh words can mean a lot of different things and so that makes makes it very hard to pin down on what the user really wants and this ambiguation is definitely going to be um a baking in the future another thing is the non-determinism um these big matrices they're probabilistic right even though you're talking to it through natural language the stuff that's happening in between before there's an output it's probabilistic meaning there is some differences even if um you know if you've played with open AI before there's a temperature setting if you set the temperature to zero in theory all of the outputs should be the same given the inputs but often times a single word has changed and if a single word has changed you can imagine the downstream effects right if a single word has changed in this first prompt in the response to the first prompt then this prompt is going to be different um and and as you go further and further along the the downstream effects of having a single word in one prompt earlier on uh is going to be large massive um and uh the other thing is that obviously you get reactions or responses or changes from The Real World and the real world is not determinist we know that there's random bugs that come up sometimes the internet doesn't work you know humans do a lot of debugging dayto day um and then and then the other problem is alignment I mean this is kind of falls into reliability but um this is also why reliability is so important um you know you have the potential for your reversible actions and and I'll get into that a little bit later um in a couple slides so The Benchmark for reliability is the robotic process automation industry while agents are sexy you know and you use language models and you can talk to it The Benchmark for reliability is you know companies like zapia which you can you can put together a deterministic flow and it'll work 100% of the time it's connected through apis you know quote unquote mimics cuic actions um a sequence of steps happens without human intervention so you can't talk to it but it works 100% of the time um and it's deterministic right uh and you know this is a big this is a big problem for Asians is you can't have a deterministic flow that you set up because every single time your actions are different every single time you can interpret the words the task that that that that uh that a user asks you differently um and uh again the other problem is alignment um there's very little margin for error when you have things actions that are irreversible for example you don't want to send an email that isn't you can't you can't take back emails right so your error rate has to be zero% you can't send 100 emails and one of them are just plain wrong that could have huge effects you have to have very very high reliability for sending emails for working with the file system for example here know that I I thought the suite was pretty funny um there was a open source um version of uh Chach PT's uh you know code interpreter and someone asked it to ask j delete the Json files in a specific folder and it ended up deleting all the Json files from the laptop um you know which is funny but at the same time you can imagine like you don't want this happening if you if you have a commercial application you can't have uh uh you have to have a a perfect reliability rate you can't have stuff like this happening and you know this isn't malicious right this is this is innocent this is innocently malicious I guess you can say where it didn't mean to um you know harm the user any way but the truth is is that all the Json files are deleted and um there's a lot of things that need to be improved as of been mentioning there's you know prompt engineering needs to be improved um I guess this is this is more of a a point I'll get into a little bit later uh but everything that that that that constitutes and agent needs to be improved it's more in the field of research and development than it is in commercial application we haven't figured out the best summarization techniques for example there are context Windows um you know uh open AI has a certain context window of you know 16k with its 3.5 GPT 3.5 model and you want to be able to fit everything within this context right but when you have a history of 10 actions right you can't fit the actions without decompressing it right and it's not use to because you want to only get the right AC actions um and so that's where memory comes in as well there's been a lot of talk recently and I know there's been a couple talks here at the conference about uh retrieval augmented Generation Um you know using memory or using um you know stored information to retrieve uh to reduce hallucinations and retrieve information about the world um there's other issues as well right with with the complexity of understanding like sometimes um you know one of the big problems is that GPT doesn't know its own you have to prompt it in a very specific way to say hey I don't know what I'm doing and ask the user for advice or to disambiguate um and also the tokens per second is quite low um I know there's been some talk about tokens per second um but this is this is a big deal uh you know there's there's there was a a talk by Martin I saw uh a few weeks back Martin shreky where he talked about you know the analog of tokens per second to herds per second and how right now we don't have enough hurs in order to have um you know complex proc processing at the order of the brain what the brain is able to do um but in the future when you have more tokens per second and when you have uh more complicated processing within those tokens we're able to do more um so there's a lot of work here to be done autog gbt is also working on a paper around like misalignment and security and safety monitoring um the that's actually due today we're submitting to NPS um you know around essentially trying to find uh creating a monitor to um you know make sure uh no response no external action is misaligned in trying to predict if the world state is going to be unsafe afterwards um whether it's through malicious misalignment or innocent misalignment like like here um and um when we talk about improving Asians it's also like important this is at a high level right how do you actually improve the agents how can you actually make these changes and see things improving some of these things are obvious right like gp5 but when it gets to prompt engineering uh it's hard to know that when you update a prompt it improves your entire model and so when we look at how going back to reinforcement learning agents work is you have some sort of action in the environment you get some sort of observations uh and you get a reward right and you want to perform actions that maximize your reward if and you select for agents that maximize rewards you have iterations of agents that are better at maximizing reward right and it's the same thing for agents language model agents right but instead of a giant Matrix that giant Matrix is is is abstracted away abstracted away with this um architecture of the agent you have a collection of prompts um and then with every iteration you improve the agent to maximize the reward or how good it is and what does it mean to improve an agent you iterate on one of these things but typically um today a lot of iteration happens mainly in the prompt engineering aspect because prompt engineering what it really does is and without having to modify the parameters of the network you can change how the activations flow and what kind of response you have from the network um I won't I won't dive too deeply into that how does a prompt engineering workflow work right if we go back to this graph from before you have an input prompt this input prompt is a summary of the Roman Republic in a txt file please uh and then it hits this brain right which is a language model and you have some sort of prompt in this case this prompt is reflect on the user input and create a plan um then you have the response from the language model which is one you search the American Revolution two you find the website blah blah blah and then you have uh based on this plan you construct another prompt that says select an action that can be executed to advance this plan and then you have a thought which is execute the web search with the search query um and then executes the actual command to perform an action in the environment in this case searching for a website that contains this information and this Improvement happens in this prompt category you change the prompt and you get different responses which are better but the issue is is where's the reward function right there's no way to know when you change a prompt or it's very hard to know that when you change a prompt or you make any of those improvements I was talking about before that it actually improves your agent and we were having issues about this at aut GPT right we had many thousands of Po like 2,000 PO requests and at the start of the project it was very hard for us to be able to uh see if someone had actually tested their Pro request and had actually understood if it improves or not and how it improves or not and we didn't have the time to be able to to comprehensively T Test all these P requests either and so the project was pulled in all different directions right the way that it was done traditionally was that you know you have this this input uh the summary of the Roman Republic in a txt file and then the agent would do its thing and then you would look at it and be like oh it performed quicker or oh it was cheaper oh it looks better right but it would be a very qualitative assessment um and based on this qualitative assessment you would then update it and as I mentioned this took the project in many different directions and led to some stagnation for a couple months um you know I like this like this without I like to say this without a compass you don't know where you're going right and so we created a compass we built an objective Benchmark um you know there we just have we have a skill tree with challenges that get harder you know the simplest challenge is write the word Washington to a PT file if you can't solve this one you don't go to harder challenges you can imagine that every time you update something you check um and we would run this in our CI you know we we we tested a whole bunch of Agents but we would run this for auto GPT and we would check when you make improvements does it actually improve autog GPD when you make improvements to the code uh and now you can quantify now you can objectively get you know the cost you can get the time it took to run and you can get some sort of score basically a reward function to see if it's improving in a positive or A negative Direction essentially um and you know we we this is um 25,000 logs worth of data logs meaning responses from a language model uh over the month of August and you can see that a few of these agents are using the Benchmark and they're has been steady improvements in the field so the future is looking bright um and uh we we at aut GPT we truly believe this that right now you know there was this hype in in in uh you know agents but now it's time to build some real stuff right and so we have a hackathon going on at the moment and we're kind of piloting some of the things we built recently uh around um you know having some sort of stem cell agent uh which uh implements an agent protocol which allows for standardization and gives you a way to kick off building your agent testing we also have this testing Suite that we then converted into um you know being able to comply with this template uh and you have a friend in now so you can view your agent working and you can run this Benchmark from your friend um so if you're interested you know join the Discord send us a message uh feel free to message me as well um and join the hackathon if if that's something you're inclined to do um so yeah I I just want to wrap up by saying you know the Future's bright um I I I don't think I need to explain why we all see the Sparks there's a reason why everyone's excited about agents um and uh you know I think within the next two three years we're all going to have our own personal assistant this is this is going to be the future um so thank you thank you all for listening hope you learned a thing or two dude awesome stuff I knew it was going to be good but I didn't realize it was going to be that good I really like this idea of the stem cell agent that you were talking about right there and so I encourage anyone who is doing anything in this field to check out the hackathon because I know you all are um putting out some incredible content we have a question for you though we have a few questions that have come through if you're willing to answer them I don't know if you you are that daring but yes let's do it let's do it man here we go so has any model shown better results than gp4 at Asian task if fine-tuned for that specific task we haven't experimented too much with fine tuning um honestly prompt uh prompt tuning is or prompt engineering has been effective enough to where we haven't needed to dive into promp tuning yet um there is some interesting results from The Benchmark and some of the things that we saw was that GPT 3.5 is actually better at following instructions surprisingly so a lot of the times if we wanted a specific like Json output we would use 3.5 over four so four would be useful for like complex processing and reflecting um but then you know 3.5 would be like it's much quicker but also it's just better at following instructions so if you want like a Json format you would ask like it's easier to ask 3.5 than four yeah okay fascinating any other little tidbits about like that that you can share that you found um gp4 has probably gotten worse as well that's the consens you're the yeah you're the second third person to say that this conference today right so yeah I mean I think uh matate and the incredible group that he has a Stamford has put out a paper on this so it's not like a super hot take but it's interesting to say that you you've seen it like you felt it yeah yeah and and when you when you use the March model versus the uh like the UN unpinned model you can see there there's a difference on The Benchmark wow okay all right so that that's that's awesome to hear I mean this is uh there is something that Nicholas threw in the chat that I want to mention too before I before I go to the next question which is the reward function or closing the loop is the key part to systematically improve AI systems and it's incredibly hard in practice yeah that I think it just Echoes what you were talking about in your com in your talk so the next uh question we've got for you is how do we make sure that every time we update something to improve one thing there's no degreg in other skills that's a that's a great question and I love I love the way whoever asked that question is thinking um and it's it's very valid and it's something that we have thought about so for the Benchmark the UI currently doesn't have this functionality um but in our Command interface we've added functionality for like a dash dash improve um and a Dash Das maintain F so if your um you know your your if a challenge is passed three times in a row then we assume we add it to the regression tests category and we assume that your agent can beat it um and then you can do a regression test which is Dash Das maintain so it just runs the tests that you already um you know that you beat so like maintenance and typically we would run you know maintenance tests when we're approving a p request um but then you would run like a full run you would run dash dash improve once a day to check you know on these more complicated challenges did it um did it actually like improve today um so yeah good question excellent the next question is kind of uh pinning on the question before this or not necessarily a question but the statement before this around the reward functions can you talk a bit more about the reward function and how you measure it have you revise this over time yeah and we still are working on it like this is this is all the work in progress we haven't figured out the the best way to do this um but you essentially have a collection of variables right there's it's not it's not as objective as with reinforcement learning right with reinforcement learning you can get a number that shows your reward and you can use that number to adjust um here it's a little bit more soft because you're trying to map out this uh this space using uh challenges or tasks that you ask and then evaluate um and so you know in terms of in terms of the The Benchmark itself what we realized is that you need to be specific about what what kind of challenges you create before we were thinking we wanted to make the challenges incremental and so you can have some sort of uh some clear distinction of like this is where this is where um my agent doesn't work anymore uh and where I can improve it but um challenges are expensive to run right it's like it's asking an agent to perform a task um and so it's better to have challenges that are more sparse but measure more and are more based around actual use cases um such as you know like adding information to a CSP file or getting links from from people rather than having proxy tests like we had memory tests before that would just directly measure the memory of an agent um but those were expensive uh and those can also be measured through actual use cases um so that's you know that's kind of skirting around the point but that's some of the learnings we T well speaking of money and things being expensive we do have a question around that how do you deal with high cost Associated when using agents yeah I mean get VC funding that's the question man that's the question um you ask open AI for credits uh you you do your best to cash responses um you know we've experimented with response caching in terms of like you know if you have a similar flow you just use a cach response but the issue with response caching is that you just it it's very hard to make anything in this system deterministic because anything that you make deterministic will have Downstream effects and it's not actually uh true to what the prompt response would have been um yeah I don't I don't have a better response to that yeah yeah is it's so fluid right and each time you ask it it's like caching just puts too hard of requirements on it that it feels like it can't doesn't have that wiggle room that you're looking for when you really need it to execute things so last one for you and then we're going to jump what makes you feel that gp4 is worse it is well first of all it's not as good at instruction following as I was mentioning um second it gets confused more easily um it it seems to make more assumptions um rather than just listening to what you're saying uh and I know that there was a paper about this as well which with actual hard evidence that that would probably be worth more worth reading than what I'm saying right now um but yeah I would say I would say those two things assumptions and instruction following Okay I lied I just saw there's a whole plethora of questions in the chat right now so we have another minute I'm I'm going to ask him try and do rapid fire how did you set these Benchmark tests how do you evaluate how hard the task is objectively that's another great question um so rapid fire we did it with human evaluation I just looked at it I was like Y is this hard or easier and then I set a response for it and you know the data shows that agents are worse at meeting the challenges that were more advanced but the um that that I said or that we we object or we uh subjectively said are more advanced but you can also do like a evasion like um uh you can you can run some some math to figure out what is actually harder versus assigning actual difficulties to them we just haven't done it yet all right is there any common protocol or understanding way of defining various interactions among agents because having many agents and then having many ways of interaction interacting is a bit too much or no yeah there there was a few pap around this around um multi-agent models I I recommend you check out metag GPT um there was also a paper and agents paper that came out uh around a framework regarding standardization that's that's something that we're really focused on as well to be able to you know plug in our Benchmark R UI into any agent um there's currently an initiative going on with with an agent protocol through the um AI engineer Foundation um which essentially just is is a common interface for all agents to use so that that um anyone can plug into any agent um and you know this came from a need to yeah be able to to standardize and plug in tools into any agent um and to call any agents in know way well my man this has been an absolute pleasure and I'm so thankful that you came on here you taught us a bit about agents as you know you're feeling it more than anybody they're all the rage we're all trying to figure out how we can get them to work reliably and what that in Tes also what we can build with them and so I love to see what you're doing I will encourage everyone if you are wanting to do stuff with agents you just mentioned there is a hackathon that you're doing with uh right now on the lab Lab email or uh sorry the URL we'll drop that in the chat so that anybody who wants to get involved can and Silent dude have a thanks I guess is all I can say yeah of course yeah if you're if you're an SF as well DM me there may be a couple opportun unities for for people here but yeah thanks for having me oh uh oh you guys are hiring uh no we're holding uh we're holding a little hackathon um on the seventh to 8th so all right sounds cool more exclusive than than the than the lab Lab one oh you can get a special invite using the promo code mlops community lm's in production awesome yeah we'll we'll set that up just add just put ml off me when you apply and there you go that may be a good thing or a bad thing you'll figure that out soon [Music] enough