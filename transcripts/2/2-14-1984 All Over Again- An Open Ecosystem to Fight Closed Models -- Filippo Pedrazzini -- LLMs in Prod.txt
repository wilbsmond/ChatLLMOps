let's see physical Pro I see your screen you're good I see full screen yeah game on baby oh there he is there he is all right cool man hello my friend how are you ciao Amigo ciao it's good to see you here and I'm excited for you to present what how long did you think I was gonna hang on there man sorry you weren't even watching weren't you yeah you were pure panic with chrome that's it so people don't don't know but behind the scenes sometimes the streaming software that we're using the speakers will crash their computer as soon as they share their screen and so Filippo got a first row seat to the computer being crashed and that's why we threw this in we improvised man we improvised so all right you are super cool at that so it's perfect we can go with it all right I'm gonna let you get started and let's uh let's cruise okay back in 20 25 minutes man perfect thank you very much so hello everyone I'm Filippo and I work as a CTO at the Grand and I am also a co-founder at the Prem uh this is like the outline of my presentation today so first I will try to speak about the AI personas and I mean in the current ecosystem and then like uh the my business personal problems I would say uh and then the I mean the obvious solution given like the title of the deck uh but I mean theoretical obvious theoretical solution which is open source and unlike the challenges that you can face when trying to use these open sourcerer lamps that a lot of people are talking about and then like as a final point I will give like a practical solution uh to these uh problems so starting from like the ecosystem and the different groups in the AI ecosystem the first group it's obviously about the data scientists and researchers um I mean they they are the ones that actually know about the muffin and and statistics and without them we wouldn't be here speaking about other lamps in production uh then the second group is composed of mlops Engineers I mean they they are the the guys that bring I mean take the model and bring it to production for real um and uh obviously I mean like they are the the serious guys and uh uh then like the the the last group it's about like the web developers uh that are I mean they are having a lot of fun building new AI applications uh using launching and Yama index uh and I mean luckily they are a year it will pass out because uh otherwise uh all the web applications will be like based on streamlit or radio and I mean this consolidates like in the in a single spot why because basically like uh I've been um I started my career as like a data scientist so I was a fine-tuning and training deep learning models um and then like due to the fact that we were a small team I had to learn how to actually bring them to production and so like I entered the rabbit all of mlops engineer and all the tools and Frameworks related to that to that and finally I mean like it's a more than one year now that I work as a CTO and in that case I mean uh you you touch and and you have like big I mean the full picture on the entire Tech stack and obviously I mean like you some issues that I was not expecting to to face for example related to UI and ux actually like they they are super important and like a front-end development it's more it's harder than expected I would say and uh and yes so I mean like uh I've been let's say facing a lot of issues during my career and like solving one problem after after the other basically but still today I have like big issues and like the main one is that I cannot use open Ai and it's not because I'm Italian uh it's uh it's I mean like the mere reasons that I'm dealing with the sensitive data so and like very sensitive data and on-premise infrastructures so in this case obviously we cannot use third-party providers um and like we have like a big limitation considering like I would actually charge a BT and gbt4r okay and for this reason I mean after like the first release of HR GPT just a few months after uh we know uh we know I mean we all know what happened so like the madness started in the open source community and all these I mean incredible animals have been out so from Miami Etc and due to the fact that like there was this a huge hype on Twitter I started to dig into uh actually these models and see like if they could actually work in a business use cases like a production use case okay um but I mean it's not as nice as as expected basically as always I mean things seem super super cool and nice but in the end I mean you face a lot of challenges and I will show you like I mean I will give you like some problems that I faced so like the first one obviously it's about quality so I mean yeah like I mean it's like a joke of course but I the number of models beating gpt4 on Twitter versus the number of models beating Jupiter 4 in reality uh what I can say is that um data sets are one thing I mean like and we have I mean I think that the community is improving day by day in terms of benchmarks but then like empirical evaluation is another one and of course like the things I mean like comparing the the current uh open source models with the gbd4 it's a bit uh too drastic I would say at least right now but progress is like doing amazingly um and like for these let's say I mean um for each problem I will try to give some tips and tricks and for this uh specific problem I mean like the first starting point that I want to say is that I mean in a single specific business use case you don't need the AGI okay so uh you don't need like the capabilities and the reasoning of gpt4 so it's not a big deal even if these models are not as good as gpt4 then like another I mean error that I have been doing like a lot during the sun doing some evaluations of these models it's like about prompt engineering but not in the sense of like defining the good prompt in order to have like the correct result it's more like the fact that each model has been trained on on different data sets uh these open source models I mean and obviously I mean each uh each one of them has I mean the the team behind each one of these models has done like different data processing and data cleaning and uh obviously uh there are different from templates for each model so when you do evaluation or like you test for the first Times Like These models try uh to be careful and read a lot of the documentation and how these models have been trained uh then like another uh tips and tricks or like a suggesting is that uh a lot of I mean um for a lot of question asking use cases you can actually use like these seven billion Q4 models uh coming out of the community but obviously like they have a big limitation in terms of context Dimension so given the fact that I mean they given this limitation you need to be careful in terms of uh creating the right dimension for the chunks so um obviously I mean given these a small prompt that you can put there um you need to be very careful in in in this sense okay uh and like then my suggestion is a related I mean the second point it relates to the first one in this is that just use sentence Transformers which is I mean fast and and good enough for these use cases and and also Italian those like a small context so um I mean the combination of the two Indian like trades like it took to your data a solution that actually works fine I would say um then like uh I mean if you really need the uh go for a fine tuning but I would stay away from that as much as possible which is I mean it's not that it's complicated it's that it's cost intensive and like you need the um a lot of experiments in order to do it right and if you can I mean if your use case just uses embeddings keep going with the embeddings and don't complicate or like I mean if you enter the fine tuning Planet then like uh it's difficult I mean like yeah you will have to maintain a lot of stuff and like you will have a lot of drawbacks um so like keep I mean keep it simple and use embeddings as much as you can then another problem that I faced it's about Hardware constraints so I mean I saw this message like too many times lately uh and the the main reason is that in a on-premise infrastructure you don't have like the latest super cool GPU with 80 gigabytes of of memory so you need uh to like adopt yourself and like for a I mean let's say proof of concept you cannot like a start Distributing the model on multiple gpus I mean you you need to start simple and and and and for this reason I mean like my my main suggestion is that start from the simplest model the smallest model model possible uh to test your use case um in an open source manner so and and also like I mean bigger model doesn't mean better quality so on the left you have like an example which I mean you see Dolly 12 billions failing on a just I mean Hello uh Quest I mean it's not even a question it's like just a low explanation explanation and you have like a as an answer like the welcome message from stack Overflow um on the other side is that the corner seven billion Q4 and it explains like very well like the difference between nuclear efficient and fusion so uh I mean it all depends so but but in general I mean and this is like of course I did these screenshots and I found the specific uh failure for Dolly and like the specific good answer from for vicona in order I mean for this uh sake of the presentation but the message is General so start very small and then like I mean try to to to see if like uh that the small model can fit your needs and then go go bigger uh if you have like limitations on on the first one then like problem frame so I mean let's assume let's create this scenario in which you are like I mean uh checking on Twitter the latest updates and you see like all these new models super cool going out and they I mean all seem to to beat the gpt4 or like at least have a good quality and then the day after you say okay let's uh let's uh speak internally with my team and see okay let's do it and I mean like let's try them out and see like if we can actually use them in in our use case uh and like done of course like the first thing that your boss or stakeholder says that okay give me like an environment where I can test it so I can create a POC create like a micro service to do whatever in order to for me to test it you know and see like if the quality it's actually good as as you think no and then this is this is what happens I mean and I think it's it's I mean temp is always in the AI industry because we are very used to to to to to to to like embed developer experience okay uh but I mean these are the all the steps the the two screenshots that I put uh behind the Nana is that I mean all the instructions in order to run open Assistant 30 billion store on the other hand is like the installation process in order to run like a a simple seven billion Q4 model so I mean obviously like we are we are very used to to bad developer experience uh but I mean it's it's it's not nice uh and and like in other Industries I mean like in other Tech Industries these things are not happening um so we are we have like low standards I would say uh and yeah like I mean in terms of tips and tricks not much to say I mean like just uh keep calm and maybe the the next day it will work maybe not who knows but uh I mean you will see okay given a independent okay independently from I mean the data type that you have to take care of and like a privacy Concepts that you have still I mean going for the open source something it would like and do like the first step towards that um it will for sure bring benefits on the long run and it's all about inference and cost I mean if you don't care about privacy uh and and this I mean it's it's it's a it's part of the the reality of like um of going for an open source solution uh and yeah like going back to the to the initial slide um obviously I mean like these three components are part of a complex ecosystem they work alone but they work together well I mean like from a broad perspective but they are also super different I mean and it's different I mean in order they have different mindsets they have like a different attitudes and and like a obviously I mean like creating like a good communication across for these three groups it's very complicated uh what we're trying to build at Prem it's like a super simple open developer friendly ecosystem in order to handle that and the like in terms of I mean for the data scientists we will soon uh release the fine-tune capabilities uh similar to how open AI does for develops Engineers you will be able to run in a few com in a few comments and clicks um production ready container uh while for web developers I mean just run Prime with a few comments or locally with the desktop app and you you can keep going with your launching integration and just switch the base URL to your server IP or like another instance I would say uh I have a small video prepared for that which is the same that you will find in the GitHub repository uh we have like both the desktop app and the server installation which I mean it's like an installer script that is also dependencies and it runs like a Docker compose um you you can I mean with just few clicks run a service and then integrate launching accordingly um and yeah I mean like we expose only few Services now we are like still in beta Alpha phase uh we are looking for help of course and and uh I mean but but soon we will expose more services and we want to go multi-model in the sense that not only a lens but in general like all the models [Music] um and yeah this is like a I mean you can try it out on GitHub and there are like all the instructions in order to install it or to run it in your server infrastructure okay uh yeah okay and uh like uh starting now I mean uh we start like right now the prime challenge uh with the price of like uh 10 000 plus dollars and it will last until like I mean it will last for two weeks um you can check on GitHub at this link uh I think that the meters will share it with you like probably like in the later uh and and uh and we will like do a blog post after this presentation and uh yeah I mean it's about composability I mean try to build on top of Prem and see like how things work in only only using open source llams we want to see like developers doing what they've done like a few a few months ago with the challenges related to charge GPT and and open AI in general like layer and models uh join us I mean like you can follow us on Twitter uh we have like a GitHub you can uh I mean enter in our Discord for any problem technical issue I mean uh we are there to to help you out and uh yeah I mean you can also check out our demo instance if you don't want to to install the uh the app in your infrastructure at app.prime.ninja and uh thanks a lot I think that uh I took less time than expected maybe I don't know nicely done that makes my job a lot easier then I mean I'm saving you time no nice yeah wait I just want to clarify some things because you know uh you're giving out some money here and you're not talking about small dollars so what's the challenge and how much money can I win I mean you can win uh I mean like we need to speak with the CEO because I'm not like in charge of like the accounting part but I believe that it's around the ten thousand dollars um and it's like uh uh across the the best projects I would say and uh but there are more like information about the challenge in the in the readme of that repository uh with some examples in order to how to actually get started with Prem so that you can already see like how it works and start building out of the box oh that's awesome so basically if we build if we build an app with Prem I'm looking at the challenge right now on uh GitHub probably even just share this with you all yeah actually and uh so if basically print challenge uh done specifically for the llms and production conference I like that what is a web app using many or one of the Prem AI services and when is it happening starting today until June 30th it's a virtual thing you can be on a team or you can be solo Oh my God up to 10K will be awarded to the final selected project so basically it's like if you can create a cool app with some open source models and Vector database from Prem then you're going to be getting some cash thrown your way I like the way that this is shaping up man this is really cool okay now I threw the link for this in the chat in case anybody wants it and all right does it say how to how can you submit it yes uh there's like um uh a link like a submission process Google form link basic steps I mean uh just to send us like your GitHub repository and uh all right nice dude well I'm glad that you guys did that and I know that it is uh it's something very cool and you're you're doing some incredible stuff at Prim I really love it and if anybody wants to know more about what you are doing at Prem I will direct them towards this good old tab on the left hand side that says Solutions and you can go there check out where is it from Prim enter the virtual booth and you get hit with some cool stuff from Prem I love the project I mean I know you guys just launched a few days ago and so it's cool like everybody out there listening give them some GitHub GitHub love because it's fully open source and you're trying to do some absolutely awesome stuff with it man so thank you Filippo thank you very much I think you may have the best slide that I have ever seen I cannot use open AI not because I'm Italian I gotta give it up for you on that one that was brilliant all right man thank you yeah