we got our next speaker on Deck um it is Sebastian with a very good question that he will address llms versus XG boost can a fine-tuned llm beat XD boost on tabular data tell us Sebastian uh can it be done thank you lady hi everyone all right today I will talk with you about what llms can do for Predictive Analytics and if they can beat XT boost on tabular data so at iwt we are doing full stack data science Consulting everything from architecture development to Predictive Analytics tasks and most of our data comes in the tabular form so I ask myself can we use those large language models on those tabular data sets and apply them to standard Predictive Analytics tasks and yes today I will present you the my findings from those experiments so I will start with again a short introduction finding suitable data task and data set then how you we would go for fine-tuning those llms the results and then a few experiments conclusion and next steps um I I looked at yes so large amounts of structured and tabular data is already in use and our around and used all the time and the question is do the llms know something about the data that's maybe not in the table itself so the large language models are trained on vast amount of data and maybe they're afraid to finance subreddit and know something about it that about some data set that the data itself doesn't know and um yes that's kind of what what I wanted to evaluate in the past I was already experimented when when bad came out and how bird Works compared to XG boost and that was still a clear winner for for XG boost but with the starting hype and the great results that the llms created I wanted to look at it uh again and then see how the performance goes and going for that the first step was to find a suitable task and a suitable data set for that and that proved much harder than expected so when you look at the UCI machine learning repository or some other standard data set you can just copy the CSV line into chatgpt and ask them hey Will that customer churn yes or no and chat TPT has or GPT and the large language models have read the data set 200 times during training and can use 100 accurately say if the customer will return or not so it needs we needed to be a data set that's not in the training so we looked for a data set that was published after the the training period and also it needed to be a data field where from a domain where we can expect the llm to kind of know something about so going with some iot device with of course on some other sensor data set where we have 200 columns X1 2x250 with all flow data we can't expect them to kind of know anything about that so we ended up with a customer churn prediction data set from a telecommunications provider that got recently uploaded and then how do we go about fine-tuning those tabular data for the um and basically three steps the first one is to translate the CSV file into natural language I guess you could even go with the comma separated list of each line but having a data set with with a code book and then translating it into a real sentence is more natural than than what the other ends are are used for fine tuning itself then works with uh because there are dildo sequence to sequence models and they are predicting the next word so you are providing basically only the word yes and the word no and then fine tune the model to predict that word outcome instead of the zero one you have in a standard machine learning model and in the end we compare those results with XC boost so I have some trained test split and evaluated so the whole pipeline basically is you have your your CSV data you translate it to natural language we used chat GPT for the whole experiment we used to open AI uh apis because it's really easy to use everything's there you have the translation with GPT to get your data prompts file and then you they have a fine tuning endpoint that gives you the trend model and that you can then use for the for the predictions in the next step and then to compare them to xgboost and with the churn data set classic metric is the the AUC you can use to evaluate your classification and on the right hand side you can see the plot of that the blue line is the XG boost which is an AUC of 93 and the Other M comes with 91.6 not far behind so we can see that the llm actually performs reasonably well on this churn prediction task so I was quite happy to see that the llm even though it's not as good or it's not better than XG boost or at the same AUC it's it's quite negligible with the small difference and we can see that the MPD performs well on this this data set I also tried it with chat GPT you can see the red line here when I adjust without any fine tuning ask jgpt hey here's a customer from the telecommunication company so one prompt from the translated data set do you think it return or not and after 100 tries I I stopped because the other bites were worse than random so some fine tuning is necessary one carrier to the whole thing is that I also tried the logistic regression and you can see that's even better than the llm and has an incredible High AUC so maybe the the data set wasn't the best to really gouge or you would need a bit of more or another data set or something to see how it goes um one problem was or one thing was maybe does the llm actually know about the data set um or not and um to kind of understand that we try to add the customer ID to the prompt and see if that improves the fine tuning so if the data knows about if the llm knows about the data maybe that would improve but it was not the case so adding the customer ID changed nothing in the results it was just noise that was added and then uh another point was because the initial idea was that maybe the llm knows something the the table doesn't what happens when we remove like the most important feature for XG Boost from the HD boost and from The lrm Prompt uh will that actually change the performance or it will decrease the performance of the of the models but with the magnitude of decrease be kind of the same between the XG boost or maybe can the llm compensate for that a little bit but there it was roughly the same decrease when removing the most important variable for no win for the LM there um also I tried a bit with the train test split trying the different seeds so each time only only one change but the variations changed not as the results change not at all so I think the results for this one data set are a reasonable robust for for first experiments so as a conclusion I think we can say that the llm performs reasonably well on the classification tasks um of course it's a bit more data preparation than directly using XG boost with the table but when you think about uh for for mixed data sets like a classical tabular data and then you have the text normally you can try to kind of extract features from the text and then put them into your into your machine learning model but now you could actually try it the other way around uh translate your tabular data to to the sentence added to the to the other text Data you have and use the fine-tuned llm to do the predictions and be I guess a bit more flexible with the whole uh yes we're supposed to approach I think the cost of the whole experiment was I don't know 100 bucks or so so it was cheaper than expected and perfectly way for for like first experiments working with the open AI API was slower than expected so when I did my predictions it took like five minutes for a thousand predictions I was quite surprised but it um not sure if that was uh if that's reasonable 244 actually their production use case or if that was something I can still improve upon next steps open questions are of course always there so one I think is very interesting can I concatenate all churn data sets uh and find and translate them to natural language and see if that joint data set fine tunings then better than every individual one could we use chip layer values to identify what part of the input prompt was used for the classification and then Compares with XG boost or also not sure if that we could have or not can we extract any relevant information from the llm to enrich the tabular data set itself so maybe those them hallucination everybody tries to get rid of are good for something after all when we can enrich our data sets with some reasonable guesses from that yes yeah the The Links of of the open AI API data set and thank you for your time awesome thank you so much uh we are going to kick you off the stage and get ready for our next speaker thank you so much Sebastian this was great bye oh all right so long