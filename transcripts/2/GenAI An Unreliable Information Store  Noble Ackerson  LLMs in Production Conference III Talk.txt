[Music] no are you with us I am can you hear me we can hear you okay sorry for starting at a bit of a delay but the floor is yours take it away great uh thank you so much I'm going to jump right into it um this is this um Talk is called exactly what it says right there on the screen uh unreliable information store um and you know perhaps ill advised I decided to in sort of looking at all the other talks I decided to sort of switch my slides up a little bit probably not the smartest thing last minute uh but I think you can do without another rag code walkth through uh and so to frame this talk I'm going to try to focus on what we can do to sort of bring consistency faithful relevance to large language models uh for the practitioners that are actually struggling uh through this and hearing all kinds of new ideas through this fog and and sort of this entire transformation and this technology I want to at least sort of empathize with you I'm going through the same struggles I'm Noble Akerson I you know for my full-time job I work as a director of product uh in Virginia uh and what is a director of product doing talking to you guys about large language models im atpm a technical product lead I founded a company previously called uh Biden Adam research we we shipped a a Google Glass based app uh a long time ago and and we're still doing a lot of product uh research and product delivery so we're not just an R&D shop we actually sort of ship product but for my full-time job that's a Consulting gig uh but for my full-time job I actually sort of take some of these practices and apply them to sort of the federal space with Venta we're based out of Washington DC are right outside our the US nation's capital let's jump right into it um I'm going to ask a question uh I'm unfortunately I I'm not going to be looking at the chat because I'm sort of locked in here but I'd like to ask you know what do some of these items on the right uh over here like Google Glass or 5G crypto genetically modified organisms that kind of stuff have in common what do sort of nuclear power crypto GMOs n T's what do all those have in common right um you know take a beat to sort of think about that uh and I'll probably sort of walk you through that so we're talking about things like nuclear power um think as you're thinking about the answer to this sort of think Fukushima uh you know or Chernobyl um GMOs think about how you feel when you hear GMOs in your food um nfts um don't marck it uh there's a lot of impressive technology that powers crypto I'm not into crypto but I I'm I the underlying technology is extremely impressive but there's a Common Thread here right um you know after Chernobyl maybe Fukushima public fear overshadowed at the efficiency of and and the what they call the low carbon efficiencies uh of uh nuclear energy um you know it was shut down in the case of fuk GMOs have a potential to improve crop yield to improve the lives of many um but those technology Marvels are mired um with the potential risks not enough guard rail not enough checks uh crypto just you know during the pandemic just went to medior rise I was on the sidelines while everyone tried to revolutionalize the financial system however was so complex and versatile um but the entire Common Thread here whether it's Google Glass or nfts or self-driving cars has a common thing it's a lack of Data Trust right so all of the hard work that we're doing to bring the reli you know to to bring BL large language models into production may not even matter uh in the narrow sense but collectively for all of us we can't cut Corners uh we add the you know we include that added complex but it pays dividends uh for the entire ecosystem so uh that's sort of the frame of this conversation um I've said Data Trust uh I want to sort of um sort of Define it and I Define Data Trust in the context of AI as the sum of you know your responsible delivery of an integrated deep learning or machine learning model uh The Continuous um value that you deliver uh by being transparent in what the model can and can't do for example uh in in a production setting and some of the checks that you put in to to not just to deliver value but to sort of protect uh your your your your your your your end users uh and the consequence acceptance because something will go wrong uh but the consequence acceptance from your end users uh if um you know because they believe that you're responsible in how you deliver it and you you know you provide reliable value delivery in in this so you're essentially calibrating uh your end users trust and so that's sort of the first lesson and so today um you know hopefully that is a coherent thesis uh that sort of guides us through uh the journey that I'm going to take you uh today and um you know I'm going to sort of start with sort of a field report of sorts like you know how my customers are using large language models um some of so won't be too surprising you may already know some of this um you know we're going to talk about you know why generative AI uh is unreliable um in you know as far as the why I feel we shouldn't use it as a zero shot information store and I'm sure a lot of you guys aren't doing that uh but there's a reason why I framed the entire thesis behind that and what to do about it so yes these systems are super powerful um yet they're flawed uh and we acknowledge this fact today uh and so today we're gonna sort of talk about where it fails and and what to do about it and in order to walk us through those I've sort of you know the failure points are many when you're dealing with a a non-deterministic system you know there's so many areas uh where the system can fail and so I've sort of broken this up into uh you know the data input your system IO uh and of course the product input which is where typically your users will interact with the tool and so I'm going to start at the end we'll Quint and Tarantino this thing and start from the product output and work our way back and product output meaning you know you have an interface with your end users so it's your user experience uh you're also probably doing some testing and some evaluation with your experts in the room and so it's also developer experience focused uh in this sense and just to sort of frame that even further um you know even though I uh as a practitioner I've been working with machine uh machine learning or at least NLP model since 2016 or so uh when Spacey was pre 1.0 whatever it was um my first interaction uh with the generative AI was in February of this year uh and as I do with every new technology I went ahead and ego surfed and you know typed in my name and realized that you know bang had just announced they were going to sort of launch uh generative AI features into bang and Google was following suit and that terrified me right and so I started wondering you know we use these tools um these tools are built to model language right that's why they call them language models right uh they're autor regressive meaning they're trained to generate tax they're rewarded by predicting the next coherent word in a sequence they're non-deterministic by Nature they're probabilistic by Nature uh they do not reason deductively natively without additional help or access to to other things or other um language models that kind of stuff and they're epistemologically blind to what they don't know so given a prompt you may get a very unpredictable answer right uh every single time and the notion that at an autoagressive system that lacks adductive reasoning that um that is blind to what it knows you know you know being incorporated into the Enterprise also terrified me right and this is where I worked so I decided to sort of put together a couple Frameworks and sessions and sort of slow my customers down uh before we went full bore in integrating uh these powerful tools into our system because there's there's also a cost associated with this you know the cost for my customer who doesn't have the the Deep wallets of a Google or Microsoft or an IPM to ship a generative AI solution without consideration of factual correctness repeatability consistency reliability that kind of stuff is too high right you know so we require guard rails we required tests we required uh human experts in the loop uh people on you know humans on the loop as well to provide feedback to get that flywheel to further improve your model and these are some of the use cases that we C it might be a combination of some of these uh say for example an RPA project that uh robotic process automation project that is now llm power that I just shipped uh for one of my uh bite and atom research customers uh for example com combined a few of these different categories um there's so much complexity that I empathize um as we sort of saw our way through through uh um this process you know as we added more complexity it you know internally it fell short every single time it felt like our customers you know the feedback that we got felt like they were yelling at a PA system or W uh and so you know we started at base zero all right so maybe we shouldn't use uh chat GPT maybe we should use an API based system and surely enough uh you know we ended up we didn't go with chat GPT even or see the open AI API even though that's what I'm showing you here we went with a Google offering uh with the Palm uh the the maker Suite uh playground and still the same thing you know added complexity Falls over when we put it into production users complain uh they feel like you know they were yelling on the wall uh so they don't use the app because it's unreliable they do not trust it will be consistent uh and then they fails but we can't cut Corners uh and so we add more complexity uh we start thinking about ways to version our prompts and there a bunch of talks today uh that will sort of talk about that uh or have already uh there ways to sort of frame and better write your prompts all right and as a creative technologist as a product uh mind or product leader I start thinking about what came before us and I think you should as well in the late 60s Paul Rice identified something he called The Cooperative principle and I think that this is a powerful frame as you go forward uh with your product outputs and and and sort of create um you know prompts and and ver hopefully version them uh and follow the best practices that you've heard about uh all throughout the day he talks about four maxims you know he talks about quality which focusing on how truthful um we are uh with each other talks about quality how informative we are relevance and manner how clear we are right and so you know these can be use as reliable gu guidelines when we uh you know when you're in doing your in context learning strategy or how you frame your prompts for example like when engineering prompts for a large language model the maximum for Relevant is super crucial you know you should craft your prompt uh directly related uh uh to the task at hand you know rather than tell me about AI or who is Noble something like explain the key principles of machine learning uh is is perhaps better you're not here to learn about how to craft a prompt um so let's switch directions a little bit I want to sort of attack the complexity Paradox uh in a different way a little bit more practical way there are many uh failure points for these tools I've mentioned maybe three here there were more but um I've sort of wed it down it seems um overfitting um that comes into play today those whove worked on this of machine learning in the past our old friend is back uh when you want to fine-tune your your your larger data sets or at least if you don't have enough uh data uh will find find uh your your your models not generalizing well uh probability it comes from the foundation model and sort of carries over um you know with the knowledge gaps um therein and so back to this um sort of framework we're sort of working our way back uh with the systems uh and we're going to sort of talk about some of the system input and some of the data output so with uh you know uh um trying to get consistency by teaching your private data new tasks uh you know by leveraging large language models um you know simply put overfitting occurs when your model fits exactly against your training data essentially you know you're you're you want to mitigate that by reducing the variance between your error and the validation data set it's not generalizing well uh like I said and so I'll SK I've skipped the I'm skipping the code part uh and so we're just going to jump right in I'm sure to show a sort of framework how everything comes together a little bit so jum to the end um lots of hype like I said demos do well but they fall over once they get into production uh here you see sort of a serving framework uh that I uh designed and I that works for me your context might be different uh your data sources get uh prepped um you know across a you know this is your reliable information sort this is your domain information you can add feedback data for your fly you know your your model Improvement flywheel your red and purple teams might uh uh uh appreciate some some Telemetry data that that you may feed them but you the point is to version that data to feed that uh into your system um to teach it new knowledge you may want to like I said fine-tune or perhaps not you may want you know you may have too much data you may want to have a a distribute transfer learning pipeline uh to sort of find to multiple um models at the same time uh you've got online valid uh online evaluations uh you've got um performance tests I'm sharing this to say you know before you integrate uh and have the you know the benefit of you know selecting the best performing model uh and doing a post processing to to to sort of persist your chat history for longer conversations that kind of stuff there a lot that goes into um uh this in a batch sense before you're streaming into your front end uh application and so the drawbacks are simply you know overfitting is you know our old friend is back um you know um you know if you have a small data set you got problems uh the cost um is really high because a lot of people sort of think that initial cost you know you're spending somewhere close to $2 million just to depending on your scale it might be less um to to get everything up and running but we forget that there's an opx there's an operational expense to continue training as your data changes data changes in the Enterprise um you know by the minute most cases ethical risks don't just stop because you fine-tuned your model right like so without guard rails uh you have some issues and also notably uh these models sort of um uh uh you know have a point where they uh they sto training so your fresh data this is where rag comes in so another demo that I was going to give you we're going to skip that as well because you know I recommend you go to Rahul in our community who's got a course on this matter but we sort of take from that pipeline that you saw um and sort of bring it into a streaming context right and then we sort of give feedback back to our our batch pipeline that you saw before I'm going to skip the code um complexity Paradox uh so what do I mean by that so I began with these llms are powerful beasts right uh we know they're super powerful but yet we put in the work to make sure we constrain them uh and it's very important because they can get scary uh right um and when we find a technique that works for our context uh we we we make him a little less beasty and more friendly uh and we add a lot more uh to uh to a lot more uh uh content a lot more hard work a lot more complexity a lot more cost that goes into making these things safe yes I should acknowledge that if you have money of a nation state or Google or Microsoft or whoever open AI certainly uh you can probably train your own llms but for my customers we don't have that luxury uh and so just to land This Plane um you know there I've enumerated a few areas uh across the you know data input system input the uh system output and the product output uh these are some uh mitigation tactics uh that we tend to use um if you're looking for a little bit more uh meat or code uh I'm going to share my GitHub uh here at the bottom leand corner and I'm going to be doing a lot more YouTube videos uh By Request and so um I hope this is has been useful I hope I'm on time I was not timing myself and so I'm going to hand it off to Adam you're doing good you're on time uh and also your book so you're writing the book it's coming up yes uh data practical Data Trust for um AI oh I I called it practical Data Trust for MLA aai practitioners and that's because I was writing the book long before um General day the general day I boom uh but I think it really matters because a lot of my customers still work on uh traditional ml or narrow ml as we call it these days and so yes that should be released uh next year and so again um you know just type in my name on I'm I'm no type in my name on the socials and and you'll probably see me announcing it next year nice uh and yeah I mean if if folks want to follow you on on YouTube or your GitHub I'm sure they'll learn more there uh we got a couple of not sure if this is a question it might be more of a comments but perhaps I'll address it to you either way uh Chris Thompson is saying so happy to see this laid out so much is overlooked when deploying llm apps thank you for the stock uh and then they continue with not only do you have ml monitoring but also user monitoring the need to address negative Trends in user feedback is another crucial aspect to watch for then you need to eyes on not only the model but also the output coming from the knowledge store and robust logging from the code itself problems can arise in many different parts of these system spot on spot on and and one thing that I didn't get into I hinted at it in the beginning just to add to that comment uh is sort of calibrating trust right um one clearly communicating what the system can't do I know I I wrote that here gracefully you know gracefully uh failing that's important but um you know if I'm under I'm calib I'm optimizing for under trusting my system people are you know it's unreliable it doesn't it it has factual errors people are not going to use my tool and so that needs to be logged and persisted and to help improve your your model if your system is working well perhaps too well you're over you you have a new Paradox a new problem which is over trusting uh where someone might you know if you a multimodal model that you know you can sort of scan a plant and it tells you um whether it's safe to touch or not or eat or not you know if it's you know to right all the time without any context without any modalities or at least any uh affordances within the experience so let them know to double check uh depending on the criticality of your your tool you're over trusting and that's also not good uh you're optimizing for over trusting so that calibrated trust is sort of somewhere sandwiched in the middle and it's very very hard work it's a it's an art not a science and so again I commend all the practitioners here today uh to hear from a a product mind uh rather than a classic engineer uh to sort of think about these things yeah yeah I'm I'm curious also about this The Cooperative principle that you put up nice quote by the way I wonder do you feel like there is also a possibility or some potential where the fact that we need to be so thoughtful about truthfulness and relevance and formativeness and Clarity will sort of like even back influence and add back pressure to the way we communicate with one another too right so this is something that feels to me like at least a potential for llm and so far as we have to then focus on improving them so as to facilitate conversation more effectively what can they then do for us in the way that we converse with one another you're your your right on I I I learned this by when know back in March when chaty PT came around I'm not a linguist by any stretch of the imagination but you know I have I'm a parent I've got um you know a girl a young lady in in in in elementary school and even to take that a step further how I interact with my tool and I picked up this book uh syntax and semantics I Not only was I able to sort of you know get research for my book but I was able to sort of realize exactly what you're saying you know how to converse with my child right how to and and that sort of maps you know how clear how relevant how you know how informative I am with you know with with with you know with my colleagues and and ultimately um the machines that we want to get the best out of right so totally uh I will smuggle one more question in here because Leonardo ass it and Leonardo is the champion of the quiz Leonardo says Noble this question is a bit off topic but in your experience how do you approach the ideation and development of new products or features with AI ensuring that the focus starts with the problem to be solved rather than the technology itself in our Squad we are centered around AI Solutions but as a PM I am to create solutions that address real world issues rather than merely leveraging the tech for its own sake they share some insights or methodologies that help in aligning the Technology Solutions closely with the problems identified I love this question so much and so I'll be I'll be very tur um one understanding and communicating internally like this is there's a lot of hype right like so in communicating internally that the model is not the product the even the integrated model uh that is being served as an API is not the product or at the edge is not the product the problems that we're looking to solve the human humans have uh whether it be narrow within your part of the world the humans have that you're looking to solve um is the challenge um AI is simply augmentative or maybe autonomous in in some cases um and and doesn't really solve the problem all the time uh and so I guess to enumerate uh some things uh to help address the question one the old Steve see jobs added right start with a problem work your way back to the solution uh let's not do I I I ripped on I didn't rip on crypto but you notice with a lot of emergent Technologies I don't care if it's crypto Google Glass or VR or Mr or whatever it is we get mired by demos and we sell those demos and we don't look at the systems that we uh that are interdependent and when I say systems I'm not talking about tech technology put technology aside for a second and bring in regulations and bring in harms uh and potential uh um detracting issues from pushing your product forward put bring in uh is socioeconomic is issu sort of falls under that as well bring in you know other products right so that bring in technology and see how that they sort of interconnect once you start framing that uh and framing your problem right and sort of thinking from we call it systems thinking uh and sort of thinking about all the dependencies start forming your principles around that uh and your principles May reflect as as going back to sort of wrapping this up with how I sort of framed this conversation your data inputs having data cards uh and and and and you know model cards to better you know have the lineage Providence and understanding as to how the how the data came to be um having know for your system input and system output uh being able to sort of build on top of you know machine learning operations this is the mlfs communicate uh community at the end of the day build on established standards rather than want to sort of roll your own uh and then of course on the product output side you know again those practices of you know um Pro you know protecting your system from bias and explaining what the system can and can't do yeah some of those soft skills will will help you afford the trust that you need uh from your end users and and that's important for not just your product but for the entire everyone here doing something with it thank you very much Noble Leonardo I hope you enjoyed this answer this was a brilliant answer no and very inspiring thank you very much for coming to share your wisdom with [Music] us