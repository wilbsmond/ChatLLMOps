[Music] um our super cool t-shirt okay actually maybe I'll give the floor to someone else talking about me yeah so I just had to come over here because I've been getting a lot of crap from our next speaker I had to come over here and let him know how everybody that is on this track you should go over and watch the track number one because this one you're not gonna get anything useful from Diego right now oh what's up man I just wanted to rub that in your face for a minute I left the guys hanging I left the fireside chat hanging so that I could introduce you right now oh there we go there we go okay you even got your shirt on oh look at that shirt look at that shirt so if anybody wants to buy that shirt you can find it right here Diego keeps asking me for this specific one I hallucinate more than chat GPT but I already sent you one man we're a community that is funded by a great uh actually your old company was one of our first sponsors let's just say that right now and give a huge shout out to algorithmia your old company and also I am going to come clean a lot of the no where is it the report that was right here the report that we just wrote and we just came out with I kind of copied the structure from the last algorithmia report that you guys put out I'm gonna come clean and say that son you know like it's uh whatever gets the information out it's it's all good oh that's not that's not what you were telling me on Whatsapp you're being all nice on the live stream I love it all right man well I'm gonna let you talk because I gotta I got a fireside chat to get to that everyone should come over and watch uh but I'm sure you got incredible information Diego's for everyone that wants to know this Diego's uh talk at the last conference it was one of the most viewed talks so I have quite high expectations of this talk right now and I may be zoning out of my own fireside chat because I'm trying to follow along with what you're saying Diego I'll get off the stage now and I'll see you in a bit great hey everyone uh happy to be back here at the conference uh so uh it's a quick talk you know everybody's favorite subject which is actually kind of like governance and compliance uh so the general idea is that um for for those who don't know me I'm Diego Oppenheimer currently the uh a partner at uh AI Focus fund I used to run a company called algorithmia I've been uh I worked at the robot for a while um I was in the space of model risk management and governance of ml models for a long time uh and I thought it would be really interesting to kind of understand a little bit how this needs to evolve in an llm world uh obviously this is a very short talk but uh you know I'm happy to chat more uh later about this so the general idea here is that you know why how do we have ai governance like why does this exist why do we care about about it it really is because um if you're especially if you're in a kind of like a in an industry that has some sort of Regulation or compliance concerns you know we really need to have an understand how this technology is used what kind of accountability and responsibility we need to take for AI actions you know how do we actually put provide safeguards for privacy protecting personal data uh in particular in this kind of like World of like what are the kind of guard rails that we need to build to ensure robustness safety and risk assessments of these AI systems and this is not about um you know kind of fear-mongering in any way this is literally about like how do we become tactical and practical about applying these kind of principles and these uh you know in a way um that you know is is useful in that we can use this technology which is some of the greatest technology that we'll probably see in our lifetime uh you know to uh things that really matter you know like medicine and the government and financial services you know like things that really kind of like move forward Humanity in one way uh we need to come up with these systems so that we can use them in a proper way and a lot of the governance principles are really have a historical content of you know why why first of all why does it exist super rapid advancement in a technology I think anybody on this on this conference can probably attest to that uh but also it's actually building on traditional governance Frameworks things that already exist in it which are really about like how do we do security how do we actually understand that we're strategically investing in the right things how do we look at uh you know the ROI of our investments versus the risk that we take um and this is really about kind of like prioritizing compliance fairness transparency and accountability of AI systems so one of the things is like if we go look at I think calling old ml is not the right thing but like previous two foundational models right kind of how AI governance workflows uh really were really about right and they have a couple categories that really matter especially if you were in financial services or in government or not you know kind of like life sciences you you know we're usually required or should have had a um the um you know complete catalog of models uh which would had uh you know model risk documentation description of the sources uh training predictions how you got there what methodologies you used you know you were usually looking for hopefully having a flexible uh model risk management framework which kind of give you a gradient of like this is a high-risk model hey we're using this for you know credit decisions versus things that were like didn't really matter it's a model that we're using maybe for you know kind of marketing decisions where like you know they need at least at least scrutiny so understanding like what level of scrutiny you need to put on a Model perspective uh really mattered you know but you looked at kind of like where the data came from how it was developed what algorithms were used you know to develop it how you trained it um you looked at you know how you would have an efficient process you know really around ml Ops to integrating into Legacy systems and data architectures of those models um what you look at how kind of like it kind of generated tooling to be able to operate manage monitor some of these models and and their Health in production and you know you would look at the kind of like tools to model that model accuracy and data consistency uh you look at why the model changed if it changed and you would record those things and then you'd try to have some level of standard audit and Report logs around like what's happening with these models who's using them how they're being accessed Etc and and so that you could provide that especially if you were in financial services and you might have Auditors or you might have a uh you know some level of being able to have to give this out so the interesting thing if you look at these uh you know kind of like you know is it's very model Centric right it was really about how did we get to this model how we built this model how did we put the like the elements that we put into that model and some of this actually like actually doesn't really translate um into the new llm world and I'll explain why so some of the you know challenges that the traditional AI governments are going to have right so there's just like a lack of documentation of how we actually trained and these models and where these data sources came from and it doesn't necessarily mean that you need that but like I'm just saying like it's very hard to actually have provenance of saying hey I know exactly where all these data sets came from where we actually got that data how it was actually trained and how we got to that um we see this kind of like uniform model based approach of risk management like is this a high degree of model a risk model is this a low degree model well in this case we're Sometimes using the same foundational models or these large language models and we are maybe using them in different instances and in different they're not fit for purpose they're more generalized so that kind of framework no longer applies to the world that we're looking at um we're looking at how to integrate these into uh existing uh you know into existing Frameworks into existing Legacy systems and this is actually a fairly complex thing to do especially today with a specialized Hardware that's needed and how you integrate these capabilities into those Legacy systems um the tools for operating llms are changing as we can see you know in like half of the talks in this conference and so how are we actually adapting those for these new kind of Frameworks uh for a guy governance uh really matters and finally we're looking at you know how do we actually monitor accuracy and data consistency like does that really matter what does it mean to actually have drift or degrading input qualities in Alm World um when there's actually changes that we're fine-tuning with these uh you know uh you know components like what does it mean for that like how like how are we tracking those how are we including that in our usage and then what does it mean to do auditing and accountability so these are all challenges that really kind of present themselves when looking at alums but the one thing is like does that mean we should not use llms absolutely not right it means that we need to start thinking a little bit differently about how are we going to evolve uh you know into these Frameworks and how do we can actually continue using it because one of the things that's for sure is that the LMS are some of the most powerful machine learning components that we've actually seen um in the past you know number of decades and we absolutely want to be able to use them so we just have to find how to evolve these Frameworks into a world that we can actually use them and really just understand the risks and where there's a risk and there's not a risk around that so some suggestions uh in this very quick talk about what you can look at um is you know how do we think about cataloging where we use the workflow so it used to be about like what models we have and how we're using them and I think it's much more important right now to understand the the catalog the workflow right if we're going to be using llms let's look at the entire workflow right and what versions of what models we're using from the different providers but also understanding like how we're using them and for what components we're using them right we should include in that documentation what guardrails we're actually building around the calls of each one of those models and identify any risks that we see for that component but then also extracting into that full workflow you know we really want to focus Less on the how the model was built that doesn't really get us anywhere and more on the overarching risk of that workflow right is this something that actually could go wrong where can it can actually go wrong and try to understand that right so the type of error the cost of the errors and the frequency of the errors is what matters but at the workflow level so then we actually want to think about risk management right and say okay well how do we apply that at the full workflow versus just at the model okay so let's document the potential failure modes right of the workflow and this is the part that you can actually understand if you actually look at if it's actually look at the box and say okay this is the box of risk if we can Define the outer bounds we can't really understand where exactly it's going to fall because you know these are stochastic methods but we can understand the outer bounds of where they might go off and document that and then accept or reject those boundaries for each workflow and understand what risk we're taking in each one of those we want to be able to constantly uh you know do constant change management validation so as llms and especially when we're using external providers right you know are providing frequent fine tuning and refinements how does changing a provider fine-tuning on new data or changing versions of a model affect that workflow and finally implementing and documenting guardrails that add structure type and quality guarantees to those LMS use so everybody's been talking about hallucinations and you know what the problem that is but like actually validating the correctness or not of a workflow and what correct this is a little bit of an overloaded term but how do we understand is the llm providing the proper out you know out for that component the proper structured output the tougher type the proper quality and guarantees for them and how can we actually provide more importantly you know corrective action when we actually started thinking about hey this can go wrong we have defined that boundary and now this is how we're actually going to go in and uh you know and Define the corrective action this becomes particularly important so the second ability to fall back on documented past for human experts is really an other one so even when we go into a world where you know we have llm-powered workflows we should understand the and have the ability to fall back on a documented path of human experts uh when the workflows can't provide certain quality guarantees so how are we actually going to understand hey we can actually not automate this we actually need to fall back on something and provide those Pathways can be super important for this net new governments framework and then finally again going back to like what is the standard audit report and logs look like if I can actually go in and understand every single time I have put in implemented guardrails into my organ into my llm workflow again multiple guard rails multiple components we're looking at the workflow level right every single time we've seen an error state or we've had to provide corrective action we should log that every single time we've got to correct uh you know kind of like component we should log that as well so as we can actually create these audits reports and and logs you know we can actually review these workflows this is how we would actually get not only Regulators you know kind of content with you know the level of uh scrutiny that we have for these models and but it also actually shows us that the boundaries that we've suggested and how we've suggested those and the corrective actions plus the human interventions are noted and this will actually just increase that Assurance around those AI workflows ultimately the power of llms is there and what we really want to do is increase AI Assurance of those workflows so that we can use them more and more in the components we want so with that said I think my time is very much up uh I really appreciate you taking uh some time to listen in today and uh if you have any questions I'll be in the slack or can hit me up on Twitter or on LinkedIn awesome thank you so much Dan works for incredibly helpful and I know I personally am really looking forward to going back through the slides and the recording once it's out there and published so thank you so much okay happy now cool all right [Music]