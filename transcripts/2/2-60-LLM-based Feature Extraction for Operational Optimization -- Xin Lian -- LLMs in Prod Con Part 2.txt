all right so we have come to our last Talk of the day it's not over yet but I am getting a little emotional already and where is Yin oh there you are you're already in tomorrow huh yes I mean excuse me I'm in the future you are it's already tomorrow where you are I can't believe it and uh anything interesting happen in the future yet um it's a nice day in Melbourne I'm based in Australia so that's what I am in the future so um good things will happen that's what I'm talking about that is so cool and so I'm gonna let you take over and bring us home um this is very cool to have you here and thank you for waking up early because I know it is a Saturday where you are and you're waking up early on a Saturday to do this so that shows some serious commitment and I appreciate it and I'll let you I see your screen is shared here so I'll get off the stage and let you jump right into it thank you thank you very much and hello everyone good morning good afternoon or good evening depending on where you are in the world I'm Sheen from Canberra and today I'm going to talk about using LMS as a feature extraction method for operational optimization a bit about myself I'm engineering machine learning within canva in the content and Discovery area I haven't been playing around with um lateral language processing computer vision and ml Ops for a while now for those who is not familiar with canva just yet it is an online design platform aiming for everyone to design everything and it's how these slides are made so LMS or large language models that we have been talking about in the last two days and it um is the hottest kid in town right now a lot of Hypes and a lot of discussions are centered around them and what can we do with them LMS have revolutionized AI breaking down entry barriers lots of Cutting Edge um AI applications and we have seen many applications built on top of LM solely copywriting software chatbot coding assistant pay a programming body tooling to understand you know lengthy legal documents the list can keep going forever that is if you give llms the input and then it will generate the desired output however another way to utilize llms Which is less talk about is how to embed llms into existing Technologies whether it's to improve existing mlai Technologies or introduce mlai capability into existing Technologies so simply put elements can be used as a middle layer reaching any upstreams and downstream tasks and this is what we are focusing on today LM based feature extraction for Downstream tasks but why do we use llms as a middle layer in such as you know building applications on top of it solely because we found that it can provide Solutions with higher performance and accuracy and a greater philosophy and at reduced cost sounds too good to be true I will use two case studies to illustrate my points here so one of the use cases for llms in canva is for informational categorization we have a variety of contents to form our information architecture from design template media assets to informational articles and we would like to organize our content and information in a way that is most suitable to our information architecture or for categorization system this categorization can be applied to various areas within canva content and information including our user search query um to categorize them and understand users interests in our content and another area is our contents page for easy internal retrieval so let's take a look at our first case study user search query categorization internally we would like to understand what users interests are in our contents by aggregating them and then grouping them into different categories our content has a tree structure like this and when we categorize user search queries there's a series of steps to funnel these queries into different branches of our content structure and a categorization models would be developed for each step or node of this tree structure to funnel the search queries into different categories three for example an intense classification is needed as at the first layer to class classify aggregated queries into different intents intent in this context is about what type of content users are searching for in our architecture for example some users search queries about template some of our feature and then the others so even though it was a straightforward test classification problem with a few classes it still needs to go through the full model development cycle to develop this classifier in a more traditional ml route so when anal names came out and our intense classes increased we thought it would be worth evaluating LMS for this task in comparison to our existing classifier and here are what we need to do to capture the text intent into in these two approaches respectively the first one is this single purpose text intense classifier and it requires the full development workflow including to collect a large amount of label data tens of thousands of data points and set up the training infrastructure to train the tax intense classifier and then set up the deployment infrastructure to deploy the trained classifier then we can extract tax intents feature at inference time with Airlines API as text intense classifier we only need to collect a handful of examples under 10 examples and then we need to design the Trump structure to Output the predefined classes and then at inference time we could use field short learning with a few examples that we've lost previously um and to extract the tax intent feature at inference time so this process is much more simplified no training and deployment infra setup and with a much smaller annotated data set so in terms of development time timeline these are the timelines for the two approaches in comparison and talk about four weeks for us to develop this single purpose classifier including you know one book starter collection exploration and then the intent classifier training inference model development and then deploy the classified well it took about a week to achieve the same task using lm's apis and it significantly shortened the development process and timeline especially for straightforward tasks on Tech starters such as the intense classification in our use case and this is the overview on the operational effort for both approaches the um for the single purpose intense classifier end-to-end development time for weeks and that of lm's apis as well for eight to five days in terms of operation cost for the single purpose intense classifier it costs about 100 per month while that of um the LMS apis is roughly less than five dollars per month in our case study and operation course is based on the you know available public pricing on the vendor website respectively and in terms of the scale um and the operation of our jobs it is a scheduled jobs running weekly with hundreds and thousands of inputs so from an accuracy perspective llm's apis is higher um than the single purpose intent classified in our case study with only few short learning no fine-tuning involved and here are some takeaways from this use case of us evaluating the LMS when to use lens API it is effective on straightforward tests uh tasks on Text data and it helps for prototyping and speed up the solution launch of the first iteration to gather more production information and because effective when the scale meet the cost saving advantages which it does in our case studies The Prompt engineer prompts design future learning can be sufficient to show custom logic on the text tasks instead of like you know any fine tuning required and it is important to consider the standardization of the completion format whether it is a Json format or restructure a prompt to um have the finance set of answers and only to Output the answer in terms of error medication LMS apis do have rate limits so to handle hitting the raise limits throttling estimated with mechanism and it's also good to have a full back solution when possible to mitigate the negative impact of API downtime in under fine tuning as I mentioned field short learning is good for Rapid solution release which is you know in our um case at first iteration however when there is a scale on the data and or the custom logic fine tunings might be needed to maintain both performance and cost effectiveness and we found that small training data sets are sufficient for fine-tuning roughly about 50 to 100 data points per class in our cases in our use case which would give us acceptable performance and this is our first case studies to illustrate you know the operational efficiency and optimization with our lens apis in production our second case studies is on the content page categorization and we have contents pages of various characteristics from the pages within a collection of similar templates with short form metadata text to an informational articles with long form texts and we would like to group our content pages together based on their relevance and proximity in our information architecture so essentially we like to group our pages together um based on the semantic similarity into different topics clusters due to the vast difference in the text lens and content information and metadata among these Pages variations of further text feature extractions are necessary using free llm natural language techniques such as like keyword extraction with a keyword extraction specific python Library text summarization with you know assisting model architecture or libraries to do so and then to adopt um keypoint.point extraction to further distribute these key information from these different uh variations of the pages into a similar text forms and then we can convert the text of similar forms into the embedding space before we're categorizing and grouping relevant pages together using these features and this feature extraction requires different methods and Frameworks and libraries as I mentioned which is a pretty you know scattered rather than a one methods to go to and do everything for us so when llms came out and the variations of our content Pages increased we thought it would be worth evaluating llms for this task and see if it can simplify the feature extraction steps all at once so we have experimented with different feature extraction method with the combination of Open Source embeddings such as the sentence Transformers birds embeddings or the llms in buildings and these are the metrics for our contents page categorization um using different text and feature extraction methods we Define three metrics to evaluate the performance of this task balance to indicate how even the pages are grouped across old pages in scope ins to see happens they are instead of a concentrated set of pages in all groups completion indicate the percentage of these pages in scope are grouped into relevant topics or classes instead of being left out as outliers and coherence as the way um that the pages grouped together is coherent and sensible instead of non-related pages being grouped together so we experiment um all these different methods and combinations of with the embeddings and it turns out that llm embeddings on Plain page text without any text feature transformation gives the best outcome it achieves the most um balanced grouping and it grouped 89 of the pages into relevant categories which is the highest among all the other methods and you can see here also it achieves the highest coherence score among other feature extraction methods and this is also the overview on operational effort for both approaches so for the um single purpose methods Plus open embeddings the end-to-end development time is about two weeks it's only for the feature extraction step while that of LMS embeddings about three to five days the operation costs um for the pre-alion methods is about three dollars per month while um as part of you know the training steps that's the training you know uh cost and the that of the LMS in Balance will be about one third of that and as we discussed in the previous slide that um the metrics for this task actually um llms embeddings can achieve you know um the highest schools for all the metrics defined for this task so in these case studies we also have a feel you know take away learnings using LMS as the future extraction in terms of feature variations um since it's a it's one foundational model to perform various text feature transformation and extraction such as keyword extraction text summarization a significantly simplified development process on the other hand the text feature extraction can be non-deterministic depends on you know your configuration and settings um so the Milling that's the output of the llms can be slightly different every time you put even though you put the same input in so therefore the suitability of which depends on use use cases and obviously in our use case this is not a problem and in terms of embeddings we found that Island embeddings appear to represent the text input um better than other available text in balance and the format and the length of the text inputs doesn't seem to affect the semantics understanding much in the llm embedding space in the future we will also can see the open source llms which can be utilized for text embeddings and other feature extraction tasks when students and potentially lower the course even further at scale in production so these are the two case studies where we evaluated LMS as a feature extraction method to understand its operational performance better and we concluded that llms are performed other methods on the natural language tasks and text feature transformation and extraction tasks in our case studies with minimum processing and logic required the utilization of lm's API in our use cases also simplifies the development process and reduces the development effort and significantly shortens the timeline as well the cost of llm's API is model dependent however in our use cases and appears to have lower operational costs for both model training and inference and I hope and with the above we found that llms based on feature extraction can be operationally optimal especially for Rapid solution productionization and I hope these two case studies illustrate you know this point um enough for you to give it a go and see whether it actually helped your development process and that's it thank you and I hope you enjoyed my talk oh so good what a closing you did not disappoint and there are some awesome questions coming through here in the chat which is how you know it is good uh because if we haven't even finished and I didn't even have to ask for questions and they already are coming through excellent quality here so there is uh there's an awesome question about which API was used when you were doing these llm embeddings yeah um oh in terms of the llms in beddings um we use the um open AI API um and with the the text a the B2 model which appears to be the most performance and the most cost effective nice so um what do you think about the use case or use cases we're processing millions of samples daily is a requirement any thoughts on how you would go about that kind of workload yeah yeah so I guess for millions of um codes that's at scale um and in our case there's two ways to go up on this the other case is that um we when we do you know the users search queries categorization what we did to customer minimize or like reduce the actual volume of the data is to do the aggregation if possible in our case again our search queries are also at you know at the magnitude of like Millions per day however we are able to kind of do an aggregation and do some of you know the uh pre-processing to you know to do some a little bit of a hair lifting work to kind of like um group them aggregate them together to reduce the volume and then use you know LMS to do Downstream tasks um if that's possible to reduce the volume and LM still you know provide you a simple uh simplified um uh methods to do the processing that you know there's one way to go on the other hand if like you know that's not a viable approach then I would probably look into as I mentioned in the learnings and Second Use case probably would look into the um open source LMS and see how to utilize it within you know your infrastructure and hopefully that could set up your you know open source um LMS um and then you have an in-house API and that hopefully can provide you with you know and imbalance and source and that could also be utilized with the you know your organization um that way you with like a little bit more you know upfront setups and hopefully you know um down the track it will pay off without you know the the API cost with external ideas nice so answer the question there yeah there's some awesome questions coming through here and that definitely answered the question uh I'm wondering about all right so Memphis is asking did you also try coherent beddings was their multilingual content in your test yeah so our Focus um is so this is like in the in the use case um and I did try the um I did try them with like multi um different languages probably not super into the the in in the imbalance how when I tried the um you know the modern language essentially is to operate only operate in the text space so that I could you know input text and now get the different um uh sorry the output in the relevant languages and then I use the open source embeddings um to convert the the text in the you know in other languages into embeddings so I did explore that way um as the you know in the initial experiment but we found that the I mean the yeah the element embeddings on Plain text is the way to go and our country scope is more on the English side so I did not um thoroughly experimented with the modern language um in terms of the embeddings but for the text uh more like the translation or like the localization of the in only in the text space I think that still works okay for us in our in my exploration so I don't think I told you but all of the material and all of the creative that we have for this conference and specifically your virtual background all made with canva so beautiful yeah yeah I have to say that right now yeah it's super awesome we love it and now I got a hard question for you coming through from the chat and you may or may not want to answer this depending on how confident you're feeling with the pr team so canva had a major data breach a few years back has that influenced the way that you work or your current direction in any way well I believe the um bleach that um we mentioned is probably in a few years back up probably more than four or five years now um and I think um we would definitely you know we you know definitely experience like you know different things and then we learn from it um and that definitely you know especially you know from a security of a security perspective definitely learn a lot from them and since then with different things you know strengthen um our you know um from organizational and I was like um the technical perspective suffering just strengthened you know the area around your security and I believe since then there's no right after that so definitely it's like yeah definitely learning experience from us and we learned a lot from it and I think we kind of you know handle and eat well um since then for sure awesome so this has been super fun and I thank you so much for coming on here and talking to us and just sharing what you've been up to at canva as I mentioned we're huge fans the the small team that we have we love using canva for everything design and it's almost gotten to the point where uh I don't even know why I'm paying for Adobe anymore don't quote me on that but Ken was the way to go every month I'm like do I need my Photoshop subscription do I need my Adobe Cloud uh subscription and yeah I think there there are so many pluses it's just so easy to use canva and that's a testament for what you all are doing and how you are making it intuitive and I can tell by the way that you're thinking about how to use llms you're really thinking about how you can make the AI aspect of canva also intuitive definitely yeah that's definitely and I'm glad I'm glad that um you know canva is being adopted with you know the community and it's like it's easy to use and you know design everything for everyone and I'm glad that um our mission is you know on the right track yes yes all right well awesome thank you for coming thank you for presenting this has been amazing you have a beautiful new day to begin and I have a beautiful night to go to sleep thank you for having me it's been fun it's been great life-wise I'll see you later and with that I'm gonna close it out [Music]