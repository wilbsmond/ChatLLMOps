friend dimma is up next and for those who do not know I've had dimma on the podcast before and he has been doing some incredible stuff but he had where there he is I found him now where where is he can we throw him up on here there he is what's up dude hello thanks for having me so I just got to give you a quick intro because for those who do not know you were heavily heavily involved in pytorch and the pytorch ecosystem still are kind of and you also were one of the creators or the creator of Onyx which I'm pretty sure everybody at this conference has heard of and if that is not a track record I do not know what is okay thank you thank you for having me and yeah I am Dima or full version is ditro which I think is actually the same name is dius just best name ever best name ever created I know it yeah so thanks for having me and as since for the intro so I I was doing a lot of PCH and framework stuff and I'm still do but since recently I confounded company called fireworks Ai and today I want to talk about our platform and also some aspects of patterns how we see different organizations kind of serve llms from and challenges the experience as they go from experimentation to production phases so when looking at experimentation I think one Trend which is pretty obvious is there is a lot of popularity find lately and if you look at kind of how people has been using llms there's this transition from just prompt engineering to find you in for several reasons one it for a lot of business tasks it actually gives you more control over the output because you can c m to dark at hand uh and often there's only that much can do by feeding a few short examples into the prompt for existing P train model whereas this funion kind of opens up your broader breath on making the model do what you want surprisingly it also helps you a lot with serving cost because you can have shorter promps which means that your requ will be served both faster and cheaper and often you can get by by much smaller model for the same quality where you know something like 70 billion model might be required for good prun you might be able to generate with it good uh training data set and find like 7 billion model which sols your more narrow expertise case much better another surprising reason why f is popular especially in bigger organization is actually how to make multiple Engineers work effectively on different tasks if you look at traditional ml then usually there's like if you have multiple tasks you would usually paralyze in between Engineers he would give everyone one t they would build a model he would deploys production this Foundation models you have this okay I have like one model but many tasks uh if I use it as this it might be good but if I want to customize um and tun for for different outcomes how do I paralyze multiple people collaborating on it so the interesting pattern which we see in bigger organization is that uh people would start this bigger model and kind of bran up different experiments through it create different F Tunes either for particular production task deployments or maybe for some experiment they that they want to run allowing them not to step on each other's toes and kind of run actual production tests on Fain food models in parallel with periodically taking all the best discoveries and merging them back and maybe doing like a single big fine tune or combining data sets and actually like retraining some the base model from scratch this pattern is surprisingly common I mean I used to work at meta before actually even ear before like LMS and Rin recommendation systems which power like Facebook and Instagram and there of like that was a common pattern how like hundreds of Engineers would collaborate for on a single model uh while kind f for so many different kind of use cases and spe specializations uh from what I heard from friends at open AI they kind of have similar process of find for different tasks as it develops the models and once in a while they you know merged back into one GPT n plus one uh which gets released to the world uh if you're trying to do funion uh you you can do full funion uh which is often expensive and tricky today get right on small data sets if you were following any H pH you know that parameter efficient F unit is all the range where you can take your inference you take your main model keep it in inference mode and learn only some parameters and over time people discovered so many ways of kind of doing this one particular trick which again is pretty is pretty popular is called blor which you probably heard of uh this the idea is that you would apply small adapter to every layer in the model and uh tune only that M set of parameters while still using fewer gpus to the training and if you have low adapter it allows you to also serve it more efficiently at run time for deployment so this so the pattern which we see L of experimentation is kind of people fion a lot of adapter often parameter F Union the question is how do we serve it for uh for productions for real applications without breaking the bank if you look at some of the offerings uh on on different platforms for example open a Pion it actually often comes with pretty high premium for for example there's like aex premium F GPT 3.5 turbo and you might ask why is that like why why there is such such big increase that actually comes to some uh like under of underlying kind of implementation details of how people serve llms I think Tim was talking about some of that in his Mr talk earlier this morning but fundamentally like LM inference and like training is really the bottleneck on readin from memory and that's actually the reason why like clama CVP running on your laptop works pretty well because memory laptops is not as much slower than gpus compared with computer stut so if you really want to use those gpus and you want to kind of have good utilization and hence low cost you need to send a lot of requests to a single model in parallel and kind of BGE them together for parallel processing good R of samp is that you know sending like 16x parallel requests on usually on models is only well like 50% slower so given us like 10x efficiency Improvement that's how all works really great if you have like one big model and you're trying to send a lot of traffic to it however if you have this Fain TW pattern which you want to get overs and you maybe have dozens of hundreds of different model variants flying around often being spun up or scale down uh that doesn't work work out very well efficiency wise all you can do is basically allocate separate gpus which don't really serve much traffic but you still pay in the bill for them luckily it turns out that F and Laura specifically can help us with that so the one trick you can do is uh and which we're doing as part of our platform is hold cross model matching uh so if you try to uh if you have multiple model variants which were trained still on the same base model uh you can kind of Be Clever and deploy them on the same on the same GPU uh draw all the traffic for different model variants to the same to the same cost and uh using clever implementation allow it to run the bch of multiple requests only which only differ in the lower adapters that's still getting you good computer efficiency and hence uh lower cost while allowing you to customize models uh pretty flexibly and kind of experiment and enable new variants specifically it also means that if you have new mod variant which just fine tune you can just upload it to platform get deployed on the same Hardware uh in pretty much matter of seconds without kind of having to go through exercise of allocating new hardware and increasing your B so uh this cross cross which we deployed you can go and try it out now by grabbing uh grabbing a model which you find to upload into our platform uh it allows us to pass those cost savings down to you so specifically for like our pricing model is similar to other providers per token but it but we our price for per talken for f models is the same as for the base models so s to this optimizations also uh we Appo best best state of thee art inference optimizations for llms uh such as multi tension various types of BS disaggregating computes and optimizing Hardware where Hardware set up for part for particular R Cas which means that end to end cost of deploying models and serving them per token end up being or of magnitude lower than what you would end up with trying to deploy fun or even based models on various platforms where you have to pay for GPU hour allocation so uh so with that uh I want to again uh encourage you to If You're Building LMS and if you're using open source models or if you're trying to find tune encourage you to try out fireworks AI we have wide selection of best open source models available which you can go and play this right away running this uh very fast Best in Class inance engine uh and uh uh and also also in terms of integration we are open open AI compatible which means that we can use existing Integrations or just it just change the based URL in even existing SDK for uh we are good friends L chain so trans harison for for helping to integrate fireworks there too so actually playround Integrations for LMI right now powered by fireworks and of course you can use any kind of SDK or Integrations you like to call the rest API for fireworks uh so with with that I conclude this talk kind of where we talked about how Model F Union uh really empowers experimentation phase and how some of the some of the tricks which you can do for more cost efficient and fast serving of those many fun variants in inference uh all of those is deployed at firewor so please uh follow us on Twitter or try to uh try out some of those models on on our website and good news is also we have three CR for developers so feel free to try try it out his on pained down with that I want to thank you for listening and yeah the mat back back to you great news you got free credits for developers I'll tell you what don't say that too loud people are definitely going to take you up on it so we'll drop the links in the chat for all those that are interested and dimma it's always a pleasure talking to you man I love the work that you're doing and I appreciate you coming on here and giving us this lightning talk and I hope that we can catch up some time very soon to have you on the podcast or something [Music] again