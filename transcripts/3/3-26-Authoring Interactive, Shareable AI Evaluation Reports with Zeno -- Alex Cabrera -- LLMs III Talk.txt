[Music] and next we have a talk with Alex Alex I believe you're going to be our final discussion today right um and you have slides that you want to put up yes and hopefully a demo but we'll see demo okay let's see yeah thank you guys so much for having me thank you Adam um glad to be excited to be closing out the the conference um thank you for all you guys who stayed my favorite part was definitely Demetrius dancing in the in the missions um it was quite awesome so take it away cool so yeah today I'm excited to talk about Zeno and and we specifically um kind of a an interesting take we have on evaluation we had a bunch of interesting talks today on on evaluation and all the challenges associated with it um so excited to add our own perspective here um before we get into it I I want to shout out our awesome team um we're all researchers at we have undergraduate students graduate students professors we've all been um contributing to this everything from designers to ml Engineers so um it's a a really great team that that I'm a part of um I want to just give a brief little overview of how we've been seeing the evaluation landscape today for llms and generative AI um so this might be a brief overview but kind of the buckets that that we've been seeing um one is the most classic AI valuation or benchmarks um these are often large cated sets of data uh for a bunch of different tasks and a recent Trend we've seen is these kind of agglomerations of tasks things like Helm or open AI vals which basically combine potentially thousands of different tasks um to try to test every possible aspect of how your particularly llm is performing um what tends to happen more often in the real world is human evaluation um there often aren't big Benchmark sets for the specific tasks that people have like retrieval augmented Generation Um so a lot of people just depend on human feedback this can be a developer just spot checking instances sometimes you'll ask your users to give you feedback like a like or dislike um or you might even ask users to do head-to-head comparison there's an awesome project by the ml LM CIS group um on the chatbot Arena where they had people pick between two different chatbot outputs um and lastly um an area that's getting a lot of traction um I know the L chain group's been working on this and a lot of cool stuff here on doing modelbased evaluation um and I I kind of split this into two groups one is kind of the llm data Generation Um and this is where you're actually just generating input data like ask GPD 4 to ask a bunch of questions like a user would ask um and then the other side that people are are testing out are actual metrics um so given that you have some input output pairs ask a bigger llm maybe gbd4 to grade the output um you can grade it on things like fluency or factuality or to try to kind of approximate an evaluation instead of a human so those are three main buckets that we've seen of how people approaching evaluation and I'm not really going to dive into the details but there are a ton of limitations and issues with these that people are trying to tackle um classic ones for benchmarks is that they're not representative Ral tasks sometimes there's data leakage so people will train the models on um the actual evaluation sets this is happening a lot with the open llm um leaderboard that that hugging face has which has been interesting and then for human feedback and modelbased evaluation these are often just very expensive and slow um for human feedback you can't do model comparisons so if you have a bunch of feedback for one model and then you update that model how can you compare you have to put it back in the world and get more feedback um and in the classic modelbased evaluation um the models if you're using a model EV a model to evaluator can also hallucinate or make mistakes have this all the same limitations that your base model has um so there are a ton of open questions designing these evaluation sets um but that's not really what I about today interestingly enough uh I I think there's this parallel side of evaluation that isn't as deeply explored given that you have some evaluation set whether it's a benchmark or you gather a bunch of user data or generated it with LMS there is the actual result that you report and use to make decisions or to update your model and across the board almost always the results of these models are reduced to a single metric or a single number so we actually look at a lot of the most common um uh leaderboards or evaluation results everything from the LM leaderboard I mentioned to the chatbot Arena to helm papers with code they will always almost report the results in a table with a single aggregate metric maybe they'll split it across a couple tasks um but that's almost everything that you get to look at it's like hey this model is better because it has this number is a higher number than the number for the other models but I want to kind of double down and I think this is a very lit of of understanding our model performance um and I want to kind of talk about this in one example of audio transcription so say right now I'm talking and we were doing live transcription of my voice into text um we'd maybe be picking between a few models and looking at the average word eror rate across a data set how often it's getting the the word transcribed correctly but if you're actually creating a real product you have a lot of other questions you probably want to answer um does it work for non-english language if you're using the transcription model in different languages if there's a lot of background noise or people are speaking very quietly um these are things that you really want to know um and as a product designer product engineer you have to decide hey this model is much better for English speakers but doesn't work for other languages but given the demographic of my users maybe that's okay you need this information to make inform choices and the the simple uh takeaway here is that these single aggregate metrics even if it's generated by an l even if it's a bunch of human feedback it's just insufficient and I have a little analogy that that I'm testing out here see if it lands or works is kind of teachers and students when a teacher is trying to evaluate how well a student knows a subject they'll give them an exam have them fill it out they get the exam back they can actually look through the exam to see how the the student maybe did reasoning or tried to solve a certain problem or what types of questions the student getting wrong so they can tailor their teaching and improve their teaching developers do something similar with their LMS they'll give it example look at maybe inputs and outputs um they'll give it a benchmark to try to understand the limitations of their model to either improve it or pick a different model what happens though is we limit the information we get back by just having that single metric that single value we get back so instead of like having that whole exam back and looking at the specific questions or input data we just look at this overall metric that makes it very hard to actually decide in what ways our model is limited and what ways we might want to improve I think our our hypothesis that that that we've been noodling on for a while now um is that we really need better interfaces to unlock the insights in these evaluation sets um so there's a lot of Rich data and information in the evaluation sets um that's often not the limitation a lot of times it's actually the interface we have to make sense of that data so that's what we've been trying to accomplish with with the Xeno project um it's this o Open Source AI evaluation platform we've been building over the past year um and it has kind of Three core asps ects but the one that I really want to talk about and really what we think is a really promising direction are these interactive reports of analysis so I'll give hopefully this demo Works um if you want to follow along the the two demos I'll be showing are are live on on our Hub um so you can click around if you want to so the first principle we have that that we think is super important is actually looking at your data so this is if if you go to the The Hub home um I'm looking at the audio transcription example this is essentially the example I was just talking about um and this is what a typical Xeno project looks like so here we're looking at a very interesting data set a bunch of people saying the same exact phrase what's interesting is they're people from all over the world with different experiences speaking English so you could look through the data and listen to audio if you want and here you see the label is all the same because it's the same base but here we have the audio transcription output in this case we're comparing a bunch of the whisper models open AI state-ofthe-art transcription models that a lot of people are using so we're looking at some of the smaller models with some of the bigger slower models the base models um so we can start looking it looks like some of these transcriptions are pretty good for the tiny model some are very odd like transcribing in the wrong text the wrong language but what's more interesting is on the left hand side here we have all the metadata associated with our data set um so every instance has the speaker age or when they learned English um or where they're from so we can kind of see these bars are colored by the word error rate so we can clearly start to see some of these more nuanced patterns in our data um so we can see for example that our model has a much higher word a rate for speakers from Asia so we could actually click and interactively filter our data um versus speakers from for example uh North America so you click on North America and it would dynamically update and filter and we could even cross filter so we could say okay what about speakers from North America who learned English later in life um and then we can see we have a much higher word a rate. 34 than than average um so you can start kind of finding these intersectional insights of how your models performing the specific edge cases um maybe that are just edge cases that you care about or use cases that you care about or maybe complaints from users Etc and we we think that the the the magic here is that the only part that's really data specific is this instance view this this how we render the data um and Zeno makes it really easy to add your own instance View use for specific data types so we have a classic uh openai uh API render for seeing chat conversations including system messages say of looking at a conversations from Ins Insurance Agents that you could do the same so the filtering is the same so we could look um say at the chat context length so looking at like very short um conversations or conversations that are very early on um or any type of of ablation or or test that You' want to do we have other crazy ones this is looking at 2 million uh prompt image pairs from stable diffusion so here we don't have a metric that we can really look at as the quality of the image but we can do some interesting analysis like calculating the average not safe for work level of the images and we can start seeing patterns like im images that with the prompt generated by The Prompt that has the word girl in it have a much higher not safeer work level than those that have boy um so this is a very interesting um Insight in potential biases us in your data where there might be more um implicit biases of of um the types of images that are generated for prompts with girl than boy um which might be interesting lastly we have other Crazy Ones like looking at um this is IMU sensor data from iPhones um we don't have a model associated with it but we could look at what does it look like to jog U versus what does it look like for someone sitting um start getting some idea of what the the the sensor data looks like so that's kind of the first key principle we have that that kind of Zeno lets you do is look at your data specifically look at the model outputs start getting a sense a qualitative sense of how your models performing I think this is something that's super important to actually get a develop High performing um models that actually work well for your Downstream test got the next principle that that we look at that super important is actually quantifying these behaviors it's not enough to just get one-off insights of how your model's doing you really want to come up with reproducible tests of of your model's behavior and model performance um so we can go back here let's look at this other example that we have the um this gbt Mt Benchmark um and this is looking at a bunch of different translation models um specifically comparing kind of state-of-the-art translation um with kind of these open source or sorry llm based translation models um we can do the same we can look at how the models are performing across different languages um but maybe we see that hey our model's doing really well for um friend um and we want to compare specifically how our model is doing French versus say Romanian so we could actually create a specific slice for this and be like hey language French um you could create any intersectional group you want like French speaker or language equals French where the text is very short if you wanted to and create a specific slice so now we could compare say language French across different models like um gbd4 versus chat GPT um and we see the accuracy see is slightly so it's a little Annoying to have to go back and forth and create change what model you're looking at maybe change what metric you're looking at across these things um so this is where our chart creation feature comes in um we think it's super important to actually create these interactive visualizations that you quantify these complex behaviors uh so we can go in and we have some very simple charts like bar charts um here we're looking at the overall performance across different systems for all your instances um and you could create different typ of charts if you wanted to you could look at different slices of your data so how do we do for all instances versus short Lael instances um looks like it's about the same pattern um nothing that different but you can start kind of interactively changing let's not look at the stateof the art and let's only look at at at gbd3 or gbd4 um in the charts the same as with the other features doesn't depend on the data you can create any slice you want um create kind of any visualization you want um we have another one that I like is looking at this this spider radar chart that have gotten kind of popular with looking at LMS across a bunch of these Dimensions um and this is actually looking at the script of the language so like Latin script versus Georgian script what's really interesting here is for some reason cilc script all the models do the same um no idea why we talked to some of the people running these analysis that are more familiar with Linguistics they also didn't know um so a lot of interesting behaviors that you can start exploring and start thinking about um cool so that's kind of the the next step is look at your data start quantifying these behaviors so you can actually do formal comparison between models um and lastly is telling stories and this is something that we've been thinking about more recently um is how do you actually just make sense of all the ways in which your models can perform it's very hard there just so many dimensions of quality depends on your users on your use case on what they're using it for um and there's a really rich history of kind of interactive data storytelling um from the world just the kind of data visualization and data analysis um you've probably seen some of the New York Times as upshot articles that are interactive that have simulations um you might have seen distill Pub that was unfortunately discontinued but it has some awesome explaines on machine learning um or our world in data is a really popular one creating visualization so we took inspiration from this um and I've been thinking about how we can tell stories about how our models are performing so what we have is our AR of reports that you can create that are grounded directly in your data and in your model performance so here we created a report about this these translation models have some markdown some description of the of the model but then we actually have the charts that are directly pulled from our project so we can go straight from just the raw data raw input to these nice interactive charts to these interactive reports that we can share um so this actually before it was talking about the overall performance and only had the all instances but it's actually when and edited that chart it live updated in the so we can dive deep and start understanding why are llms um worse than state-of-the-art translation models turns out a lot of times they have a lot of repetitions so we could actually go back and we can actually look at the data specifically at repetitions um so there are a ton of instances where we see these hallucinations that we can actually quantify um and formalize into a slice and do analysis across um yeah so we have a bunch of charts here um some of the ones I mentioned before like the radar chart we can just pop directly in here um the idea is we're working actively on making these very reproducible so if you had a new translation model that you thought was state-of-the-art um ideally with just a couple commands you could upload that new uh that new model and it would update all your reports um dynamically yeah we have another one here we have a Bor chart looking at all the languages so you can kind of expore languages where maybe is actually better than the stateof the art translation model um we find a couple interesting one this talk pisson language is a creole language um that's based in English and Latin script so there's some hypothesis that gbd4 is really good at inferring or or understanding kind of English like or Latin like text and is actually better than maybe the state-ofthe-art language models that hasn't been exposed to that much of this language um so you can really start getting that kind of more granular view of how your models are doing we have another one on the auto transcription that you can also also explore yeah so that's more or less what I want to talk about today is I really think that people should be diving deep into these benchmarks into all this Rich data they're collecting of how their models perform it um and really create these narratives uh that that really dive into how their models are actually performing Beyond just a single accur there's a lot of interesting directions to go in um a lot of open questions one that I kind of go off over that's the whole point at the beginning of the talk was actually building these evaluation sets is very hard um how do you create Ben benchmarks um how do you effectively Gather in user data um that you can then do this analysis across there's another point of error Discovery there's a thousand different slices and ways you could slice and dice your data to find interesting patterns that's very hard can we help people automatically discover these eras areas of areas of data with high error highlight interesting differences between models and lastly can we actually kind of move from storytelling to testing once you start identifying the interesting areas which you're you care about your model performance um can you encode that also as a specific test um there are a lot of interesting talks earlier today about how people are thinking about regression testing and evaluation um kind of what does it look like to have cicd for a model is a very open question that people are actively thinking about it's very exciting time fors Alex thank you very much we had one question here uh maybe I could get that out of the way very quickly if somebody wants to make sure that they're on stage one they can move ahead um what about label free evaluation it's a great question we have thought about that and there are some metrics that you can you can do model graded evaluation where you just given the output you asked llm to give it some measure metric you can do analysis across that in in Zeno using that as a metric what we have found is that it's better to have labels um if anything you might as well just use the llm to generate labels and have a small set of label data that usually gives you a much stronger signal than doing label free evaluation so we have EXP that it is an active area but I think what we found across a bunch of different use cases is it's better spend either some time with small data or even just use an LM to generate the labels and then do evaluation but Zeno doesn't care if you wanted if you want to do without labels you can go for it however you get it yeah uh I'm extremely bullish on this direction glad to hear it this is fascinating I had interviewed about a year and a half ago somebody um Professor Jose Hernando Jose her Diaz and about this like aggregation perspective on of machine learning evaluation and I think either he or I I don't remember the course of the conversation we called it practically a scandal that we're looking at this it's it's such a like multifaceted and highly dimensional uh like uh concept this performance right like when you evalue the performance of a human being you never just aggregate it like this okay this is their score I guess maybe you do for certain behaviors like I don't know your credit score or something right but it just ends up hiding way more than it reveals and I think like one other example was it's like what happens you let's imagine you have like a self-driving car and you are it's 99.99% accurate whatever if you want to bring it up arbitrarily High you just keep serving it very simple examples that's all you have to do but with a platform like Zeno it feels like this is almost like an automated what like error analysis on steroids something you know what I mean like you I think at the simplest it's like you can just get so much by just looking at your data like if you just let people like do some sort of structured way of exploring your data in a systematic way just everything comes at you like wow uh but I totally agree I I the analogy of like you wouldn't hire someone based off their IQ test and that's how we're picking llms like no you actually want to know how well they're going to do the job and you got to like dig in and understand all the dimensions of that job so yeah totally agree thank you very much I also think this is um it's have a piece with I believe like what the spirit of the ending of this conference should be which is it's I mean a lot of people have showcased and displayed and shared with us like best practices and things that they've done but ultimately I believe a conference like this should not be about giving Solutions but inspiring as all the questions that are still open that need to be dealt with I mean this is such a young space and we don't even it's I imagine we don't even have the right abstractions the right terminology and language to talk about most of these things yet and this is It's struggling to be born but ultimately um it will be and it will be because of people like you doing the work and people like the audience tuning in Alex thank you very much exit time thank you so much [Music] Adam