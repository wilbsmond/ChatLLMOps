are you with us thank you Adam and thank you everyone for joining I'm excited to be here um I'm excited that language models are finally making it into production um just let me jump into a little bit about me before I dive into the material um before this uh I co-founded ml perf which is a standard uh organization that helps uh companies um get the best possible performance in um training deporting models including language models um before that I built a research team in by research the Silicon Valley AI lab where we invented language models we invented scaling models uh scaling laws for language models and had many um of the people from that lab went on to build um the language models that we know and love today including gp4 including Claud to including um metas llama and including Megatron at Nvidia so I'm so happy that we're finally at the point where uh this stuff is like out in ruption and everybody is using it um and then a little bit before that I used to be an architect on Cuda where I helped build the first versions Cuda um so yeah super happy to be here and I want to talk about a really specific uh technology for um language models which is called fine tuning uh fine-tuning is the technology that takes us from research projects like gpt3 um into Chad GPT or from gpt3 into um GitHub co-pilot so it allows the language model to learn a new interface and also to understand um specific data uh that's important to to solve a particular use case like to learn how to write code um so here I want to talk about how we're doing um f tuning at lini and I want to uh see if I can get through the entire process um in just a few minutes so let's let's dive into it okay so I'm going to go to presenter mode and I hope that everybody can still um see me here all right so at LOM and I we're building an Enterprise platform for fine tuning just make it really easy for every single company to do fine tuning um in this example we're going to look at what it takes to F tune llama to uh the 13 billion parameter model um so that it understands uh legal cases so that it can actually understand the material um inside of um a particular legal case that it's never seen before um so the base model llama 2 has no knowledge about these cases these are new cases uh where imagine the proprietary data that um the base model was not trained on and remember that most of these models were trained on the internet so they were trained by scanning um the common crawl or the pile or a data set like that of information that you can readily find on the internet but not all data is in there so hopefully my video works okay so this is what it looks like if you just ask llama to some the questions about these cases in Bell versus turer did the appeals court um like a question about the appeals court and most of the time the model will correctly say that case doesn't exist I've never seen that before right and so there's no case such as Cruise P Packer versus cherto not aware of any legal case so completely makes sense it wasn't um any information that the model was traded on so of course it won't know um how to answer these questions but you have a lot of data um so if you're an Enterprise you've been accumulating data in a data lake or a data warehouse for the entire history of your company and you might have pedabytes of data and language model knows nothing about that so let's look at an example this is an open example so you can reproduce it um we're training on this uh Harvard Law C case uh data set and we're going to inject this information into the language model so that the language model can Now understand and answer questions um about this data set and this is the raw data so you can just scroll through this this is just uh The Core case um just directly uh as a string exported um into a giant CSV file um so you can see here you might have you know thousands or tens of thousands of core cases in a data set like this so it's a really um big data set and it's just straights so there's no postprocessing that you need to do on this it's just a giant stream okay so now the next step this is where the magic actually happen so um here we're essentially going to use the lini docs to QA interface so you can see on the right hand side there from lamini import docs to QA way we're going to create um essentially two models using prompt engineering so this is going to create two models that are going to pretend that they're paralegals um graduated from Harvard Law School and they're here to um ask detailed questions about these documents and answer detailed questions about these documents so you can kind of conceptually think of this as um having the language model read through the entire data set and then generate data in the format that we want so we want this to be a question answer model model we need to generate data that are questions and answers um so we have two language models here one is generating um questions and then the other one is answering those questions and as the input to those language models they're actually reading through these documents so this is a big computational task we kind of having expert language models read through all this Core case information uh to generate a data set so that finally generates you know thousands potentially tens of thousands you know you could generate an arbitrary amount of data um about uh these um these core cases and now finally we're going to feed that into a language model for fine chaining so the last line line 16 right there is going to um kick off a fine-tuning job we're going to take the same model um llama to 13 billion and F tuned on all of that data that we just generated okay uh so you can also connect to um other places where your data might come from in that example I was loading it from a CSV file so we can also connect uh to data bricks data Lake um we can also um connect to Snowflake and you can pull data out of a warehouse okay so there are lots of ways of getting data into this um and these are examples of some of the questions and answers that are generated by running the language model over on the data so we generate a lot of data there um all right so then we run in um when you call train it immediately submits into the system so you immediately get a model that comes up um and starts fine-tuning we use optimizations um like retrieval and light heft um to make sure that fine tuning uh is done efficiently and has all the right context okay so this is what happens the model finishes in about 15 minutes okay so let's see how it performs so remember this was the original walut so it it answers these questions like oh there is no such case um but what happens for the fine team model what would you like to know um what was the issue regarding the custodians claim in McGrath versus manufacturers trust and so um the issue was whether the C custodian was entitled to interest on the amount owed by the bank so it actually understands um the content in the core cases now and can answer questions very specific factual questions about um the information that's uh that's in the data set so we've essentially um downloaded this information into the language model all right um we have a number of other examples here that I'm going to skip in the interest of time but um let me get to the main point if you want to learn more about the technology underlying this and how it works I'd encourage you to take our short course um co-hosted with deep warning Ai and corsera on fine-tuning large language models this explains how F tuning works and has several practical examples you can play with and we have a hosted platform that's out right now go to l. um you can sign up you can read our docs you can run exactly the same application so um I think I'm about at time so I'm going to stop here and yeah just excited to see what what everyone builds with this thank you very much Greg it must be wild to just to see right from the beginning of llms to where they are today uh and it's it's how do you feel about this I mean this is this is crazy we're writing scaling laws so they sucked you know in 2015 they could do like spell checks so now you've got like GPT 4 so the thing I'm really excited about is 2025 and 2030 because they're going to keep writing scaling laws they're going to keep getting better so this is really just the beginning thank you very much [Music] Greg