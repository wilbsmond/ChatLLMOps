we've got another talk uh the next one is for my friends at get your guide we've been doing local meetups in Berlin since the beginning of the local meetups and get your guide was the first company in Berlin to host us and do these local meetups and now we have the pleasure of hearing from both meana and Tina meana it's great to see you again Tina it's great to see you for the first time I'm excited for yall's talk I also have to keep the time very tight so I'm just gonna hand it over to you all right uh welcome and super excited to be with you so let me share my screen and jump into it so hello everyone I'm Tina I'm product manager at gure guide and today I'm here with my colleague meanna who's an mlop engineer to talk about llms how do we prioritize them and put them in production at geter guide so before we jump into the the nitty-gritty stuff just a quick note about what get your guide is it is a leading Global Market Place for unforgettable travel experiences it's basically a place that connects suppliers and travelers and as a traveler you can book and discover Unforgettable travel experiences from culinary excursions booking your museum tickets and even bunch jumping and um kind of we believe that llms are key to our future success and redefining how customers discover and plan their trips and before I kind of go into the specific areas of opportunities where we look at prioritizing llms I'd like to really break down the principles that we use when thinking about llms in product development it's essentially from our point of view a new technology that has asking questions like what are the things we weren't able to accomplish in the past or were limited due to technical budget or time constraints secondly um which is a more exciting question is like if we were to start from scratch in this era of llms how would we build our product and operations differently and thirdly what parts of our product and business are especially slow and not scalable what are really the bits where we're asking ourselves like if you know if we were to scale this would be able to scale this only with people or is there a technology that can help us and these are essentially the questions that are helping us uncover opportunities where this new technology can really transform our product and experience for our customers so if we go deeper into specific use cases and areas in ad getri guide where we're looking at LMS being truly transformational uh these are kind of the the three key bits uh so first of all how do we transform our search and Discovery experience and how folks plan their trips we believe that llms can truly revolutionize this from going like you know from basic uh surgeb input to really like a conversational experience and to take this one step further how can we build a new understanding of our offering that is like growing exponentially and how do we actually translate that understanding in ux components and even advertising messaging and lastly how can we speed up our day-to-day exploratory work and business operations to increase the scale at which we operate the business for example here we're now looking at our supplier on boarding and making that experience much smoother how can llms just like accelerate that 5x and so thinking about this conceptually we have a lot of appetite to change how we develop our product and invest more resources into this but there's obviously risks and challenges I think we heard about some of them in the previous talk and we truly need to address these first and before we head dive deep into this in terms of challenges um you know there's nothing new I think the group is very aware of this but they're very crucial for us and so what we've seen with GPT and the likes is that without truly tight constraints there are inaccuracies biases and misinformation and the output produced and we've had an example in an exploratory internal project with GPT where we're kind of keen to see is there a way how we can different summarize our inventory and present kind of USPS or unique things for customers and while it looked overall promising we had red flag cases like we had this one experience of watching the changing of the guards in London which is like you know one of the top things you do when you go to London and when it summarized it it summarized the experience to getting up close and personal with the guards and this is not essentially what the experience was about and even you know if we were to put this in front of our customers it would be truly damaging and not accurate and besides this uh we're kind of also losing track of what is the source of Truth and kind of with many llm and supervised models interacting with each other we are creating closed loop loops and kind of losing a grip of what's happening in our product and then what is actually the end experience for our customer so kind of between these challenges and risks we started discussing internally how should you know we develop these products what should be the principles when we are working with llms and what are the practices we can actually put in place to deliver high quality products and the goal of this was to kind of keep up with the pace of you know putting llm products in production you know going with the low but you know still doing that without damaging our customer trust and also our brand loyalty and of course these principles are to be interpreted depending on you know whether you have a highrisk use case or a lowrisk use case um so yeah sort of we sat down and drafted these uh core five aspirational principles and Megan is going to talk about you know some of them more into the details of what it actually means um but first of all we are customer Centric first and foremost we prior building high quality products by tightly scoping them out following data product development best practices and having very clear guard LS that tell us you know are we on track or are we off track secondly we build for reliability accuracy and safety this is truly about ensuring we don't put any false or misleading information in front of the customer um and it's not just about misleading it it could actually lead to a place where customer books the wrong experience and they have a terrible experience because of this and we achiev this by thorough pre-launch checks and Bug bashes that are really a mix of human feedback loops and also automated checks third we prioritize customer trust through transparency it's it's truly about having that Clarity internally how and why models are making certain decisions it can sometimes be truly a black box so how can we make it less of a black box and we have believed that there are steps to get there as things like documenting the limitations biases and potential errors associated with the model this is truly especially important in the event of another team leveraging or building on top of this model and here I think a really good external example that we've looked at is are the model cards that I think were started by the Google Cloud team and now are like adopted by all the Google data product teams and we believe that once we have this Clarity internally we're able to also explain it to customers in simple terms how does this product actually work uh that they're interacting with essentially the fourth principle we have is that human oversight is an integral part of the development and R process we believe that shipping quality products and maintaining set quality over time requires kind of that magical mix of human and machine humans are great at noticing weird things patterns and we can leverage that human intelligence to improve and fine-tune our models to deliver even higher quality output and lastly and most importantly uh we be accountable for maintaining Integrity over time this is really about catching these model regressions we truly need automated checks evals and think about llm observability overall and kind of with that I can segue and give it over to mean who can talk about some of these automated evals and what we're doing at geter guide Megan over to you thanks s for the intro yeah forgive me if my voice is a bit uh uh shaky today um I'm not feeling too good um yeah but um yeah let me let us dive a bit deeper into the aspect of uh when to choose llms um over uh the classical models so the biggest bottleneck uh in data science in the data science pipeline that we have is having a well curated uh good quality and good size data set and that takes up like the bulk of uh the time from our data scientists and uh basically our Capac capacity so that problem is solved right now with llms um they help us in iterating really fast and we able to like launch experiments really fast and we are able to fail faster which means we also able to generate learnings really fast and in case if we want to um if we decide that we want to uh develop our in-house models so they're also like a good way for collecting this data um over time next slide please sta so what has changed um for get your guide uh in the llm landscape so um so I'm emphasizing mostly on open AI because um that has um gained wide accept acceptability uh within Geto guide so uh what changed for us is uh we saw a a a great increase in the number of projects that actually used uh um llms and uh people across uh teams and get your guide are coming up with innovative ways to solve their problems um and that means we also have lower burden on our data scientist teams and now we have an established um yeah interest groups where people from um diverse backgrounds get together to um discuss their challenges or experiences in using Ai and basically anybody with uh a coding experience can deploy um a data product to production so we have like really fast iteration Cycles where we get to production really fast so um uh everything seems cool but um there are still some challenges so um some of them are especially with fine-tuning models on open AI they can be really expensive rather than just using GPD 3 uh and when um when the cost across teams add up um it can yeah really get expensive for us um and openi has um a Quee where um before we submit a fine-tuning job um we have to like wait in this Cube before our job actually gets submitted so this can take considerable times uh considerable time even days um so that's exactly not feasible like in the long run and um open API apis change and um and this can uh result in um uh changes in behavior and user experience and are not necessarily always positive um and the most frustrating part is we we cannot have control over the output and there are like some hallucinations and once we deploy our model in production we have like no idea what's going on in there right um and what about the performance metrics so we have performance metrics for let's say classification we have like the F1 score or the accuracy metrics yes cool there like a great option when you have a data set but what when you don't have a data set so that's why we um decided to iterate with LMS in the first place and um how do we evaluate free text uh in a recent project on AI based onboarding where we um want to um uh give a platform for suppliers so that they can um post uh activities on get your guide without any anybody from Geto guide as an intermediary how do we evaluate uh the text that was generated by the openai on behalf of the suppliers was actually of good quality uh we do have some uh metrics called Rouge and blue out there but um on testing they didn't exactly correlate uh with the human um score so uh what next um we still um explored um and we came across um open AI evals so for those interested um in digging deeper I've attached a link for the repository so what does open AI evolve so um open AI a few months ago open sourced their um the framework um which was like an internal benchmarking tool um where they ensured uh that the models that they released uh were up to um up were performing adequately were on par with um yeah basically the metrics were good so they this was their internal benchmarking tool and um it provides like a systematic way in which you can structure um your yaml so that um you define your metrics um you define um the eval class um which is executing along with a few few short examples um and also like some samples for um for learning uh basically and uh there's like this class that you see below is nothing but um a class that has like a set of prompts uh which does various tasks next slide please yeah so um you can uh basically um use these uh evals out of the box uh so there are like some standardized um evals there which you can use for your use case or you can Define your own evals by uh writing your own prompts so um all in all it's uh the Crux of uh Opia eval is it's just one llm basically evaluating the performance of another llm but more structured and in a more um in a way that follows best practices around um uh around um llm benchmarking um so next slide please okay uh forg your guide um there's no official documentation for open a evals uh so which limits um which limits us to extend it or use this for use within Geto guide um but we are really inspired by the idea uh and we went on to build something similar to Opa ials within Geto guide which um suits our use case within our or organization um so uh the way we did it was for this project called AI based on boarding uh where the uh where the model generates uh the free T text on behalf of the supplier um so we want to test we basically fine tune a model that uh that is fine tune on the historical um uh historical data um that's there as part of Geto guide which contains the supplier text and the text that was corrected by a freelancer so it can the data set consist of a raw text and uh and a refined text and the model was fine tuned on this and uh when a supplier inputs a new text which is um at the inference time a new RW text uh the the find you model uh outputs a refined uh output um so how do we test uh how this model is doing um we the best way we had till now is to do do a human evaluation and of course this was not scalable so um uh we uh implemented the open AI evals kind of uh framework uh within Geto guide and uh we came up with like a set of prompts um on uh which which basically evaluates um our our main model so this basically contains multiple iterations with the uh with the human evaluator and until you get the prompter right um and uh contrary to what many um I heard in this uh conference about GPD 4 uh I mean that's exactly what I used in this case and um it achieved like a good performance especially for the scoring kind of scenario um so it uh it also out outputs like the thought process behind the scoring uh which is also helpful for like the human evaluator to understand um the score that was assigned and in some cases we also saw that it um it identified um uh mistakes that was uh overseen by the uh human evaluator also so um sometimes even uh surpassing um the human human evaluator so yeah we got a pretty good performance um uh overall on this evaluation framework but yeah the only thing is that we need to iterate a lot on the prompts and the prompt engineering that's done around this um so yeah we have the main model we have the evalu evaluator model and we have the score so what next so the next thing is to observe it like have observability and basically to monitor how uh this is um doing when the model goes live so we use a tool called arise for model observability so where we can monitor uh the data where you can monitor the predictions um and uh this is what we'll be using also for our llm observability so there we can also configure the custom metric as you can uh see in the figure uh or you can like also configure thresholds and alerts and when you see that um uh the the data point has um has violated a certain threshold you can also configure alerts uh and for all those uh data points that have violated the threshold Um this can be sent for um human inspection uh next slide so all in all uh our eval was um basically a set of prompts uh which Define how the scoring has to be done and which is uh usually in line with uh a human um human scoring and the output um of the llm is usually uh like a score which is on this uh on the scale from 1 to 10 and when the model goes live so all the rows which have like a threshold uh which are violating the threshold are are sent for manual inspection and here there's two things that happen either you iterate on the main model or you iterate on the scoring model and um this is uh when we enter a cycle and then yeah um we do it until we are satisfied with the performance of yeah both of them yeah and um yeah that's pretty much it that's all I had uh for the talk um so um yeah I hope you end enjoy the presentation and any questions please awesome uh so good thank you so much for this I am going to wait for the questions to come through and magana thanks so much for doing this while you're not feeling super good so I appreciate you powering through it and going over this now you were mentioning so from from my side I'm gonna uh ask a question before the different questions come through on here you're mentioning that you were getting this you were getting better results from the GPT 4 evaluation than the different people that you had the contractors that you had did you look at is that because of this uh little Paradigm that happens with GPT 4 being bias to its own output like did you think about that one uh so they were not always better but we we could see that it in some cases it um it could point out the information that was missed by the human evaluator and we could also see that um it was kind of flagging the false positives like uh bit more so yeah uh I guess that was still the part that we had to work on when we say okay like tone down on this like it's not that important so it's fine it's it's fine if you know if you don't uh detect the scores for like certain aspects but yeah this was like a behavior which we could see that it it was like generating like a lot of false posi in meaning in the sense like detecting a scores where it wasn't like really necessary but I mean this could be still be improved with um with improving your prompts yeah yeah totally so as you and this one Tina this one's more for you on the product side have you thought about different I mean you mentioned documenting stuff and how are you looking at this Roi and being able to quantify which which projects to take up and which ones to say yeah you know what maybe that's not enough of a loow hanging fruit for us to actually do right now um yeah excellent question I think it it really depends on on kind of the use case um you know whether it's like internal external with internal use cases I think we have a lot more um flexibility for taking risks and just letting people hack things and and see how it works as for external use cases um in terms of low hanging fruit um I think with llms um while there is like upfront investment you need to make with observability I think over time the cost just reduces um so I think it's more about really strategic thinking and seeing like where you want your business to be and like what are the biggest blocks that are slowing you down and I think Mega to mention this um AI supplier on boarding that has been definitely one of the parts of our business that we are keying on accelerating so that's essentially how would look at it so we've got one that's coming through the chat here that is let me see if I can interpret this correctly what were the parametric it was benchmarked on so um I'm not sure if that makes sense to you I we may need some clarification on that and in the meantime raat if you can clarify that there are a few other questions that came through here what are the validation metrics that are used for gp4 um so we not like uh evaluating like GPD 4 but more like the evaluation uh for the output from the main model and we are basically using gp4 to do that and so this is being done by prompt engineering and also like feeding in some context about like yeah the information of how the scoring has to be um has to be done and uh these metrics are defined by our internal uh content team uh which um yeah which is very relevant to how a Content has to be um presented and this is like our uh our own metrics and it's also our yeah internally that we are defining you know how these these has to be scored and how much weight each aspect of the scoring has to uh um uh take take on excellent and you mentioned hallucinations in the talk how do you guarantee that llms which you are using AKA gp4 I think for evaluation don't hallucinate in the evaluation process so absolutely yeah they can hallucinate um so that's why um yeah we have like this threshold right and then we um for for all the data points that are um yeah um witing the threshold let below the threshold then we inspect them and uh like I said the observations are more like uh false positives which means we're likely more likely to um see um um lesser scores rather than like higher scores um which yeah which also kind of limits our risk um so I guess this observability tool really helps us um to understand hallucinations on either front and um and also like this human in the loop process um and thresholding which which can help us um basically solve this so this next question is awesome and I guess we can I can add a little bit to it maybe to make it easier for you but on a scale of one to five how reliable is it to use llms for evaluating other llms output and then you can elaborate maybe give us the number and then elaborate on why that number is the one you chose I mean uh so far considering the metrics that we used I mean of course there there can be uh still other uh methods to do this and we are still open to try them but um of all the things that we tried so far uh from the standard metrics out of I mean out of the box metrics and like a model graded one uh this worked out uh for us the best it's um I would say it's still not perfect uh the sense I mean uh it has like false positives uh but I think that's something which we can handle for us as a business um yeah so I think it's like around seven or eight out of 10 uh I mean the deduction is like because it has like false positives but that's something that um that our business can take and of course like your prompt engineering and uh refining is also very important to um to get the level of performance and that that that's also where like bulk of your uh time um goes in uh developing something like this 100% yeah seven out of 10 ain't bad I mean that's a pretty good number so last question Tina I think this one is for you how do you ensure that the cost does not just get out of hand when you go into production yeah good question um I think first of all we we tried to evaluate before launching how much the cost could scale and I think there's like a lot of uh things you can do with caching and like optimiz optimization to kind of uh bring down the cost and one thing we do at gri guide always is we we have a good overview and monitoring of the cost of different services and anytime there's a spike we try to understand where is this Spike coming from how we can bring it down and then see are there use cases that are actually not needed then we can kind of keep it within bounds of normal uh cost and then the r decent incredible so I will just mention that this talk was awesome when it comes to evaluating llms and how you all are doing it in the wild in production I love hearing about it and I also want to talk about how we have an evaluation survey that fits right in with this I think maybe even meana you may have filled it out for then even if you didn't just say yes because no nobody's going to know the difference so I if we can pull my my uh my screen onto the screen I will go through just a few of these cool questions with you all because it fits perfectly with it and I want to make sure everybody can see this so you can see things like what data are you using to evaluate your llms how are you using it and then here do you have ground truth labels for your data uh here's a great one are you using human evaluators and so it's just like sew right up your alley with this talk and you all are doing it there's a ton of other people that are trying to figure this out with you and if you haven't filled out the survey yet I encourage you to go do it we also have all of this data it's free for you to check out and that is not it that's my run of show this is all of that data you can go and you can look at right now if you want and we'll drop all these links in the chat but for now Tina and meana thank you so much I appreciate you coming [Music] on