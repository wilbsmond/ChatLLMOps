[Music] Ben are you around yep okay awesome hi everyone I am so excited to be here today in this talk I'll will be going over how to evaluate an llm for AI risk through stress testing and red teaming I lead the ml team at robust intelligence which is a 50 person startup based in SF and we're building an automated validation engine in firewall to manage and mitigate AI risk so to start to kick things off I want to start with the why why are we focusing on red teaming testing right now there is a huge need for independent third-party testing of models due to this major wave of generative AI we've seen statements and regulations coming out from the White House nist the National Institute of Standards and technology and also the EU which are all pushing for mandatory external security testing and just general validation so to start what is red teaming uh historically red teing has been in the cyber security field the broadest definition being an exercise in which an organization simulates adversarial conditions for improving defenses it has often been thought of as a group of it employees um use to simulate malicious actions but now we've actually come into New Field of AI red teing which in today's definition is actually Pro it's a little bit more Broad in terms of a process where the input is an AI system the output is a set of deficiencies with actional recommendations so we've seen this big shift due to generative Ai and that it's no longer just concerning cyber security but actually we're thinking about how to test and how to attack AI systems so how do we go about doing this um we think that you should test both your isolated base model but also ensure you're testing your application through the normal user experience and with real world data um we should think you should test not just pre-production but also continue to Monitor and test post-production especially if your company or use case is more prone to security risks and adversarial attacks as these adversaries are always changing and improving and finally how to test there are multiple ways to test and I'll go into this more but I think you should be doing both open-ended breath first search um and actually doing more of that open-ended red teaming and then also uh Focus depth first red teaming which is like specific failures specific data points um and there's sometimes it feels really overwhelming of what to actually test for these llms so at robust intelligence we've broken it up into operational security and ethical risks and you can see some common failure modes here so hallucinations inconsistencies um irrelevant answers or random answers for ethical we're thinking about harmful outputs exclusiveness and for security there's the classic prompt injection examples data privacy Supply and supply chain issues ETC and these are just a few examples but as we know there are so many so one way to think about this is to automate what can be automated um there I think about this as like a macro vers micro so there's the open-ended manual effort and I think there will always be some of this manual effort and you can really think of this as the system level looking at the cracks between components um to find out where these failure modes lie and then focused is more of like a unit test so actually from yeah the software unit test we can have these automated tests to really probe the model and um the one thing for llms that we have to be really careful of for some of this automation is thinking about cost um because that can also be a big detriment so think about automation also make sure you're uh worried about cost too so automated red teaming we think of as solving an optimization problem um so giving given an existing data point it's finding a close data point for which the AI model returns a slightly different response um and I think red teaming llms requires an even harder optimization problem because it's really um there's so many different responses that it's actually can be quite intractable uh so now I now I want to give you some actual concrete examples of how we're kind of thinking about this automated red teaming into stress tests um in our product and how I've kind of explored lots of different um testing for open- Source LMS that are out there and seen some failure points um and so from this it's kind of an iterative cycle of of doing that manual creating a test from it trying to automate it and seeing how that works so here's an example of a prompt injection attack test um we have a large library of different types of prompt injections and here are testing adding a prompt injection which is the the red into a user's input data in the context and the question so you can see here that we have the original at the top which is asking this um question and the model output is answering it um and then we actually add the prompt injection into the context and see um in this case it's actually passing it does not fall for the jailbreak succeeded answer and then in the last example we are adding it to the question so kind of testing different points of the prompt and then we can see that um the llm actually falls for this it says she'll break succeeded and this is like a silly example but you can think of how this can be um quite detrimental if you're asking it to say certain things or maybe giving it a toxic language or false information and this is um like an example of how we've turned this into a unit test and I think um one of the hard things about this is actually figuring out from the output if this is really passing or failing and so this one I'll I'll skip through pretty quickly but this is another example of um a security test which is a prompt extraction so we're actually trying to get um to try to get the original instructions from the model so the system prompt and in this example we actually provide like a random token um as the system prompt and so we test whether we're able to get that out of the model which we are in this example and then some other types of tests for that are not necessarily security but is like a data transformation test so this is really going back to that optimization problem thinking about how do you tweak different things in the data to see if the output has actually changed so this case is actually just um lowercasing everything and there's not that many uppercase examples but um it in this response it actually just has like a completely different response because of that uh which is very interesting and then for these Transformations we're like thinking about uh that example as lower casing but we're also thinking about character swapping or um OCR errors or just adding characters adding synonyms Etc and then we're basically looking at the before and after and looking at some similarity metrics and seeing um how the output differs and I think again this is um kind of an ongoing challenge of like how do you see if that text was similar enough and how to really measure if this test passes or fails and lastly I'll give an example of the model alignment test so kind of similar to the um other to the security test we have a ton of prompts of misinformation sexist racist prompts and we feed this to a model sometimes giving it tons of examples of this type of behavior and then we see if the model will actually go along with the behavior we're giving it um or if it has guard rails up and uh we'll say that no I cannot answer that question so in this example we're asking it um some giving it some like sexist question and answers and then we see that the model answers um according to that which is toxic so that's um yeah I know we don't have a lot of time but main takeaways there's these operational ethical andsecurity risks that you should think about uh llms are very vulnerable to these types of attacks and uh we think a combination of manual and automated red teaming uh is great to actually thoroughly test these all right you very much I one thing I would have loved to see is the conversation between you and Casper who will soon be joining because Casper can you can you hear us are you here uh yes I am here you can hear me yes because while you are evaluating and testing open source models it also feels to me like there's more and more of them and it's going to be an increasing mess to do that and keep track of all of the developments that are taking place in the open source space and that's exactly what Casper is going to be here chatting with us about Finn thank you very much for presenting and for all of the work that you're doing thank [Music] you