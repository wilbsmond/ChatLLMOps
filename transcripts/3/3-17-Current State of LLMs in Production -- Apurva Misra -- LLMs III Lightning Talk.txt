porva are you around somewhere there you are you get the honors you get to bring us home how's that feel so good I wasn't that feels really good all right I thought you were speechless I thought you might have uh gotten your breath taken away because you heard out about that so it's great to see you again I know you've got an incredible talk for us and I am very excited for it and I will also mention that you helping organize the Toronto local Meetup chapter so if anyone is in Toronto and wants to hang out with more like-minded people that are doing this kind of stuff with llms and machine learning hit up a porva and go to one of the local meetups okay um since we have very little time I'm just going to get started um uh I'm going to speak about current state of llms and production uh my name is Aura misra um and I'm a senior machine learning engineer working at truck stop and I have a background in natural language processing and like Demetrius said I'm based in Toronto reach out to me if you want to be a part uh of deml Ops Community Meetup here um so I'm going to talk about uh llm models in production and because it's hard I asked D to generate images with the text hard which was really hard for D to to do uh so um let's get started um what I've been seeing um with different companies is generally uh racks are the ones which go into production they seem like a simpler um system to put into production at least initially when you look at them um since we already have like had like a couple of um um presentations about racks I'm just going to go over this very quickly so what you have is your you have your data set um it can be any data source it can be Confluence documentation it could be YouTube video transcripts it could be any sort of help articles and what you do is you chunk them like break them into pieces because generally like embedding llm model models have a limit to the context length so you you would want to chunk them and embed them um using the embedding llm and then store them somewhere um that's where Vector DBS come in and then when a user comes and ask a question what you want to do is go look for similar chunks which would be helpful in answering that query um pull that in and send it to the llm along with the query so that it generates a natural language answer so this is the gist of racks um and what I've seen is generally it's this rag model a uh rag system which goes into production or it's um rag is acting as a component of a larger system you so you're using semantic kernel to build a bigger application which could U answer a lot of user queries um and then when when the user comes and asks about help articles what it does is it figures out oh this is the plug-in uh that I should be using which is a rack system so it'll call the rack system if if the user is asking about something else it would call the API to answer that question or maybe call the search engine to answer that question so um so it's it it might just be a whole system by itself or it's a component of a larger system um so I'm just going to go over um the rack system break it into pieces and see how um each each of those pieces need a lot more detail in there for example uh to start with you have the extraction module that is you have your Source data somewhere and then you would have to break it into pieces like I mentioned because llms have a embedding llms have a context length um and then you would want to embed them so um for the chunking either um you would have you would have to decide like what the chunk size should be um and how many number of chunks you would want to break down the documentation into for example if you have PDF documentation you can just chunk each page at a time or like multiple Pages at the same time and this would completely depend on your use case for example um I've seen in some cases in which they break it down into smaller chunks and when they're pulling like similar chunks what they do is along with the one which match the similarity they pull the chunks around it as well um or uh other times what I've seen is they generate multiple embeddings of the document for example they generate the embedding of this page along with that they summarize the page generate a summary and then embed the summary as well um and sometimes uh depending on the industry you would have ter terminologies which um which the llm model doesn't know about so you would have to take that into account as well um so the next step would be the embedding so after you have done the chunking you would embed the chunks so you would have to choose which embedding model again this would depend on your use case and then you have the hugging face leadership board that you can choose the embedding model from but what I've seen is it it does it the top most embedding model might not give you the best um results so it would depend on the use case so you would have to test out multiple embedding models and see which one works the best for your use case um and then once you have done the embedding you have to store it somewhere so that's where the vector databases come in you can use something as simple as postgress with PG Vector I've used it it works really well um but then these Vector databases they are customized for this use case and they have a lot more functionality in there um to help with that and for choosing the vector database there are a number of factors you have to take into account like on Prem versus Cloud the indexing speed versus uh query latency like how long it takes for it to like go save it in the vector database and like pull it when you have a query um and then like good call versus low latency Spar dense vectors hybrid search would be keyword search plus semantic similarity um that's again like something that Vector databases provide um and then uh you want it in memory on dis depending on like which algorithm it uses uh it either of them can be faster and then uh does it do pre-filtering or post filtering after getting the resist does it like do reduning um which might be necessary as well depending on your use case um and again like the uh the icons that are have put the logos I've put it's not exhaustive there are more Vector databases out there so that's uh something of this is again a rabbit hole that you can go deep uh take a deep dive in um okay so after you have saved your embeddings in the vector database when a query comes in you have to retrieve um the relevant chunks which would be useful uh in answering that query um so um for retrieving that there can be different criteria you can take into account one is like semantic matching that we were talking about you would want to find similar um text uh and which would be able to answer the question so if you look at the picture underneath the image underneath um it's the distance metrics that uh postgress provides postgress with PG Vector like cosign distance inner product these are like uh provided by PG vector and then uh the image on the top is the ranking so after doing semantic similarity and finding the uh relevant uh chunks the top K chunks relevant to answer your question you would you might want to rerank it to figure out which is more relevant so coair provides um an API through which you can uh do reranking after you have pulled out the chunks from the um database and then uh you might want to do like I mentioned uh in the previous slide hybrid matching that is you might want to use keyword matching along with semantic similarity because again like different Industries have key terms which um the embedding model is not trained on um the other thing can be you want to do deterministic plus fuzzy matching you want to do s semantic similarity but you want to look for documents only from January to March of 2023 so a combination of that to get you the results um and then after you have built your system you have to figure out like if it's working properly is it worth putting it into production that's where evaluation comes in uh and like e was mentioning like you have component level evaluation and you have end to endend system evaluation so for component level U especially uh for the rack systems you have the source um Source where all the data is then you have the context which it pulls uh from the vector DB to relevant to answering the query and then uh you have the query and you have the answer so um you can bre down the system into retrieval how is the retrieval uh functioning so um given a query does it retrieve relevant context from the vector DB to answer that query the other thing uh you can um measure it on is answering quality given the correct context and a query is the answer generated um of the quality that you expect it to be generally what I've seen is um um companies using llms to score um the rack system and they would ask it to like score between one to five and then again like Demitrius had asked this question before in a presentation like there is bias like gbd4 would say gp4 answers are the best or GPD 3.5 answers are the best so you have to keep that into account as well when you're evaluating using llms um okay okay after you have done the evaluation what you want at the end of the day is a system which is performant and um it's cost efficient right so it needs to like perform at the best quality and it should not cost you a lot um llms get expensive as you scale um but I have seen um this is uh an image from not diamond. it lets you route your queries to different llms based on the complexity so a more complex query um might be answerable by gp4 but a simpler query maybe can be answered by Lama which might be cheaper so it lets you do the rooting um and um you can do this yourself as well using logistic regression with coun vectorizer like be build your own classification system when the query comes it it Roots it to the right llm to answer the query the other thing is caching um so that you are doing less llm hits when you're answering um the query okay um just this is the last slide I just wanted to go over um my experience Building llm Systems um recently like I'm working with software engineers and what I've noticed is it's really hard for them to get a grasp of um how different it is to build with LM systems like the code flow is very different it's like a blackbox and there's lots happening and it's very different to keep track of what's happening when it's calling which plug-in and how dependent it is on the prompt like sometimes changing the code doesn't affect the result but like changing the prompt does um and then testing is also very different it's not deterministic you cannot say you solve the problem um all the time and sometimes what really happens is you change the prompt or you change something in the code and you have fixed this problem but you might have like um made something perform worse so it might degrade the system overall and then again like token limits so there are like different factors um which which are very different from traditional software Eng enging engineering so it's like it's really important to keep communicating and collaborating really well when you're working with software Engineers um that's that's all from my own oh my God I love ITA that is awesome because I could tell you were trying to cram in as much information as possible in those last moments it was incredible thank you so much for this talk how was it for you did you get everything in that you wanted to say yeah yeah almost but awesome well I think that is it folks a POA huge thank you for anyone that has questions [Music] fora