we've got another lightning talk coming up here we have got hamsa on Deck hamsa has been creating all kinds of AI applications for the last 20 years and I'm so excited to learn from you you're currently at Google we talked about what this talk was going to be about I know you got 10 minutes so I want to be very very conscientious of time I'm going to be the timekeeper and as I've been saying the the whole talk or the whole conference that I'm the shaman guiding us through the Journey so I'm going to hand it over to you Hamza and let you get cracking and I'll be back in 10 minutes perfect thank you so much can you just confirm that you can see my screen I can yes we see it awesome thank you so much thank you so much for having me today um and I'm really thrilled that we have such a big audience today my name everyone is hamsa bagen I lead AI Solutions at Google cloud and being in the cusp of innovation and emerging Technologies I'm really humble to share some of my observations around implementing llms today so llms and generative AI we all know has a potential to revolutionize so many Industries and aspects of Our Lives when I was asked to come and speak about the llm use cases I wanted to step back a little bit I think when generative AI is taking off at full speed we need to first of all have an understanding of the use cases but we need to also look at the use cases that drive the most value for your industry and I was referring to this McKenzie report on the economic potential of generative Ai and it really provides a really good framework for assessing your own industry so if you look here it shows you how you can highlight some of the use cases that drive The Highest Potential value and in this case it's showing banking Life Sciences retail and consumer Package Industries our teams have been working with so many different customers along around llm use cases most of you are familiar with these use cases it's really common nowadays from document summarization to content disc Discovery we've already seen so many benefits I work very closely with media companies on unique content Discovery experiences I also work with retailers on product catalog enhancements work with again U marketing companies on multi-channel marketing and healthcare companies on Healthcare dis research and information Discovery and for all you developers out there there are so many different developer tools now and um built on top of llms that can help you improve your developer productivity now before I go uh more deeper in I wanted to talk a little bit about the llm application stack and I was thinking of how best to bring in a diagram all the moving parts that in that is involved in llms abstack and I came across an article U from Anderson Horowitz and I wanted to walk you through to the stack using this depiction at a very high level the workflow is divided into three stages so the data pre-processing and embedding The Prompt construction and retrieval and the prompt execution and entrance so in the data pre-processing embedding this stage involves storing the data to be retrieved later so this can be any of the uh private data that you have in your organization and typically the documents are broken into chunks and so they are passed through an embedding model and then stored in a specialized database called a vector database during the prompt construction and retrieval phase when a user submits a query the application constructs a series of prompts to submit to the language model and a compile prom typically combines a promp template hardcoded by the developer and examples of valid outputs called few short examples any necessary information that's retrieved from external apis for example and a set of relevant documents that you're retrieving from your own Vector database and then you have the prompt execution or the inference and so once the promps have been combined compiled they submitted to a pre-trained llm for inference and this includes both proprietory model apis and open source or self-train models some developers can add operational systems like logging caching and validation at this stage so the reason I wanted to kind of share all the things that is done in an llm application stock so there's a lot going on and it's really great right super exciting but but I just definitely want to point out that although this is all exciting the LMS are really far from perfect and many of you know we've all played with it for the last couple of months and it's not really really perfect so while they are powerful and versatile they come with a lot of pitfalls so the users uh I mean as users you need to be aware of this so there's issues with accurately citing sources there's inherent biases uh it's generating false information U there's difficulties with math suceptibility to prompt hacking they are all challenges that need to be addressed and by understanding these limitations you can use your llms more effectively and responsibly and work towards improving these models and of course the cost of training and serving these models we know it's really huge uh from what I've heard U training a large language model such as GPD 3 could cost over $4 million now among all these risks and pitfalls the one thing that I want to dive in a little bit is mainly the part of making sure that whatever is generated is credible so how do we ground our llms how can we unlock the power of real world llm use cases but also make sure we can ground them and deliver accurate results now the one thing that most people have to think about is when you want to deliver accurate results you're looking at some source of data right or some source of knowledge that you're referring to and where is this knowledge this knowledge is very much in our own boring databases and that is actually our source of Truth so if you look at it most organizations don't realize that now with all the cool stuff like where is the data getting access from right so databases provide the most upto-date data and can efficiently now with Vector databases they can efficiently Store and search like can use them to S Store and search Vector embeddings they are your trusted familiar data store so essentially databases with Vector support bridge the gap between llms and Enterprise gen AI apps so first of all the database can provide you the most upto-date data for augmenting your llm prompts so this will help you increase your accuracy and relevance so you can combine the data from your production databases with the power of llms and the generative AI applications now can create more accurate answers than before second most of the data that we see nowadays is like conversational it's unstructured so semantic query is really a key feature that ensures the right data is retrieved and Vector embeddings can encode the semantic meaning in unstructured data like product descriptions help desk tickets and conversation history and lastly the best part is that your databases use your trusted source and you're already it's already supporting like all the part of the business critical uh things that you need on reliability data protection performance all of that so developers are already familiar so you don't have to worry about now figuring out a yet new system for you to understand the other technique that has become really really common is rag so rag is nothing but a retrieval augmented generation this is a technique and this was this came out first um in a research paper that was published by meta it's used for improving the quality of your llm generated responses so now what you're doing basically you're grounding your model on external sources of knowledge so that you can supplement this um supplement the lm's internal representation so what you can do with rag is rag can be fine-tuned on knowledge intensive Downstream tasks so you can achieve the stateof art results compared with even the largest trained sequenc to sequence language models and unlike these pre-trained models R's internal knowledge can be easily altered or you can even supplement this on the fight so this way researchers and Engineers such as yourself can control what rag knows and doesn't know without wasting time or compute power retraining the entire model and rag is more accurate at Q&A than purely extractive models and you might think this is somewhat surprising because you're extracting Snippets from existing reference and you're thinking like how is this accurate so now the way you want to think about this is uh the two sources the parametric and non-parametric memory complement each other and what the research paper found was rag uses its non-parametric memory to cue the sequence to sequence model into generating correct responses so essentially combining the flexibility of the closed book or parametric only approach with the performance of open book or the retrieval based method now the performance improves when rag has access to documents that contains cues to the correct answer but where the answer is never stated verbatim and rag even generates correct answers in certain situations where the correct answer is nowhere to be found in any of the retrieved documents and that's pretty cool now if you don't if you want to take away one thing from rag is a simple two boxes that I'm going to show you so the approach is as simple as you augment the prom sent to llm with relevant data retrieved from an external knowledge base through what is called an information retrieval mechanism The Prompt is designed to use the relevant data as context along with the question and it avoids and minimizes using the parametric memory so now if you look at it the external knowledge base that you're seeing is is your non-parametric memory and so this approach is essentially Rag and really great for generative QA and it's not just for generative QA this is just an example I'm I'm showing you now this is a depiction of how we have done internally we built out a QA system based on rack pattern and some of our products and services around our Flagship Vex AI that responds to questions based on a private collection of documents and adds references to the relevant documents there's one thing that you want to keep in mind is semantic search is very much part of rag and semantic search goes beyond our typical keyword search because it determines the meaning of the questions and Source documents and uses that meaning to retrieve more accurate results and so sematic search is very much part of rag so guys like we are part of a revolution right now and in fact very much the start of a revolution there is so much going on and my guidance is to just keep up to date on up upcoming research and tools which is what we are doing ourselves here although we are innovating so much and incorporating some of these techniques in our llm use cases and most importantly staying grounded so thank you everyone excellent amza thank you so much for this talk it's so cool to have you here and I love the work that you're doing some people do not know this but you've got a book coming out pretty soon so can we talk to people about what that is real fast oh my God that's a Shameless plug yes uh I'm writing an anthology on persuasive leadership and I'm talking about some of the challenges in technology leadership especially for women and I'm really really excited uh the book is coming uh probably in the next two weeks so stay tuned well a lot for that yeah I'm excited for it and I look forward to getting my hands on it and I know that it's really a lot to ask of you to do this conference while you're finishing off the final touches of the book so I appreciate you coming on here so much and doing it and thank you