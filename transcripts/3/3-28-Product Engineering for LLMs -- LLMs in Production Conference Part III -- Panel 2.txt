[Music] Char very good to have you here we have a big panel right now and I'm going to start to bring on do you want to like introduce people and then we'll like dramatically push them onto the screen I love that yeah um to thunderous Applause yeah thanks Adam um yeah so I guess I'll uh start by introducing our first p analist uh shamala praga who is a senior uh software product manager at Nvidia um sh some great work on voice assistance um in uh and I'm looking forward to hearing from her about how LMS have changed that landscape nice I think you're here great um keep on bringing folks on the stage next up we have Sahar um who is currently doing product at stripe has worked as an engineer and an engineering manager uh which is exciting the topic of this panel is product engineering for language models so at this intersection of making a useful product people like and making a computer do stuff when it doesn't want to um that uh we got a lot of folks with that intersection of skills and uh Sahar is one of them I'm excited to have them on hey everyone great to be here and I just left stripe few weeks ago but same knowledge i' would like to share so oh awesome you're still wearing the shirt um yeah all right and then uh next upep we have Sarah guo um who is uh founder of conviction a VC purpose-built for software 3.0 um and it's exciting to hear from the capital providers who make this possible what their perspective is on how we're going to make you know a new kind of software with with good design and good engineering great to be here uh and then uh last and of course not least we have William P uh who created Feast uh and then we detect on so some familiar names maybe for folks in the mlops community um and William is currently founding in stealth um so we'll see whether he can share anything maybe you know blink twice um if you have a secret to share uh maybe not yet but hey Charles and hey everybody great um yeah thanks for coming everybody I'm really excited about this panel um I think with large language models um we are now about a year into this kind of hype cycle and we've spent a good chunk of that in what Alan Cooper uh the interaction design guy the father Visual Basic would call the dancing bear phase of Technology um where like the exciting thing is not how good uh the the product is but the fact that it can do anything at all like a bear that can dance um so the the kind of theme that I want to talk about here is how do we move out of it and in the past with uh with goys in the 80s with browsers in the 90s with Mobile in the 20 2000s 2010s it has always required both product and design sense and Technical and Engineering sense to like get out of that phase into something that both like works is like technically sound um but is also a good product um so uh to start off um let's talk about some of the successes uh in using LMS in products or as products um so I'm curious maybe starting with uh Sarah uh with your broader perspective uh um up there uh on the heights uh of venture where have you seen llms or LM powered products succeed and what do you think were some of the key aspects that came from the design side or the technical side that made them successful yeah I mean I'd start with the premise that uh these are um the reason that this era is different at all is because we have um General technologies that are applicable to many Downstream tasks and I think what we've sort of seen in this first year is um the new capabilities we get out of them are easier to use in real environments in some tasks than others right um and so you know we tend to think of it in like a few different framings one would be like how robust is the task to error um another is just like how much work is um in a particular task is um automatable uh sort of end to end with the reasoning um models and the generations we can do today uh and and so some of my favorite examples are actually in like wildly different areas right so you have um people I think like there are a lot of companies in this area now but uh people who are trying to autom at um an workflows in the legal world because it's a lot of text in text out right you email the client you read a document you summarize the document you try to reason with a particular framework you generate legal motions right like models can do so much of this now um and I think there are other professions that look a lot like text generation right code is a form of text generation we're just progressing along that Dimension um and another sort of field that I think surprised many of us has been uh sort of um uh media creation right I think everybody attending this conference will have spent some time with mid Journey but I've been super excited about the progress uh that's happened in video generation just because it's so um you know useful from it's expressive and it's useful commercially so everything from um from Runway to Pika to haen which is like virtual avatars and translation uh that really is sort crossed a quality floor over the last six months yeah um so you mentioned text generation media generation and with a lot of focus on like robustness and quality so I'm hearing a lot from you that's very much on the like technical capabilities side I'm curious um focusing on those two examples of text media generation maybe um does anybody have any uh thoughts about particular product innovations that have um that have like allowed those to crossover from Cool Tech demo to usable product that people will like pay for or uh be able to to you know deploy confidently at scale I can talk a little bit about that like especially I have seen uh you know with all these great uh Technologies coming up there's one advancement which has happened is the right and how we are able to compute these things of course uh uh gpus are much more capable of now uh training the models at scale and you you have all these different things but one thing one advancement which I'm seeing is the noot platform so up until now if you have to train the models it has been like a very big task in itself like you need to have the right kind of infrastructure in place but also you have to have the right kind of data set and then the whole training process itself and how long it would take all of that is a very tedious process right if you have to train the model from the scratch or if you want to find tun it from an existing model but now the notb platforms which are evolving and there are many many of those and almost every company have some variant of that if you see right including Amazon and Microsoft have some VAR of that it's now helping us it's like becoming no-brainer the bar is becoming so low like okay you come you select to base model you select uh or upload your own data set you select whatever hyper parameters parameters you want to adjust and then boom you have this uh uh training pipeline running in the background and now you have a model which you are able to now test it out so I'm really seeing that as a breakthrough in the industry because that's helping in anyone and everyone to come and try out a variant and then see how the model is working that is also opening up opportunities in the community to have lot more OSS models out there for people out and explore and you know building that infrastructure and ecosystem yeah um oh did you want to take over I just want to add on that as well that one of the things you basically ask what enable this revolution in many ways the ability to have these larger data sets of text for example gpt3 was trained on Comm and these kind of data set so having more data also well curated you want one of those catalysts to what we see today with all instating langage models definitely um from a from a commercialization standpoint I mean it's hard to ignore the big players right like GitHub co-pilot moury and just chat GPT they're lowlevel well at least in chat gpt's case it's a lowlevel abstraction but it is such a powerful tool um and if you learn how to prompt it um you know you've seen the large amounts of money flowing into these Services um of course get have copal has many years Head Start in front of many many other folks but yeah we're obviously seeing a lot of new products being developed but those are the obvious ones yeah and um I think it surprised some people that chat gbt was such a huge explosion relative to the gpt3 API um but I think if you've sat a non technical person down in front of something like the playground or the API versus the chat interface it's like it's night and day um so I'm wondering maybe uh from shimala you've worked a lot on voice assistance in particular so I'm curious um what have you seen that are the kinds of like unintuitive or surprising or high leverage changes to these voice assistants that it's not necessarily just about their engineering capabilities their latency or whatever but the actual sort of like product design the whole product thinking that makes them sticky or better yeah I mean if you look at and that's a very good question so if you look at the traditional way of Designing I mean in my mind uh when you're designing a digital assistant a lot of different generating capabilities are there I mean I I don't like to call it uh like conversational AI but I like to call it generative AI because ASR is a kind of generative AI to text or text to speech is so uh digital assistants in general the traditional way the way it used to work was this some sort of uh speech recognition system which would you know interpret what you are saying and then there was the understanding that is where you know a lot of advancements happen we used to have nnp and it was very much rule based you had intent based and you would have a couple of atanes TI to an inent then you would say like okay if the user is asking this these are the different erences and it would not go beyond that so there was some advancement which happened eventually like yeah so for example will it train in Seattle do I need an umbrella or whats the weather in users's mind they all meant the same thing but it was not very natural for us to or the language models to have that kind of capability and we had to you know tune to each of these different utterances I still remember those days where I used to collect these utterances from users through different mechanisms and then feed into the system and Tra the model that has changed so large language model in my mind is like a rapper around the conversational AI technology but in an advanced way so not now it's so much open it's able to understand because it has so much you know Knowledge from different sources it a it is able to interpret the intent without I having to you know manually feed the intent now the other advancement which has happened so when I used to work for for Motor Company one of the challenges we had back then was we had this huge owner manual right and then uh if we have to Voice or voice enable or make these uh um uh you know owners manual as a con conv AAL assistant think I can chat about it or I can talk about it believe it was very complicated with all the things like we had a lot of manual effort back there to be able to uh create a bot out of or the knowledge base out of that uh owner's manual now with the advancement with the rag and the you know all these different things coming together I feel like it is becoming much more easier you are able to feed all these different knowledge sources to the system and it is able to you know generate the answers based on that so that is really going to be the next big thing especially in the digital assistant world because it takes away the effort of someone having to Manu do um all the effort which was the case back in like couple of years you know I I think while we're talking about assistance um I think it is uh like Sam ultman who said like we're trying to replicate the median human and I think in the voice assistant space um or generally with assistants and um and like personified Bots it's not clear to me people want the median human right they want either the best m&a lawyer or they want Elon Musk or they want their personalized version right they want like their Ai and so um you know I think one of the things that has been really interesting to see um you know pigging backing on um uh what was just said is uh there's so much to do in terms of development of empathy right like media richness like voice and video um interrupts fluidity of speech and then also on the um sort of more uh U model development side in terms of like personalization memory skills like things the assistants can can do and so um you know I'm an investor in inflection also great companies like character out there I I I don't know that we're going to end up with median human versus like experts and personalization and like you know humans are interested in indiv individuals right I think there's a tendency to say it's a median human because the models hallucinate and so we're trying to say okay there an expert and the expert can judge the quality of the system but really I think Sarah you're right there's a new class of product that can still fulfill the job to be done say it's a legal team and they've got some problem but what you want is a assistant that can scan millions of documents where a human couldn't have done that before so the breadth and perhaps even the depth can go and find correlations between documents that the human would never be able to find so You' be augmenting your team and unlocking new possibilities with these products yeah actually exactly about that what I usually see with like median human kind LM applications and really great applications everything around evaluation so they have the capability already it's mainly there on the application that being written in a written up way which then needs to donation so teams that do it really well usually have a whole evaluation set a whole data set that allows them to constantly Monitor and make sure that the performance they see is actually what they expect and then iterate on that and that's what usually takes from a median human health level to an expert level so um it seems that this personalization with the like successive character. for example this like personalization is clearly like a pattern that can be applied in other places um I'm wondering people have any thoughts on other patterns or anti-patterns that they've seen in the creation of like an llm powered feature or an llm powered product yeah I think that everything that involves latency is really challenging for everyone so I mean probably the one one of the hardest intersection is latency and user facing and open-ended so many companies building open-ended chat or interfaces that behind the scenes we have lowered apps it's really difficult to make sure that we we see the the exper or we have the experience we expect to have so again having this evaluation that that's really helps um but also latency is a big challenge so how do you make sure you like only want to get great performance so you may have like multiple prompts and multiple techniques like CH forication from recent from mea and the like but you also want to make sure you experience it really fast so if you have let's say a chatbot strip we launch these strip docs that allow you to ask questions and for documentation then how do you make sure that the experience is fast enough but at the same time you don't see inated text so I think Microsoft had one of those examples with Sydney when they just launched people would play with the chatbot and then they got all this off results sometimes and then um after a few weeks or I think even days they deploy this update that after you get a generation there is another prompt running behind the scenes asking the generated answer is actually not harmful or biased and the like and if so it literally like delets the answer there are many ways to sand like latency and also so and maintain performance but it's definitely a challenging space yeah I think that particular example is one I saw a lot of people pretty upset about to see an answer appear and then like suddenly disappear feels a little bit dystopian so that's one of my favorite examples of an anti pattern for how to solve that problem with the latency versus uh correctness and checking yeah maybe I I'll mention two different patterns I think are are kind of interesting um uh one's in a company that we invest in and one is just something that like we're continually looking for that we we think will work so um uh a a common issue is that humans are like not that good at prompt writing or specifying exactly what they're looking for right um and so an example I'd use is where investors in this company seek thatai they um uh use among other Technologies like uh SQL generation from natural language to enable business users to ask questions of structured data sources like the warehouse but as you might imagine like any company that has a reasonable amount of data 16 data teams the idiosyncrasies of like how they actually transform their data and like what you know customer count means in this department versus the other department means um it's actually very hard to go from a general usually under specified question from a from a business user to the right generation right like the SQL is actually not like we can be best class but that is not actually the hard part um so the pattern I think is kind of interesting is uh companies figuring out like what is the workflow to be predictive right like use for example like you know um query generation or ranking Technologies or iteration with the end user to get better specification about what they're trying to do versus expect every end user to be good at well essentially prompt engineering right so I think that's one one interesting um uh sort of Frontier that people are working on um another is like everybody loves um co-pilot but co-pilot is at once like one of the most um like very limited and also crazy ambitious things to try to do right um like we work with Alex graveley who's onto a new Mion which and he's the Chief Architect at at co-pilot and he was like well like the the dominant feature we've talked a couple times about latency but it was like you know if I type a 100 words per minute like you got to stay ahead of me to be useful like that's a pretty tight latency Bound for Anything um for anything that is LM powerered um and that also means like you know you're you're doing like syn almost synchronous auto complete with local context like that is not all we need to do in in software engineering and so particularly in these areas where um uh there's been some you think it'll there's like going to be success in the modality I think the um the interest in trying to do more things asynchronously and figure out how to come back like go run a job right you know if you're an engineer you work on my team like you go do something and you come back to me and say like I figured it out or I didn't or I have a question right and so I I think that there's both like product and research effort around um how can I sort of use the this to some degree the scalability that William described to go try more things do test time search like you know try and compile coding and come back and and tell a user like oh I did I did more for you and it's like let me notify you that something's happening or gather more data and I I I think there's gonna be more emphasis on um sort of the more asynchronous uh patterns as well I think a different way to put that is where do LMS really annoy people and suck it's to put them on the critical path and you put them synchronously in front of the user on a job that they're familiar doing but whether's writing code and a get up copal it's on the side it's additive it comes in and makes a suggestion but you can quickly glance and say you know you're crazy I'm going to ignore you and I I'll continue doing my job I think that applies to all use cases like humans have a specific way of doing things and you just incrementally introduce yourself and then as these co-pilots move closer to Pilots you know we'll defer more to them but for now they're still hallucinating and they're very slow um and so as that improves we yeah and over and over MH yeah I think um a lot of the time for that kind of asynchronous communication or that ignorable communication it's like text based or visual you know it's a it's a toast that says hey maybe you want to do this or the ghost text in co-pilot yeah related to this sorry one more point is like the fatigue that I think will set in so even in Co generation if as humans we're going to constantly be reviewing the agents so this is the take C generation like that can become very overwhelming to engine engers right like it could become taxing and so you want that accuracy bar to be high or you want to introduce the agent in a way that is reviewing your code and not you reviewing its code um yeah if you 10x the amount of code to review you 10x the amount of code review to do which people don't talk about when they talk about boosting productivity yeah um so uh the question I was uh going to ask to shamala was with audio that's like something that really captures your attention it's like you know even a tiny glitch in audio is like something that you just immediately snap onto um so I'm curious do you have any thoughts about how this sort of like asynchrony um or this like you know deferral might work with something like a voice assistant does it have like can it call you on a cell phone um or or how how else can they achieve that phenomenon so your always kind of broken with me if you could just repeat it I don't know what's happening with my internet so yeah incredible an audio issue while talking about the trouble ex audio isue the way we're talking about B yeah so a text assistant can like you know pop up a notification or do ghost text um and that allows it to fade into the background um but for audio um it's not obvious to me how you would do that and I don't think I've seen that pattern with anything like you know Alexa just like interrupts my family dinner if I happen to say its name U so curious if You' thought at all about how that like asynchrony or or um you know Fading Into the background and co-piloting might work for audio uh so audio is just one part of it I I'm glad you brought it up because I talk about some of these things in my book emotionally engaged digital assistant right like I'm trying to bring bring all of these things together understanding personalization so in my mind uh you know it's it's all about affordances like how uh it should not always be proactive like you know every time I'm saying Alex I'll do this you know it will do it right but in certain cases it is very uh you know proactive in the sense like if something you know it's giving me gestures there's multimodal it even in that context you have like different devices so I feel like that's very important like how the feedback on notification is coming when I'm doing a certain task and especially with all the different kind of automations I'm able to do uh around the you know for example um I'm able to uh send the entire schedule for my day saying you know at this time I want this to happen at this time I want this to happen and then you know like set up an alarm turn on my AC or whatever you know I'm also setting up my routine nowadays how does that happen right how is the user getting notified the job is done or how does the user know what's happening sometimes there's audio cues sometimes there's visual cues sometimes there's some notification sometimes it's just like you know uh everything done now this is the task so I feel the same kind of pattern uh will be lbed in the world of propilot as well um you mentioned U multimodality um in your response and that's something that like looking a year or two ahead once GPT 4V is generally available and once other uh proprietary model providers and open models have caught up in capabilities is a clear uh big win um has uh does anybody have any examples of products that take advantage of that multimodality um that they have either seen seen a a demo of uh that they want to share we have a product called Nea the Nemo Vision assistant it is it is a multi model model and openi just launched a multimodel as well um so with this model uh it's available for trying out you can go to our playground Nidia AI playground and try it out as well what this does is it's an image and text understanding model so it's not just generating all the things what we spoke about like text to text but it is able to take an image and it is able to so if I have uploaded an image and I ask it like hey describe that image the model is able to describe the image for me uh if I just give the image and say write a story based on the subject of the image or the background of the image the model is also able to do those things right and uh the model is able to work Standalone in the sense like I can just have text and I can use it as a llm any any text to text model or I can use it in the combination with text and image and it is able to generate and give me responses based on whatever the subject L of the image might be that is one level of M modality which really is interesting I don't think about multimodality like yeah I mean just generating an image kind of context but I think about it more on the utility standpoint like how it is helping the users right and that's very interesting because think about people who have accessibility issues if right now everything which we have everything which we have even when I'm posting on LinkedIn or somewhere else I have to add the all text so that people who are using screen breeders they are able to actually understand what I uploaded otherwise they will not know what's happening but multimodality can change the Dynamics in that context completely I don't have to the llm is capable of doing that for me so it's very descriptive so even a person who have any kind of disability or want to understand what's happening in the context what's the image and all they are able to get much more information than an an ALT tag which I will write which will be like a person sitting on the bed that's it right that is the kind of thing so I'm thinking about uh the the multi modality in that context and it feels great what it can do for the advancement of the industry or the community yeah yeah I also adding about that maybe like the first part is about what does it mean for like product and engineering and design in that sense so the first one unlike maybe text we should remember that H multimodality usually involve video audio which are more expensive compute also store which is going to change probably profit margins and also latency constraints that are already quite challenging even with text at the mod at the modality and then the kind of prodct that I've seen doing it relatively well so I think multi modality that well is one that you don't really feel there is like multiple modalities behind the scen so in the way Google's search generative search experience is one examples where you write text or a prompt and you get multiple results based on video and image behind scenes and then two areas I think that are currently are about to expose once multimodality gets there I mean the first everything around content generation like imagine we can create Tik Tok videos like seemlessly if we use like 11 Labs or combine with SoDo and like all these other amazing products to generate our videos without all the efforts currently associated with that and the second part is I think Charles we talked about a few weeks ago is everything around robotics right so robotics is also having its moment so once you can combine like vision and other sensory and signals you can do some amazing stuff with robotics as well I I think one of the um things going back to uh making it easier for is it's not necessarily focused on um multimodal understanding in the sort of um final uh more extreme sense but the ability to take richer user input um versus expecting everything to be text I think is really powerful right and I I'll give you a few examples um and Sahar just made me think of this right if you um and being very use case specific is is very impactful so for he Jen like you you could as a user like string together a bunch of things and say like okay I'm going to use element T I'm G to use 11 to um you know take do speech to text and then do text to speech in a different language and then like try to match it up with um uh the sort of uh video Avatar models that allow us to match like lip movements and motions right or a product company could do that for you right and there's definitely research involved in um designing models to interact well that way but um I I think the fact that now like what users want to do is just give you video in one language and get video out in a different language that works perfectly I think that's one example another example would be something like Pika being able to um uh you know create video from a given image uh is is um it allows for much simpler controlability than right and we've also seen people figure out how to go back and forth with editing that uh so like you know managing structure in images and videos is still very challenging right so a smart product person way to go handle this well integrate with workflow tools that allow you to manage structure right that could be a um a Photoshop or a 3D um uh a 3D modeling tool change the structure in a very uh specific way and then re render for example right and so that I I think um there are uh lots of interesting product Innovations happening right now to allow people to and users to get the outputs they want in more intuitive ways which includes like editing and input in different modalities all right um I think that's all the time that we have for this panel so uh uh want to thank shamala SAR will and Sarah really great conversation a lot of very distinct um but complimentary perspectives on how we're going to make use F products out of large language models and large multimodels multimodal models in the future thanks a lot thank you very much everybody and Charles thank you uh for being such an excellent host and [Music] moderator