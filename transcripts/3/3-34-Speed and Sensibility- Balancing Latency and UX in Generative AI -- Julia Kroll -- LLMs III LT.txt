[Music] hi everyone I'm very happy to be here my name is Julia Cru and today I'll be speaking to you about balancing latency and user experience in gener AI as an applied engineer at Deep g i serve as a technical expert for customers using our unified API to build natural language understanding into their products I hear every day from customers who are excited to create an endtoend generative AI dialogue in which first a human speaks then automatic speech recognition recognizes the words that a person said transcribing from audio into text next an llm processes the text meaning and generates a text reply and finally text to speech uses the llms output text to generate a spoken AI voice reply to the human as I've worked with customers building their own chat Bots assistants and AI agents one theme that I've often heard is that they're looking for low latency Solutions but why does latency matter so much when you're building conversational AI one clear reason is that this archetype of an AI dialogue requires three consecutive steps which requires packing a lot of processing into a second or less another reason is that we're all aware humans are attuned to the Natural flow of a conversation and low latency makes interacting with an AI feel natural it preserves the impression that the other party is listening thinking about what you're saying and responding in turn within a reasonable amount of time having an AI speaker respond after too long of a pause gets into uncanny valley territory and even becomes aggravating or unusable as you go through several interactions on the other hand if you allow an llm a longer amount of processing time then maybe it can generate a more accurate and relevant response so it could become a worthwhile trade-off to allow the extra latency in some cases next I'll cover three primary sources of latency that affect your AI response times the first source of latency is model size one Infamous llm GPT 4 has 1.7 trillion parameters it's an impressive feat and gp4 is a powerful general purpose model that can field almost any question however in a production use case you typically aren't intending to field an infinite variety of questions from your users and that's where smaller model sizes become valuable rather than being backed by an llm your product may be best equipped with a task specific model that was trained to do one one thing well such as topic detection or information retrieval models may also be domain specific which can allow you to exclude a vast amount of training data since your model doesn't need to know for instance how to code or how to write a college essay it needs to understand specific topics around what your users will be asking it if you can distill your model from trillions or even billions of parameters down to merely hundreds of millions of parameters while while still enabling it to learn all of the information that's relevant to its use cases then you have a small efficient model that can be substantially faster than a huge lln then once you have your task or domain specific model you need to host it often models are hosted by a third party you send an API request to another server which returns a response to your server after hundreds of milliseconds or maybe a full second or longer longer however an alternative more advanced option is to host the model on your own server this is commonly referred to as on premises or onr this could either be in your own data center or collocation facility or on your own virtual machine via a cloud provider hosting a model yourself can easily shave off at least hundreds of mil milliseconds because your requests don't leave your server they also o don't need to be processed on a thirdparty server that may be congested with many other users requests leaving yours to wait in a queue to be served that's why un Prem model hosting is a very popular route that many of our most latency sensitive customers love it's also one reason why open source models like llama are so popular because anyone can host their own instance of them it can be more complex to set up and it requires you to have your own gpus either by owning them or by getting a hold of them via a cloud provider but hosting your own model on Prem is a reliable way to reduce latency and establish consistent response times another bonus for privacy sensitive customers is that their data never leaves their own environment and finally the third source of latency is all the cool features that you want to offer your users this gets into an intriguing question about gener AI besides low latency what does good user experience actually mean in the context of llms we usually talk about ux in the context of website design or visual media but our AI dialogue is mainly about listening and responding with speech and language in order to understand good ux and generative AI first recall that your users have a certain core goal that they want to accomplish by using your product by the simplest definition of ux your product should enable them to accomplish that goal so you need to understand your users's intent in order to help them succeed however we're all striving for more than just not disappointing our users we want them to be pleasantly surprised delighted trusting and rushing off to recommend it to others when you design your generative AI product prioritize the goals and features that will offer the biggest positive impact to your users so that they're even excited to wait an extra moment or two for the sake of a more enjoyable user experience I'll give a couple examples of richer features you may want to support one is language detection maybe you want your AI to be able to listen not just to English speakers in the United States but to people speaking dozens of languages worldwide language detection will add some additional latency but it's worth it for the smooth user experience users won't have to specify which language they're speaking in the model simply infers it and replies in the same language another feature is sentiment analysis maybe you want your AI agent to be able to analyze a customer sentiment in order to recognize when they start feeling frustrated or angry that will cost you some extra latency but in a setting where customer emotions are often running High it would be a valuable addition to provide an empathetic touch to an AI mediated user experience you can see that the goal here isn't to slash our latency to near zero at all costs some latency is inevitable rather it's to isolate and understand each source of latency within your AI and then interrogate whether each one is providing a worthwhile addition to your user experience for sources that provide no tangible benefit to the end user such as model size and model hosting you can ruthlessly minimize that latency to the best of your ability for sources that do provide additional richness to the user experience you can incur that latency strategically to the extent that your users will enjoy and find beneficial remember that in the end productional ising generative AI isn't only about staying on The Cutting Edge of ml advancements but it's also about enabling a natural conversational experience with people who have their own goals expectations and emotions and the further Tech technology advances the more opportunity we have to seamlessly integrate it into our everyday experiences for the better thank you and please reach out to me with any thoughts or questions thank you very much Julia let's see if we have any questions here I think questions might come on slack so uh folks can find you there Julia thank you very [Music] much