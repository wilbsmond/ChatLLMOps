I've got somebody in the studio here with me right now Mr Paul where you at dude come on get on stage man let's get this happening we don't have to man you too so what's this right here well I know you like to hallucinate so we brought you a plus one asset for your hallucination what is this so this is uh as I've been saying I'm the shaman on this journey and now you're coming bringing me gifts huh so the journey I heard that's a custom so uh welcome well dude I know you got a you got a talk for us you all have been doing a lot of cool stuff when it comes to plus one and for those who haven't been experimenting with plus one it is happening now in slack so go ahead throw something in there ask it questions but you're going to share all the learnings with us right that's the idea yep all right and a little bit of peeking behind the veils showing what kind of models you have going on in the background why there's some hard problems some easy problems that's it did I just give away your talk no that's it little spoiler I'll let you go for it have some fun I'll see you great thanks Demetrio so um what I wanted to take the next uh 20 minutes or so for uh for is to essentially share um how we've uh tried to bring some of these models into the products that we have in the uh companies across the group as Euro mentioned earlier and Demus alluded to we're a very large group of different companies doing different things and um I I just wanted to kind of share that with you let's see if this one still works there we go um so uh across the group we serve about two billion users through the companies that are uh part of the group in various categories in education food delivery and groceries and finance and so on some of the companies that we've worked with very closely on uh helping them understand where the value of AI and machine learning and now the new generative AI uh tools can be useful uh are here and um we want to share some of the things that we've learned in uh in doing that so um we've actually been working on these tools for some time I'm part of the press AI team and uh you know our goal has been to work with the companies I just mentioned to help them understand where they can apply machine learning bigger better faster and so of course generative AI is uh isn't something new as certainly most folks in the in the audience will know um the GPT models have various Generations before them there's been generative models in other modalities in in in speech uh in code and so on and we've been working with them for for quite a couple of years and um and basically have been trying to understand what be what could be the potential value of some of these models um in the products that we build for the users that we serve across the group and so that's something I want to share with you now some of the questions that we continuously try to ask and uh and you know we've got the Stormtrooper here uh and this will come back at the end of the presentation when we answer these questions but are at the beginning of this gen wave about you know maybe 12 to 18 months ago when we were seeing that these models were becoming uh you know much much better uh where first of all is this a parlor trick right everybody's trying to write a poem everybody's trying to create a funny image or is it actually use for some of these models that we're uh working with um so that's that's one thing that we uh we're certainly trying to spend time on the second is what are those use cases across the group that um you know made sense and so trying to discover those uh making sure that we invest the right uh you know time and so on on on then building out those use cases uh into the theme of like this conference into production right you know do users want these capabilities and if so how does it help Learners does it help folks that are ordering food and so on um should we build our own models um you know like uh Timothy just talked about uh for some of the things that we're doing or are we better off uh from a just business perspective to consume the commercial models out there and if so when and which model and so on and what does it take when we deploy these things at skill not just for uh you know for a couple of users but as we talked about hundreds of millions of users um that use these products so how do we figure that out how do we try to actually discover that so we're basically trying to explore in the last year and a half or so very specifically what are the areas that we could use this um and one of the ways that we tried to do that was through a tool that we called plus one because it looked and feeled like a plus one team member um and it's uh available to uh all of the teams across the group um basically it's got um any and all large language models other generative models under the hood that uh you can interact with by asking questions like generate an image transcribe this model transcribe this audio uh review this code document this and so on all of the questions you might imagine but in one interface and making it really easy um to to talk to it mostly through slack uh thousands of users have been experimenting with plus one uh to kind of go on this Collective uh journey and one of the things that I wanted to briefly show you how this looks and feels so just so you can get a a sense of it so it lives in slack as you can see here um and it can do a variety of things you can ask it to generate images of a pizza uh and it will do that it uses some open source models uh for that you can also ask it to for example transcribe this meeting again different models for that um and then maybe suggesting quotes um other tests that are happening you can actually ask it to summarize internal Google Docs and it will do that this documents about plus one's PDF processing features with it which it's able to do and um of course you know I also wanted to make sure that you know if Demus is still here he sent us this little picture from him on holidays and we asked plus one if it could actually caption this image and very aptly it said a man standing on top of a beach holding a stuffed animal in a life preserver so it looks like de's life was preserved and he made it all the way to Amsterdam safely so uh good good on uh good on him so anyway that's just to give you a a feeling of how this looks and feels and so we've had as I mentioned thousands of users over half a million questions asked to plus one in the last year or so and we try to understand through some uh you know some some essentially uh history of these questions what do people go to this tool with is it a parlor trick what kind of task do people find useful and you can see on this chart it's a whole range of things right so a lot of actually software development things it's a little bit the bias maybe the makeup of our group a lot of tech companies there um and the folks that are more maybe more early adopters are the are actually often in those product and Tech teams but not exclusively lots of other different tasks like product management tasks General writing tasks and about a 50/50 split between engineering and non-engineering also interesting is when you look at who is actually uh executing a variety of tasks there's two things that we wanted to highlight and that was especially surprising to me was on the one hand if you look at the software engineering tasks yes of course you know the engineers are asking those questions but it turns out there's actually a lot of non-engineers that are also asking technical tasks the folks in finance asking to build helping build a forecasting model the folks in marketing that are now asking how to do more complicated maybe formulas in Excel and so on so it in a way democratizes skills which is a kind of a cool thing for any team for any organization trying to build things if you can give them access to these tools that seems to be one of the effects that we observed and the second thing is writing and communication which is something that of course everybody would be inclined to use but especially also Engineers are apparently you know asking uh in this case plus one and other tools to for example document the code or help me communicate with the user impact would be of this pool request and so on um and then if you ask people what actually they find what's their experience when they use a tool like this one there's a couple of things that come up so the first is that they work faster that's the productivity uh piece but also second order effect there is that they're able to do more so more types of tasks right people let's see maybe the finance analyst example that's now able to write Python scripts with much more ease because they've got a a co-pilot essentially helping them to answer questions and debug where they get stuck um and generally that's the piece here getting get going right so not not be stuck not have to wait to An Answer not have to uh you know go through maybe documentation you're unfamiliar with it helps you get going and when we actually did some uh some very specific studies on trying to measure productivity and speed and so on generally people are extremely uh clear that it does help um them be more productive on a a subset of tasks obviously that's also corroborated by you know other research out there and so the interesting thing is that you know if you're interested in using these tools for your teams essentially one of the conclusions we were able to draw is that it's able to improve productivity uh the moment you uh make this available to uh to yourself or start using it or your teams now there's another process here which is well what are the business use cases that actually come up so besides productivity which is sort of a horizontal implication of using these tools there might be other very business specific things that these uh kinds of generative AI models could be used for now um the example I'm showing you here is from uh one of the food delivery companies in uh in Brazil where of course they will talk and we inclined to speak to uh plus one in in Portuguese and you can see that not surprisingly of course it understands Portuguese the question here is what are the ingredients and quantities for one person serving uh for a chicken strogov dish why is that person asking that question because typically in this business there's you know tens of millions of different unique uh items menu items uh think of it the dishes that restaurants upload that folks can order and try to understand what goes in those uh specific dishes is important to help uh users discover that food and so this is a question it looks like uh uh uh you know these models under the hood plus one is able to answer it responds with the list of ingredients and the quantities and then interestingly uh this user then makes the next uh conclusion says well wait a second um you know we've got tens of millions of these dishes uh uh we want all of those enriched with uh this information can you structure it in Json and those this is one example where we saw hundreds of cases where people are collectively discovering what these things can do what these models can do for their use cases and now you've basically identified this could be one of those uh use cases that would be extremely valuable where in the past we know we have talked about training for example our own models like food BD and other smaller language models to do exactly this task basically uh categorize classify h do enrichment on these dishes that now you can do essentially uh with a uh a model that isn't specifically trained for that task um and and that these are the use cases that were being discovered now maybe one one step back on the bottom left of here you can see that this particular case there's four emojis right thumbs up thumbs down heart and the little Pinocchio and that's a mechanism that we use to kind of collect feedback to understand you know how well how good is this response how valuable is it and uh users have used this a lot to give us information and and also signal hey this this response was good we could then go back to the models and so on to figure out uh in which case which model uh version was doing better on specific types of TXS and so on and so we're collecting all these things now the first three uh emojis here you will of course recognize what they do I think it's very intuitive but the fourth one uh is related to uh the mushrooms of Demetrios right it's the hallucination we call it lovingly the Pinocchio Emoji uh that uh that we put there because when the model would make up answers uh we wanted to make sure we captured that um so we could see if there's something we could do about that and if you look at uh let's say the progression of um the uh feedback that we get uh specifically for this hallucination Behavior which as many of you know is is one of the Tendencies of these these models you can see that we started with one out of 10 or so answers uh you know fell into that uh category but over time we worked hard to figure out how do we get that down and actually now this is a stat from June we're uh you know comfortably below 2% and there's a lot of different things that we did to get there and that continues to be a lot of effort especially if you want to deploy these models in a safe environment and some use cases that we have whether it's education or otherwise you want to make sure that you you you don't um uh have hallucinations and so how we did that is we looked at what are the types of reasons these models hallucinate Which models hallucinate can we solve some of that through prompting which we then did can we solve that they're actually connecting these these um uh models to specific uh knowledge bases and in fact in particular in professional context for example in slack that turned out to be one important thing because people do ask uh these models in that team team context about information that's internal and so a lot of work was put in to actually make sure that you could ask these model these models through plus one to surface information for example that lived in Confluence right so that's sort of the traditional uh rag approach making sure you do retrieval of the relevant information uh by having it available in some way having it indexed either through embeddings or otherwise and then uh put it into the uh The Prompt at the mo moment of generating the answer and then doing appropriate attribution to the sources as you can see here now this is one of the things we did that we did many other things to try and improve that it also it's one of the lessons that we learned it's very hard and um uh context specific how you build this rag pipeline you know are you for information retrieval are you doing uh just using embeddings and which embeddings do you use it's very different if you use documentation or text versus code do you do hybrid search uh what's the chunk size all those hyper parameters are very sensitive to the use case um and so we spent quite a bit of time trying to figure out what the the best way is to do that for uh your your context that you're trying to pull in now all these lessons are the things that uh you know we were able to uh to share with the group and some of those have been used for uh some products that have been launched publicly uh you will hear about some of those later also in the day and uh in the rest of the agenda uh you know we'll have brainley uh one of the largest uh you know K through 12 education uh platforms out there talk about the the work they've done to make it easier for students to get answers to their questions good is an online uh uh education platform as well um that has done similar things to creating educational tutor uh as we talked about you know in the in the food space making conversational ordering easier in in in in Brazil in Portuguese that's work that ifoods talked about and I'm sure many of you might have seen the Overflow AI announcement that stack Overflow did where um again some of these same techniques were being used to help uh folks like ourselves answer questions and get unstuck and there's many more in the pipeline um that will be launched soon another thing that matters for us because all these examples I just mentioned they're uh these are not small companies they each serve Millions sometimes hundreds of millions of users the first question is does it work the second once we've answer that is can we afford it does the cost make sense so we continuously try to understand what are the costs of uh of of answering or fulfilling this question fulfilling this task and um we spend a lot of time trying to bring those costs down so we can actually do deploy these things at scale and you can see that we've been able to reduce the cost for a specific task over time and again there's a whole bunch of things that we do to do that some of that um as you will understand has you know simply to do with uh basically choosing a simp a cheaper model uh right so if you have a smaller model then uh that of course will be cheaper either per token if you're consuming commercially or if you host it yourself fine-tuning some of the models um doing uh things like uh you know more efficient prompting uh more efficient answer generation uh picking you know between large context models small context models all the different things that you can do here to depending on your use case uh pick the right uh model to make sure that it's uh um you know economically uh feasible uh to actually launch this I want and some you know some of the things are are maybe more obvious than others but one of the things I thought was interesting in particular also highlights what we see in in our group because we operate in many different uh languages is when you look at prompt economics um um you know many of you will know you pay actually per uh token that you when you interact with these models and how does that total token uh billing happening well you get essentially the tokens that you uh typically would put in for the instruction that you give the model second the actual content in this case it's a translation example the sentence you want translated and the response now I want you to think about this for a second because what stands out when you look at this uh these tokens for each of these line items you know we talked about different languages somebody mentioned M STW and the reason for also potentially training on non English data sources you can see that this token uh uh tokenization uh approach here is actually much more inefficient for non-english languages than it is for English so for Spanish in this case you're paying double the tokens um uh and therefore double the money uh for uh having that generated and that also applies to things like code right so here you've got 1776 uh characters in English on the left that require 356 tokens but the same number of characters in Python are more than double so again this is something that we discovered because we're trying to figure out how we make this economically feasible we are working in many different languages almost a 100 countries across our uh uh group that we serve users in and so most of those or many of those will be non-english so thinking about about how do you effect effectively use uh the models what tokenizers you know as you will know it's not easy to change uh uh you know when you're basically bound to the tokenization that the models the base model is trained on but so picking the right tokenization can actually help you a lot when you go um uh to non-english to avoid the sort of non-english token tax that happens on many of the large models out there um then I also want to talk a little bit about agents because again this is one of the areas which we try to discover okay does this work right people talk about them um and uh in fact um it turns out um uh you know I have you an example here with plus one we have created some agents here's one that does data analysis where you can upload an Excel sheet it then goes to you know a python execution environment um and through you know variety of uh let's say uh building blocks here you can then have um uh plus one generate uh in this case a chart and do some of the analysis now uh our conclusion has been so far that it's really early days in agents uh when they work it's phenomenal and it really does provide you kind of a magical moment for a user uh but they're very brittle very sensitive to updates in the model very sensitive to the prompt um you know when you start doing executions the kind of room for error compounds and uh they're very you know uh let's say non-deterministic that non-deterministic behavior also compounds and and creates kind of a strange user experience because you never get the same answer twice uh even if it if it does work so we are exp exploring this this area we do think it's promising but it's something that's still early days now the Stormtrooper is back which means it's time to sort of wrap up and try and answer the our own questions we had at the beginning um and uh and recapping the lessons that we learned in essentially about a year a year and a half in deploying some of these models um you know across the group The First question we had is okay how do you find Value where do you find Value one of the areas that we can confidently say we do see there's value to be had already today is on the side of productivity and whether that's using you know GitHub co-pilot whether it's using you know a specific legal tool for marketing teams or plus one or whatever it is we can see the teams that use it um are consistently reporting that their productivity uh goes up and so um that's something that can be done can be had today then there's the question of sort of candy versus vegetables and the use cases and what I mean with that is there's a lot of hype today and so a lot of senior leadership in particular but also other folks are very keen on going for the flash of use cases let's try and do something completely new um and I think there's a lot of innovation experimentation happening in that front uh and that's great but at the same time uh there's a lot of uh use cases that are maybe much more obvious where uh we're already solving the problem today it's not really an innovation question it's more about can we solve uh this existing problem in a new way using these LMS where I think this data enrichment example that I gave you about taking a dish and understanding or asking a model what its ingredients are is it vegetarian Does it includ include allergens and so on is something that previously would require collecting your own data set fine-tuning a model and then running that now is much much more easier and it's actually very valuable so you can do that today so finding a balance between going for the SE stuff and uh at the same time also making sure that you pick up the unsexy low hanging fruit that's the second point there the third is that um even for those sort of Novel use cases where you say hey we're going to build something that's entirely different changes the user experience or it's a conversational food ordering bot whether it's generating images uh you know for users and listings doing automatic content generation and so on um it turns out that it's often easy to do a PC but especially when you're operating at scale um shipping that product in a cost effective way um is hard and uh making sure you actually have impact on your metrics um is something that still needs to be demonstrated in many of these cases so it's much much harder than often uh would appear when you see a successful POC um and then the second uh set of learnings is more on putting these things into production right uh the first piece here is that often when we try to discover where this value is you actually don't need to find T your models yet I mean we love to do it and we're doing it but I think for understanding where you can actually have this task or this question answered Tas done question answered with any of these models you don't need to necessarily start fine-tuning I think Euro talked about that sequence right you can do some prompt engineering you can maybe pick a specific model you can and so on and so on to make sure that you can actually try and experiment fast before you start thinking about uh getting all of the the things in house to fine-tune your own model which then needs to be hosted which you know has all sorts of other uh complications down the line the the second piece is around retrieval augmented generation it's um uh let's say the the the Hot Topic in town and everybody believes that and and we do to certain extent it can solve a lot of the hallucination problems I think that's uh certainly potentially true but actually making this work in a um in a generalizable way where it works for end code end uh documentation and different types of formats of information whether that's a book or paper or J cards or you name it and making it generalizable is very hard um and that has to do with as I mentioned your information retrieval strategy your chunking uh method uh whether you do um um you know how you actually put it into the prompt do you do that all at once or do you have separate you know prompts and then you combine and there's different approaches and strategies that you have to consider to make sure that you actually do that in an effective way with the right attribution um the third piece here is around a couple of really key fundamental uh challenges that still remain unsolved uh the fact that a lot of these models still don't have upto-date information and and facts that are relevant to specific context of users whether that's in a professional environment in a team or an educational content or whatever the fact that they're non- deterministic and reliability is sometimes Challen challenging and the fact that we don't fully understand what these capabilities are and last but not least on agents um as I mentioned they are very promising and they're you know when they work it's uh it's really it gives you uh a lot of excitement but they're certainly it's early days so putting them into production our experience as they're brittle and um and hard to kind of um uh have work in a consistent way for your specific use case so that's it um and that open up uh for questions I think deitos is flying over in his Lama dude awesome we've got some good questions coming through I've been monitoring the chat and seeing them and and I appreciate you giving some funny shoutouts to my mushroom days you know I've been actually Scavenging for mushrooms uh but not those kind of mushrooms just the good ones back home because it is mushroom season for those who know so it's always mushroom season in Amsterdam so I was told not to bring up Amsterdam and mushroom connection because the Dutch don't like it that much but you are a Dutch right so all right we'll take it now did you hit any capacity issues while running did that affect the user experience um I I assume it's for plus one you mean yeah so all the time actually especially in the early days um doing um essentially solving for the capacity constraints we had on the various models was something that we learned we had to solve quickly so we do that through intelligent model routing where we essentially make trade-off between three things cost quality and latency uh in particular latency is basically a capacity issue right if you don't have capacity your latency becomes infinite and so where do you route your request depending on capacity is certainly important um and then ends up being a one of the trade-offs you make or one of the dimensions you trade off against when you C send it to one model endpoint or another yeah I'm trying to remember this joke it's like you can have it you can have two out of these three things fast cheap easy but not all three exactly so you have to choose and so having a way to actually route between your models is one of the key components of a system like plus one I can imagine so in time do you see yourself being able to predict hallucination responses with inappropriate confusion metric like iow what's iow iow are you improving your ability to monitor guard rail quality responses based on human feedback yeah I think Absolut so human feedback is one of the elements that we as I mentioned with the Emojis and so on we've been able to use to detect sort of the broad buckets that we see so you know people are asking questions related to uh internal you know information that these models can't answer unless you give them access to it or people are asking about recent information like what's the weather in Amsterdam today or and so if you have these buckets you can sort of fix them in different ways some of it's through prompting some of it's through Dynamic prompt building some of it's giving it access so we uh once we know what the issues are we can then monitor for the performance not just through human emojis but some evaluations that we do basically on the back end to control for that so this is a great question from a past participant of the marketplace Mariano is asking in which cases do you see using llms more cost-efficient than classical models in industrial applications uh well I think uh that's a a great question I think especially at our scale we often find that you can still with some of the classical models far outperform especially on cost um and sometimes on quality if you've got the right training data to tr find to train your own models like we've done um but I think there's also another case which is like again going back to that classification example data enrichment example um you know there's a lot of effort you save by just being able to use this powerful new model that actually you never you didn't have to collect your own training data for you have to train your own model for you can just out of the box use a powerful model like this one to uh answer it so so we've got one more question and this again is another this is a bit of a throwback but deep is coming in and asking what do you think of the dangers of llms providing help to non-experts who may take its output as literal especially for questions like show me interesting insights which deep I feel like you're talking right to me well good to hear deep and Mariana by the way too uh um are are here so I think the uh this is a a really important Point again when you start deploying things at scale this is a really really important element um for now this is one of of the reasons why we put plus one out there so folks can test it in in their teams right I think one of the really different things that we see here is as opposed to many of the other tools out there which you're using by yourself you can actually use this in plus one in slack and you can have your team uh you know look at what the answers are that it generates um and often step in to sort of have a social quality control which is an additional layer of uh of ensuring that uh you know you don't end up uh confiding in an answer without verifying it either yourself or through colleagues there are other use cases where in particular in education which I think we'll talk about later you can actually use some of these uh you know existing processes to ensure that the quality of the answers is what you like using the subject matter experts that we have the teachers and so on that you know traditionally would either curate the answers that are generated by other folks on the marketplace or in the case of uh let's say if you're if you're dealing with uh content creation teams that actually create questions and skills assessment they will then you know curate and see this content before it goes out to the user so that's these mechanisms exist especially in our companies which are large and there you can make sure that you don't end up in this uh terrain where you're not sure whether the answer is actually to be trusted or not so yeah so I I want to ask one followup and before we do I'm going to mention to people that right now we get a little bit of a break and I am going to be serenading you Paul oh while we are in this break and I also it's an interactive break so you can throw in prompts in the chat and I will sing about them so while everybody is getting their prompts ready throw them in the chat let me make a song about it but Paul there it brings up this question the last thing that you said on having people see it before it goes out where are you thinking about having humans in the loop or how are you thinking about having humans in Loop and touching on different pieces of that Loop like if there is a loop yeah where do you know where to put the humans or how do you know where to put the humans yeah I think especially when there are uh you know user facing applications at scale like in education and so on I think uh in most of the cases I think almost all I can think of now there's essentially a human in the loop that somehow checks the quality uh continuously to make sure that the you know the answers behave at least to to the Quality that we expect from the existing processes that we have um and in cases where it's basically used for you know data enrichment that becomes maybe less critical um becomes basically these models provide input for next step in the system especially if you're doing recommendations and so on uh but I think uh as we are seeing with with the way we've designed plus one it's a it's it's supposed to operate in a human context with other team members right it doesn't do things by itself um I think that's uh clearly part of uh of where um the value of some of these models lies in combination with uh with you and so which is what we're about to see in fact in a song oh boy oh boy are you ready for this I'm not sure but I'm going to give you [Music] the