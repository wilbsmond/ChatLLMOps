is he in the oh dude I'm here if you can hear me yes oh that is so cool so some people may know you from your newsletter that goes out to 50,000 people right some people may know you from the work at lightning AI that you're doing others may just follow you on LinkedIn and you've been in the game for 10 years now it is so cool to see what you're working on and how you share with all of us and I love your talks and I'm so thankful that you've come to give us a talk here I am going to put you on the spot though real quick because you don't accept my LinkedIn request what's up dude didn't I oh I lost total control over yeah I think my LinkedIn feed maybe there will be or I hope there will be an AI for that in the future there we go but yeah I need to double check that one then all right all right I hope so I hope I hope that uh that happens now I am super excited because you've got a conversation for us about fine-tuning and anyone that has been trying to put llms in a production for the last whatever six months has probably gone through some way of fine-tuning and so this is very very topical I'm G to hand it over to you I think you've got some slides that you're going to share and we're g to get rocking man feel free I'll be back in 20 minutes and anyone who has questions throw them in slido I'll be ask to Sebastian all right it's all you yeah thank you for the kind introduction and uh yeah I can see you can see my slides so I think I can get going here um so yeah in my talk I want to talk a bit about fine-tuning open source large language models and um so this is an L mlops conference but this will be not about the deployment this will be essentially the steps uh prior to deployment how to um find youe open source large language models for for certain tasks so um there was already a kind introduction but yeah about me briefly I um a i researcher and educator at lightning AI now previously I was uh in Academia where I was working on deep learning research mostly focused on um yeah deep learning for ordinal regression and computer vision but yeah in recent um years I would say shifted more towards um yeah large language models um it's an exciting topic so uh I wanted to briefly talk about um using pre-trained large language models first just to get everyone onto the same page um in terms of how we use pre-trained large language models and yeah of course the everyday typical use case is that we prompt something like cat GPT to you um give us some explanations about topics we um care about and what's actually quite interesting and nice is that we can be pretty I would say precise we can say um summarize something in let's say two sentences and it will most of the time um do this job pretty accurately um just this morning I tried out the new cat GPD Vision interface so here I just posted a latch equation and asked it to extract the latch code which is also something that is super useful when I write papers or articles where I have an older article I just um or I see something I just see okay um I want this datat codes I don't have to completely rewrite this from scratch I can just make a screenshot upload this and it will do a quite good job for me for me for that of course I should mention there are also other tools for that like math picks which I'm using usually but I was just trying this out with a simple example and yeah it works so this would be one way we interact with a large language models there's a technique called prompting where um this was more like I would say initially when large language models were developed but it's also still relevant where we want to sometimes ask the model multiple times so for example if we have a translation problem where here I want to let's say translate this uh large this large this um language um joke um I can then ask the language model in multiple ways so here are just three different ways I can formulate my uh input prompt and depending on the llm you use especially let's say with a smaller open source large language models you might get different results and some of these prompts work better than others and uh yeah if you follow the news there's also this um concept of Chain of Thought prompting for example I won't go too much into detail here but the essence is essentially the way we format our input prompt currently can sometimes really make a big difference in terms of the results maybe in the future we will see that this will go away where when we develop um yeah more capable even more capable large language models that this becomes kind of redundant but for now that is something yet to keep in mind another way we use um pre-trained large language models um it's a technique called um retrieval orenta generation where we essentially uh develop a hybrid system a hybrid between a total LM application but also using let's say existing Knowledge from documents so for example imagine you are a company and you have a let's say a documentation for your software or you you developing an application there is a yeah a big let's say knowledge base that you have um you could technically use an llm or another embedding system to Chunk Up This document or create embeddings and put that for example into a vector database and then if you query an llm with a prompt you create an embedding and you can search for the most similar embeddings in that database then and when you have the llm generate the response it would um combine both um your query write its own response but then also uh incorporate some of these embeddings from the database and this is an is a very useful approach if you are developing a system where you already have a knowledge base and you want to return specific examples let's say from a user documentation so here it doesn't make sense for the llm to really regenerate it if the answer is already in some documentation but again this is another use case it's not the focus of this talk the focus of this talk is on fine-tuning where we take a pre-trained large language model and then fine unit on custom Target tasks so first wanted to briefly talk about um building llm classifiers and the reason is um I believe that most real world applications involve some sort of classification so when we're developing applications for example email spam filters detecting fake news or even uh Twitter uses that for detecting uh toxic content and so forth um there are a lot of classification tasks that we um encounter and practice where we want a model to classify something into categories so um for that that we can also fine tune a large language model so there are different approaches for doing that one would be similar to what I talked about with embedding it's like what I call the feature-based approach where we take a data set we take a pre-trained Transformer or llm and then we extract these embeddings and train a classifier on top of it so the classifier here um that would be anything you like um support Vector machine logistic regression something simple usually and um this does a okay job in terms of being a good classifier going one step further there is this approach that I call usually fine-tuning one where um It's relatively similar to the approach on the left except that we insert one or more hidden layers and then have an output layer that we fine-tune so we keep here so the key also is that we keep the Transformer Frozen we are just extracting the embeddings here and we are just working with these output layers and we can then fine tune the these output layers and the idea is that embeddings contain useful information that the last layers then can use to become a good classifier going even one step further there is something I call usually fine-tuning two where we update all layers so instead of just updating um the output layers we would update the whole Transformer or the whole large language model which can give us even better performance in certain cases but it is also of course involving more updates and um involving updating more parameters so this is also more expensive usually so when I was plotting this here as a rule of thumb usually this feature based approach is the cheapest and the fineing two approach updating all layers is the most expensive one but then also you have a trade-off in terms of training efficiency feature based is usually much cheaper than this other one so it's really um depends on the application it's really something to try out um and sometimes there's also something in between fine tuning one and two where you can get let's say most of the performance already ready by updating only a few layers in between um so when I benchmarked this on a simple data set 50,000 examples of movie reviews where there were positive and negative movie reviews whether someone liked the movie or not um I got 90% accuracy in like 3 minutes 93% when I was updating the whole model and that was for a small model that was the still bird I think it's about 60 60 million parameters so it's not even a large language model really it's a small large language model and um I just want to show that large language models are also capable of these tasks in addition to generating texts and uh also here for reference when I was trying to find you or to classify these examples with a logistic regression classifier or recurrent new network I never got above 90% so I was always about 85% so in that case it's worthwhile to consider large language models also for these um let's say simpler classification use cases but yeah if you're interested interested the code is uh the link is at the bottom so I won't go into much more detail about that um what I wanted to talk about now is I wanted to talk a bit about instruction fine tuning so this previous slide was a bit out of order I wanted to talk about instruction fine tuning which is a different type of fine tuning it's not for classification it's for um yeah for llms to perform certain instructions or to answer with um certain responses given an instruction so here uh an instruction could be for example write a limeric about a pelican and the output is then what we want the llm to generate and this is a data set that we would have to collect where you can think of it as a labeled data set um in contrast in pre-training we usually use unlabel text where it's just predicting the next word here the llm has to uh predict the next word in the output specifically and um there's also an optional input so for example you can have an input field where you have an instruction like identif if y the odd one from the group and then you have different inputs like carrot apple banana and grape and then the llm is supposed to answer with Carro and you can also think of this as a type of classification task in a sense but this is more General this is like this um instruction fine tuning Paradigm and uh here I wanted to show you that this instruction fine tuning is actually quite worthwhile if you compare it to a base model so at the bottom there is a gpt3 base model which is just a pre-trained model and then above that we have gpt3 plus prompting where um this goes back to the beginning of the talk where I showed you that we can format our input in different ways and then if we add supervised fine tuning to it we get even better performance on the y-axis here so this is an older paper using gpt3 but of course this also generalizes still to the newer llms as well so fine tuning is usually worthwhile and most of the models you find on leaderboards on the top they are usually fine-tuned in a supervised fashion so fine tuning can be expensive um so when you think back of the slides that I just showed about the classifier updating all the layers is quite expensive so there are of course methods that only update the last layers but what we really want is we want to update update the whole model but in a parameter efficient way so for that researchers developed parameter efficient fineing techniques and there are really literally dozens of techniques out there but if I had to pick one specific technique then that would be low rank adaptation Laura which is currently the most popular one so low rank ad adaptation in a nutshell is a method for factorizing the weight update Matrix so if you have a task where you're fine-tuning a model you have on the left hand side the pre-trained weights let's say you you're updating um GPT model then um usually if you finetune all the layers you have this um grade descent process where you learn the update rule like the updated weights so how much you want to nudge the weights in the direction that minimizes the loss and if you represent this as a separate Matrix here as a weight update Matrix this weight update Matrix with the Delta weights the changes have the same size as the original weights and in Laura we factorize this um large Matrix into two smaller matrices and this would be essentially a low rank approximation and um so here I'm showing you this uh figure for one particular weight Matrix but of course in a large language model there are lots of these weight matrices so we have the qk and v m matrices and we have also multiple layers we have the multi-ad attention blocks and we also have the output layers the multi-layer perceptron layers at the end and so forth so there are lots of places where we can do this factorization and just to give you an example with numbers so on the left hand side if this Matrix is 5,000 by th000 um weights then we have 5 million parameters whereas when we factorize this for example into two matrices like 5,000 * 10 and 10 * th000 then we end up with only 60,000 parameters in total so for this particular Matrix so you can save a lot of um yeah let's say parameters by just using this low rank approximation approach and according to research for specific tasks this approaches the same performance as the whole weight update but of course not in a general sense only for specific tasks that you find you in this on but it is need something worthwhile to consider because it saves you a lot of computation time compared to updating all the weights so I was doing some um speed tests here so I was comparing this full fine tuning updating all the weights compared to for example Laura and some of the other uh parameter efficient fine-tuning techniques and as you can see here the full fin fine tuning took about 9 hours on six gpus so um that was using deep speed with a CPU off looing so that took additional time and that was necessary because even on six gpus I couldn't fit the full model the 7 billion model onto full six gpus even if I distributed it um whereas with um Laura I could fit that on one GPU and train that in about one hour so just for comparison and this was for 50,000 training examples in alpaka and memory wise um yeah for the full fine tuning I used the full memory for all the gpus whereas for Laura or the other methods it was about 16 GB and um this is the regular Laura when I was using a quantized Laura that went even down to like 13 12 gigabytes so uh quanti Laura takes a bit more computation time because there's an additional quantization step but you can even bring that further down so just to illustrate um parameter efficient fine-tuning is a technique that makes it possible to really train relatively large large language models on a single GPU making it more accessible than also for deployment scenario um yeah so to give you an concrete example and application I wanted to talk a bit about an open source library that um I'm yeah helping to develop where I'm a contributor and so this is fully open source we developed this at at lightning but it is yeah available to everyone no restrictions on Apache license so that's something you can also use in deployment scenarios um so I wanted to briefly show you how how it works and how you can use it and of course there is more information in on the GitHub repository here at the bottom so here it's just an overview that there are a lot of documentation um documents that we wrote so in case you want more specific let's say um guidance you can find a lot of information there I will only go over the very basics in the last um two to three minutes so uh if you're interested in fine-tuning llms so and you are interested in using let's say Cutting Edge state-of-the-art llms like um f F mistol llama 2 and so forth so for this repository you would um so I also should say maybe why this repository and not let's say some other repositories what I like about it is essentially that is easy and hackable so what it is is essentially scripts that you can look at and modify and you have a lot of options there of course um it's not all encapsulated where it takes away all the options which is nice for some people but it allows you to modify things and um you know play around with that so here I'm showing you the very Basics so if you want to set it up you would clone the repository uh install the requirements and um the Second Step would be then downloading a model and as I mentioned um most of the current models are available and if there's a model that you would like to use and it's not available it's usually not a big deal to add it so there would be Lama 2 Falcon long Chet stable LM code Lama mistol F 1.5 which is a nice 1.5 billion parameter model for for example um so you would download the model just one command P download the name of the model U convert the checkpoint into the format that is supported by L GPT it's really just like two lines of code um then preparing a data sets so over the last couple of weeks we also added a lot of data sets and there's also way you can uh load a data set from a custom CSV file for example so um these are just yeah available and these are some data sets that I'm currently also using in some other projects for example there's currently the neurs efficiency challenge which is quite interesting um and yeah so you prepare a data set and why is that necessary why do we have to prepare a data set specific for a model so here I'm using the mistol model which is really the most popular model since of last week because it's pretty good for a 7 billion parameter model um the reason is that each model has a different tokenizer so not all of them have different tokenizers but it's really um um most of them may have different tokenizers so you have to prepare a data set specifically for an llm to make sure you use the same tokenizer that the researchers used for pre-training that large language model so you prepare the data set and then really you can already get going with the fine tuning it's really just yeah one um code snipp it here where you for example use Laura fine tuning with a checkpoint the pre-trained model here I'm using two settings a Precision bf16 true which means um brain flow 16 so to save you some compute memory and to make it faster um that is usually what most people use if your GPU doesn't support it you can also use a regular 16bit position and then I'm quantizing it here with a normal float four which would be a QA essentially which is even more memory efficient and then yeah it should train about um it would about be about one hour here or two hours I think it's like a slower GPU I used here it would be I think a A10 which is quite cheap so so you can train it on a small cheap relatively cheap GPU you and um yeah if you're interested also there is this uh neurs large language model eff efficiency challenge which is about training one llm for one GPU on one GPU for one day and I'm just mentioning that because if you care about llm deployment there may be a lot of cool llms coming out of this challenge because people really work on making llm training more efficient so developing the best um llm in a given compute and time budget and this LPD Repository I just mentioned this this happened to be selected originally as the official stter kit so it's still the official starter kit and people yeah use it to compete in this competition but of course you can use any framework but legt is I think the favorite here because it's so easy to use and pretty customizable and so forth um yeah so I was just doing some playing around here on the weekend where I did some submissions which ended up being first on the leaderboard but it's not first anymore so some people were already um coming up with better Solutions it's pretty interesting to kind of keep track of that um and with that I think I'm also already out of time it was quite the Whirlwind to here um highlighting different fine tuning techniques but if you have any questions I think we have five minutes for questions oh dude there's questions Che more yeah questions there's a lot of questions coming through people are really stoked at what you're doing so I'm gonna fire off a few of them for you first ones about how do you see the performance trade-off between using techniques like Laura and full fine-tuning for complex tasks demanding reasoning abilities can we use lightning AI for full fine tuning what are the advantages compared to other libraries um yeah that's a very good question about so the first question was about Laura and the tradeoff if you look at the original research paper uh I don't have the figure here but they showed that um with Laura you can get the same performance as full GPT fine tuning as a full fine tuning and also you can do full fine tuning in um Li so I have this I specifically added this script for full fine tuning um the performance trade-off is yeah you get better computation performance but you may not um so if you fine tune something with Laura you usually have a goal in mind um and if you take a general model so you assume this model is good on General tasks you assume that um so for the fine tuning you do a low rank approximation that you have for a specific task a low rank um situation where you only update the weights for certain uh targets or Target tasks so you may yeah so technically you may preserve the original performance on the original tasks but in practice uh I notice you get worse performance on tasks that you don't find unit on or for but this is kind of like I would say by Design where um if you already have a pretty capable model that is a generalist model then there would be no need to find unit with Laura in in my opinion so you could just use it as is if it's already very capable in general tasks so in my opinion Laura is really to make it better on certain tasks you care about in a certain um application scenario um so if that answers the question and also oh one important thing is um Laura has a lot of hyper parameters so if I go back here if you look at this R here that's like the hyper parameter for the width of these matrices here um that is also a big factor in terms of how well it works it's also trade-off the more parameters you allow here the May the better it may be and also um you have to consider there are multiple layers in the network so that's also hyper parameter so if do you want to find tun the output layers only or the all all the layers and so forth and you can change that also in um in LPD so there if you go to the script there are all the settings for which layers to update and so forth so you have to um also kind of like depending on what you try to do you can try different settings too so yeah if that answers it yeah more than enough dude awesome next one up is about when you would consider full layer fine-tuning and how effective it is for text classification tasks um yeah so I have so I honestly I must say I removed a lot of slides because otherwise it would be a 50 minute talk 20 minute talk but um so for the full fine tuning if you go to this repository I think I have a figure in there if not I will um I have it somewhere I will share it and so the full fine tuning here it takes much more resources I it's much much slower uh but you can get better performance but I noticed if you so what I did is I did an experiment starting with the output layer and then adding one more layer at a time or one more Transformer block at a time and I would say like two or three Transformer blocks in the performance already saturated so I think this model had like six or seven Transformer blocks or maybe it was 12 but you didn't have to fine tune all of them if you fine tuned the first couple of them you already kind of saturated the performance so in that sense it's maybe not necessary to do the full fine tuning you can really just get away with um funing a few of the first Transformer blocks from Counting from the output layer and yeah but this I guess depends a bit on the data set but it's I would say it's worth trying just to start out with the output layer and then working your way towards um full fine tuning and see if you can save some time by just you know doing the output layers well speaking of full fine tuning and kind of pulling a little bit more on that thread do you still recommend the full fine tuning or sorry do you still recommend the fine-tuning to updates to all layers for instruction fine-tuning or does it have some sort of tradeoff um I think it's worthwhile um it's a good Baseline if you have a lot of uh resources but yeah like I showed here um where was it uh it is pretty infeasible if you don't have the resources and it's very expensive if you consider uh usually I think a uh 100s if you have eight of them it costs like 30 $40 an hour um if you fine tune for 10 hours it's like um it's 400 bucks for one experiment right so it's yeah I think it's worthwhile if you have the resources if you don't have any budget or limitation well I would say full fine tuning but otherwise with Laura and so forth you can get pretty good approximations and um depending on the target task you may actually get better bang for The Bu you know so you can also so here I'm showing you full fine tuning for a 7 billion parameter model if you have the resources um instead of all throwing them on to full fine tuning for that 7 billion model you may just want to use Laura on a 10 times B biger model for example right so it would be also something to consider so I'm going to give you a real easy one now because keep in mind the next speaker is the CTO of mistl himself Timothy oh so this question here is which llm do you usually use for fine-tuning does this vary based on the use case yeah so right now actually I did not know that we have the CTO of mistol here but uh yeah we just added mistro support to L yesterday so um that is right now something I'm playing around with and also for the nurs challenge it might be an interesting candidate so for the nurs challenge uh I would not use the instruct version because the goal is really to start with a base model and then come up with the own instruct version but yeah um so that is currently my hottest candidate too because it's um I guess fresh interesting and works or seems to work well yes I know a lot of people love it and I'm excited for his talk dude this is been awesome as everyone can see fine tuning is not as straightforward as you would think it is I made a shirt design just for you and this talk and I think we can pull it up it's on my screen right now let's see can you see this oh wow he's speechless there he is there it is that's just for you I don't know if you can read it it's just as easy as find tuning right Sebastian if people listen to your talk hopefully it is as easy as fine tuning now man this was awesome if anyone wants to continue the conversation with you as I mentioned before you've got a newsletter you're very very active on social you're sharing a lot of incredible information with us all and we appreciate that so much I'm Flo that you said yes to coming on here and giving this talk and so I want to thank you and now we got to keep [Music] move