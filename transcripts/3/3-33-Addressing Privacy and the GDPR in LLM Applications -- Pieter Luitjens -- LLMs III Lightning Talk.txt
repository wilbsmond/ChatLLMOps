we have up next Peter you around I think we have you coming up where you at Peter to the stage maybe there he is there he is all right dude how you doing good thanks how are you I am incredible so I'm going to put some time on the clock and then I'll be right back I'm going to let you go and have at it you ready thank you yeah I am thank you excellent and H yeah thanks very much for having me speak today and uh yeah thanks everyone for coming to listen uh yes my name's Peter luchin I'm the co-founder and CTO of private AI we're a company based primarily out of Toronto like many I guess we an international team these days uh yeah and we're focused as you might might imagine on privacy so for a number of years now we've been building a primarily a text Data anonymization tool so uh you can go out and build things like chatbots uh and ml applications uh whilst mitigating privacy concerns uh yeah we've been as I mentioned we've been doing this for a number of years uh we were around before Chad GPT and uh yeah that's that's been a very interesting development uh the whole llm Space so that's what I'm going to talk about today just addressing just a few key points with uh privacy in the gdpr in uh llm applications uh so I guess just to begin with uh just to take a bit of a step back to the beginning of this year opena and especially Chad gbt was in the news a lot Peter oneing just uh I don't know if you meant to be sharing your screen but we can't see anything ah sorry yeah that uh that definitely helps thank you don't worry happens to the best of us and I can't tell you how many times this happened to me all right I think we can see it now well uh thanks yeah good on all right on the first slide I'll be back thank you uh yeah so as I was mentioning uh they had their first dat leak on March 21st and then 10 days later so end of March uh chat GPT was banned in Italy by the gdpr regulator which I don't I don't want to use the word unprecedented but usually they don't move so fast and and do things like that so that was uh that was newsworthy let's say and yeah although it was just B in Italy a number of other countries such as Canada and Germany also launched investigations uh so that was a St of this year so fast forwarding to today a lot of these concerns have been fixed so really what was uh the core of the issue was that there was no way to opt out of data collection for for model training uh and also there's now aure open AI which for a lot of companies uh ticks the boxes in terms of for data security and privacy but uh whil this this being said it's still interesting to note this is a survey um from last month that a lot of companies still have hesitation around using uh commercial Cloud llm Solutions just because yeah data privacy uh so sharing sensitive and proprietary data was was still is still top of the list in terms of concerns for companies doing deployments uh so I guess this is where um data anonymization can be really useful tool so by stripping out all the sensitive information so like names phone numbers but also anything that counts as personal information under the gdpr such as uh what religion someone might adhere to or their political beliefs uh removing all of that and then sending that through to the llm um also offers a really good protection for privacy so this is something we've built uh a number of others have too uh this is a pretty common pattern now I think so uh doing deidentification or anon anonymization on premise and then feeding that anonymize text to the llm and then in the response repopulating all the pi so from an enduser perspective it's a seamless experience but the llm never sees uh like the really sensitive stuff or credit card information whilst open AI does block that at that point it's already in their systems so making sure that it never leaves premises to begin with but this all being said um for many um the improvements open a has made on the Privacy front and also as your open AI um for a lot of people that's good enough but uh I guess the the new development that's come out recently is the ability to fine-tune the open AI model so uh uh gbt 3.5 turbo has just been announced and gbd4 will follow soon apparently by the end of this year here and I think that's that's really the the next question on the on the Privacy front and what it really all comes down to is that llms are very very good at remembering uh training data uh so this is a piece of research from three years ago uh where uh researchers based on just simple prompting techniques uh just putting in some information gpt3 would actually complete all the the personal information oh sorry this was conducted on gbt2 but the principle Still Remains with um the latest uh llms so this is really the the core of um the I think the the privacy issues uh that we're that we're facing today uh and as an example of how this can go wrong um this happened a couple of years ago in South Korea uh this dating company chainer trained a uh chat bot on about 10 billion uh conversations and then when they put put it into production the chatbot without anyone doing any exploits or hacking or whatever it just started regurgitating uh training data which contain things like people's names nicknames and home addresses and of course that that made its way into the news uh I guess there's been a number of techniques uh that um purport to like unlearned data from an llm that's been trained but this is actually a piece of research which came out the last couple of weeks which shows a lot of these techniques don't actually work uh well even anywhere near 100% of the time so I guess the point is is that once you've trained an llm on data which might include personal data it's it's once it's trained it's very hard to get it out again so this is particularly difficult for servicing access to information and requests for deletions uh under the gdpr and yeah again I think uh this is where data anonymization and also synthetic data really comes in uh like the best it I guess in my opinion the best way to treat this stuff is to make sure that personal information and sensitive company data is not going into the machine learning model the llm in this case in the first place uh so the reduction markers sometimes can affect system accuracy so one thing that we've bu for example is where we create uh synthetic Replacements uh for all the information and yeah just at least for our system few of the key features we've been building it for a number of years now uh so yeah it works really really well I would say and based on the tests our customers do uh we do over 50 different languages uh we do over 50 different entity types also covering things like Phi so health information and uh like credit card information and the key thing is that it's not a cloud API it's something you deploy into your premises so that data never gets shared with us and just a separate call out as well like I guess bias um is another big Topic in llms at the moment uh stripping out personal information does go some way to mitigating bias as well so things like race religion um if someone's an immigrant like removing all of that stuff also reduces the the scope for buyas and that's my talk finishing two minutes early thank you uh if you want to try it out uh we have a demo on our website you can just go and play around with uh it's free to sign up for the um the chat interface we have and yeah more than happy to if you want uh you can also grab an AI key to to play around with our system excellent thank you yeah