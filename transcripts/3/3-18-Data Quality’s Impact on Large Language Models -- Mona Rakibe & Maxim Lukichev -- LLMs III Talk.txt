this time it is a Duo we've got two speakers It's Mona and Max are they here can we throw them on the screen yes excellent so how y'all doing fantastic I'm going to start with one thing I was at Google next with Lynn from fireworks and I was like damn when people start talking about fireworks Ai and py talk it's a big shoe to fit in and we did this again today now so max we are in trouble TR talk it is uh I would look at it like you are um you're right there with them in in the what do they say these days the cool kids say in the arena in the arena with the cool kids also yep you're with the cool kids so I'm excited for your talk I know you all got a lot to tell us about when it comes to data quality which is very topical to is very important and so I'm going to hand it over to you and let you get it Kracken awesome thank you so much all righty let me start so while Max uh brings up the slides first of all thank you so much this is really a big step like in an ml conference Spotlight on data quality is exactly what we want uh a little bit about us uh I'm Mona ribe co-founder CEO of T my uh Max my our CTO is is also on this call this is the first time both of us are on the call so this topic is really close to us uh a little bit about Tel my we are an AI based data observ data observability company uh two and a half years old but both Max and me are not new either to data science data in itself data quality specifically or observability through our experience at relo signal effects and many other companies so when we started uh TM what we had noticed is everything we do today in terms of analytics data science really hinges on the quality of the input the data that's fueling all of that and we really wanted to solve this problem but we also knew that the tools and technology that are there today or were used for to solve this problem was really not solving it at the potential because today we are the volume of the data the velocity the speed um the type of sources the formats of data is is very different from what was happening 10 years ago so that really kind of was why we started TMA and how we started Telma was really using the foundation of strong data science ourselves and it's a combination of a lot of stuff our tool is built using a lot of like statistical analysis machine learning uh based approach to solve the data quality problem but also we feel that we sit at the foundation tier of uh data or data engineering which helps in a nutshell accelerate both Ai and analytic uh analytics use case uh a little bit about uh us we specifically are closer to the B2B and a prise space and this is important because a lot of our research a lot of what we are showing today will be influenced by our conversation through B2B customers specifically large Enterprises and where they are seeing the objections of adopting Ai and so on and so forth uh um so uh next slide please Max so really uh just in the recent past AI is not new machine learning is not newed Max and me have used it at our past lives so many different places but since last November the adoption pressure has been super high and when we are getting into these conversation like specifically in Enterprises B2B Enterprises what we are seeing is like there is years and Decades of data which is in different formats in a different uh storage devices with all of these a lot of teams have started adopting specifically llms and when I'm seeing adopting we seeing Enterprises adopting like in this print next print uh for use cases like classification of internal documents using getting insights from document documents which are high highly distributed across different teams across silos across uh different Source systems and so on summarization of all of this like then uh creating building a lot of search capabilities on top of that with prompts and so on also use cases around chat box and we are now seeing a lot more adoption of new user interfaces around this and some of this is really close by I mean this is like happening in this year next quarter and so on so we also went ahead and asked them so what is the number what are the top objections and this is no news right we've seen the sessions today also so what are the top objections that teams are seeing when they even adopt some of these use cases so it really came down to three things cost which throughout today's sessions we have seen ways of how you can bring down the cost what can you do to bring the cost and optimize the cost security and things like that but the last one is data quality the input of the data heavily influences that what Roi you are getting from your large language models and like U and this is again yes today we are talking about JL M but this problem has existed irrespective of llm irrespective of data science and machine learning but even for simpler analytical use cases so that's really where our focuses the company's focuses the focus for this uh specific session is Max is definitely going to show more like uh re reality in the light of reality I'm doing the talking he's going to show through examples but really the focus on the data quality and how we can totally influence the out output of machine learning based um initiatives using data quality is what we want to cover today so again something uh just couple use cases max if you can go I'm not going to go detailed into the use cases but there are adoptions of du AI there is adoption of uh uh data brakes Lakehouse IQ which we have seen across the board for the use cases that we are seeing there's also like co-pilot and other things these are all prime prime time adop and there are challenges with lot of different uh companies specifically fine-tuning existing models uh and we are going to actually showcase and I'm going to hand over to Max how through a very prescriptive approach and through experimentation we have found how data qualiity impacting your models so I'll hand it over to the smart one in the room Max who can take this forward and show you some real metrics want I did a brief introduction okay what what's the point of this talk and it's really you know what we we we talked to a lot of Fortune 500 companies and you know what we learned through this conversations how adoption of LM is going and where are the challenges are so one I would say very interesting thing was to discover that pretty much everyone investing into this area it's always almost like a copy it's a top- down uh you know ask from the the management you know we need to do start doing llms and you know good luck go figure out what to do um oftentimes you know what companies are trying to approach is you know they have vast amount of that Enterprise data which was sitting sometimes we are talking to companies which are like 50 years old you know vast amount of data been sitting there no one touched it they're trying to make sense out of this data you know through you know various categorization uh search interfaces you know chat like uh or different summarizations of uh all this Enterprise data and and obviously because it's such a sensitive data they are defaulting to use open source models uh for various reasons mostly concern privacy concerns but also licensing uh constraints of open AIS and they are running into a lot of challenges and this is um very I would say dangerous situation for many companies just because a lot of uh a lot of success of this project depend on the quality of data is being used and you know data quality is our bread and butter like we we're dealing with data quality with helping companies manage it and you know discover problems in the data all day long and we know that there's no perfect data but no one knows how bad the data it is until they start doing something with this data and specifically when we talk about llms it kind of magnifies the problem because you know it starts including things like the the the cost of the model training or model fine tuning what most of people are doing is uh increasing this you know vast amount of garbage they're trying to throw into the model it's very timec consuming because it's like iterative trial and another process you know they they try one data set it doesn't work they figure out why it doesn't work they try something new just to figure out later like why what was wrong with that data set and so on and it always very uh cost inefficient because you know it accumulates a lot of infrastructure costs in the end so in the result you know unpredictable this unpredictable results means pausing off this project changing the priorities um you know kind of not a good situation for the company as a whole because you know it could have been a breakthrough new feature new capability for the product um how to and and as as I mentioned a lot of these problems are coming from the data quality not only from the data but most of them coming from the data quality which is not a new problem it's been there forever uh it's been ignored for a long time now with the rise of um you know on top of data quality tools data observability tools we see Enterprises starting to be more diligent and more uh investing more money and time into solving this problem so let's talk about how this affects llms But first you know what kind of these data problems are you know Annoying us so much well the most popular one is missing data you know data can come with values missing like no and empty strings when they supposed to be it can be imbalanced data sets you know you want to do a classification task and you know suddenly uh data from certain region is missing or where sparse versus the other regions which are over represented uh very common problem in the data sets polluted data uh data which you know due to human errors just the other day I was talking to a customer where the problem was uh humans human salese uh it's very very common as well were making errors on purpose just because how they were paid for entering the data making duplicates and you know doing all sort of garbage to the and damage to the data which later on uh when used by data scientists will cause all sort of problems um and more importantly the data itself is not um you know used directly it's being transformed it's being joined with other data sets it's being enriched and the earlier these problems are introduced the bigger impact it will be in the end after all of those Transformations and Joints you know good example would be uh duplicate values which were in The Joint key which would result in the cross product and blowing up the the section of the data which was completely irrelevant um and thing like that another typical problem that we see in the data you Mal form data sometimes it's a human error you know in the code bugs or just errors of entering the data unexpected truncations of the values inconsistent patterns and things like that also affect models big time sometimes the data is just uh invalid it doesn't conform to expectations for example there's a logical expectation that you know if uh area geographical area is that the ZIP code should be certain format or there should be some the values should be in a certain range and and things like that or data can be inaccurate which is more interesting uh problem is it looks right but it just doesn't represent the reality you know sales numbers don't make sense even though it's kind of by looking at it uh with a human eye nothing wrong with it and finally the stale data or outdated data just another example of data quality problem right if the data is old it's as bad as just wrong data so when you're trying to uh you know use such data for you know model training or even more importantly you know we we had bunch of really good conversations today about you know longchain and the Agents because llms uh can also serve the data um you know Enterprise data for example they can generate a query and then return the results um the the serving that bad data is just as bad as like having the bad model itself because in the end end user that's what the end user will have right the wrong irrelevant results so if you're on this journey building the model uh what are the challenges you you know have to think about uh and you know either you're building something yourself in house or you're looking for a tool which will help you but just few things to keep in um keep in mind first of all it's going to be Integrations even more than for a typical analytical task or typical machine learning algorithm Integrations will pay even more will be even more significant when you're working with llms because you will have to pull data from lot of different systems uh vast amount of data sometimes the systems are Legacy sometimes it's something that you cannot even query with SQL uh you know crms semi structure data web gr data we've seen a lot uh you know companies do having their propriety logic you know extracting relevant pieces from web uh internal databases or ticketing systems for example all of these combined fed into the model uh can be very powerful but that means you have to be able to very quickly uh understand what this data is it reliable can it be used uh without spending too much time on building this Integrations yourself on top of that uh when we talk about Enterprises especially you know in finance or uh in medic uh medical U environment this data might be encrypted like not readable at all like at record level you have to decrypt every record before you even look at this data also have to solve it somehow and then finally you know deeply hierarchical data of you know this pretty much Json structures which is very typical when um you know there's a class of systems called Master data management you know Consolidated records which are uh joined and and combined from you know multiple hundreds of sources about the same entities of the company let's say customer data coming from you know support system from the sales database from somewhere else uh kind of in the single golden record so that creates all this you know very heavy structures which also very it's a golden source of this data for llms but it has to be quable and you have to understand the quality of this data in the first place before using so Integrations is one aspect to be very careful about next one will be scale uh especially when you start training model uh you know you you you can use whatever tools available you know stack Overflow is a great way of doing something simple but the bottom line you have to analyze the entire data set you know analyzing just a sample a tiny sample random sample especially is no good uh it's not going to help you finding the blind spots in the data you will not be able to tell how actually good the data you you can get an idea what's in there but you cannot measure quality based on that especially you cannot measure accuracy or um like learning on the behavior of the data historically and predicting that it's today it is not accurate because it doesn't conform the you know what we've seen in past um that's why you know whatever tool you choose or whatever you know path you go uh make sure it is scalable ideally it's Autos scalable so it can you can throw anything from megabytes to gigabytes or terabytes without spending too much on infrastructure infrastructure planning and and spinning up the services and and stuff like that uh this is this is very important this is quite honestly like a big um it's it's very timec consuming effort like doing it right and finally getting to the anomaly detection you know one way and a lot of companies start this way because it's simple and readily available start writing SQL queries and analyzing attribute by attribute and seeing what's going on other anomalies anomalous values missing values and stuff like that it's an absolutely inefficient and expensive way of doing things uh very manual very error prone and in the way it will leave you with still lack of confidence in the data so it's better to use some data quality of profiling tools which can do it at uh larger volumes and do some apply some statistical analysis to highlight possible anomalies and I will show you know what kind of in the next slide but pretty much like you know anomali based on the the length of value patterns you know distributions of the values can be off and and and things like that matters a lot um when trying to evaluate the quality of the data set and finally uh you know we've been talking about data equality but you know this New Concept emerged like few years ago the data observability which is kind of a sibling brother of uh the data quality when we talk about the data quality it's more of like property of the data uh you know how complete how correct your values how accurate this or you know how fresh is the data set uh data observability puts it in the perspective of uh historical analysis and can learn on the behavior of the uh of the data and you know predict it today it is it is off it's anomalous because you know we have seen values of different patterns in this attribute you just doesn't make sense or you know business numbers business kpis don't make sense they off the charts uh here's an example what I'm talking about just the screenshots uh taken from telm my our Tool uh you know just working on the simple experiment with llms um trying to fine tune the model and you know identifying me right away that you know I have bunch of duplicate records and I have to be careful about this or you know in one experiment we had accidentally truncated some percentage of data and the distribution of uh you know lengths of the values would easily highlight where these uh things happening and more importantly when once you are trying to uh do it in a continuous manner uh let's say this daily job or fine tuning or retraining model or like data which is going into the serving to being served by the model you can automatically alert on these things and you know be aware that there is a problem in the data uh here is an example of this time uh kind of historical analysis of uh the you know business kpis where you can tell that you know sales numbers don't make sense for specific region or specific customer and also be aware that you know even you have the best uh llm model which generates the perfect query which can be very relevant and nice it will return the garbage result the user will still think that you know the the tool the solution is not working another very interesting aspect and very important aspect exactly about serving and uh putting these models in production um sometimes you know you cannot just block the bad data when it's being detected right because this especially if it's happening at the very early stages of the pipeline if every time there's noise or Garbage detected uh if you stop the pipeline to do remediations which is good it doesn't it means that the the batter is not flowing in your sensitive systems and damaging them or corrupting the model but on the other hand it creates the delays it creates the urgency for someone to be on call and fix it uh jump in the middle of night and and do some work very interesting concept being emerging in the last you know couple of years and specifically in the data observability area is called data beaning and um circuit breakers where you can plug in tools into your pipeline which is processing the data you know and especially early in the pipelines to the left as possible of the pipeline and start splitting good from bad data and if the amount of this suspicious data is not significant just let the good data flow it's not going to hurt a lot uh and we'll you will have have the timely and relevant dashboards and you know the models uh can return the freshest data possible versus when you know everything is stopped and you know the team has to jump and investigate and fix and reprocess everything possibly making a delay of the entire day um yeah just it's it's it's more of like a design partn uh now with the with the data pipelines rather than just specific tool to use and now it's a little kind of fun example uh you know how big of a deal is actually if you know the bad data gets into the uh llm tra fine-tuning process uh what we did is uh just to demonstrate uh the impact and that you know data quality is is not an after afterthought in in all this endeavor but actually the Paramount uh you shouldn't put it at like very last phase you know you should start with it we did an example where we took uh from from hugging phase uh this U model which predicts genes of the books based on their titles uh and uh we injected Some Noise to demonstrate uh what's the impact when it's happening and these are all very typical you know data pipeline problems we see with the customers all day long uh so there it's not really made up it's like real kinds of issues that we see and you can see the the impact is actually pretty significant when the titles got truncated the Precision dropped down quite a lot uh sometimes we got mixed up the uh titles with the book authors also a very very common problem when the attributes of like after releasing new new pipeline version like mixup of the attributes happen all the time and the imbalanced training data sets right when the one class is over represented of another class especially when it gets very significant uh difference between them also impacting the models in a very negative way and this is just an illustration that uh you know by leveraging the right tools before you start your journey with training the model funing the model you can very easily get much higher quality results at a cheaper price with less trial and error uh and basically save a lot of time and money by the way this QR uh code uh kind of is a link to the blog we wrote about this if you want to see more details and just like explore it a little bit more with some code examples you can follow it and you know if interested and as a summary I just wanted to highlight few points uh unreliable data feeding into the llm model training uh will result on the higher cost and the poorer performing mod model it is obvious but on top of that if the model especially like longchain uh applications uh serves the bad data it will look like just as a bad model right it is just as important to remember that you know whatever data being quitted result of all the applications of LMS it has to be reliable data so data quality is the the par amount of the architect not an afterthought uh so it's best to have it in plan when you know thinking of productizing and uh building your own llm app specifically for the Enterprise data which we know is quite dirty and I have yet to see a good quality data set with a customer it all of them have a lot of data quality issues and if you want to just try it out you know you're working on your project for example you want to try out some profiler or some tool which can help you um you know get a CS of the data quality issues in your data set you can use TM for free just follow this link if you if if you want to try that's it thank you so much excellent thanks