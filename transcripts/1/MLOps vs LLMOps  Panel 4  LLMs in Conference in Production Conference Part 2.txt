well without further Ado I think it's time for a little panel reach out what's happening well I'm hoping my phone doesn't ring and say Seek shelter because we're in the middle of a thunder and tornado storm in the Greater Philadelphia areas of fingers crossed well if it does um then you get you stay safe I'll jump on and and play your part for you so then we'll um we'll also bring on where who else do we have here Mr Willem what's up dude and Chris oh this is a great panel where's aparna yes and last but not least we've got Alex but Alex calling Alex to the stage where is he there he is all right so there's a ton of us I'm gonna hop off reach uh I'm gonna hand over the Talking Stick to you and we'll get it started all right thanks Ringo um welcome panel um let's just take a quick second to go around the room introduce ourselves and then we'll dive right into the questions I'll kick it off my name is Richard suchdev I work for JPMorgan Chase and I'm in the Chase rewards department and lead a team of data engineering and operations and we work on the business side to help them make data-driven decisions I'll pass it over to you aparna awesome hey everyone good to get to see you all uh excited to be on this panel my name is aparna founder of arisii Rises ML observability and I guess llm observability which would be the topic of today so excited to you know have a great combo with all of you wonderful thanks aparna over to you Chris hey everyone I'm Chris Van Pelt co-founder of uh weights and biases I'm repping the brand today uh our mission is to build uh developer tools for uh machine learning or llm prompt engineering uh engineers welcome Chris Alex hey everyone I'm Alex I'm one of the co-founders and CEO at snorkel I'm also um affiliate assistant professor University of Washington uh and before that I was working in the snorkel project at Stanford all things we call data Centric AI so uh labeling but also curating sampling filtering cleaning all the data that goes into either training models from scratch or fine-tuning free training instruction tuning um large language models or Foundation models as we like to call them so excited for this uh this chat and Richard thanks for hosting you curse and this panel would not be complete without William well I'm your next hey folks yeah my name is William been in the dead end AI space for about uh you know six eight years now already um leading ml platform teams and more info teams and building in the open source space uh recently most focused on feature stores but uh really sinking my teeth into the LM space over the last couple of months well thank you all for being here this is definitely a great panel and I'm excited for all of the learnings and William nice to see you again so I'll kick it off with you can you walk us through what do you think in your experience at the high level differences between llm Ops and mlaps and it's quite a tongue twister llm Ops I I always make sure I have the right number of L's yeah I feel like this question we can probably expand and talk about for 30 minutes to an hour so I'll try and keep it as brief as possible but um I think a big part of it for me is abstraction and generalization um to me LM Ops is um language oriented right and so there's a big question around how you frame your use case as a builder um to what an LM can do because it's so generalized um but I think we've seen a democratizing effect of these llms bringing in new folks that couldn't previously um uh traditional Mrs stacks and solutions um but I think a key difference for me is the offline flow right that's an area that has become somewhat optional right now and I think for application builders that has enabled new use cases to be unlocked and for folks that you know weren't in the space to suddenly bold right you can just start with the model in production frame your use case um around an llm and and get that in front of users and and get traffic onto that model and then figure out the next steps and the offline flow where there's their collection whether it's training um that that whole step has become a secondary step is as opposed to the first step um and I think that's that's a key thing and I'll kind of leave it there and with the rest of the panel can expand on that yeah uh thanks Willem anybody wants to add on to what Willem just said yeah I mean I think Bone's point was was a really good one you know when we think about data scientists and ml engineers and what the workflow has been like for you call it statistical AI traditional AI there's there's a lot of names I'm hearing floating for you know non-lm type of models um but but what happens is a big chunk of that work is in the training of of the model and I think that you know when we think about how do you go improve the outcomes you think about well I'm gonna go and train it to get X percent increase in performance that's become a fundamental shift in this new LOL mops world is that there's so much you can do with prompt engineering and actually I think this is what kind of what William's point was which is in the you know the offline world might not look like training it might not look like fine-tuning it might just be iterating on what is the prompt what is the context that I might want to incorporate into the prompt and it's a very fundamentally different different workflow and different way of thinking about Improvement than than in the past which is um which is where you know from for me the the one big thing I've been thinking about is well how does this change the Persona does it you know our are there going to be just data scientists who are building these models just ml Engineers does it potentially open up the Persona a bit to you know I've actually already seen job postings for llm engineers and so does it open up kind of that persona for a wider audience because there's less of an emphasis on on training and more of an emphasis on call it prompt and prom prompt iteration um the one thing I do think will will stay true in both ml Ops and llmops though um is it's very easy in the llm Ops World to I think try things really quickly get a Twitter demo up post it on Twitter but just as an LML Ops very very hard to get something to work robustly work work consistently and so um that happy to I think dive into that and talk about that more yeah for sure so yeah it's interesting you're saying that probably the way the teams are going to be structured the personas the life cycle itself might change but for like companies that are getting up to speed with the llm world I know like mlaps is still something that is not very easily understood with the introduction of mlops what does it mean for companies to strategize can they take what they've already built and carry forward is it a completely separate initiative so curious Alex if you have any thoughts around around that yeah I'll uh respond to some of the great points that while I'm in a partner raised I think there's a um I guess my short answer to your direct question is I think there's a lot of the mlops infrastructure that is going to remain the same um and this has a lot to do with the appropriate models to actually serve in production for some subset of use cases we often call them predictive use cases classification extraction a lot of the traditional things that um actually a lot of Enterprise Value still rests on today versus some of these larger more generalist agents that kind of have open-ended you know q a chat dialogue Etc um I guess I I already talked long enough that I'm I'm uh I'll just cut to the punchline in my opinion there's a lot of those um those tasks more predictive tasks that still um probably are going to make up a lot of the the value that enterprises get out of AI they can be distilled down um you know orders of magnitude to traditional models basically think of it as like going from a generalist to a specialist um one way that we've been thinking about these these Foundation models or large language models is um as kind of first mile tools uh part of your point about you know it's very easy to make a demo but the last mile Still Remains very very hard right we all everyone is you know an AI knows that problem and knows at least for a subset especially a highly complex and valuable problems that's not changing that's why self-driving has been here next year for the last decade um that's why the last mile is always difficult um but you know I think these these big generalist models will give us a very powerful base to start the explorations and to uh we often refer to our platform as warm start the problem um and then you need to trained Specialists that Branch off of them and those Specialists can be much smaller and often will just look like a traditional ml model artifact um we had a paper with some uh one of my great students at UW and and some Google AI collaborators on a distillation a new distillation approach basically of kind of using a big model to teach a smaller model but this is an old technique and really you know the intuition of how you can get this crazy result of a tiny model that's doing far better than the big model is it's just generalist to Specialists you don't always need a generalist jack of all trades that's hundreds of billions of parameters when you have a specific narrow problem you need to do really well at so I think a lot of um a lot of Enterprise Value a lot of the problems that actually get shipped to production really are going to need Specialists and I think for Specialists we're going to see a lot of building off of big often open source models that are more than good enough to give you that that warm start and then specialization and those Specialists will most likely run on existing ml Ops infrastructure so the serving infrastructure will look almost identical I think when you truly do need a generalist and I think that we're still figuring out what subset of problems are real there when you but you know when you do need some kind of you know generalist co-pilot or chat thought um then it will look like a new type of serving mechanism because then you can't shrink it down as much that's serving and then training I've already talked too much look at myself but training I would just say I think there's a there's a chunk of the world that will look like you know part of what you were articulating quite nicely where it's more about the prompt engineering there's a chunk of the world where actually because you're working on Specialized data or high value and or high value use cases you are going to need to continue training a piece of the model an adapter fine-tuning Etc um actually a lot of that we're going to see converging in context learning and fine-tuning um there's some cool new papers out about how those can be you know conversion so it will in our view come back to just the kind of engineering of the data and context you put into the model whether it's through training I.E fine-tuning or through context Windows it'll really be more about that um so that's I think on the summary the trading side it's gonna be a lot about these data operations whether it's a prompt or a tuned prompt or a prompt that includes a set of examples which is now back to a label data set or a mixture of all these things plus some external context it's going to be all that data engineering um and then more so than the actual kind of hyper parameter algorithm model architecture selection um and then on the serving side it's going to be kind of a split between these highly specialized models that'll look like traditional ammo Ops and traditional models actually would be my bet and the kind of true General generalist where you you do need a gigantic model and the Ops will look a little bit different so that's my that's my hot take um but I'll pause there if talk too long yeah thanks for the great insights Alex [Music] um Chris I have a question for you so in your own world when you're talking to different companies data science teams Etc practitioners that are in the reads how are they thinking about using open source rules let's say for a financial firm um how are they making sure that the data is safe and it's easy or is it safe for them to Leverage The Open Source tools that are available out there or is there a lot of talk of trying to build some of these llm models in-house as well I mean it says still early days um we're all obviously very excited about llms but I think getting these into big Enterprises doing real work is is you know still yet to really happen in a meaningful way um I talked to security teams and and data teams often and you know it is a big concern um to you know just send this data out to to say something like open ai's um API they've mitigated this with uh you know allowing you to have kind of a an isolated Azure native API in the in the case of openai but I think there's still folks that that wants to own the entire stack or literally you know can't send out the data to any third party at all and that's uh that's when either you know building something from scratch or or partnering with um folks like uh Mosaic uh to help you know build out something um for themselves and there's a whole there's a bunch of startups now just focusing on like hey we'll build you in llm um it remains to be seen if that you know that's a big burden to to manage and deploy and deal with one of these large um llms so I think we'll we'll still see where this where this ends up but it's it's a bit of an unanswered question the other day it was I think uh early this week there was a a startup on Hacker News talking about this exact problem like we're trying to give better data observability and and lineage and governance around you know what is is going into these things potentially redact so I think we'll we'll see kind of where where this lands but uh it's definitely a big question on on the Enterprise's Minds no thank you and and the follow-up that I have is like how are they thinking about the cost as I understand there's a huge cost Associated so how are the companies rationalizing that around the ROI that they're going to generate from it or is still very early to determine that yeah I mean it's expensive for sure um so I mean I think a lot of these bigger orgs they have a budget and they can you know very kind of rationally think about would this make sense to take you know this massive chunk out of the budget uh but it's also still still early and there's a bunch of stars trying to make it a lot cheaper too so yeah we'll see yeah yeah I think there's a lot of great progress oh sorry go no no good obviously there's a lot of great progress to Chris's point on on uh startups and a lot of that coming from Academia on on just making these these operations at all stages cheaper and then you just have um yeah I think as this the um the space matures a bit although these are all very classical ideas you have these different stages you can start at so it's not either start from scratch or taken off the shelf you know API based uh Foundation model you can you know you can take an open source space and you can you know build off of that you can there's various there's a whole spectrum of kind of customization uh you know kind of entry points so I think a lot of organizations these large Enterprises that Chris you know Chris talked about that you don't necessarily want to send all their private data to um you know to an open Ai and don't necessarily have to because the open source models they're they're not quite as good uh right now as you know kind of General open chat and taking you know I don't know MIT entrance exams and Elsa and all the things that apparently you know baby alums do these days in their free times um but they are more than good enough to kind of base off of and build a highly accurate specialist so you have all these options of kind of where do I start and how do I specialize it and I think in terms of cost rationalization one of the appealing things is both being able to use this as a base for kind of jump starting multiple Downstream ml projects if you have 100 teams that are going to use your you know Foundation model as a base to speed up by 20 that's a pretty you know good Roi calculus and also the cool thing is that you know a lot of how these these Financial models are trained it's not just pure okay go look and do self-supervision train yourself you can also use Downstream specific tasks through what's called multitask supervision to kind of make it smarter so basically you can both get a boost for those Downstream 100 tasks but you can also create this powerful kind of data or supervision flywheel that makes the bass better if you own that so I think a lot of Enterprises are getting quite interested in what if I own that and I both speed up everything else I do Downstream but also kind of have this way of kind of collecting and scaling all the work we do um that's going to be very interesting and it doesn't necessarily mean open source versus closers it just means they need to be able to own their copy of the model and there's some interesting models from the cloud players and others emerging around that along with open source yeah and that's a great Point well um I'm sorry Alex the the one thing that uh we definitely should mention on that front too we're talking about foundational model you know the more private foundational models than the the OSS the OSS foundational models I you know in both of those cases I've been seeing the huge rise in using Vector databases to be able to connect that you know it's another you know I think about it as two ways you you know there's there's a privacy component where you might have to use um an OSS model or build your own um but then there's also a connecting your data component when you're fine-tuning or you're training up your own llm it's one approach to be able to give you know your your llm context of your use case and your data another way of doing that if you want to just if you can use just an off-the-shelf llm is to be able to connect to um a vector store and pull in context from you know and I've been seeing a lot of different use cases for this getting actually deployed into production things like chat Bots or I want to be able to answer questions on these certain documents and what teams you're doing is is really supplementing the user queries with context from their own knowledge base and it's it's a way less complex approach to find you know building up your own LM or fine-tuning your llm as you add more documents or knowledge to your knowledge base you can you can kind of augment it without having to pre-train the whole llm um but it's a great way to connect the llms to your own data and you know the way I think about it is if you can use a you know an out of the box kind of kind of public foundational LM like open AI well you can directly call it get get back a response if you need to connect it to your data throw in a vector database if it's still not giving you good responses you want to fix a more General problem um you know you might need to fine tune it and then if if you really can't do any of those options then I'd kind of uh you know we've been seeing people then resort to using more of an OSS type of llm but typically that's kind of the process I've been seeing people go through to select the right approach for what what makes sense for their use case you go uh I think we should if we have about seven minutes left let's hear any hot takes if folks have about how ml Ops is going to change but I wanted to uh just type back uh Alex's point and Chris's point on the Enterprises I think if you're mature and you have high value use cases um You probably have an evaluation stack and already existing systems in place and for a lot of these companies they have a little bit of leeway to try open AI or other mature models and do a like for like comparison offline and I'm hearing that from a lot of folks I speak to and if so if they have data sets already that are produced for or by ml systems or even human labeling they can already see how well these llms are performing and in some cases they're you know cutting down nine months of human labeling to you know minutes or hours of work so there's definitely demand and I think a lot a lot of companies they see this as a differentiator so there's pressure to get adoption and now this is figuring out like how do we actually do this and they're exploring open source and other ways to do this reliably um but yeah I'd love to also like get into a topic of perhaps like how does the stack change right like vector stores versus feature stores um or you know any other uh kind of Stack changes maybe on the observability side um have you seen any any kind of like use case differences there yeah I so that great great question so I think Vector stores is definitely I'm not the first person to say this the vector store is definitely you know a core part of what I'm seeing in the llmop stack um I think the embedding analysis and being able to understand we've been doing it on the Deep learning side for CV and NLP but definitely um something that's been cropping up more on the observability side real alums and then um one other Nuance point that I think I'd mentioned is that um for llm observability if you are using a vector store being able to trace back was the right context added if the right context wasn't added how do you trace it back to you know are the most similar documents that are selected necessarily the most relevant documents or all the types of questions that we're getting asked and we're seeing actually our our users troubleshoot um in in production so um I think the the observability stack for llms will definitely need to have some some components of looking at the vector store and being able to troubleshoot the context that that was retrieved um you know one thing that I think is different with llmops is exactly what you you set a part of this like need to really debug or diagnose or or get into these ever more and more complex chain of Agents like all of the the cool stuff trending is it's like pretty complex and you can have many different calls out to different sub models or different services and um if any one of those things goes wrong you're going to get a bad result and then uh Willem you mentioned this like evaluation stack like companies that have a mature evaluation function I mean this is this is really important like it's we're getting a lot of software developers Now using these things and it works and it's a cool demo but if you can't measure how good it is there's there's you know that's not going to be good in the in the long run and it may degrade and um so I think these are the the big differences here and and you know there's going to be a lot of tools that that will hopefully help solve these things but it's early days I think one way to look at this was one quick point I think the LMS versus ml Ops comparison puts them kind of at the same level but another way to look at this is if you look at a feature store you know the features could be produced by an llm or it could be you know this is a system that is at a higher level and it compiles down um your data pipelines or produces the code produces the features and touches on the points that Alex made of the Specialists right like you know you could start with the LM and then ultimately you have a more performant perhaps more brittle but reliable code driven system that is optimized for your use case and and so they're not perhaps peers in the same level but more kind of student teacher Alex you can go yeah I definitely agree with that um and and with what Chris was saying in terms of how these systems I mean they've always been complex chains of interdependencies between deployed AI powered applications but the kind of democratization of that and the kind of blooming of complexity we'll see how many of those actually get shipped to production but you know it's definitely kind of coming more to the Forefront right now in really interesting ways that require this kind of yeah more software like debugging through these long traces um I guess uh I can give a um let's see a a quick hot take a medium take and a cold take I don't know which is just like a lukewarm statement but the hot take I already said is I think a lot a huge chunk of the models people actually want to build uh the problems they actually want to solve with Foundation model stay in the Enterprise will just boil down to uh traditional ml models and traditional ml Ops we want to classify this it'll start with a foundation model but it'll end up with just a smaller model and you could look at this and you know um look at uh um uh you know some of the mature deployers of foundation models for years like mainly I'm referring to Google right they don't serve the biggest models in production they distill down to smaller models um uh you know in ways like the work we did with them so I think that's the hot take is that a lot of lmops will just boil down to ml Ops as hot as I can get for a conversation this nerdy uh you know right now when there's so much agreement in the group too maybe a medium take is um I think evaluate I agree with you know well I'm agreeing so not that hot that um evaluation super important super challenging we've taught a generation of data scientists basically uh well not to Peak at the test set right don't test that hat hack Don't cheat and what that is translated into is don't look or think don't look at or think about the test set and that's always been a problem in machine learning right you know um you actually have to engineer your test set to be representative of what you want your model to do in production that's actually still a problem for you know simple predictive models but it gets tenfold more messy when a tenfold Messier when you now have um you don't have a simple accuracy score you need to build like a custom Benchmark and you need to you know be careful about where the data came from and whether it was in the training set of some gigantic you know web-wide uh trained model and so I think evaluation is is very much in the hot spot right now and a lot of these demos they look amazing until you actually do a rigorous evaluation then you're like it's still really cool but it's not production ready but we're not even we're baby steps there so that's the medium taken then the lukewarm take might be just that this goes back to a nice articulation of partner you were making a kind of the the steps of things you do a lot of the questions that people are asking about like should I do a prompt or can I just rag it I heard that phrase at Microsoft build uh you know retrieve login the generation of putting in a context database or do I need to fine-tune a lot of these are just pick the right tool for the right job right and it really is it's always the problem data Sciences start with the simplest approach and you know in some cases yeah simple prompt you'll get a great solution and then other cases well your model is missing some context from a structured or unstructured data store so plug in a database in other cases it's actually the you know ability to get the right decision boundary is not even with the right information is off so you need to fine tune it in other cases it just doesn't have a good enough representations for your specific specialized domains now you actually have to pre-train so you know I think it'll it'll all come back to it's not on either or it's just to pick the right tool for the right job that's my lukewarm take um kind of just Plus winning what a partner said yeah you've all been a great panel but I believe you've run out of time um Demetrius yeah yeah so this is how I know oh you can't see me but this is how I know the panel is full of Executives they're right on time literally one minute over and they're like we're burning money here we gotta go run businesses so I love that I just want to know last question from me and there's some incredible questions in the chat so maybe we can convince some of you to go into the chat after this but Alex said it already so I don't need to hear it again the rest of the panel do I need to Rebrand is it ml Ops going away do I need to Rebrand in llm Ops that's what I want to know Chris I'll go with you first uh I'm in agreement with with Alex's takes I think uh it's it's spot on um it is it is still early days we're all kind of like watching and and wondering how this is gonna how this is gonna play out but um I don't think a Rebrand is necessary the the core fundamentals of let's you know measure and have uh an audible auditable system of record is is going to hold true and um that's good for for my business so maybe I'm I'm seeing the world as I want to see it but we'll wait and see I love it well yeah I mean maybe a little biased from my side but I'll go with that one what about you a partner I feel like I'm gonna maybe be a little dissenter here um I I think that um it's it's definitely changed how people think about um but like I think the potential of LMS is massive I think it's changed how people think about um you know should I build this or should I just call an API and and choose that approach um I think the world of today you know llms probably don't replace all of traditional ml use cases come on it's so expensive it's late and it's not going to work for internet scale applications but um there's some things that I can't imagine people like sentiment analysis or classification I can't imagine people continuing to build models for for that when an llm does so good right out of the box and so I think you know it's probably not an overnight or all level of mops and we're kind of living in this Middle Ground of statistical AI still the most commonly deployed ml that's out there but llms are coming for us and I think all of us are kind of thinking about well you know I actually just had a call this morning where they said CEO has three llm initiatives and those are the biggest ammo the biggest projects that are ml or DS organizations focused on and it's it's a little bit of I think this you know wave that's coming towards us and we're all I think we should you know the needs of this model type or this new wave might be different so being able to adapt to something I I think we're all trying to do yeah I think from my son um mlops rolls off the tongue a little bit better so I think you're safe on that front but I 100 agree with the Parma um yeah I think I'm excited about the new use cases that it unlocks reasoning and judgment certainly um these are genetic flows and the debugging thing was an important Point earlier there's a lot to be learned still I think it's early days but you know nobody's ever said you know this is an existential risk to you know us from XG boost right there's clearly a step change with llms and we're all waiting to see how this impacts us yeah over the next couple of years um but yeah I think for now wait and see is the best course of action and last years I got to get a word that you were you were trying to pull fast on me you know I think it should be FM UPS I know you've said that last time I could never that never happened and keep ignoring it we can't have another Ops man give me a break there's too many odds oh so yeah foundational model Ops does make a lot more sense though I will give you that so Rita last one before I kick everybody off is it in or is it out am I rebranding or what no you're in for the Long Haul you've got the right name and you got to stick to it and we all know you as somebody who founded this community and we're gonna stay true to it there we go all right so thank you all for this and thankful chat I I do not know if I have ever been on a virtual call with so many hard hitters before and it is uh an honor to be able to say that I rub shoulders with you all and for those that are in San Francisco I'll be there in two weeks so hopefully we can meet in person I'm going to kick you all off now because we got some trivia games to play but this has been absolutely awesome thank you all for the fun times and insights and good stuff [Music]