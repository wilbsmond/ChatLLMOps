um and I'm so pumped to introduce our next speaker Rohit um he's going to be talking about confidence checklist for llm in production um which I think is incredibly important and very timely um so let's bring him on stage without further Ado he's gonna do a full presentation actually our first one of the day so Rohit you'll have 30 minutes if people have questions please drop them in the chat um I think we'll have a couple minutes at the end to answer them so let's bring Rohit on hello how you doing very good how are you doing lady I'm good I'm good were you playing the trivia game yeah I was that was fun I mean I didn't get a lot of it right but yeah I think dimitrios makes it fun yeah definitely um it was the speaker one right okay cool yeah I feel like some of the like fun facts about speakers have just been like blowing our minds so it's a good it's a good trivia game absolutely I mean there are people who've DJ'd in the clubs in SF which is like super awesome um all right well here are your slides can you see them yeah yeah go for it thanks I hope all of you are having a great time um glad to be talking about the confidence checklist um you know please keep asking questions we'll also take questions in the end but if there's anything coming up I'll keep looking at the chat as well um awesome so let's get started a little bit about me I'm at jumbled on Twitter if you just want to reach out or see what I'm thinking but I'm right now building portkey dot AI previously I was building pepper type I was at freshworks frame bench interesting facts I've written production code and prds using AI so I'm a product an AI person and if anybody tells you that hey it's not really you can't write production code with it um talk to me I think there are definitely ways to do it and it's really good at it uh a product I've built we've generated over 900 million tokens in production so we've seen it over a period of time and I think it's just massive uh the technology impact and what what is possible using this I have still not trained my own private album so I think uh still naive in the GPU space but I think has seen a lot of stuff in production cool so I think what I'm trying to do in the next 20-25 minutes is talk about some of the things which have been production learnings for me uh what are things that have really impacted production so if you're thinking about going to production think of this as a checklist six things that you should really do we'll try to keep it as actionable as possible so that you can go and Implement these learnings in real time perfect so first think about output validations when I was launching my products and I've seen many other people do this as well we just rush code out without worrying about output validations and what it what will often happen is that while this code Works in the test environment or development environment because your prompts are very limited to the things you're testing about you might be getting great outputs but when it goes to production you'll have twenty percent of your customers who are not getting great outputs and that 20 is actually a really large Churn number to see so you'll quickly see a lot of people starting to churn out because it works 8 out of 10 times or your product works eight out of 10 times and that will cause a lot of challenges a very easy way to solve this is using output validations and let's talk about how can you implement output validations so this would be the format of the entire deck we'll we'll go through why is what you should do and then we've got some solutions which is in the basic Advanced expert format stuff that's worked out for me but if there's more stuff that's worked out for you uh let me know very basic stuff is just check for empty strings in character lens so if you know that your output needs to be at least 100 characters or 20 words just add that check so that if it's not passing that check you can just retry it again you can check the format of output so if you're generating code or you have to generate Json just check what you generated guardrails is a very good library to do this I've used it personally and it's a very easy way to just make sure that your output matches the user's expectations so that's the basic stuff right definitely get this done probably takes 10 minutes to get it set up so no reason not to do it when you go a little more advanced uh focus on can you check for relevance uh if you're if you're done even basic NLP checking for relevance against the user input query is very easy so just check for relevance uh try ranking results so if you're showing multiple results or if you've generated multiple results then you can rank them cohere offers an API for this or there are very simple re-rank algorithms that are available this also really works very well if you're doing a rag type of use case if you're fetching multiple vectors from your vector store you can actually rank those based on the input query which makes it even better to pick which one will make the most sense to answer a user's question third piece I think experts have told this time and again is try to answer questions over a closed domain so if you're if you're worried about getting the facts right and if you're worried about being truthy the only way to do it and the only way to have llms not hallucinate is to do this over a closed domain so give it all of the context tell the prompt or tell the llm that only use this context to answer my question and then ask your question that just improves fact uh truthiness by a by another level so definitely do that expert level would be model based checks this is a very interesting concept that open AI evals launch and now almost everybody is doing but we're not talking about badge evals I'm really saying can you evaluate in real time the output that is being sent out now obviously this introduces some latency but there are ways to go around it but there are model based checks so basically you get an answer from an llm model and you can reply to it if it's a chat model or you just send another problem saying are you sure and just uh the fact that you ask the question lets the model think about the answer and come back with was it right or it can correct its answer extremely helpful are you sure is such a powerful prompt uh you should definitely check it out if not in production at least in testing but yeah that's how you do output validations moving on uh secondly and I'm sure nobody is really thinking about this but prepare for a DDOS prepare for a lot of users starting to use you and I speak from experience right I mean we were building an app we were having fun and nobody really expected that we'll get ddosed now the problem with ddosing in most cases you might think is hey we'll go down yeah so you know the fail will comes up people know they're popular and that's awesome but what's actually going to happen is if you stay up you're gonna make hundreds of thousands of requests to another server uh to maybe open Ai and that is going to bloat your bill like crazy you'll not realize it but in a matter of days uh the token costs add up very quickly so you really need to figure out how do you not get a lot of bad traffic to your website basic stuff just add a captcha if you are getting DDOS if you immediately start to see a lot of bad traffic coming to you just add a captcha that solves a lot the problem so that at least simple hacker cyber attacks don't get in but right after that I think you should really invest in rate limiting users and organizations so how do you make sure that all of your users across your application have a great experience because you've rate limited uh the folks who might be abusing your system so it's always good to have these checks in place right now there are tools to do this very very easily so no reason not to do rate limiting across users or even across organizations it's also good to build this into pricing plans so if you're building a product and you're going to price it then it's also good to rate limit users or if you're doing it internally again individual users can have quotas so you're not exceeding it and everybody has the best performance expert level would be uh we actually ended up implementing IP based monitoring and fingerprinting which is a lot more deeper concept but there are companies available that help you do this much more simply as well so definitely I think if I were to talk about IP based monitoring a little bit more how it helps is instead of you just identifying a user by their email address or by the user ID whatever you have stored in your database you can actually use their IP address to figure out if it's the same user creating malicious accounts on your system so a captcha would obviously help but if a user is just they love your product they're just using it extensively you don't want to be caught up in a situation where you end up spending so much money and it doesn't make up for what you're charging the user so definitely I think think about doing it another interesting variant to this uh to just getting ddosed is there are some languages where tokens are very expensive so a short word in Korean could actually cost you a lot of tokens so keep a lookout for multilingual usage of your product it's a good way to keep tracking which users which language what cost is it happening so that you can manage those costs and better it's better to be safe than sorry in these cases because open air costs just blowed up I remember at one point in time we would pay uh our open AI bill was 6X the AWS bill and when I say that I would uh I'm sure a lot of people will agree that we've seen these in many cases as well so just look out for this and adjacent to this is building user limits I think it's just important that uh you are not winging it and user limits are again really really easy to build how do you make sure that all of your users have a really good experience and they're all able to use your product with the highest level of reliability and accuracy so I mean I keep stressing on this point all your users because for sure some of your users your power users will have an amazing experience but you actually want to capture all of your users um the basic stuff you just start with client-side rate limiting just debounce it so if somebody uh and this is weird even if I say it but there were people who would just keep clicking the generate button multiple times now what's happening is the request is eventually going to take much longer you could have just prevented it by using a simple debounce if you see chat GPT already does this today so if you're using chat GPT till the time one response is not complete you cannot open up another chat um you cannot open up another chat you can't send another message so that's debouncing on a very different level so you can have debouncing on button clicks you can have a debounce on is a request already in line you can even have rate limiting on how many requests can a user make in a given time period again chat GPT implements it with gpd4 they say you can only do 25 requests across three hours um but yeah again simple stuff implemented don't don't just worry build it out you go a little more advanced and you have multiple organizations coming in you might want to go rate limits you want you might want to build rate limits specially segment based so you could say for organizations in my Premium plan or for this set of users these are the rate limits versus for some other users I've got higher limits lower limits um it's also helpful to create sort of a allow list where people are now trusted and then you can grant them higher rate limits so everybody coming in can probably start with a lower rate limit and then as they build trust with you you can start increasing their rate levels uh how you implement that is you just put people into different segments so any new user within the first seven days stays in that untrusted bucket or like a quarantined bucket when you're ready to take them out based on some amount of usage or some amount of concurrent usage then you can move them to the trusted bucket which has then higher rate limits or maybe paid customers get higher requirements so that's an easy way of implementing rate limiting on the user side obviously an expert level is dynamic rate limiting which I was just talking about how can you really identify abusers and fingerprinting is a way to do that but can you identify people who are abusing the system and decrease limits for them right um I hope that's not too complex but the same concept as bucketing but now you can actually dynamically set rate limits for people so companies already do this uh there is a concept of credits or Karma that you can build into your system where you can say that everybody gets a certain level of rate limits and then that is increased or decreased based on what is the kind of usage we are seeing from your system coming into ours a general practice is you start with a lower limit and keep increasing as trust in your system trust in the user increases as well we've talked about fingerprinting um extremely useful when you're launching to production and launching to a lot of users perfect number four um caring about latency and I sort of love this meme but I think latency is the biggest factor which is different from older user experiences to the llm user experiences you know do you remember times that you would get frustrated if an app even took two seconds to respond to you you would get finicky and you wouldn't want that you actually want to spend time on apps that are really fast Google said that hey we're searching through millions of documents within milliseconds but then today that entire thing's changed and open AI calls or inference endpoints can take really long to come up with an answer uh so it's natural for us to stop caring about latency because you can say hey my request is gonna take 15 seconds users just have to cope with it but what's happening is that users are having a bad time and they're not used to systems like this it's interesting that chat GPD is probably normalized it but they use a variation of things to improve the perceived latency of their product so I just keep that in mind maybe you can't handle latency because inference takes an amount of time it takes but you can improve the perceived latency of your product and I think that's what is really important so how do you do it basic stuff uh just Implement streaming it's the easiest way to get started one parameter in most API calls you can just say Stream True and that improves the perceived latency so that 15 seconds that the user has to just wait for that Circle to go around and then come with an output you could actually show them the data coming in and it reduces the anxiety in the user so perceived latency goes down very very much because the time to the first token um and probably this should become a metric Zone saying TFT so time to First token is actually maybe a second which is the kind of experience people are really used to and that's also what chat GPT and other products have gotten them used to so just Implement streaming get done Advanced um Implement streaming properly implementing streaming just saying Stream True is just a simple way of doing things but the technology of streaming is just so complex that you'll have to start handling a lot of edge cases for some users streams can break streams can have multiple date objects coming together so there are multiple implementations of this but it's always good to just test out streaming really well so while it improves latency it can also break your app in multiple ways I think just yesterday vessel released their AI Library I think their AI SDK and I think that has streaming done really well but if you're implementing streaming I would say just look at some of these edge cases where you can have incomplete chunks or multiple chunks coming into one um it just makes for a bad user experience if your stream breaks in between um the second piece is handling rate limits with back off so what I mean is and again goes back to user rate limits I think that is just an important concept wherein if you have some users who are completely uh taking up all of your usage then uh you will I mean the other users are going to suffered so what's happening is you are hitting rate limits of your endpoint provider and that is going to start hurting your users as well so it's always good to handle these with exponential backoffs so just Implement like a tenacity in python or again there are solutions out there to do this but just handle your rate limits gracefully do that with big with exponential back off do that with random Jitter Concepts that have existed in engineering but we just somehow Miss applying these in llms the third is caching uh again super simple concept but if you can semantically cache your queries and we've actually you know we're just launching this is a plug because we're just launching a product around semantic caching but semantic caching can actually improve your responses uh dramatically so imagine for a percentage of your responses you don't have to go to GPT or your inference endpoint get the inference out and show the answer you can actually serve from a cached response because these are either the same queries or similar queries going in especially useful if you're building a rack system or you're chatting with documents chatting with data you'll realize that probably 20 to 30 percent of the times users across an organization are asking the same queries so you could just cache it and whenever you see a similar query coming in within time frame obviously you could return from Cache that's a very big performance boost people don't wait that 15 seconds they're getting some of the frequently asked questions they get answered really really fast so you should check it out on the expert level you can think about really building fallbacks so if one providers fails or one provider is taking time can you fall back to another provider as well and the most common use case I've seen production is where you start with an open air and you put anthropic as a fallback or you have gpd4 as your main model but 3.5 is a fallback it's a graceful degradation but users still have a good experience with latency uh second you can Implement queues so instead of having all of the requests coming in and you're Fanning out the requests to your API provider you can actually queue these systems which helps in obviously rate limiting at your end and users can have a more consistent experience so you can always say that for every user one of the queries is going to get executed simultaneously yeah uh yeah I'm just reading traditional role of a software engineer I really feel that I'll a lot of times what we're doing with llms is more engineering than machine learning because for a large part machine learning is already done so somebody's built pre-trained these models and now you're just using this so it's almost like it feels like I'm reminding everybody of the concepts we've already implemented in other apis but there are just minor nuances on how can you implement them in your llm strategy as well perfect so moving on to logs and monitoring so this is a again a hard lesson that we've learned so when you start with saying let's log all our requests let's monitor for latency cost tokens the first thing that comes is I already have a monitoring logging system in place let me just push everything to it and what happens is a lot of these systems were not built for unstructured data so llms you have these large prompts you have these large outputs and all of the logging monitoring systems actually exist for these structured data so they expect smaller strings they expect to index everything and this quickly becomes either very expensive or very slow and they're not built for probabilistic models right to give an example apis until some time back were returning a success or an error now apis are returning a success but you don't know if the output is accurate or not so there's like a whole variety of accurate partially accurate completely inaccurate but the API returned true so data dog or any other monitoring software cannot really monitor it so you'll need like a specialized system to start monitoring if the llm apis are working for you well or not um the basic stuff is you have to either live with some amount of poor visibility as you're getting started which is fine uh because as you're getting early users and they're trying it out you may not want that level of complexity or you just pay for a data dog or Cloud watch or the likes and spend some time setting it up uh here I don't have an environment option I don't think there's an easy way or an advanced way to do this the expert ways obviously I have seen companies who've built out their own monitoring layers with like an elastic a kibana for probabilistic text models so you've done some amount of evals on the models and then you're pushing this data into a column in a data store and then you use a grafana dashboard on top of it so if you're building a really large app and this is something that's core to you I would really say that you should think about what's the devops layer look like for monitoring logging analytics I think that becomes important as you're building out the apps if somebody is using datadog I think it's great to get started as you hit walls you can start to upgrade migrate try out different solutions but just something to watch out for because it becomes important as your starting to scale your app lastly I think the sixth thing is implementing data privacy as you get started most companies aren't really worried about it but to some extent think that a lot of private data is going across the wire to multiple applications and then to your you know API provider of choice so how do you implement data privacy early on and there are very easy ways to do it it's important that you do it because when it hits you that you're not compliant with gdpr and you're not compliant with CCPA the fines can be drastic it's also something that people value a lot so building it early on in your system makes users really really happy so these are one of the quick things that ensure a great user experience early on itself basic stuff I think there's no basic stuff you can just close your eyes and say it's fine privacy is the way it is I'm not going to worry about it to begin with and a lot of people do that um I don't think it's okay but I think anything to get your product off the ground right so basic is closing your eyes advanced mode is let's amend our gdpr cookie and privacy policy first so let's tell people that hey your data is going here we either have a data processing agreement with them or we don't but at least talk to your users that this is what we're doing and then secondly you can actually Implement pii masking so if you're using Lang chain or Lama index or even if you're using the base apis you can pick a library to Implement pii masking within your code so what's happening is whenever users are giving you in any information you can make sure you're anonymizing all of this before they get sent to other systems so um fairly easy libraries that exist for this just do it when you're going for a more advanced mode or especially your Target user segment is really focused about privacy so in the medical sector financial sector I would recommend using something like Microsoft's Presidio library or Azure also as a service that can identify pi to a much higher degree of accuracy so when you want to make sure that no pii should ever leave my system or even within my system I need to store Pi really well then just implement this perfect so if I were to just give a quick rundown of everything that we discussed um one make it reliable so what do people really care about right make it reliable and this is true for almost every software that exists and we're re-implementing almost all of that as we talk about envelopes for llms or llm Ops or whatever you want to call it but one stable API is no down times faster responses um second is making it accurate but how do you reduce hallucinations on llms so provide relevant moderated consistent Generations you can solve this you can start to do start to use the biggest baddest model that's out there go down improve prompt engineering then maybe if that doesn't work keep going and maybe fine tune your own models uh evals are a great way to continuously monitor if your responses are accurate uh consistent and moderated and compliant so just do that and if that doesn't work then usually it's it's a good idea to maybe consider building your own models and the accuracy is important third piece is uh making it cheap I think I have seen this in a lot of applications with friends and my own application that you can get started with the POC you're seeing a lot of traffic you're excited things are going well but then inference costs can be really expensive uh if you're trying to build a SAS product on top of it SAS products are generally enjoyed very high margins but insurance costs can very quickly eat up those margins so how do you make sure that the economies of scalar working in your favor or after a point in time you're transitioning to smaller cheaper models without impacting your quality as much so I think that's a balance to maintain but then reliability accuracy cost are probably these three things that your users worry about the most um and you just have to implement these Concepts in your llm app itself so what should we do Implement telemetry you cannot fix what you don't know Telemetry for LMS is a little bit different it is a little more nuanced so check that out build systems to improve resilience fallbacks retries streaming ux pieces that you can do to just improve perceived latency perceived reliability and then collect human feedback early so that you are fine-tuning your data and making sure things are really going well for you so while you can launch your poc in probably a hackathon and everybody is excited I think getting production ready is is a fairly long journey so that's a marathon that's not a Sprint so make make sure that you're spending time on getting through the seasoning yeah I think lastly while you know I hate saying the buzzword but don't just be a rapper but I think what people are trying to say is that a foundational model system is only the compute layer so think of this as your server or an ec2 um you know that's your bare metal server that exists you need to build a product on top so while every software was probably an AWS wrapper before this uh here you just have to find what's your value endpoint and then be production ready with it so you can hack together a solution for a POC or an MVP then going to production is a completely different Beast so yeah I wish all of you all the very best in getting to production any questions I'm available on Twitter uh but happy to take questions now as well and Rohit on Twitter that was an awesome talk but we have to kick you out to prepare for our next panel absolutely no fantastic doing this thanks so much Lily thank you so long [Music]