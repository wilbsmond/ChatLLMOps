who do we have up next none other than Omar hey how are you I'm great man it's great to have you here I know it is uh what a Friday afternoon for you almost Friday night so yeah it's like 8pm but yeah it's very sunny so it's gonna look good so you can handle it and I appreciate it I appreciate you taking the time out of your Friday evening to come and talk to us yeah thanks a lot for the invitation I'm excited to be here dude so I'm gonna kick it off to you and I'll be back in 10 minutes I'll get this uh this little uh ad off the screen and I'll share your screen and then we're gonna keep it rocking go ahead man awesome perfect yeah so hi everyone I'm Omar I work at dragon face as a lead of machine learning and today I will be talking about end-to-end modern machine learning so this is an extremely complex topic it's changing very fast and there is no way that in 10 minutes I can talk about everything here so my goal today is really to increase awareness of which are the existing tools out there that can make it easier for you to use the state of the art ml models in your products in your services so I will divide this stock into two parts so in the first part I will talk about how to do inference with these models and then in the second part I will talk about how to adjust these models for your own very particular use cases yeah so if you have been looking at Twitter this year every few weeks there are new exciting models here in this slide I put three example examples of popular recent models from the last few months so star coder at the left is an open source community-led co-pilot replication it's a model that can generate code for many many different programming languages there is llama by meta that has caused a great number of tools and research around it so from Lama C plus CPP which is acid plus plus version of it to many other applications and Tools around it and two weeks ago there was Falcon it's a very very recent model and it's uh right now considered the best uh the best open source llm a large language model and each of these have a bit of different licenses as you can see here all of these are quite large and they go from a few billion parameters or 7 billion parameters up to 65 billion parameters in that piece of llama and so yeah so there are many challenges on how to use these models in inference event uh so the previous slide showed some common things so for example as I mentioned before the models are huge a falcon for example requires a lot of GPU memory it requires 90 gigabytes of gp2 memory and that means that not even uh a100 which is considered like the one of the best gpus to use and you cannot put that model in a single a100 so it makes it very hard to use as the model one fit in a single machine but it's not just a model size it's also the evaluation of the large language models and there are many benchmarks right now you can evaluate these models but the benchmarks are not necessarily representative of real world usage or like the real world use cases in which you will put these models and most likely most users or companies really want to adjust or tune these models for their own data their own use cases and then you you will expect very fast latency so there are many things that have been popping up in the ecosystem in the last couple of months the last half year so for example there are techniques such as loading model scene 8 or fit a 4-bit mode that allows you to use a less memory to load these models so for example there are open source levels such as bits and bytes or accelerate so for example with a 4-bit mode you can load the larger Falcon model just with 27 gigabytes of RAM which is still a lot but it's much less than the 90 gigabytes that I was talking before and you can even do like some interesting things such as putting part of the computation in CPU which of course means that it will be much slower but you can use very large models then there are also tools such as text generation inference that are optimized entirely for the limbs so I would like to talk a bit more about text generation inference which is how open source Library you can go to GitHub and find it and there are many features that this library has but I put here like a couple of the interesting ones so for example tensor parallelism allows you to use multiple gpus for a model for a single model uh pretty much what it means is that you can split a tensor into slices and each slice will be processed in a different GPU uh if you have used chat TPT for example when you're talking with it chatting with it you won't generate you you want to receive the full response you will be receiving uh characters at the same time or tokens so that's called token streaming and it's essential for fast latency so rather than waiting for the full generation what you want to do is to have the server just answering as soon as it starts to generate tokens it provides a much faster ux and it's a nicer experience a matrix monitoring quantization with two such as bits and bytes which was what I what I was mentioning before and there are many other optimizations such as a flash attention for fast attention mechanisms and many other things so this tool tax generation inference is actually being used right now in a couple of different uh places in the open in the open source ecosystem there is code in chat which is a open source UI like chat DPT so an open source UI for open source llms there is a effort called open Assistant which is a reproduction of very large llms and not fitment launched a couple of weeks months ago llm playground so all of these are examples that are powered by text generation inference so it has been battle tested and it's again a fully free open source tool so this was the part on how to use these models uh to do inference of uh research lab Community share some model to put these models in in production but most likely as I was mentioning before you will want to adjust or tune this model for with your own just case most people here are probably familiar with a with some of this uh so in the classical ml setup what you want to do is that you will want to train a model you will require lots of data lots of compute as well as the asparities on how to train these models that's usually quite expensive so in the last five six years many people have been doing fine tuning fine tuning requires a much less data so in fine tuning you pick a base model a very large model that was usually was a most likely very expensive to train shared by a research lab that has lots of compute and then you adjust your fine tune to your own domain or data so it can be your own company data or your own personal data and here you need much less data much less compute you can train models much faster but now with very large llms it's becoming very hard to just do fine tuning and that's where path or parameter efficient fine tuning is quite interesting so it enables phone use cases such as a fine-tuning whisper or falcon I was talking before in a free Google collab instance so without having to pay for expensive gpus so the idea of best in 20 seconds is that rather than tuning or turning the full model you will freeze the model you will add some adapter or some additional parameters and those are the ones that you will train and you will have maybe a bit of a quality uh a bit of a quality hit but even then the the performance will be almost on Parallel and the inference will be just as the same but the training will be extremely fast so I will talk about those three quick examples about a path so for example there's whisper a whisper is an automatic speech recognition model and that means that it can transcribe audio files to text and fine tuning whisper usually in a Google collab would crush your memory you will get an out of memory but with Laura Laura is a parameter efficient technique this is a path technique you can just add a small adapter which will be much smaller than the original model and it will enable much much faster training with similar quality and without a requiring requiring having like extremely large additional models a path can also be used for things such as stable diffusion so for example a sustainable efficient if you don't know it's a image generation model and you can have different adapters for different concepts for example but in the context of llms a pest enables things such as fine-tuning a falcon even if you don't have that much compute power and there was a very recent technique from about a month ago called a killer killer allows you to uh to a fine tune a model with 4-bit quantization an adapter tuning that means that a so a long story short it allows you to require much less memory to train models and that enables very interesting things so for example in the context of rlhf uh reinforcement learning with human feedback which was a previous presentation what you can do is have a single base model and then have multiple adapters so for example for the preference model you will have a different adapter and for each of the components you will have different adapters but you will always keep a single based model uh this is a very recent area this is just from the last couple of weeks months but this enables some very interesting things so yeah so again what I wanted to do today was just give a very high level overview of what's the current state of the ecosystem and I hope this was useful everything is in GitHub everything is open source so feel free to check it out feel free to give some Stars thanks excellent Incredible or thank you I get the feeling I'm listening to your accent right now and I get the video from some Spanish-speaking country maybe yeah so I'm from Peru originally and I grew up in Mexico yeah all right here we go man so if anyone wants to continue the conversation with Omar throw it in the chat and he will cruise on over there thank you so much Omar this is awesome man I really appreciate you coming on here and joining us yeah thanks a lot for invitation thanks everyone [Music]