[Music] oh can you hear me hey how's it going oh yeah man when we got Jared Daniel all right Mario and the guest of honor that I am going to end up replacing myself with Lina Lina you are the one who has been tasked with leading this panel that is coming up I'm gonna hand it over to you and let's get it started thank you all for joining us heads up oh I wanted to Triple to you earlier amazing uh to have you all here we're going to talk about um cost optimization and performance and so I'm also an ml engineer and I have an amazing panel for you today so I have two people who run large language models in production we have Daniel who is a research scientist at Neva which is an ad free private search solution tried it uh quite nice and they're using um large language models in search for all kinds of questions like summary semantic retrieval career generation so I'd be super interested to hear what you learned by applying it to the search space then we have Mario who is a staff engineer at intercom and they put one of these early gbt4 powered projects out which was a chatbot and then we have two people from the tooling space really also very excited to have here Jared which is the co-founder of prompt layer um hey Jared wow hey um it's a platform to manage your production llm apps with a reliable prompt engineering monitoring of the problems like versioning course latency and you can track historic usage history and performance so I'll be excited to talk more about evaluation and last but not least we have Lewis who wears many hats he is super hero signs in Washington and also the co-founders of octo ml where they offer easy and efficient deployment and there are interesting part for me especially was that you can just upload any computer vision um or NLP model and it will be optimized and it will tell you which instance to run it on and soon you can also run your models there so I'd be very interested how you help users to make this model run very more cost efficient and faster okay um so maybe let's start with costs we know generally running large language models is quite expensive much more expensive than normal paid back-end apis so so um Mario and Dania maybe because you run it for practice um was cost an issue how did you approach this um the cost angle of running and production uh yeah so class is an issue especially with scale and I think that's kind of like where we started out we started off with some of these kind of like foundational apis as a way to figure out is this a product that's useful does this help our ranking signals to our users like this and then once we actually started getting that both the cost of running that and how much we could run this on really because of bottleneck so if you think about like summarization systems uh potentially running it like in production real time for users that's very possible the API it's kind of slow it can get expensive but what if you want to run summaries on let's say the entirety of a web index which is on the order of billions of documents at that point no longer can kind of become actually a cost possible so you have to move in the house to some type of smaller system and at the same time it's just a question of like how widely do you want to test and that one using these models how many people you want to access them and what is their expectation of their turnaround time yeah do you have a guideline like how long would you start so you have a working prototype and also maybe could you speak on imagine you're running it in-house now but it's still expensive right so do you have a ballpark uh compared to other services how how do you evaluate that and how do you bring it down to which level yeah so on the evaluation side or initially using kind of these foundational models it's about kind of like having rapid iteration and the ability to kind of refine things via prompt and get this in front of users to actually kind of figure out what they're thinking and once users are actually saying like oh this is great this is the use case that I'm working on then we tend to kind of focus in and figure out how can we distill this extract this into a smaller model within these kind of smaller models the process we generally think about is less around uh just the dollar cost more on kind of like where can I run this and for us the golden threshold has been can we make this model run on a single a100 GPS or 810 GPU or on any type of CP requirement because like when you move from the a100 to an 810 say you're serving AWS that moves you from having to pay 40 bucks an hour because you have to get eight at them at once to on the A10 you can pick up a spot instance for 30 cents and so like at that price point you can do pretty much anything and then if you even move it to CPU the game changes even further where you can move to kind of billions of items without necessarily even thinking about this and so a lot of our work we're moving like for some of our semantic search these are small queer encoders these are 30 million 40 million 100 million parameter models and so in that case we can actually run the query encoder on CPU so it sits in the same system that's actually doing the retrieval and so that can just simplify production it's just a question on really like figuring out what scale you're going to what is your latency throughput that you're looking for and what is kind of the size and one of the things that we saw was the foundational apis are actually really slow and partially that's actually really good because it changes user perception whereas like before people's perception via Google search was like anything over 100 milliseconds is terrible now people are used to waiting three four five seconds and we've been able to see that we can take if we're running to here we're running open AI uh what's it called anthropic they're on the order of like three seconds for our response for a single batch we can get a model that is 20 billion parameters to respond in 400 milliseconds so that like net Improvement in speed is huge just by bringing in-house and that's just because you don't have to go on the internet it doesn't have to go through their orchestration you can do with your own and the other thing is when you bring this in the house you really become a master of your own fate where one any type of latency optimization is actually owned by you you don't have to worry about API rate limiting you don't have to worry about like their entire system going down I think that was actually one of the things that really pushed Neva to bring these things in-house because shirt open AI or has 99 uptime well that's 99 for everyone that was not what we were experiencing at all and when we're trying to deliver this product and we're like well this use case doesn't work because their overall system's down and we can't do anything but sit in our hands it really came as kind of like the impetus to bring this in-house yeah we experience the same issues we also had regular they were regularly not available sometimes 15 minutes at a time so that was a problem um okay anyone else on on course consideration can I can I just jump right in here to say Amen like I love it was just what he just said Mario like this is exactly our point of view is that hey often you don't need the highest End Hardware right so you can live with Hardware that actually hits your latency throughput requirements you know even though there seems to be um a strong um let's say uh shortage of high-end silicon many people don't realize that they don't probably don't need high-end silicon to deploy their models um and you know basically what Octonaut offers exactly what you talked about but in the general way in a fully automated platform for broader users to deploy their own model their own custom model or another source model in a platform that they control automatically choosing the Silicon for you and turning models into high performance portable um artifacts and code that you can run on different Hardware I just have to say you know we should definitely follow up after this because our our view of value here is 100 aligned so but in panels we need to disagree with each other so somebody should disagree with with him to make this interesting right so You can disagree about the most promising um ways to go about customer so if you each had to name like your top two methods because you can do so much right if you if you start reading on the internet what are the top things to do to reduce costs you have a list of like 20 items so maybe you can disagree on that that let me give you give me each your top two okay first of all um make uh pick a model that does the thing that you need and optimize as much as you can because then you have better performance in a given Target in a second choose the right silicon for it the one that has the lowest cost possible and hits your uh performance requirements so you're also saying that a total type like a smaller type of type of exactly yeah completely right model optimize as much as you can hopefully automatically and then still pick the right hardware for it so Yeah my two things would be kind of like structured Printing and knowledge distillation where you can take the large model and then you actually just remove portions of it until it fits in whatever systems you have and like I'd like direct example for this like we're serving a flan t511d parameter model that physically doesn't fit in an A10 GPU until you remove at least 60 of the weights so you have to just because like the actual size of the model once you've done that you can make that model fit in that smaller GPU but then you have to use some form of knowledge distillation so the model doesn't actually have this like uh decrease in quality in generally speaking those two things together really provide like big big performance improvements and if you get really crafty with your distillation and I used to work at neuromagic which focused on distillation for CPUs and everything else and we were able to get 30 40 50x improvements on text text classification tests by basically just pushing in that like knowledge distillation as far as we could with the biggest teacher model and the smallest student model and that just works extremely well the downside to it I guess I'll take my own uh going against myself in the other corner there's a non-zero cost actually moving all these things in-house like if you think about the beautiful part about these foundational models is the feedback cycle is super fast so if you think about like say you're doing something with the prompt for search engines uh you want to rephrase how your system works and so you just change the prompt and it runs it's good it's in production it's good to go if you want to do this in a house you basically say you're like you want the system to take a more neutral tone you have to go through all this iterative approach to make data sense just throw this data set into something smaller take this model compress it into something smaller figure out how it moves to production so like your iteration time goes from being on the order of minutes to potentially days or weeks and that's kind of like this like nice trade-off of like how much do you want to explore how quickly you want to be able to iterate on products with customers to how quickly do you want to be able to optimize and control your own state um if running in-house what do you say what size of team do you need just for the upside of running uh I don't know few models but let's say two I guess it depends yeah maybe it's too general question depends what type of model probably but just maybe can you give listeners like a ballpark on on the on the amount of work that you had to do so I would say the bringing these models in-house and making these models there was roughly there's about four of us that were working out on anything from data modeling to I'm the compression guy so that's what gets me up in the morning uh but there's other folks who are working on the modeling side and otherwise I think overall at Neva directly working with models there's probably around 10 of us okay and Mario Jarrett thoughts on the cost side uh I can maybe give like uh a little bit like a few thoughts on um how you can control cars before you actually move anything in-house so we don't really like we don't train or fine tune our lamps ourselves so right now we are pretty much on the consumer side uh just calling already apis uh and It's Tricky uh it's expensive and um I think even DaVinci uh can be a very expensive model depend how you use it and um most of the time like we just first do pretty much feasibility exploration of is it possible to do what we want to do um in some like really isolated cases uh like can we summarize one conversation for example and to like to assess that capability we would take the best model first probably see whether that's possible and try to go down down you know just try to Pare down the model uh to the lowest model that gives us results that are okay like Daniel mentioned that's often like I think uh producing games uh 25 years ago where everything is too slow and you're totally like limited by how much you can process and uh it kind of like Cuts both both from like cost latency perspective like you just can't summarize all the conversations in the con because we'll Bank wrap straight away um there's like too many of them so but like one thing that's one thing that's interesting for like cost reduction is just basic engineering thing which is just call it less and that's not always possible of course I think like like probably like in Daniel's case in Evo that's a little bit more tricky if you're doing a semantic search but like the systems we build are usually a complex set of things that talk to each other and there are multiple there are multiple components that are implemented with llms and sometimes you can put like a really simple classifier before doing something to figure out whether it's even whether it makes sense to call a lamp at all it's like that's you know that's one technique that I think can work quite well because not all tasks are so sometimes you can find a tool that's really simple and it's going to be able to say whether the more complex and smart tool is going to has has any chance in finding an answer so I think like that's one technique that helped a lot yeah and I just Echo Mario's first point there that like the first step yeah you could do this bringing kind of bringing it locally or whatever but if you're using these uh large Foundation models from open AI or whatever the first step is just knowing what's going on and kind of just building version one and kind of see just understanding how the cost works and this is like uh it's a bigger step than you'd expect and just understanding which prompts in your system which parts are expensive and like knowing where to go from there is a is a pretty important actually so just echoing that otherwise yeah so there's some great tricks you can employ when when starting off and then maybe later when you scale uh to bring it in-house okay let's talk about latency it's uh okay I just said one more thing I think you would think that I think was was implicit but I don't know if it was said explicitly was you know there's plenty of Open Source ready to go models that can do I think Daniel mentioned you know T5 plan for example they can refine for users and people realizing that you don't you don't do not need to pay the open AI or cohere tax if there is a use case that you know a local Source model can do well and you can much more efficiently in a platform that you control that you might not need to build on your own that that already gives you a very very significant room for cost savings there right so just know your use case you know you don't kind of like there's an analogy of use a CPU because it's General but the CPU is expensive but then use the GPU to the work of the computation because that's more efficient I think there's a direct analogy here where you have these uh you know General expensive models like pt4 they have to pay to use but then for things that you know that you want to do and you can specialize it and use an open model you're probably going to pay a lot less like probably orders of magnitude so just to make it interesting and push back a little bit I'd actually say that please yeah I the way I see it if you're if you're I mean it depends where you are in the stages of development if you're building version one if you're trying to ship something where honestly most people working with LMS today are trying to ship version one I say don't bother with open source models get something working on GPT yeah yeah yeah but just like hey if you're writing code here you write in a CPU you make sure it works and then your floats to the efficient compute unit that does what you need to do similar thing here you start simple with the model that you know is more expensive and as you understand what you actually need from it then I'm I'm fairly confident you can find an open source cheaper model that does what you need to do right question answering summarization and stuff like that I I think the part that's important to consider in the cost reduction also just is Staffing a team like the story I have here is I've always worked in like the search World of like big search engines so there's always been a lot of money behind this there's teams everything else my brother used to work at fine Tech and at some point they were looking for something that could classify businesses and when they poked around and figured out like what would this cost to have our own system you have to hire people you have to hire at least one ml engineer private info engineer you have to have to run it and pretty much their bottom Barrel costs would be on the order of I don't know five six hundred grand a year just for like the team and they could make 99 like what Alice was talking about but in reality they just needed something that was 70 and like if they just plugged it into a system they never had to hire an NLP team and who never had an ml team they had the system that was beautiful because like they could go in and someone who could just tune The Prompt until it works well enough and it just like by using this large system they save money because they didn't have to hire expensive people like me and it's an understanding use case because they were only pushing 10 000 examples through today so like their cost is not as High versus on like a search engine side or when you're like dealing all the interactions that intercom has like if you wanted to do all the summaries then that kind of cost calculus changed yeah it's uh it's a question of scale so clearly if you if you use it like the exit plus type of thing with the human in the loop then using as the API will be always more cost efficient it's a good point okay so um moving on to latency this is another huge problem so just getting uh or maybe I'm a bit um you know spoiled from normal classification models but just moving into the llm space and seeing that just one token takes 0.7 seconds on P90 was uh shocking and then you normally you need like a whole sentence um so that can take several seconds and how do you deal with that if you have a user on the other hand who is like you're really very impatient things after one second the application is broken and what are typical how do you go through the life cycle when do you that you start out maybe not looking at speed what types of speed UPS can be um are possible to expect do you do this trade-off between model size and and Speedo like how are you thinking about this and maybe also throw your best tips like if you had can do one or two things uh on speed uh what you do if you don't have a lot of time literally yeah I can walk through it directly that we didn't even we've talked about is our initial summarization model and uh initially use a large language model you get the summaries you're basically looking at about three seconds per item and we're doing a batch of 10. so we're basically spent 30 seconds you we did this and figured out that like oh it turns out this is actually useful and very good iterator and prompt and then brought this into a smaller model of a house just going into a T5 large model but serving that on like a batch size of 10 on an A10 still took like eight seconds naively so it kind of went to about all these ways so first off there's a great Library called faster Transformers Nvidia kind of supports it they don't fully support it so like you got to fiddle with it a little bit but just moving from uh Native Pi torch serving to faster Transformers basically brought us down from like eight seconds to like 1.8 seconds so huge gains geost on changing how it served it worked on the GPU had a funky build process but it works super well and then from there we threw in some notion of structured pruning and asymmetrical pruning on the encoder decoder side just because like the encoder produces this contextual representation on a sequence sequence model and you want the best contextual representation possible and it also only runs once so the cost is pretty much doesn't matter versus the decoder runs again and again and again soon heavily compressed with decoder so we basically took this model that started off with 24 layers on either side and brought it down to 24 layers on one side and four layers on the decoder and that meant that like by the time that we were done with faster Transformers this compressed model we got down to roughly like 300 milliseconds per batch of 10 which that allowed us to basically say hey in now will cost us twenty thousand dollars to create a summary for everything in our index and we can actually do that because we'd like decrease these costs versus when it was 10 seconds 30 seconds I think just not tracked yeah yeah I just I just wanted to underscore making sure to use the very best kernels the very best binaries for the giving harder they're running it on because that's if you can do without changing your model at all or any batching parameters any other system premise start with that and see how far you can go you know if you and the problem is that this is dependent on the hardware Target it's very specific in the hardware Target so the more you can um ultimate that the better I'll put a plug-in for what OCTA ml does we can help you search automatically for what's the right library or compiler using tensority or TVM or uh you know Cody and then for NVIDIA or the equivalent for other Hardware targets and then after that you know change the batch size sure you might pay a little bit more Hardware utilization but this latency really matters to you reduce the batch size pay a little bit more so I I'm really happy to hear it sorry again Mario I'm really happy to hear like with Daniel and Louis are talking about because like if you're in our world we're just taught like talking to some external API your hands are so tight so effectively you have like two ways to uh improve the latency one is by shared capacity um if they if they can give it to you uh and then you know you can play a little bit more with like throughput latency um curve and like where you want to sit there and the second one is maybe like what I said like if if you're okay with not finding an answer and if you can detect it quickly maybe you just don't call it and return the answer straight away like there are ux paradigms like I think it's becoming quite for like chat applications you know just piping the tokens directly from llm to the user so user is you know saying straight away that something is happening or sometimes streaming do that streaming effectively yeah like streaming like directly from llam to um to to the end user experience like sometimes you want to check what's you know the answer when it's fully uh generated and in these cases like you have to wait for it and for example for gpt4 uh shared uh pool of capacity uh like the the latency I'm seeing is something like uh 100 millise per token um for English it's usually one word per token they change the tokens for gpt4 but it's extremely high uh so I'm really I'm really happy to hear with Daniel and uh who is there talking about because I hope they'll be able to actually you know move some of these things um like once you understand like which use cases are actually the most important for us and which use cases make sense to uh to optimize and bring in-house there's there's some tricks you can still do on um on when you when you call an API you can work with a prompt because the main database in the output not in the size of the input so you can you can tell it to be precise or give you back a number of paragraphs like be a bit shorter in the output so this is something I mean we can clearly see from the panel that if you want to be really fast you need to move it in house but let's say you're in this first stage where you still need to understand the cost and and the speed uh there's a small amount of trick like semantic caching and what Mario mentioned that you can do and maybe Jared you also have a dashboard where users can track a latency they observe is there some insights that you can share like how do they like what You observe in terms of your users with respect to latency yeah I guess uh the interesting thing here yeah is what is the low hanging fruit to kind of help with latency imagine one is obviously like how long the response is but the other one which is also very obvious but falling back to different types of models so obviously if if you you can detect uh how how badly you need cpd4 over 3.5 if there's a use case where you can fall back to 3.5 a faster model or even something kind of let although that might be the fastest one they have now but or maybe Claude or something like that kind of there's becoming uh in fact there already is probably this whole axis of which engine do I use for this problem as opposed to kind of just saying we're using GPT for for everything or we're using our in-house for everything I think these are all tools that have pros and cons and one of the pros and cons of each is latency yeah I I can't resist big enough again like one one thought that was mentioned very briefly you know that streaming the output like I'm I'm more on the user side these days interacting with this boss they go and buffer the entire answer before they show it to me a simple thing that just helps human patients is to say it's being typed or streaming it to the user because then you see that something is happening right so he detectifies black Bots here it buffers the entire output before it shows it to me like if you just said hey the bot is typing just let this GPU work hard for you that will already go a long way so don't change the latest you just manage human expectation they will probably go a long way as well Ah that's such a good point the actual UI experience and there are some drawbacks in terms of like safety of the response like if you stream it you can check the response so that could be a concern for some type of applications but I like your second idea of like typing like when you see someone typing in some different types of like a human I don't know different shared applications that helps you see their typing it's a good idea okay yeah I think uh just I'd add real quick yeah I think there's a lot of ux ways to do this to solve this problem um that's one thing and then also just one other addition is these latency amounts of like gbd4 these latency amounts of the models being offered are not constants either um we've like been looking kind of at our high level like user data and the lane season like very spiky of things like tpp4 and like it's very laggy some days and not other days so if you really care about optimizing that's probably another thing to look at yeah yeah oh that's that's they don't support this lace yet so I guess that's we'll be having a latency page to kind of help with that soon so stay tuned on something we talked about earlier I mentioned it's like batch sizing is hugely important both for like the streaming component but also yeah if you think about like when you're actually doing this like greedy token by token decoding if there's any difference in the length of each batch everything basically defaults to the longest batch so like there's some literature out of Washington I think that in machine translation they found there's like 70 of tokens and batches were effectively useless tokens because it was driven by the longest padded sequence and so like especially if you have a high variability in your output doing anything without a small batch size can like be bad because it's like if you have a short response you don't get to enjoy that basically your all your responses are limited by your longest output hmm makes sense do you have do you control for that and your prompt somehow or do you how do you handle that the control of output size I'd say on our side we've done a bunch of optimization there even as you mentioned on like making the gpt3 output more concise which then we kind of can distill from but uh it's really just kind of fiddling around with it like one of these things that we saw was uh with this kind of asymmetric pruning that we did when you move the batch size is larger all these gains went away and that's because like when it was smaller you really could like highly optimize it because you're not waiting on this decoding versus when you have a large batch size you're just basically dominated by the longest sequence so all your compression is completely kind of just broken interesting that makes total sense okay let's talk a little bit also about monitoring I'm very curious uh what you guys found useful both like uh human based LM based modeling any other um type of techniques that you found useful um I can jump in here with like a high level overview and I know I've talked to Mario personally about this I'm sure he has some things to add too but kind of what we've seen our users doing kind of breaks down into three big categories first category is which I think is the real source of Truth is end user giving you a thumbs up thumbs down or you can use behavioral based maybe like clicking refresh or closing out is a thumbs down as a negative signal on like how the prompt is doing that's one category another category is this whole mturk hiring out to click workers to do very boring very boring way to do it in my opinion the third category which I think is really kind of where the future is here is synthetic evaluation so how do you actually use an llm to rate how good it is and the one interesting thing I'll add here which is kind of a thesis where developing uh based on our conversations that prompt layer is kind of via negative is the right way to do this instead of trying to understand which prompt you have how good they are it's better to reverse the problem and say how can I evaluate what's failing in prod which users are getting bad results like is the chatbot being rude is kind of the most trivial example of this but that that's the way we see it yeah to be direct on that that we're calling this critic modeling and it works extremely well especially with the most recent models we found that so like on a seven point scale the gbt4 is off by an average of one label which I used to do a bunch of human evaluation that's pretty much the best you're going to get off like mturk train judges and it works across fluency accuracy other things the one thing here is there's huge positional biases so we found that if you ever try to compare two things the outputs don't work as well so it's best to try to focus on individual comparisons and then potentially using some like weak supervision signal to figure out pairwise things can you give us a concrete example when you mean critic like what is the problems you're giving the critique or how does it work you're writing a thing of hey uh engine your taskers to evaluate how good a summer is for something you're evaluating in a term of accuracy something that is completely accurate has X characteristics as a seven something and it's slightly missing something as a six provide all your description of labels give it to the model the model will respond pretty damn well but I will say I'd like to give you don't try to say hey I have these two summaries which is better because the model will uh there's position bias depending on what you put first your second the model will favor what's left and at the same time the outputs aren't as usual it's kind of like the old Costco thing where you never ask someone why they prefer something because if you ask them to justify it then they will not actually give you their Preference they will give you an answer which they can justify so don't ever ask your model to justify oh yeah okay interesting recency bias is also known in human decision making by the way like if you give a man like a verbal statement of of choices people just forget very similar to what we observe in the network the first option and intend to pick some of that indecision design very interesting behavioral research um okay anyone else on monitoring and performance evaluation yeah yeah so just like I said we've done a bunch of stuff in-house and we've found similar to what Jared mentioned where like the tail latencies can get very out of hand where you can see massive spikes and they can be caused by these weird Corner cases that you never thought of like we were seeing massive tail latencies occasionally and that turns out we were running a model on a sliding window and occasionally hit super long documents and like the solution was like oh what do you do just truncate after I don't know 10 000 tokens but sometimes and the only way that you'd see this is like you look at what these outliers are and in most cases anytime we've seen an outlier the solution is super simple it's just like oh we didn't know about this weird Behavior yeah this is the easy fix we no longer have that I call this terrible uh I have a worst name but I call this terrible experience debugging which I regularly do I wasn't recommender systems so I also monitored like the really bad experiences and they just picked one every week to to Deep dive on the root cause this is very similar to to you saying like monitoring the bad experiences like the edge cases the long latency and then you you find like groups of problems that you can fix whatever one oh very good okay cool I think we're out of time I just find one more thing if you don't mind just so uh putting a plug here come try the optimal profile let's you with a single line of code you can get your model running across different hardware and you see the profile of the latency takes for different options and so on to give a very it's actually running the model and providing it for you so love to hear from you so uh I guess can I just jump out can I can I jump in for like 30 more seconds like I'd be interested to hear from others like problem in chat like how they deal and labeling becomes too expensive as well if you need like gpt4 for labeling then you know it's expensive again um so whether people have like some smart sampling techniques or maybe like pre-filtering initially are there Solutions out there for that it seems so nascent field to me uh that pretty much like all the tooling is just in the process of building so yeah I'd be happy to hear in chat of like what people use for things like that yes snorkel there you go oh that's awesome so we will um continue this conversation in the chat yeah please let us know what everyone is thinking and thank you so much all of you were incredible and these are like these are very poignant questions and I really think that you all educated us a ton on how you're going about it and what kind of best practices you've found thus far wow all right thank you later you all off stage though see you later foreign yeah yeah