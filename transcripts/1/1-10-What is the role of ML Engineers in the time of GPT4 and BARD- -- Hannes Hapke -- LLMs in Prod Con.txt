perfect hi my name is Thomas I'm a machine learning engineer at digits uh we handle books for business owners and accountants in real time then and that involves a lot of machine learning natural language processing and these days and also large language models but we had this big question of like how does the role of machine learning engineering evolves in the time of gpd4 in barn does it commoditize our whole profession and so if you take an example project here for example you would build like an address parser with like deep learning for example to handle some more uh unique cases instead of just using erendix profiler or regex implementation you um you would write something like this you would deploy your model you'd have an internal endpoint and then all of a sudden you would get those results out so that I don't know might take you a week maybe two weeks depending on the complexity of the project and depending on the available availability of the data um you could also go to an API like open AI or in the future Com or uh Claude or other services and oh by the way then you run into those rate limited issues and like their IPA the monthly IPA but the API sometimes it's not like available but once you get over this and you submit your request then you get a decent answer back and so there's zero um there's zero implementation time from the ml side and that is sort of like hmm what does it mean for the projects and then you dig a little deeper and you come across those projects from the initial demo where like somebody scoreboards a description of our website um on a page and then all of a sudden the algorithm or the the API endpoints spaces out some ideas about like how to write the HTML and CSS and it looks very much like what they had anticipated uh in this group link and those moments all of a sudden you're just like wait a second what is going to happen to the entire uh like ecosystem of ml engineering envelops etc etc and so we're not like the little Kevin here like uh running around and screaming but if we step away then what does it mean for us as like machine learning Engineers uh ml Ops projects owners stakeholders Etc is there's a Gutenberg moment and I call this a Gutenberg moment because something interesting happened in the Middle Ages in like 1450 the uh there's a German fellow who is a quote-unquote invented a printing press and until then um books have been transcribed in monasteries um there were monks they were trained on doing this by hand and uh they did this ever with high artistic value but then uh this inventor came around and was like hey we have a printing press we can print those um documents or Bibles or books or whatever they wanted to print much quicker and that sort of like contributed to the entire democratization of access to books and uh had a lot of like in um had a massive impact on literacy across Europe things like this but if you walk away from this or if you if you zoom out from this moment you could say that um we have an machine learning engineer um and we have the modern printing press is basically the API for those models and we're at a moment right now where like as a as a community we need to figure out like where we go with our projects so what this means is like machine learning got democratized uh what I mean with that is like now we have domain experts they don't need to be experts in machine learning they can just basically take their data and run it against the apis and solved quote-unquote some machine learning problems and the interesting effect we also see in the community is like now we have discussions about machine learning and the impact of machine learning in a broader public so do you see articles opinion pieces in the New York Times talking about artificial intelligence machine learning I have close relatives asking me all this out and was like hey can I how can I get a key to open AI how can I use this and those folks struggled before um to even understand what I'm doing on a daily basis for work so those are the good things and then when we look at um is this a Gutenberg moment for us or not like some projects let's be honest some projects got commoditized like um we'll talk a little bit more about this later what this specifically means for us uh in our project and they're also we need more education around the dangers of machine learning and I'm not danger I'm not saying like to scare anybody here like but we need to talk about like what does safety mean what does bias mean like how do we train those models what's actually happening behind the scenes and that goes back to the criticism around open the eye are they really that open or uh what the name says or more like close and I think there may be more on the later hand so we also need to educate folks who use those apis that the predictions are not objective they're still subjective data behind the scenes and there is a bias in the data just because the machine has made the prediction it doesn't mean it's we can classify this as objective so before we talk about like what does it mean for our projects let's summarize like the early lesson we learned when we looked into those apis and so the first results were really impressive like no question right like you see you saw the demo you um realized what's actually going on uh behind the scenes we're like wow TPT was great but then gpd4 is sort of like that's a different level right that was super useful um then you start thinking about like wait a second those brief interactions they can be misleading and as an example like the case I showed you earlier with your dress parsing if you submit the address if you submit the same request multiple times with that prompt I showed you you would get the same values back but for example the keys in this Json structure would change so it's inconsistent um so there's therefore when we have conversations internally about like those apis we just need to be really aware that the first result can be misleading and then there is like convincing hallucinations so we have seen tasks where we try to run them through an API and the model returns something in a decent data structure but it hallucinated in the values themselves so it believes that it extracted something and it was uh convincing but it was utterly incorrect and we've seen this now with discussions about like biographies uh descriptions of people like it's very convincing but um again the whole text was hallucinating and then as I just mentioned the outbreak was so um when we look in this it feels a little bit like little Kevin is like dancing at home and just like not making things up but like pretending to be a little bit bigger um than what he actually is and so that's how it feels like right now with like machine learning projects like we can do a lot of things but boy there's like if one of the strings breaks then all of a sudden air thing stops dancing in the movie so let's talk about like commoditization that was my first impression when we saw on tpd4 that a lot of projects got commoditized and even folks in in the first few hours after the release were like hey I'm doing a PhD in natural language processing it's my entire PhD still worth doing and I think the short answer is yes because there's a lot of problems still not answered but a lot of machine learning projects got commodities so which ones would we consider as being commoditized so any project with public data available is basically commoditized because there's a good chance that it has been soaked up into the um the sort of like the the scrapers of those data sets for gpd4 or maybe potentially gpd5 so if it isn't solved yet um there's a good chance that somebody else will do this in the future um and it gets commoditized projects without any specific environment requirements so let's say um you have a use case you need a you need a machine learning model or Union machine learning solution but the stakeholders don't have any requirements in like this needs to run on device or we have security requirements then it's a good potentially threatened by a public API specific security requirements like the digits we take um the security of the data uh as a Paramount and we do not ship the data to third parties so that's really critical so if we want to use large language models we have to deploy them internally and that is a requirement we couldn't fulfill with openai and therefore we can't use those apis at this point in time and what does it mean for our in-house machine learning so we need to focus on machine learning projects on proprietary data let's say the best example is always like um there's a high likelihood that uh openai won't replace Radiologists because yes they have a lot of data but if you have like a custom trained highly specific domain-specific model you there's a good chance you'll always outperform um the specific generalized apis so we don't need a machine learning model to make a radiology uh assessment that also gives us cocktail recipes and tells us like what to cook next Sunday um any project with very specific security requirements is an in-house project so if you can't ship the data to a third party for whatever reason let's say you want to run your machine learning model on an iot device or you wanted to don't share the data with a third party that makes a good qualification for an in-house project and then we also have seen great instability around the latency and the availability of those third-party apis so if you have low latency requirements that is a really good reason to pick up those projects and do them internally instead of shipping them to a third-party API and then most important and that's a discussion you as machine learning Engineers need to have with your stakeholders is like what is the core intellectual property you want to preserve in your business or in your research project so if we want to classify the sentiment of a text is this important to your entire IP value chain if not an open API or a model API might be a really good use case to get you up and running and started and then you can focus on the actual IP related projects so what does it mean for machine learning we as a machine Learning Community we had objectives in in for all of our projects we did not always achieve them but we had a bunch of goals in the past so we wanted to make our predictions unbiased so there was a strong focus on like how do we handle the data up front to make sure that everything is as unbiased as possible balance training sits um known data sources making sure that we remove some of the biases in the underlying data there was the objective of like adding transparency around the data and the training of the model so you have seen a lot of like projects around uh model cards from Google for example or data cards or things like this to communicate what are the limitations what are the constraints of all of our machine learning models when we run versus 11 models there's this great objective to add feedback loops to our models to improve the model performance we know as machine learning Engineers no model is perfect and so if we see something which is not working well we want to approve uh improve this in the next model generation and therefore we need to capture those like misclassifications and then add them to our training set that was like a key objective for all of our in in-house machine learning projects and then we have this objective of that user privacy there are projects going around like Federated learning or ml privacy or even encrypted machine learning so we wanted to make sure that everything is sort of like uh at the at the highest standard for the use user use cases and then obviously we have undevice developments like sometimes we want to shrink models and make sure that they can run on a cell phone or on the latest iPhone um as you have seen like for example with like stable diffusion and those types of models but those were the sort of like ml Ops objectives for our machine learning projects so when we zoom out again and take a look at apis then basically we're we're missing out on the last four points and the question around like unbiased predictions is sort of like up in the air because we don't know what the models have been trained on there's very little information about like the background and so we as a community we need to be careful with like the objectives we have as machine learning Engineers when we use those apis when we talk about a little bit more about this later but here's one example let's say you get an incorrect response from gpd4 there is no way right now to feed this back into like your own training set you cannot find you in this API potentially that works in my future um but it comes at a high cost and so for some of those problems you might be able to just like use a large language model from an earlier generation let's say a T5 or um maybe a gpdj or something like this which doesn't have so many parameters um and then fine-tune it for your applications you might not need the cocktail recipes being generated when you want to do the address parsing and in that moment you can add back the feedback loops so we talked about what does it mean for the for the machine learning ecosystem but what does it mean for you and me as machine learning engineers so our role has drastically shift shifted from like developing machine learning models or creating the infrastructure of those machine learning systems to being the moderator between stakeholders so you might have a CEO or a CTO who doesn't have the full understanding of like how the complexity Works in machine learning and what the benefits are of like all this um the security aspects and the Privacy aspects and those types of things and machine learning systems so it comes down to you as a machine learning engineer to educate the other stakeholders what are the benefits of like in-house development versus using third-party apis and then you as machine learning Engineers you were the core drivers now when it comes to advising others in organizations around the risks and benefits of those third-party apis so as soon as gpd4 came out we had very long conversations with the security folks on our team um they were they were very much concerned about like sharing data with third-party apis Etc and we unpacked this a little bit of like what does it mean what is being shared um is there maybe a way we could get around this um what are the benefits of doing this in-house Etc so just to conclude where are we going from here as like the as like the community of ml engineers and emerald helps people so prompt design I'm not calling this prompt engineering because right now there's too much guessing in this game it seems like it's more like design a design process it's very much iterative than an Engineering Process where there's like a given path in the structure yes that will be part of machine learning but it won't replace machine learning that is my strong opinion like we will not be out of a job tomorrow because there's like some massive model and we we just sit there and tune a prompt it will be part of it but it not it won't be the full job there are lots of ml Ops challenges around large language models so even deploying a model with like plus billion parameters is not an easy task and we will need as a community we will need more experience and like how do we do this how do we distribute models across multiple instances drawing this across maybe a bunch of gpus for a single model and how do we get latencies down to um to like something we can use in real-time systems um also as a side note like the carbon footprint of those systems is massive and should not be neglected neglected and this is something we need to focus on as a community and then we need the integration of like the integration itself needs like good ml understandings so we need to know like how tokenizers work like how um why certain terms make uh models more sensitive and then other channels and we can we can understand this with a good machine learning understanding which we can provide as a community to other stakeholders who want to use those apis and this also goes further in terms of like bias and safety so instead of just like uh handing over the API keys to somebody else in the organization let's have a conversation with them about like that about what it means in terms of safety in terms of buyers then we as a community have no insights into those models and there could be consequences for the users so what can we focus on for the maybe a short-term future or midterm future focus on projects with proprietary data um if you have um a custom data set in your organization that is gold like nobody will replace you uh no a open API overplace you and your project I focus on subjective machine learning so our machine learning team is heavily focusing on account on the accountant space we advise on accounting questions bookkeeping questions those are very subjective machine learning problems because one classification for one user is very different from the classification for another user there is no Global machine learning model for us we use something like similarity machine learning but you could also think about like recommendation systems as like a subjective machine learning nobody will paste the entire shopping cart history from some start party system into a prompt for gpd4 to make the next recommendation that would be just simply too expensive and therefore subjective machine learning is the key for in-house developments and then avoid plain vanilla plain vanilla projects like getting the sentiment from text that is something we can uh uh this is something we can do probably through the apis we don't need another model to detect cats and dogs so um focus on the non-vanilla project and then focus on projects with specific requirements like as I said like user privacy security low latency and really hone those uh those aspects and become the expert for example in like low latency language models and then be the moderator between the stakeholders be the moderator and advisor and help other people in the organization um to understand the benefits and the the sort of like the the disadvantages of those apis and then drive also the conversation and with that you're basically in a really good spot to live alongside those open apis or model apis to uh to succeed as a machine learning engineer cool thank you