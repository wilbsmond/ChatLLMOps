thank you so much for coming um I'm Miriam I'm one of the co-founds of technomel and what we're building is a specialization platform for NRP models and today what I want to talk about is how we can make NLP deployment much much easier using specialization methods and compression methods so earlier Diego gave us a really great explanation of what all alarms are um they're essentially really really big neural networks that know a lot of things and we can customize those for the things that we we actually care about and these are really driving the NLP development over the last kind of five years have all been down to these really fantastic foundation models like bur T5 GPT and the benefits of these when building NLP applications are really really clear you need a very low data requirement because they're pre-trained on almost every single piece of data on on the internet um the state-of-the-art accuracy informance all comes from these Foundation models and they're really easy to build um into version one applications like Diego told spot earlier however these Foundation models and llms come at a cost they're really really really big which makes them very difficult to deploy and you get slow inference and you need to run them on very expensive Hardware so you get very expensive Cloud costs so they're very expensive to deploy they're slow and all of this is really because of the L they're large these are really really large models and the way that these models work and the reason they have such good natural language understanding is because they've seen so much information and as a side effect they have huge capabilities so for example chat GPT is able to do tasks as broad as you know writing plays and um brainstorming strategy right they're able to do such a huge breadth of tasks however as a business or in an application you actually only need it to do something very very narrow um and but when you have it do this very very narrow thing the way that you currently deploy it probably you also pay for the the effects of it being able to do other things like writing love letters or categorizing uh you know or reviewing contracts when you only need it to to categorize um resumes so what we do with specialization and and this idea of specialization which I don't think is particularly well understood is we take large language models like your butts or your gpts and we take the parts of that model that are relevant for just your task so if this uh blue thing is my whole Foundation model um in that there's only a really really small amount of that that's actually relevant for the task um and this is how you're able to build a much much smaller model which is equally as accurate which is much much easier to deploy now these models are just far better from a ml Ops point of view um it's much cheaper deployment they're much easier um to get really really fast inference from um but also you actually get very good quality models so a lot of the state-of-the-art benchmarks in very specific tasks are held by these specialized models um not the really really big um models that we see coming out of places like open AI so here's a uh a quick illustration of what this process might look like and how this works so here we have a graph of um like your accuracy and your size trade-off and this trade-off is always going to exist now currently your options are what's available open source so you might have your original model which might be a very big but maybe a one billion parameter but and then you'll have open source checkpoints along the way so I think this one we've we've pointed out here is maybe a base or a still bar but what we're able to do with specialization is from the original model specialize and make it much much smaller and give these what we call Specialized models along the top and then you get here this Pareto front of models which are much smaller but don't have this huge accuracy decrease that we see when moving from the open source large models to the open source more resource efficient models and you're able to get a better accuracy latency or size trade-off than you would have been able to previously so I'll uh since we don't have very much time I'll whiz through and talk very quickly about the kinds of results that you can see with specialization so on the left you can see the the graph of latency and model size so when we move from larges all the way to the Titan but the Titan variants which are much much smaller so on the order of like 100x smaller um but then we can compare that with the accuracy that we see from these models and they beat um bird base and distilled but fine-tuned on most of the natural language understanding um tasks so you're able to get models which are you know between 10 and 100 x smaller um while actually sometimes improving the accuracy on on benchmarks which is really impressive and obviously because they're smaller they're much much much easier to deploy so uh this process of specialization is very difficult and the way that it's done currently is individual ml Engineers will do one-off tasks uh one-off projects to specialize their models so they might do a combination of pruning and quantization or graph compilation and neural architecture search um but the issue is this is a very expensive and long experimentation process which quite often fails um so what the Titan ml platform does is it wraps all of these techniques up and into defined pipelines where you can put in your model your fine-tuning data set and the tizen platform will automatically specialize for you and create that model that is 10 to 100 times faster and cheaper and therefore much much much easier to deploy so I'll finish off with a very quick case study of of uh what we did with a early client of ours so what they were building um was in a uh it was the original was an electro type model and they were doing document classifications um and the problem that they were struggling with was they weren't able to get the latency that they required um on sensible Hardware the only way that they get the latency was on using two a100s which for inference is pretty silly um they'd already tried standard things like Onyx runtime and quantization um but what we did is we we took that same original model and put it through our specialization Pipeline and compression Pipeline and ended up with a model which had still very very good accuracy but could be deployed on a single T4 with the same latency and that's a bit of a game changer when it comes to the challenges and getting these models to production and getting them uh running at sensible latency sensible costs so the the tldr is large language models are very very very big but for most use cases they really don't need to be that big when you're dealing with one specific use case um and turning those large language models into much smaller specialized models um means you have much a much easier time getting to deployment and getting the latency and memory constraints that you might have as a business uh thank you so much for the uh mlops guys for organizing this you can reach me my emails down there or LinkedIn me um and let me know if you have any questions in the chat I'll be happy to stick around and answer any later thank you so much Miriam this was awesome really appreciate it thank you so much