hi Daniel hi thanks thanks for the great intro and I'm glad to to be here and I'm so happy to see some well-known faces so thanks for for attending uh yeah I'm really excited and uh because we are going to present some of the latest things we've been doing uh for the agila platform and I know some members of our community are here and I also expect new people to to join as well so it's a great opportunity to to cover the the new things we are doing and and also an interesting topic and I think a useful topic for for everyone so I will share my screen let me okay can you see the screen I hope so yeah okay yes we can okay great so yeah uh as uh as we said like I'm going to talk a little bit about about argila but my goal is to talk more about like General practices and and give like a general overview of of this topic of a data collection for for llms and and for uh LL fine tuning and rlhf uh I want to mention that the argila is useful for any other NLP task and that's what we've been doing for the past two years and and before that we've been working also on a lot of NLP projects so yeah you can use algila for LMS but you can use and get a lot of value from argila also for predictive NLP models so yeah I invite you to go to the GitHub page and then you will see there like how to get started it should be really easy we have an integration with hanging face spaces that will mean that you can have like a quick start a space to play with argila and then if you want to experiment a little bit more we provide Docker installations and other options so yeah most of the things I've been discussing here are introduced like a few days ago so they are pretty new and it's what we call our Gila feedback or Gila for llms if you want to know uh why we build this and what we plan to to do in the next in the next months I invite you to to read the blog post because there we kind of uh position where we want to go and and why argilize different to to other things so in this talk I will be covering a several topics so the first one is I try to just like Define what's what we understand by human feedback in the context of language models and then why we need it and then I just briefly discussed some of the components of the llm life cycle and then I will also mention an important topic which is llm evaluation and then I go I go through some of the stages of the llm life cycle like from collection supervised fine tuning and so on and by the very end I will do a demo and I will also announce a surprise a special price for this conference so I invite you to to stay until until the end to to the miss it so what's human feedback uh I have this definition from from a paper but it was only just an excuse to to point at this paper that I found really really uh interesting and and with a lot of insights and it's very recent so they cover all the different usage of human feedback to improve generative models and they it's a survey so you can get a lot of insights if you if you plan to to to use human human feedback to improve your your pipelines so basically in this definition uh it's all very complex but what what he's saying is that you you have an input you can have one or more outputs and then you have some feedback and this feedback can be of different nature and and can come from different places so it's all kind of like complex to just explain that there's a human like providing feedback about about specific outputs in argila we believe that feedback is not only about outputs but it can be also about inputs and it can be about many other things in the pipeline and in the data so for example you could use argila to to score prompts for example that is not really an output it's an input but it can be very useful to to ensure the needed quality of of your training methods as we will see later so yeah this is a the explanation of the same formula by a chance the eli5 of that formula and basically yeah it's talking about building a tower y from oax and then the feedback is the advice from Mommy and Daddy and their goal is of course to build better better Towers so for models is essentially the same we want to gather feedback to to make these models these models much more robust and much better than done done without this this feedback so going a little bit more into the specific details of what's human feedback I I built this uh human feedback UI in our Gila basically argila provides a way to define your own custom uis for for providing feedback and with multi aspect and custom feedback we mean that you can Define any set of questions so on the on the right side we see different questions one of them is a rating to rate the prompt the other one is a ranking to decide or to give your preference over one of the responses and then you can also ask for natural language to your users or to your colleagues if you are doing evaluation so in this case we what we see is a prompt asking about the python question and then we we have two different responses from maybe the the same model we don't know but they are there are two responses and our goal as humans is to provide feedback about the full data point as I mentioned it's not only about the outputs it can be also about the quality of the inputs which is kind of essential for some of the stages that we will see later so basically this is how you build this UI that I that I just show so the idea is that you build that data set in our Gila through the python SDK and you define a set of questions and then you start feeding data points there and then you can have like multiple annotators to actually go into the data set and provide feedback and then you can read these data data back to just train models evaluate models or do something else with the with the data so yeah what we see there is the definition of the field so which Fields will be shown to the user and also which questions will be asked an important part of this process is the guidelines so The annotation guidelines are really important and we will see some examples on when not using and not defining well this these guidelines can can cause problems so yeah just to summarize a little bit more about feedback so we have inputs so for example the inputs should I add to refer to my paella and the output is nothing but the feedback is actually a completion so this will be one of the first stages of the llm fine tuning process that is called supervised fine tuning or instruction tuning and basically we are going to ask our experts or users to provide kind of like the model response so in this case we provide them with an input a prompt and then the user will come up with this feedback which is absolutely chorizo it's a popular ingredient and that's of the type of natural language feedback but then you can have the same input with an output and you can ask users to rate the quality of the of the response so in this case the user is saying like it's not so good if we assume that it's from zero to one with zero being bad and one being really good and then another type of human feedback can be a ranking which is very important for the last part of the talk which is about like human preference and in this case we are giving the users one input and several outputs and we are asking the user to just rank by preference and preference is a really like wide definition but I will be trying to explain the type of things you try to to communicate to to your labelers or to your experts in order to Define what's preferred and of course that depends on the use case but you can have like a set of like General guidance so another type of feedback and this is more related to traditional NLP it's categorical or binary so for example you can show the inputs the outputs and you can ask the the the users to say if it's harmful content if it's positive negative and so on so all these kind of things can be easily defined with our Gila and you can in fact define your own set of questions and guidelines for for for actually Gathering human feedback why do we need human feedback so this is a very famous representation of the process of going from large language models training on like internet scale data to something that is more acceptable supervised fine tuning and then rlhf which is kind of like uh yeah kind of controlling a huge Beast that has read like really harmful and really violent content and actually to steer this uh huge monster into something that is acceptable by by humans so yeah as another mentioned to the paper I I pointed at at the beginning uh yeah joining llms on internet scale data can generate toxic inaccurate and helpful content and automatic evaluation metrics often fail to identify these behaviors so basically as models become more capable we are going to need more and more human feedback probably it's going to be less quantity but more quality and that's something I will discuss at the end as well so yeah this is a famous figure as well showing the effect of adding human feedback in different stages so going from this internet scale llm to something that is prompted so a prompt including some guidelines on how to actually behave or follow an instruction and then when you really fine tune the model to just follow instructions and do the things you you are asking and then the last step the instruction is one that has gone through this rlhf process that I will describe at the at the beginning at the end sorry so yeah how we Define preference or how we Define when a model is aligned with human values or with human preference so the anthropic people came up with this acronym HHS a3h or I don't know how to how to pronounce it but basically they kind of divided this into three different uh different factors or properties and you can read the paper and they they describe more in detail that I will do here but basically the first eight stands for helpful so basically a model is helpful if it resolves queries six necessary details and response empathetically and provides the suggestions so the way to measure this is by human preference ranking so the example that we saw at the beginning about having two outputs and asking the human to just rank the two outputs also you can measure this by preference models course which is like the model that you train with the rankings as we will as we will see later and then something that is becoming popular as well uh in the in the domain of llms is this ELO scores which is basically a way to to compare different models and to to actually compute uh the the probability of winning uh for for for a comparison so the next eight is harmless so a model is harmless if it's a if it avoids offensive behavior and all these things that we know this is probably the best defined thing and it's not like helpful it's difficult sometimes to to to Define but toxic or sexual or violent is much more easy for us so basically the way to measure this is through binary questions and we will see in some examples that basically you are going to ask also labelers and users to to provide this kind of binary inputs to to your feedback then there are a lot of bias and toxicity toxicity benchmarks and also some Works have been doing preference modeling for harmless harmlessness and basically here they are using a ranking but the ranking is to compare uh across this Dimension so rather than the helpfulness Dimension the preferred output will be the less harmless and notably this is done by the anthropic work and then the last age which is kind of ill-defined or not very well defined and has been dropped after this paper as far as I I see and this is also discussed in the paper I mentioned about human feedback is the honest part so basically the model is honest if it's it provides accurate information so there we are talking about accuracy and truthfulness but also expresses uncertainty so one of the things that is interesting about llms is that an rlhf is that we don't know what the model knows this internet scale llm might know a lot of things but we actually don't know if it's like kind of making up the information or or it already contains this this information so basically what the open AI people did is okay we are not going to talk about honest honesty we are going to talk about truthfulness and then you can come up with ways to actually measure the accuracy uh of the of the responses and also uh to to to measure if the model is actually making up some information so the most notable Benchmark for this is the truthful Qi data set that you will see also in the llm leaderboard by the hacking face people and you can see like the comparison across the open source models for this data so now going into the llm life cycle so here is specifically I'm talking about like kind of fine tuning for human preference and Alignment but of course in the llm life cycle there are many other things like evaluation that I will discuss and there are many other issues to take into account such as deployment inference and so on but here I'm specifically talking about the life cycle of the data and the models and the data you need for for improving them throughout this process so basically this is this figure is inspired by the one from tibhuyan that also is inspired by the instrument GPD paper and basically we see the different stages the pre-training this is the internet scale llm then the supervised fine tuning that is trained with something called demonstrations that we will discuss later but you can see an example on top and for sft models you have many examples many open sources examples as well like alpaca dolly vikuna the the new Falcon models the Falcon instruct model and so on so yeah basically you can start using this model because it's supposed to to be following instructions but you can improve it further by the Second Step which is the rlhf estate that is composed of two steps one is preference modeling or reward modeling and there you are going to collect these rankings or these comparisons that we discussed so for example I have a prompt two responses and the the user will tell me which one is preferred and which one is less preferred I'm not going to discuss much about the the modeling part but I will point at some good Frameworks and libraries to to to check to check this out and for the final part is the RL process and basically for that you might need to collect more uh data and that data it will essentially be prompts because in this final process you are going to use the model to just generate a one or two or or more responses and then you will use the reward modeling that you train on the previous stage to actually evaluate and score that and and the RL process will will not use the human preferences anymore it will use the preferences of the of the reward model um this is a a rather static view of of training and other Gila Widow like this we are more into like iterative and dynamic uh life cycles within since the very beginning focusing on this on enabling data teams to actually iterate on data models and we think about the same for for llms so I find this this diagram from from the anthropic paper very interesting because it's probably one of the only works that discuss this kind of like as they call it iterative online rlhf but basically what they do is like to do snapshots every week and use a UI which is a chat based UI to collect these conversations with new generations of models and they keep on improving the quality of the data every week so I find this process much more aligned with what we are expecting to see in the coming in the coming months and years for for custom llms so I wanted to share also this this figure so yeah just a few words about evaluation uh probably the youngest people here won't know this film but it was very famous in the 80s and it's about a robot that has like incredible capabilities and I when I'm looking at certain papers and certain Works discussing llm evaluation I I always think about this idea of a robot like reading input input input and probably uh not understanding much so adagila the all the argila feedback work has been inspired by several uh Community efforts that we've been doing and one of them is this alpaca data sets we've been engaging with different uh communities in different languages and we've been analyzing the the quality of the alpaca data set that for those of you that don't know it is a synthetically generated a data set of instructions and basically we've been before releasing arcilla feedback we've been using the previous version of our Gila to to actually help people out for for the cleanup efforts and here we are seeing an example of a kind of hallucination of of an instruction and remember you are going to fine-tune the the instruction following model with this type of data so the model will think that actually hallucinating is fine and kind of receiving something like attached painting it's fine and it can provide you with a response that is completely made up so during this effort what we did is to actually use our Gila for curating the data set and with all this feedback we actually train a model that has been used for different languages and has been used also in this alpaca clean effort and basically it's a set feed model that can given an instruction an output and an input it can tell you whether it's a high is likely uh uh about instruction so um yeah this is this is the type of work that we will expect to be doing much more efficiently with the new idea of feedback because you can Define different kind of dimensions for for for feedback so you can train also specific models to detect certain uh certain attributes a another comment about the evaluation is this paper that I found very interesting and very useful and needed uh that came up a few a few weeks ago and it's about the the promise of imitating profitary llms and this is specifically about vikuna alpaca and all these models that try to reuse outputs from from from GPT gpt4 and so on to to to just kind of like imitate the their outputs I know about the Orca work I cannot comment on that because I didn't read in fully in the paper but my first thought is that the process that they did the Microsoft people is like really expensive uh in the sense of like a some works I will discuss later will require only a thousand or two thousand examples to just get the model to follow instructions so I don't see a lot of value of like at least for open source to generate like millions of instructions from from gtt4 so basically uh this work uh kind of showed that humans and crowd workers so the people like evaluating or scoring this this the outputs of these models uh actually tend to to to be fooled by by the style and the tone so what we see in the screen is such a gbd response that is almost correct or accurate so the truthful truthful Dimension and then we see an imitation model that seems to be like really um with good style and authoritative but basically it contains a lot of like false uh false information so in this paper what they showed is that the crowd workers tend to like score highly this this imitation models but when you actually test these models in for kind of language tasks and NLP tasks you will see that they actually don't perform even like better than the non-instruction models so for example if you fine-tune llama with alpaca uh llama the base llama will still be better at this task so I found this really interesting because we've seen other work like vikuna claiming like 90 percent uh um closeness to to charge GPT and I think like uh in this paper they show that there's still a huge gap and that there are not shortcuts to to actually make these models like comparable to those that don't don't get to that don't that have been going through through a process of human feedback so another thing related to evaluation that I wanted to mention is this post from Deep learning AI that got really popular and is talking about like okay in traditional ml you need to get level data you need weeks or months to do that then you develop a model and then you deploy in production and what we are seeing now and I agree is that you can get going with the prompt model so you can just configure it and you can test it and then you can start deploying it and they describe like this sequence of actions so you deploy your live data and then maybe you use it on Shadow mode so you don't use it like for like getting like the real answers to end users but you use it just to to to collect to collect responses but what they don't discuss is this last part and it's about the model performance so they say if model performance is acceptable then let the model make a real decision years but my question is how you do that and there are not many good responses to that because either you collect ground truth data or you have another llm to score the the responses or you ask your users if they find it correct but of course you need a rigorous process to actually evaluate and decide if the model is performing well or not so what we propose at argila and we've been doing work on on that as well is that you can start with a prompt model say a long chain application and then you can use the official argila callback to monitor the interactions with the with the chain and then you can build data sets on top of that so what you can do is to use this from base model and continuously and frequently kind of ask your users or ask your colleagues or experts in the domain if the model is behaving well so you can compute a kind of evaluation metrics from from this production and so I invite you to to test it is still Alpha we we are going to keep it to keep it uh uh improved to keep improving it over time but there is usable right now and the idea is that you can define a callback so every kind of interaction of the llm will be stored on an argila data set and this agila data set can be configured for rating the responses so just provide like the correct response so this is highly valuable because you have a problem based model but you are at the same time and selling the the the the the way to actually maybe fine tune it or at least get kind of like a real evaluation metrics beyond the model is performing wealth so maybe we can stop uh we can do a small pause if someone wants to ask some questions and then I can continue or shall I continue hi okay okay so yeah now this part is about the stages of actually the life cycle that we saw at the beginning so how to get from a base llm way through uh instruction following and then an rlhf uh model basically I will be discussing the different data collection processes that you need the first one is from collection and the type of feedback at this stage is basically there's no input there might be some input guiding the user to please write a prompt about this topic but necessarily you don't you don't need a lot of input it will be only about like asking the user to come up with a with a prompt so in this case should I add to listen to my paella and the type of feedback is not relevant so just to situate ourselves we are at this stage so for supervised line tuning we need Proms and I will discuss ways to get them but one way is to ask users to just write them down and this is what the structed work did asking labelers and crowd workers to to come up with with thousands of from and so why do we need from collection so we need to collect data to fine-tune and align llms with human values preference and and domain and for that we need a prompts and as I said there are different options so the first one is the most obvious you can use an existing resource or database and in fact open AI combine asking users to write a new prompts but also leverage a lot of data that they had from their previous apis and I don't know exactly the the fraction of of how many labelers were Pro were generating problems versus the the API distribution but they they at least say that they use both you can also leverage user queries to your service so imagine you want to fine-tune a model for I don't know customer service for for your uh for your product or whatever and probably you half even a non-ml service for people to ask questions you could leverage that and as I said before here it's important to actually measure the quality of this inputs and the and the diversity and you could leverage this database and ask users to just trade or qualify this this queries in order to have like a high quality data set so the other option and is the one that is described in the instruction paper is to ask experts to write prompts as I will mention later this was done by Dolly as well the databricks people ask employees to come up with prompts but they also ask them to write the response and I will discuss that later because that can come with with issues so here it's important to set up the guidelines and the topics so for example in argila you can say Okay I want uh I don't know 2000 Proms and I want this distribution of topics what you could do is to create a data set with an input that is specifically telling the user to write about this topic so you can say Okay I want 20 of data points of this topic and 10 of this other and the users will go there and they will not need to think about the the topic they will be given a topic and they just need to to write the prompt so there are ways to actually control the the distribution and the diversity of this this data set the risk with the collecting problems this way is that maybe there's a disconnection with the use case and you can ask users to write like about different topics but maybe this is not the real questions you will get for your model so for example the customer service one they can come up with fake questions but at the end of the day the the user will ask other questions so maybe your model is not fine-tuned to follow or to respond to this real user queries so if you have a existing resource it's good to leverage that one and probably combine it with with this other approach so that's this question about okay if I'm asking my experts or my laborers to to actually write prompts why not ask them to write a response but uh because at the end of the day what you are going to need for the supervised fine tuning is the prompt and the response so why not do this at the same time uh well this this has some limitations uh it might work for some use cases but coupling both of them can actually produce less quality responses because imagine like if the same user is writing a response and it's not very clear but for him or her is really clear uh she will come up with a with a response but probably in even the first question or the problem was not correct so maybe the data quality of that data point is is not so good but if you separate those uh to kind of like data collection processes you can ask users to write prompts and then other uses or maybe the same users to write the responses at that point they can say okay this I don't understand this instruction or I don't understand this prompt so I will just discard it and in this way you are not collecting uh kind of like bad quality bad quality pairs of front-end response understand specific case of this and is the dolly data set from from databricks within also analyzing the this data set and we've been also engaging with the community to to to improve it I have to say that it's really good and I I really admire the the effort of of data breaks and and the and the employees but nevertheless it has some issues uh and one of them as I said is the problem is that some employees or labelers they didn't understand fully the task or or how the fields were used and and for example in this example we see this is supposed to be an information structure task where the instruction is who is Thomas Jefferson and then the context is really short and in information instruction basically you are going to need to extract like the specific information but actually in this case the the user the labeler just just copy pasted Wikipedia and basically yeah you you are kind of trying to train a model to do information extraction but the uh the example that you're giving is not so not so good and you might ask okay maybe this is just a couple examples but uh we've been doing kind of a community campaign and and we've identified and fixed more than 400 and for some tasks such as information instructions summarization and so on this accounts for more than 10 percent of the examples so we believe that improving this data set can lead to to better quality data and as I said this is also a good uh Insight on how to actually collect prompts and responses uh so yeah if you are interested in more details we actually have the data set available for for everyone we also provide translations to other languages and this is an ongoing curation effort so yeah if you are interested in in helping out a contact me so yeah this I discussed already of course it can be much more costly to to do separate uh but as we will see later maybe you only need a thousand examples and not ten thousand like the dolly or this activity paper because there are some Works pointing at that you need like higher quality but less quantity so even with 10 000 with 1000 examples you can get a good instruction following model so another way to do this is from base ml so you can use prompts from monitoring an llm as we said before if you are continuously monitoring this this model then this is a perfect source of real questions from users and then in this case you can even use the responses if they are good so in Aguila you can set this data set listening to a line chain app and then you can say okay those prompts are good the response is also good so this is probably a good ground truth example to evaluate and potentially fine tune your your moment so once we have prompts the next step is supervised fun tuning which is basically trying to uh to turn them llm into a helpful model so with ability to follow instructions and to answer questions and to and to be helpful for for the user so we are there so in this case the feedback will be the completion so I will as a labeler I will see a a prompt and then I will write down a completion so the type of feedback here is natural language again but here we are providing already the input which is the instruction or the prompt and yeah this is uh also mentioned in the literatura supervised fun tuning behavioral cloning instruction tuning and yeah lately it's more like an instruction following models the terminology used for for this so I wanted to show you an example of what we want to achieve here so this is a real example of the Falcon based model so this kind of Internet scale llm and we are asking it to write a follow-up email and we are adding a prompt to something to the prompt to to actually just Notch the model into kind of trying to respond and generate the the response that we expect but even with this is not so it's not so helpful so this is the same model but fine tune with instruction following instruction following data so basically they used an open data set that is contains this completion so the instruction and then the the response and we can see that this is much better so we don't need to add anything to the to the prompt we just say what we want and the model uh try to to answer in a helpful way of course this is not perfect but this is something that can be achieved with uh with this collect comparing with this um completion data sorry so yeah basically this is how you could set up the sft phase the supervised fine tuning phase for collecting these completions or these demonstrations so basically you will say okay I have a prompt and I want to ask the user to write a harmless and helpful response so the user will be asked to to just write it write it down and then you just push the data set and it's available in the UI for for the users and this is what they will see so for example this is a very famous prompt and it's explained the moon landing to a six-year-old in a few sentences and then the labeler will actually use the UI to to provide a response okay so yeah about size and quality so the instruct GPT paper it was 30k coming from the API and coming from laborers and the type of data set is private and then the dolly data set kind of like follow the same guidelines and they collected 15K and in this case was employees but the difference is that they were writing both the prompt and the response but the data set is open and you also have the curated version from from argila in the link I I just showed and the other work that I find really interesting is Lima less is more for alignment and there they focus exclusively on curating a high quality instruction data set and it's only 1K and they show that this gets good results at least on style I don't think they will get a lot of good results in kind of like human preference or all alignment but at this uh the style of actually following the instructions is really powerful so this might show that you don't need a 10 10 000 and you might just need a 2000 or 1000 to to at least have like a a first version of your instruction following model so the last stage is once you have this instruction following model you want to actually model and align this this uh this model for for actually providing more helpful less harmless responses and the way to do this at least for now is preference modeling and basically here what we are going to ask our users is to say which outputs they prefer as we said we are here and in this case we will get several responses and the the task is about ranking and it's about providing a ranking of this uh of these responses and this is called comparison data or preference data as well and the goal here is to train a reward model that I will show how it works at least from from a high level perspective so basically the type of feedback we are getting here is rankings uh and I wanted to mention here that you can ask experts to rank more than two outputs and this is done by the instruct GPT paper but not from for not by the anthropic people and the way they do it is that they imagine they provide seven outputs and they ask the user to rank them all but for training they will binarize this uh this ranking and they will generate pairs of ranked responses because basically the data that you will use for training the preference models is similar to this one you have an input you have a chosen response and you have a rejected response and you will teach the preference model of the reward model to give better and higher scores to chosen uh chosen responses so basically in this example we are seeing that for this question the model is preferring much much more the chosen one which is written by a human by the way and the rejected one is written by a an open source model if you are interested in this and the training process we publish this open source model and we also have a tutorial on the agila docs that can help you to to to to to get started with reward modeling using the the TRL Library and yeah I think I discussed most of it and maybe the the last mention is that preference models are useful for rlhf uh there's a new model called a direct preference of the optimization that don't need RL so this preference model modeling is done on without RN and this is a highly interesting for us we haven't tested yet but beyond preference modeling and this is also used for other things that are important in the in the llm lifecycle so one of them is model selection uh because yeah selecting the best instruction following model is not easy because they tend to overfit so basically some some Works what they do is disregard the the the evaluation metrics and just use the preference model to just select the best snapshot so basically from the evaluation metrics you can see that it's overfitting but the model preference the preference model is saying that even if it's overfitting on the on those metrics this this is the most useful so this is used for model selection but it's also used for evaluation so you can take the responses of a model and measure whether they are useful or not and this is how you set up the UI for for for this stage so basically you have three text Fields you could have more if you have more responses as we said and in this case I'm using a rating question but you can also use a categorical or as we call it label question and soon you will be able to use a ranking question where you can actually drag and drop the the records and this is the UI the after setting it up and filling it with with some examples coming from from Dolly and from Falcon so basically we have a new cell instruction and we have two responses one coming from the dolly data set and the other one coming from uh regenerated response from Falcon and this is really simple we are just asking the user to say okay I prefer a response once or I prefer response to as I said yeah if you are interested in this topic within uh closing looking and collaborating with the TRL people at having face and we actually published last week and into an example for doing real world modeling using a are Gila data set and using the new reward model trainer from from tln so the key takeaways because I want to have at least some minutes to introduce the special price is we believe that human feedback and likely aided aided by Machine feedback is key to deploy a aligned and robust llm Solutions we believe that domain experts will become more and more relevant because it's getting increasingly difficult to provide feedback to these models and you need privacy and you need all sort of things so we believe that a high quality feedback from the main experts within your organization is gonna be very important a collecting feedback is not as expensive as in my scene especially if you start thinking about it since the very beginning so since you are starting to experiment with llms you are starting also to Define your own human feedback collection processes we believe that this is not going to to be very expensive especially if you have a domain experts and you have your data team collaborating and cooperating in in this in this process so yeah I wanted to introduce this llm crowd eval price and basically it's an open experiment we are going to learn from this process and we would love you to participate and it's an open experiment to understand llms open source llms and open source data sets and I think it might be a good exercise for you as well to just look at some of these data sets and outputs so to understand how how they work how they think or how they produce responses and also it's an opportunity to win an amazing book so the idea is that we have set up an instance uh alveolar space on the having phase Hub with 500 users is completely deployed there it's completely open source and the idea is that you just need to go to this hanging face space that I will go afterwards login into the regular space with your user and then read the guidelines and then just start ranking and optionally providing feedback so the deadline for this is next Monday at 10 A.M Pacific time and the way to participate is to post a antagila on Twitter or LinkedIn with your username you will see that the username is auto generated so we need to uh you're claiming that you are that user and that one what we will do is to analyze the the different parties participations the different contributions and we will choose the most prolific but also helpful and truthful contributor and the evaluation will not be based only on the number of data points but also on quality we have introduced several control data points I didn't discuss this but this is highly important for ensuring the data quality so all of you participating will have some records in common and we will use those to measure uh kind of like if you are just not following to the next screen and just labeling randomly and the results and the winner will be featured next week on our social media and the book that we are giving away is called the human human in the loop machine learning and it's a really good book to to understand all these human in the loop processes and understand Active Learning and so on so I really invite you to to participate and I will just finish with going through the steps so basically this will retrieve one of the uses from this 500 users on this instance and if I run it I will request this one I will not use it because probably and hopefully some of you will will use it but I'm already logged in so what I do is to go to this link you need to copy and paste the the password don't worry because this password is not disclosing any data from you or anything it's just to login into this instance that contains basically open data so basically what you need to do is to go here okay so this was my user and this is the user that you need to tell us that you've been using and if you go to the data set if you want to see like the full screen you can go to the embed this space and you will see the full the full screen I think is is better so yeah you will be given an instruction to responses and you need to use this scale to say okay response a is much better so if you press enter you will actually submit this record and you can see your progress here and that's it I think I will uh leave some minutes for questions uh and if you are interested in participating please let me know I think you will have the presentation available as well yeah right um thank you so much uh uh Daniel for for sharing that I think we have a few questions but David has been gracious enough to sort of answer most of them um the outstanding question here um came from atakan and he mentioned how does uh our have um help with uh situations that require World Knowledge you know sarcasm or second order logic computations like no uh ambiguities and stuff that's a very good question I I don't think I have an answer I basically I'm not a narrative researcher or anything but from what I understand it will not help much with World Knowledge and in fact in the instructivity paper you can see that uh this relative model gets a bit better on on truthfulness which is like factuality and all these kind of things but it's not getting like much better and because of the processes like human preference uh and probably the user is going to prefer one response over the other maybe both are inaccurate or they are not encoding the World Knowledge good so probably you are going to to just use the same kind of reward for for the rla check process so I don't think like rlhf is the is the best way to to to solve this problem I think like using external knowledge bases and other things is much much more promising but that's my yeah my first thought on that you know thank you for sharing that and uh the next question um is from uh dhruv and he mentioned um how stable is there are lhf in general would you recommend it in high sticks situations again pretty new and high drove I I'm really happy to see you here I think it in itself like the rlhf process is really unstable like even for getting like the model to to be trained properly is very unstable and for high stakes I think like at least for controlling uh like toxicity and all those kind of things I think the the the open Ai and the anthropic people they've done a great job uh using RL hf2 to detoxify models and so on and that's why I think they they they've had this huge success but for high stakes like really high stakes I I don't know I I think I would provide a lot of guard rails and other things that go beyond uh Beyond just like fine-tuning or or a relative I think it's a much more complex system yeah and and trust that I think Tom also shared a perspective in the chat as well so um I think that's uh that's a good perspective and then this question from Rahul and he asked are there any standard data sets formats for exports in the training data I believe this is that has to do with uh the Aguilar platform again can can you are there any standard data set formats for exports in the training data in Aguila yes so basically uh I think daily this is around from from the team he's been working on on this kind of once you have gathered feedback from an agila data set how to transform that into something that can be used for for training and he's been doing a lot of work on kind of aligning this with uh with other libraries so Transformers TRL and so on and we'll be doing much more work on that but basically what we show in the tutorial if you want to go to the TRL tutorial you can see that you just need a couple of lines of code to actually transform the argila data set into something that is usable by by a reward model it's just like some Transformations that you need to do yeah and we're a few seconds over but I think we can take just one final question and this question is from shave and um she asks how often do you update the model with user feedback Incorporated Within that's that's also a good question and I think like most of the things I I've been discussing now like for more Dynamic environments we are not there yet like as a community so I think we will get there but for now everything is pretty static static sorry so yeah you train a supervised fine-tuned model and then you just start using it maybe for production or whatever and maybe you do a relative but you don't actually update the model but I could say like the anthropic paper that I mentioned at the beginning they did a great job showing that you can do like kind of weekly updates to to the model using these LHS process but to be honest we haven't explored yet this kind of like more iterative ways of of building uh or fine-tuning llms we've been doing this for predictive models for for a long time already but I imagine that this can become more challenging but this is something we will definitely look at right thank you so much Daniel for sharing um your perspective and uh for also leading Us in the workshop today so again Daniel is the CEO and co-founder of Aguila and uh if you want to learn more about agita you can just go to the website at aguila.io um also sharing the link there as well and uh also if you're participation in the program face space as well to ensure to follow the instructions that uh see you in the other workshops for now bye for now thanks everyone thank you