So speaking of being at the Forefront of this llm movement I've got Shreya here let's see if I can grab her and bring her onto the stage yes what's up Shreya hey dimitrios nice to see you again yeah you too so we just confirmed right we're gonna see each other in San Francisco when I'm there in two weeks yeah yeah that's right yeah so I'm gonna drop by your office maybe even with Diego and and uh we can record one of these podcasts that I'm talking about yeah I think that sounds amazing uh we'll have to get an office in time for this okay that sounds amazing living in the hybrid world yeah so no I'll go buy it but I'll let you go uh because as I said before you can call me Ringo today I am keeping time and we're already four minutes over so I'm just gonna give it over to you and hear what you got to say awesome thank you so I'm sharing my screen and that's up here perfect so I'm gonna get started uh hey everyone uh my name is Shreya I am the founder of guardrails AI a company focused on building uh AI safety and reliability for a large language model applications uh and today I am going to be talking about how to build practical guardrails uh if you're working in an industry or if you're putting nlms into production um so with that let's get started so why do we need guard rails off the bat um LMS are awesome but they are brittle and hard to control uh and as we work with them I am guessing a lot of people in the audience have worked with them as well um it's typically um a number of issues pop up in practice so for example the llm application works while prototyping but the moment you run it into production it ends up being flaky um there's you know issues with hallucinations and falsehoods often issues with lack of correct output structure Etc uh interestingly the only tool available to developers is the prom so if you want the llm to behave a certain way typically people would just print into the prom do not respond with this word or always respond in this manner Etc which just seems you know prehistoric in some ways and insufficient um so this all of these issues combined mean that any time you want to deploy llms in an application where correctness is critical you know that becomes really hard and we've kind of seen that in the in the applications that we see in practice so how what are the tools available to us to control llms uh to start out with so first is controlling the llms with a prompt but we talked about why this is insufficient uh primarily because LMS are stochastic and what that means is even for the same input you might not see the same output repeatedly so the prompt you know just does not guarantee correctness um the second way of controlling llms is via the model um so but often it ends up being very expensive or time consuming to train or fine-tune a model on custom data and if you're using the llm you know behind an API uh there's you know no control over model version Etc and the llm provider might change you know without without notifying you um so the guardrails approach to controlling LMS and offering guarantees is combining an llm with output verification um so what that means in practice and I'm going to dig deeper into this in a second is that there are application specific checks and verification programs so these are small programs that you can run on the outputs of the lens that ensure that the llm output is correct within the context of your application so that was a mouthful we're going to dig deeper into what that means in practice right over here so this is the standard way of llm development wherein you know there's some application logic that that lives uh you know somewhere and within that application logic is the llm unit which essentially like takes the prompt uh sends that prompt over to an llm API uh generates the output and then forwards it out back to the application um the alternative way of doing llm development that we believe in is that after you get the raw output from an llm you pass it to a verification suite and this Suite contains you know these small independent programs that we talked about earlier that encapsulate what correctness means you know for your application so this might be you know totally um like a totally varied set of criteria so for example like not containing personally identifying information or not containing profanity might be two separate checks you know if you're building like a commercial application then making sure that there's no mention of competitors uh may be important to you if you're working in code generation then making sure uh that the code that is generated by an llm is executable if you're generating um summaries making sure that the summaries are correct and faithful to the source text all of that ends up becoming important so we get this llm output and we pass it through a suite of verifications uh if if all of the verification tests like pass then you know we continue on with that output and like send it back to the application logic but if verification fails then there's you know a number of different ways to handle it and then one of the ways to handle it essentially is to par is to you know construct a new prompt so this prompt contains context about what went wrong uh and then re-asking the large language model to you know like uh correct itself and you know like try to produce something that is more aligned with the uh with the applications um uh context um so this is where God rails AI uh comes in so Gabriel's AI is a fully open source library that offers a bunch of functionality uh so for starters it is a framework for creating any custom validators or any of these tests that we talked about lay there's you know like uh nice hooks in the framework to you know like make creating that very easy um it is the orchestration of the prompting to verification to re-prompting logic that we saw earlier um additionally it is a library of many commonly used validators across multiple use cases implemented for you and it contains a specification language for how to talk to and communicate with large language models um so um I'm going to go through this example quickly uh and make sure I can squeeze it within the time slot that I have uh but here we're going to go through this example of getting correct SQL so the problem is we're building an application that takes natural language language questions over your data and generates you know SQL queries that represent those natural language questions um and for this specific application our databases you know like Department management that contains information about employees Etc um so this is the same uh you know guard rails development workflow from earlier and then within this workflow our verification logic has now changed where it contains these three tests so the first is that the SQL query is executable in the code base um so what that means is you know like once we generate the SQL we you know like pass it through our code through our database to make sure that the SQL query can actually work um the second verification logic is that there are no private tables that I don't want to expose to my end user like none of those private tables are you know part of the query um and then the third one is that there are no risky predicates so even if my end user says like hey delete these like drop these tables etc those uh those tables won't be dropped or or those queries you know won't be executed um so let's say that this is the quest uh like uh this is the question that we end up receiving from the user which department has the most employees um so guardrails you know automatically um like guardrail's text to SQL package has a prompt that is automatically constructed for this natural language query which will pass in all information about the schema of the table um other information that may be relevant for answering this query uh and sends the prompt over to the large language model uh let's say for this example that this is the output that we end up getting from the llm which is you know select name from departments uh and here let's say that the Department's table doesn't actually exist in our schema um so when we run this output through verification we find that two of our verification tests pass uh you know no private tables uh and or no risky predicates exist in the query that is generated but because the Department's table doesn't exist in the database uh this SQL code is not actually executable um so we now enter this like validation failure and reconstructing prompt part of the guardrails workflow where a Recon re-asking prompt is automatically constructed by guardrails with all relevant context about you know why this particular generated queries incorrect so all of this is automatically done and then sent over to the LM and we end up getting this like corrected uh response with the correct table name um we pass this through verification again so we're back in like this part of the this part of the logic and then all verification tests passed and so we end up passing this output back to our end user uh so this was obviously a very very simple and contrived example but it goes through the workflow of what building an LM application with guardrails AI looks like um so very briefly I'm just going to leave this up here not really talk through it but these are like some of the examples of the uh of the validators you know either available in the library or that you can build with the library you know and so they span a ton of use of uses um and we only talked about re-asking as a way to handle validation failures uh but guardrails comes with um you know a bunch of other options including like filtering or refraining raising an exception just logging that logging incorrect failures if they occur or trying to programmatically fix them wherever possible um so in summary uh guardrails AI fully open source library that offers a lot of functionality to make AI applications more reliable in practice uh Inc including you know a framework for creating custom validators the orchestration of the prompting verification and reprompting um a bunch of commonly implemented and used validators available in the library as well as a specification language for communicating with llms um to follow along you can um look at the GitHub package uh on Shreya r slash guardrails the dogs are at get guardrails AI or you can follow me on Twitter or the guardrails AI account on Twitter where I continuously show um the guardrails philosophy um yeah that's it so good so many great oh sorry wrong wrong button on YouTube stay on for a minute yeah you don't need my face anymore you just need like how to follow along get out of here I gotta keep it moving no I wanted to show you this I think you've seen it before but uh any time that I think about this shirt I always think like oh this should be the guard rails mascot shirt or something because it is uh it just plays into these mlms so much we all know it's true they hallucinate like crazy and there are some uh there are some uh incredible questions that are coming through the chat so I'm going to direct you Trey over there in case anybody wants the I hallucinate more than chat GPT shirt you can scan this right now and get it and I look forward to meeting you in person in a few weeks right likewise thanks again for inviting me yeah we'll see you soon [Music]