for anybody that is wondering and wants to go just deeper down the rabbit hole with Alex we had an incredible conversation when he came on the podcast uh about a month two months ago time is going a little slow for me these days so it's probably wrong might need to do a fact check that one but a few months ago and we just talked all things like changes ml Ops slash foundational models and I know you got more for us right now so I am going to hand it over to you and give you the stage that's awesome I'm super excited yeah it was a really fun chat it feels like uh I think everything feels the the effective time skills are very interesting in AI World these days because everything changes so fast and I guess one uh one segue into what I'll chat about today is you know some things change very rapidly other things don't um so um can you see my screen now oh yeah okay great so um I uh I don't know how we're doing chat um or or q a or anything like that here uh I can't see I can't see anyone I'm just seeing full screen in my presentation so Demetrius if you want to jump in or if if you're doing live q a feel free to make this interactive um exactly I I'll um I'll jump in if something comes up and there's a great uh question in the chat otherwise I'll leave it to you and we can keep all the questions for like the last five minutes awesome sounds great so um and just to do a time check I think uh I've gotten until 12 10 right yeah I mean we started like five minutes late so 12 15. awesome generosity today this is great um cool so I'll I'll jump in and um as a quick intro I'm Alex I'm one of the co-founders and CEO at snorkel AI I'm also an affiliate assistant professor at University of Washington uh where uh at both places and and then uh you know back when I was working on the snorkel project at Stanford with the team uh everything really is around this idea of of data Centric AI this idea um that a lot of the development that is most critical and highest leverage for building for building AI is around manipulating curating labeling slicing sampling data um more broadly developing data even more so than you know picking the right architecture or the right or you know tuning the knobs in the right way um and I'll talk at a high level and part of this about how that's been accelerated by this this uh rise of foundation models and I'll share some other broader thoughts as well if we get into the weeds I'll I can also share my thoughts on uh some of the work actually out of my my co-founder Chris's Lab at Stanford on at end the snorkel research team on ways of automating uh prompt Engineering in part via VIA weak supervision so that'll be a uh you know in the weeds teaser if we get to that then anyone wants to remind me or I remember we can talk a little bit about that but otherwise I'm going to go through kind of a high level uh tour today and thank you all for taking the time to to be here and watch the talk hopefully ask some interesting and hard questions either during or at the end so you know to make it interesting okay so quick outline I'll start with and to meet yourself to apologize here because I'm opening with some some shots fired I'm going to switch right away to saying uh Foundation model not llm which I get is a kind of aggressive move given the name of the of the conference but it's it's kind of critical to how we do things I like it I got to make things a little interesting right so I'll start with that and um uh you know obviously I'll actually flip back and forth probably between FMS and llms um uh throughout without realizing but I do want to you know start on that note and and just uh introduce why you'll hear me and you know in some instances saying Foundation model I'll share a little bit of this this high level idea that a lot of the future is going to be around um you know let's call it gptu rather than gptx I'll then dive into how you make these more customized and domain specific and performance Foundation models what what is the development the data center development you do I'll use some of our work at snorkel as a case study although again I'm not going to be overly focused on that it'll just be an illustration of you know the system building we're doing around these ideas and if I get time I can talk about some other you know interesting challenges that are really orthogonal to our work at snorkel in the foundation model space just picked up from some of the customers we work with and and some you know some general thoughts may or may not get to that um but that's the highable outline and I'll jump in now so it's not that uh that maybe there's a little underwhelming as a shots fired slide because it doesn't it well I don't know Demetrius you tell me but I don't think this is the most aggressive looking slide uh with my you know uh hastily Googled image of of uh house and Foundations um but I do want to kind of introduce why do we say Foundation model uh why um you know my co-founder Chris uh and and a bunch of our uh Stanford colleagues have anchored on this term with with uh the center there um there's really three reasons um the first one is that a really exciting one which is we've we've moved Beyond language actually any data type uh any data any data type that has some underlying graph structure admits the exact same types of you know self-supervision or Auto aggressive methods uh Beyond just you know a sequence of tokens right we're seeing that uh with image and multimodal data uh we're gonna see it with with everything from databases to to genomics or we're already seeing that so really this is you know a boom that we're going to continue seeing far beyond beyond you know the classic language models that's reason number one the second reason is um you know often I hear uh you know gen or generative AI as a synonym for large language models but really all AI application types not just generated but also discriminative or predictive which is where a lot of the you know the classical and high value workloads uh still live our view is that all of these are built on top of foundation models um so if we get time we can talk more about that but that's kind of Point number two and the third point where most of my talk will kind of dive into is this you know the idea behind the name the basic and simple but but very critical metaphor which is that Foundation models are are foundations and you still need to you know build the specific house or building for your specific setting and needs on top and uh we'll talk about how you do that coming also there's a leaf blower which happens more times during talks than I think would be uh you know occurring by random chance if you have any problems with my audio let me know otherwise we should be fine you're sounding all right I didn't even notice it until you said it awesome okay I did but uh it sounds like we're good so diving in I'm going to start now by just kind of opening the aperture and and uh I guess firing some more shots although again this is mostly about you know open source Community love and predictions of where um you know practical realities are gonna you know of you know AI use cases and and the market are going to take us but you know again I'll I'll say it's a little bit more declaratively just to make this talk interesting so I'll start by saying that um you know the entire field owes you know the GPT line of work uh you know a massive debt and and I'm a a huge fan and uh I hope that you know gptx continues to increment up because uh you know it's been driving the field forward and it's a you know truly incredible innovation but I do think that most uh usage in the Enterprise even in you know in individual and consumer settings is going to be a lot more around let's call it gptu rather than gptx so why is that three high-level points I'll start with here just to get us all thinking number one we've seen a lot of evidence recently you know this is both first principles but also just you know demonstrations that have been uh you know accelerating of late around the lack of defensibility of closed API models and you know very correlated with that this kind of just boom of Open Source Foundation model Innovation and proliferation number two the more durable mode again no real surprises here if you think about it from first principles is around private data and and domain-specific knowledge whether that's in a person in an Enterprise in an organization we're seeing that and some examples of the power of actually leveraging that which I think is going to spread and a third point which will be the segue into the rest of the talk and a lot of what we do at snorkel is um this idea that really the last mile all of the hard work to get these you know to go from Foundation to house for specific use cases in specific settings is really where um you know a lot of the the effort goes vast majority effort goes and a vast majority the value um and and you know differentiation is going to be captured and I'll talk about how that's done and what's exciting there so just to go through those first couple of points high level you know there's been a uh a proliferation of awesome awesome logos and and uh I guess the sunglasses thing has been taking off with these recent projects so for folks who who um haven't been you know living and breathing Foundation model Twitter every you know five minutes uh like like a staying healthy person um uh one uh recent a very interesting project that came out actually right around when gpt4 dropped so it it you know didn't get noticed by everyone was this alpaca project out of Stanford basically at a high level what they did is they spent I think a couple hundred bucks on uh chat GPT API calls and they were able to use that along with a self-construct method out of some some uh out of a team at UW so basically train a 7 billion parameter model the the open source or the the Facebook llama model to and the evaluation is not fully finished but to to have very uh similar levels of skills and and and outputs as chat GPT so basically just by a very cheap set of queries they were able to basically clone um or or come close in many ways to cloning chat gbt and then we've seen a bunch of other follow-on work databricks clone the cloning method with their Dolly model um uh and and now there's doll ev2 where they put some of their own crowdsource data into there's an interesting koala project out of Berkeley where they show that they could get you know the same or even better performance as alpaca with just very careful curation of the data sets the high level Point here is that um you know it doesn't really seem like closed API models are going to be that highly defensible if they have any kind of you know sufficiently cheap API and if the open source continues to advance in terms of supporting these models which we see no signs of slowing down and is quite quite exciting so where do we actually have more durability well one first principles concept is that it's going to really arise out of private data distributions and you know and knowledge and we saw one exciting example that's come out recently with Bloomberg GPT so they took a bunch of private financial data and they were able to train their own model that was able to perform better on that specific domain so you know a I think there's going to be a flattening of the space in terms of just general purpose web data and general conversational and other kind of generic task style Foundation models number two we're going to see this kind of family tree start to really Bloom of domain specific models many of which are going to be leveraging you know private data and specialized knowledge to be you know better foundations for those specific areas um that gets into the kind of gptu idea um and then the third thing which I'll talk more about is again the building of the house on top and so I I in no way I mean to pick on the Bloomberg paper but it's interesting that very few people these days actually uh oh this would come off a little cynical it was very very few people uh you know actually open the papers often on these these uh these projects and so if you actually look even the blog post that the Bloomberg gbt team released you'll note what you see in many of these these uh evaluations that are done you know thoroughly on on proper held out data sets which is that it does better in a relative sense which is a very exciting achievement and proof of of this power of domain specific data but if you look here at the financial and and you know the financial specific tasks it's in the 60s right for most applications that's nowhere near a you know production Deployable accuracy level so again that gets to this love this idea that even with all this development both closed and open source even with the specialization and leveraging of you know domain-specific data and expertise you're still building the foundations and you still have a lot of work to go what always an AI over decades has always been the hardest part The Last Mile as any kind of you know uh data scientist or ml expert who shipped real stuff to production those that you have to do on top so that's what I'll I'll talk about there I'll pause for a second just any any uh questions comments pushed back on this this high level framing before I get into this this uh you know Last Mile building the house part we're good so far awesome Okay so let's get into this and what I'm going to share with you is is you know our perspective that a lot of the you know the building the house really revolves around these high-level ideas of of data versus model Centric AI uh and I'll share that both in terms of high level ideas and there's a ton of innovation in the space in the community uh and that'll be kind of with my you know academic Community hat on and then I'll also give an example of how we're supporting it uh in in production settings with our platform at the at snorkel the company uh which we call snorkel flow so um and here you know partly because I'm partly because I'm stealing slides uh uh and partly because it's just a you know it's a relevant stand and I'll talk about Enterprises this could be you know a commercial Enterprise this could be a government agency this could be a an open source organization anything where there's a you know a collection of non-public non-generic data and real production use cases so that's that's you know for the purposes of this talk think of Enterprise as a stand-in for any kind of organization or setting like that so one way that I think it's helpful to think about this is at least collapsing into two Dimensions is think of one dimension where you you you're basically asking how bespoke or unique is this data this is you know for ML folks in the audience this is just old transfer learning intuition right um if you train a giant Model A small Model A giant model on web data it's going to work better on data closer to what it trained on um Part of Me versus you know very bespoke different data inside a bank or a hospital or government government agency for example and then think about the x-axis is what is the accuracy requirement or accuracy proxy metric whatever you choose before you can actually use this thing we see a lot of of a very exciting use cases for you know especially generative type use cases where no one even measures the accuracy or even really knows how to right if I'm trying to generate some marketing copy or some cool images for a kind of co-pilot style application I don't even know necessarily how to measure what accuracy means and I don't need it to be that good because it's just a starting point it's a it's a it's a part of a human the loop process so there's high tolerance for failure um versus a lot of the AI applications that are actually shipped in production have to be you know 90 95 99 accurate accurate before they can even be shipped to production so a lot of the really exciting momentum and demos that we're seeing are kind of in this lower left quadrant where you're testing on data very similar to what you know gptx or other Foundation models were trained on you know you either have kind of low accuracy requirements because there's High degree of failure tolerance and a human loop system or you don't even have an established way of measuring what you know some accuracy a style metric and a lot of where you know the house building is both hardest and more most valuable is in this upper right quadrant where you have you know non-standard data um in a bank a government agency a hospital system most most places in the world most Enterprises for sure and where you actually have high levels of accuracy you need to get to before you can actually ship something um so I'll give an example of this uh with some of our internal data um and this is using a variety of foundation models uh some you know early results of gpt4 also smaller models like Burt and clip um and here we're just yeah there's there's more data here and and we'll be releasing uh some of the latest gpt4 data from our experiences soon but just at a very high level you know comparing the out of the box performance so a zero shot and and the few shot performances are pretty similar of just using gpt4 one of these models out of the box and this is looking a range of problems from info extraction for a large Pharma company to um actually an open source case study we released on classifying legal Clauses uh um to image and chat things as well and if you actually look um at the Gap you have to fine tune on on tens or even hundreds of thousands of labeled data points to actually even start to approach uh production level accuracy and I'll note here just to be very precise that gpt4 doesn't have a fine City interface so actually if you look at these case studies with gpt4 this is fine-tuning a actually gpt3 so it's actually fine-tuning a a less powerful model but gets a significant leap in quality um and and if you're curious on more data you can look at actually this legal data case study where we're updating it pretty soon uh this includes in between the 59 the 83 all kinds of few shot techniques Advanced prompting techniques nothing really approaches uh you know the the you know good old-fashioned fine-tuning on labeled data but this is mainly just to show before we get into methods that you have to do a lot of work still to build the house on top of the foundations and there's lots of work out in the public domain I'm I'm you know highlighting one of my favorites here uh because I was uh I was lazy and I just grabbed one there's a lot of work and and you know there will be a wave coming out once uh folks have enough time to evaluate gpd4 and other models this was an evaluation of chat GPT where the conclusion was that um chat gbt was again an incredibly impressive generalist hence the bet that this is you know Jack you know models like this are going to be the foundations for for all AI development that's certainly the bet that we're taking um but it was on average 25 percent worse than a specialized model that was specially trained for the given task and it was it was over 25 NLP tasks and by the way that specialist model was often a minuscule fraction of the size and therefore cost of chat GPT so we're seeing these kind of same results out in the open source um just a little behind because these things take time to these large benchmarks to take time to run um so how do we build that house how do we get from the kind of Baseline out of the box performance that again for real AI use cases or at least for many real AI use cases in that upper right quadrant we showed are just not anywhere near good enough to ship to production how do we go from that you know generalist jack of all trades Foundation to a you know a specialist or an expert that's Deployable in production well surprise surprise uh already announced that this was going to be the uh the technical perspective but um at a very high level what we've been working on out of Stanford UW and snorkeled a company over the last eight years or so is exploring this idea of of what we call data Centric development this idea that you know rather than pursuing things that the often the kind of classical way how ML and AI is still often taught in intro or most intro classes where the data comes from somewhere else it's janitorial work it's exogenous to your process as a data scientist it's not your your job I often think of this as the kaggle era of machine learning where you just your machine Learning Journey starts when you download your data set from kaggle all nicely labeled and curated and collected and then you start you know tweaking your model architectures data Centric development is the idea in its extreme of kind of flipping that on its head where the model is now fairly standardized and fixed and may not even change or maybe automatically configured in your process and most of your data science process your workflow your journey is really about iterating on the data labeling it sampling it curating it augmenting it Etc um so it's it's not always that extreme but one thing that I'll point out is the the wave of foundation models has really made it much more extreme than it ever has been think about it from the perspective of a user you know if you find out that your foundation model based application is messing up on some patient population or some subset of the satellite images you're analyzing or some you know subset of of legal documents more than ever before you can't go and just tweak the model architecture you can't go and you know tune you know by hand some of the trillion parameters you effectively have to go to the data whether that's labeling prompting Etc it's it's all these these kind of data Centric interfaces so in our view the rise of foundation models has also accelerated and in some ways completed this shift from model Centric to Data Center development so that's a high level thought I'll leave you with uh again you know one of one one uh uh you know of our favorite examples here is the the gptx family and I'll note that that you know the advancement that got at least a large chunk of the world you know uh going crazy over these these advances was really a Delta between gpt3 and 3.5 that was all about human supervision right a lot of folks are familiar with the the rlhf term but you know I think it's helpful and if you look at recent work it's you know more precisely you can separate the the inputs from humans and the mechanism by which the model was updated which is the RL part um and the the input was just labels in this case it was uh it was uh you know labels in the form of rankings and or orderings and then there was further labeling in terms of the thumbs up thumbs down and now there's even further labeling and and uh response generation being paid for yet again so the Delta was not really about the model architecture the Delta was all about the the data and the supervision so this is you know one one really great example of this this uh this data Centric development idea so you know our idea is that um and the central concept of both snorkel the academic work and this data Centric uh you know um uh concept is that you know the the critical layer between kind of these base Foundation models and here I'll depart or I'll be orthogonal to that point that I raised this doesn't matter whether you're starting with um closed apis like open AI always a fun fun sentence to say or open source models like I pitched I think are going to become even more uh more more prevalent um whatever you start with you have this layer of your stack where you have to do development to fine-tune or adapt them for your applications and that development is done via data primarily and this is where a lot of the challenge comes in because you know manual annotation is extremely difficult a lot of it's difficult because um it's just costly and slow and especially for most uh um you know non-trivial complex settings and certainly most Enterprises they can't just Outsource it so this takes you know huge amounts of In-House efforts often from you know very highly paid and and very busy subject matter experts you know a clinician a lawyer a network technician and you know an underwriter Etc um and it's also very brittle because every time something changes you you don't really have any way of modifying manual labeling so here's where I'll segue in uh to snorkel flow which is our system for um you know for developing Foundation models using data Centric AI and again I'll just quickly pause on some of this high-level stuff that I covered before any any questions any comments otherwise we can leave it there yeah there is a few questions that came through one is like when does gptu need more than 32 000 tokens of context oh yeah great question so I mean I I think that I think that's mostly orthogonal is like I think the the extension of the context window length um a lot of that work actually has been been pushed by some great work by uh um tree Dao and others in um the co-founder Chris's lab just advertising if anyone is hiring for academic positions I believe trees on the market he's amazing his work on flash attention has been been behind a lot of this these advances in context window left um but I I see a lot of that as somewhat orthogonal I think it's it's extremely exciting um there's a ton of possibilities that get opened up when you when you open up the context window um you know you can I think there's there's ways to basically unify fine tuning and prompting by putting all the label data into the context window you could handle obviously larger contacts and and more complex documents and Etc um but you know you can still get by with a shorter context window it just puts greater emphasis on fine-tuning on on you know various kind of chunking and compression schemes Etc so I see those kind of orthogonal to to um to a lot of stuff I'm talking about today although very exciting there is another there's another one that came through also that I wanted to say and it's uh from pradeep asking we can use the expensive large language models with Last Mile prompt engineering until we get enough training data to train or fine-tune gptu and maybe that wasn't so much of a question now that I read it again I at first I thought it was a question and now I'm thinking that that was probably just a statement but maybe you have something you want to talk about on that statement well I I yeah I mean I mostly I mostly agree I mean it really just depends on your your use cases right I think one of the you know the key things in characterizing a use case is um uh you know the various different ways you could talk about this but call it the the failure tolerance of the of the use case right how accurate do you need to be to go to production and um you know some of these things are scalar meaning you you get you get better results you get better Roi if you improve the accuracy in which case yeah like starting with you know a a zero shot or prompting based technique that gets you to that kind of 60 level and then gradually kind of tuning and developing it with data Centric AI to go higher is a very viable strategy in other settings 60 or as I mentioned before like not even knowing how to measure the accuracy for some of these generative use cases is good enough because it's just a copilot use case where it's just meant to assist a creative process and there's always going to be a human editing and and you know the final result and then in other settings and as you can guess there's a lot of where we operate um you know getting in the 60s just isn't good enough for anything you can't ship that model and so you really are blocked until you can get that that data development done uh to to get it to a production level accuracy um so I guess I'd say I agree with the statement in certain use case settings in others you're blocked until you can do that that you know fine-tuning or Downstream development and and in others you're fine because it either works really well out of the box because it's a very generic or kind of standard data task or you don't care about the accuracy as much because of the the use case setting um that goes back to the kind of quad chart I shared or at least that's one way of thinking about it right awesome so I'll push on because I'm almost at time so I'll just give a little preview I definitely won't get to the last section but um uh you know I'll just give a little view of the very high level Loop uh that that we support in snorkel flow um and I'll note that you know a lot of what we've you know over the last you know many years we've kind of anchored the description of snorkel flow on is you know developing training data for training models from scratch actually a lot of our workloads have actually been using it to fine tune what's called a medium Foundation models like Bert for many years now um and and now you know obviously a lot of the world is moving towards you know and we're we're you know we've been heavily invested for the last year or two and moving to you know building on top of foundation models so the basic workflow in in snorkel flow today is starting with some kind of Base Foundation model it could be closed Source it could be open source again I I I threw out some bets at the start of the presentation but we're we're completely orthogonal to that you you bring whatever you want to uh to start and basically the process starts by defining the specific task you want to accomplish let's say you know I I you know a lot of our our customers are you know still focused on predictive tasks where a lot of the value-wise I want to you know classify these contracts with very high accuracy so you take your your base Foundation model it would say train on web data and you apply it or snorkel flow automatically applies it to your data and your task so that you can kind of you know see how it does to start and that's when the guided error analysis starts so the idea is let's apply your foundation model to the data and if you think about it how else could you actually inspect how your foundation models do it right you can't go and poke around the model weights at least practically today you have to apply it to data to be able to even see how it's doing so that's that first step and then you know you start this data Centric Loop which begins with um you know discovering error modes in your base Foundation model via guided error analysis I'll skip over details there but that's a lot of the the work we've done both academic commercial side and then your goal is to correct them and to do that as rapidly as possible and this is where if you've um you know heard me give a talk before or seen any of the other snorkel materials I'll refer you to that if not a lot of the the um uh the acceleration that we get and a lot of what our academic work has been around is these radically more efficient and programmatic ways of doing this correction this corrective labeling and as I teased at the beginning I'm basically at times so I I won't go in depth but I'll just note that um this idea of programmatic labeling or on the academic side we've often called it weak supervision is a a powerful way to unify all different types of input so it could be a heuristic it could be a knowledge base it could be using you know clusters and embedding space it could be manual labels it also could be a prompt so there's a some nice work out of the the Stanford lab on um and also the snorkel research team on how prompt can actually be viewed as programmatic sources of supervision or labeling and automatically combined and modeled so you don't need to find kind of one perfect prompt you just dump them all in along with any other source of information and it gets all combined by snorkel flow um and I won't have time to go into the how but lots out there in the academic space and then the last step and I'll end here is basically one of two pads or three I have here on the slide you can always you know take the data that you labeled and just export it but the two main paths and I think this is again broader than just circle flow is either you go back and you update your foundation model with this you know corrected and augmented uh labeled data and again the standard way for production accuracy is that you'd still do is fine tuning or and we see this increasingly in our customers you actually now distill this into a smaller model that is specialized for this task and I'll just quickly note there's a case study that we have up on this this open source Ledger data set it's a contract classification underway classification problem and the net result uh you start with actually an ensemble of foundation models we did in this in this case you do this to development and you actually get not only a 41 accuracy Point boost above the kind of this was with gpt3 the GPT route Baseline but we actually now distilled it into a smaller Foundation model that was 1400 times as small so this is a lot of where we think things are heading you know starting with the foundation models as your foundations but then building the house on top via data center development and here's where I lose the the house building metaphor but basically also then distilling it into smaller models for for production accuracy so let me cut there I know I'm at time um and I don't know if there's time for questions but at least we covered already a couple and uh thank you all for the time today dude amazing for the questions one thing that I would say is for everybody that's looking to continue chatting with Alex jump into slack Alex I think you're in slack if you're not I'm going to send you the invite right now and go to the channel the Community Conference Channel tag Alex in there and ask him about all of this stuff I want to give a huge thank you to snorkel while you're on the stream with me because you all have sponsored this event and I am so grateful for that I also want to mention to anyone out there snorkel's having a conference a virtual conference too so we'll drop a link to that in the chat uh it's coming up soon I think and when or am I speaking out of turn Alex was that a secret did I just blow the secret yeah I'm I'm not even I don't even know so I I I I don't think it's a secret I didn't even know we were like giving out socks and stuff and sponsoring so um let's assume it's not and uh at least for this group otherwise it's super super secret exclusive announcement yeah we'll be running and thanks for bringing up we'll be running another version of our our um uh future of data Centric AI conference that's it yeah we've had some exciting speakers before last time you know a bunch of academic folks uh we had the uh the incoming um I think uh uh CSO the CIA give a talk um uh so you know academic Federal industry um all open um largely kind of tilted towards academic uh type stuff um and uh all about this this intersection this year of foundation models and data center development methods so uh yeah if anything that I said today piqued your interest uh please uh consider showing it up and Demetrios thank you so much for all the time today it's a pleasure man thank you for joining us and I'll drop all those links into the chat together