we've got another speaker coming on though where is let's see Inez Inez calling us to the screen where you at there you are what's happening Inez hello hello um all good great to have you here I am so excited so now we went from these 30 minute talks and now we're gonna go lightning tile and we're gonna be going 10 minute talks and so if anyone has questions in the next 30 minutes over the next three talks please hit them in slack or in the chat because the speakers will be answering them there he knows I think you need to share your screen I don't see it uh you might need to share it again and then I'll give you the 10 minutes I'll be back in 10 minutes and let you go and of course I am a huge fan of what you are doing in numbers station and I'm so excited for this talk so I'll let you go right now take over awesome and thank you so much for having me the materials appreciate um all right so today I'm gonna talk about some of our work on leveraging Foundation models in the modern data stack uh and most people here know about generative AI uh before I dive in I just wanted to do a very quick recap of what Foundation models are and why they're so exciting so at a high level Foundation models are very large neural networks that are trained on massive amounts of unlabeled data like text or images uh from the internet and the idea is to use a technique called self-supervised learning to train the model so for instance an important category of models are Auto regressive language models which are trained to predict the next word in a sentence given the previous words and this idea has been around for a long time in NLP but what makes Foundation models really unique is their scale and with this increase in scale model size and data volumes we started seeing some very exciting new model capabilities so what are these model capabilities which we call emergent capabilities well because these Foundation models aren't trained on so much data they can generalize too many Downstream tasks via capability called in context learning and the idea is to take any task and cast it as a generation task by crafting a prompt and then use the foundation model to perform the task by completing the prompt which is what the model is originally trained for and I can then reuse the prompt on many more input examples or I can craft many other prompts for different tasks but the key here is that the same underlying model remains all right so we we clearly see and everyone here in this Workshop knows that there's a revolution happening in this space and there's a huge Paradigm Shift compared to traditional AI uh with Foundation models and it means that pretty much anyone not just AI experts can rapidly prototype ideas and build AI applications with this technology so what we're going to focus on today uh is application specifically to the modern data stack and you may have heard this term but essentially used to describe all the set of tools to process store and analyze data and starting from where the data originates which is typically apps like Salesforce or HubSpot the data gets extracted and loaded in data warehouses like snowflake it can then be transformed and modeled using tools like DBT and ultimately once the data is clean and prepared it can be visualized with tools like Tableau or power bi and even if these tools are amazing they've drastically improved things like scalability and knowledge sharing in organizations uh there's still a huge uh problem which is that there's a lot of manual work to do throughout this process so the good news is that it's possible to automate a lot of this work with AI and even more so with Foundation models and that's exactly what we do at number station essentially we're bringing this Foundation model technology into the modern data stack to accelerate time to insights so now let's talk about how that works in practice and I'll start with a simple example that you probably have seen before which is generating SQL code uh so in modern organizations anytime a business user needs to answer a business question they have to send a request to their data engineering team and these ad hoc requests usually require multiple iterations and can take a long time to fulfill so with these Foundation models a lot of this back and forth can be avoided by using the model to generate the SQL queries from natural language requests and this works amazingly well for simple queries but there are some caveats to use this for more complex queries especially for complex query when there's some domain specific knowledge that the models the large pre-trained model may not capture so for instance if there's multiple date columns in my table which one am I supposed to use that's not something that the models know out of the box and I'll touch on this point later in this talk uh another exciting application is data cleaning to fix things like typos or missing values and typically the way to address this problem is to create a bunch of SQL rules so this works well overall but it's a very long process uh to derive all the cleaning logic and many times the rules break because of some Edge case that was not captured during the rules development process so with Foundation models there's an exciting alternative to this rural development process which is to use the model itself to do the correction and the idea is to create a prompt with a few examples and then reuse this prompt over all the data records to clean the data which is what is shown in in this figure and this is obviously very exciting because the model can derive the patterns automatically from the in context examples that we provide but there are some caveats to this approach as well especially issues around scalability which I'll discuss later in the talk and another exciting application is data linkage so the goal here is to find links between different sources of data that may not have a common identifier say like my Salesforce data and my HubSpot data I want to link my customers there's no ID to create a join on and similar to data cleaning Engineers need to spend long iteration Cycles crafting rules and sometimes these rules can be brutal and break in production so with Foundation models we can create prompts for the specific data linkage task and alleviate some of these issues the idea is to feed both records to the model and then ask it are these two things the same in natural language and in general we find that both for this problem and the cleaning problem the best solution is to compose rules with the foundation model so if there's some very basic rule that can solve 80 of the problem we should use it but then for these last mile examples that are more complex we can call the foundation model all right so now that we've shown some of the possibilities for using Foundation models in the data stack and there's many more just I just had to scope it for this lightning talk uh I want to share a few caveats as well as solutions that we've developed throughout our research at number station in collaboration with the Stanford alab so the first caveat which we touched on uh briefly is the scale Foundation models are extremely large models and depending on the application and how we use them they can be very expensive and slow and if I'm using a foundation model with a human in the loop for things like SQL co-pilot then the scale is not so much an issue we care more about latency in these cases but for other data applications where I need to use the model itself to make the predictions on let's say an entire database that has millions or billions of rows this is just impossible and it's going to be extremely expensive extremely slow compared to using a rule-based solution so how can we address this uh there's many possible solutions and one of them is model distillation so essentially here the idea is to take the big model for prototyping and then use that big model to teach a smaller model to do the task and this actually works really well uh with good prototypes and good fine-tuning and we can easily bridge the gap between these large and smaller distilled models there are also other solutions to address the scale issue which I Linked In This slide one of them being as I mentioned to only use the foundation models only when we really need to so if the task is simple enough that it can be solved with a rule we don't need to use the model itself to solve the task we can instead use the model to derive the rules based on the data which is always better than handcrafting rules uh all right so another important challenge here is um as everyone know prompt brittleness so for instance uh if we format a prompt differently it will predict two different responses in this cleaning task uh and the demonstrations that are used in the front are also really important so in this example on the right we run an experiment and we saw that picking uh manual demonstrations versus random demonstrations had a huge performance Gap and this can be particularly problematic for data applications where users are used to deterministic outputs which is the case in the modern data stack they're not comfortable with potential errors in brittleness even if that can save them a lot of hours of manual work so how can we solve this um there's a few methods again to to approach this problem one of them is a method that we proposed uh in the AMA paper which is linked here and the high level idea is to apply multiple prompts to the input and then aggregate the predictions to get the final results this worked quite well we noticed some good improvements compared to the traditional prompting method and there are of course other solutions to address this prompt brittleness issue such as decomposing the prompts with chains as well as being smart about how we sample uh demonstrations and the last caveat I wanted to touch on here is the lack of domain-specific knowledge so Foundation models are trained on public data and they lack some knowledge that is sometimes uh crucial to solving Enterprise data tasks so for instance if I'm asking a foundation model to generate a query uh to compute the number of active customers in my database there might not be a perfect is active column and then what I need to do is rely on some organizational knowledge that defines what an active customer is to generate that query properly and so to approach this uh domain knowledge problem there are two types of solutions inference time and training time so for training time the idea is to Leverage The untapped knowledge that is stored uh in organizations documents logs metadata and what we can do is continually pre-train open source models on this data to make them aware of this domain knowledge and we have some work on this which I I linked as well on this slide another solution is to bring the knowledge at inference time by augmenting the foundation model with some external memory that can be either accessed through a knowledge graph a semantic layer or a search index over the the internal documents so that's pretty much it for for this talk I'm sorry I know it was rush I tried to condense everything in in 10 minutes uh I wanted to thank a few of my collaborators from Stanford and number station and if you're interested about these applications feel free to reach out to me uh check out the blog send an email I'd be happy to discuss more so good I love it Inez thank you so much that was awesome so two things uh if anyone has a question throw it into the chat or into slack Inez I think is in both of them I'm super excited about what you're doing at number station and I believe that you're you're working out of the factory offices or you stop by there every once in a while because yeah a good part of our team is there so all right now that I've got you on screen right here I'm gonna get nothing I can catch you I'm gonna be visiting Diego who you're working with in at the end of the month in San Francisco and I want to stop by the factory office and hang out maybe we can record a podcast or something and you can go deeper into this does that sound good yeah anytime sounds great and thank you for inviting me again awesome well I am glad we got that I'll reach out to you on uh get all the email or slack and we'll make it happen and now we'll keep it cruising so thanks again [Music]