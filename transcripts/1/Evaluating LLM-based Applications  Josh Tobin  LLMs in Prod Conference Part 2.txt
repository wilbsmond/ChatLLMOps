and oftentimes the main challenge is not actually building the application itself it's like convincing your teammates that this is actually good enough to go in front of your end users so evaluation can be an antidote to that and then finally if you use evaluation the right way I think it can be a great roadmap for making improvements to model performance so if you um if you have robust evaluation and granular evaluation that can point you to what are the current opportunities to make your model or your system better and that can help you decide your development path um and then you know so the corollary of this is you know if if we're using evaluation for these three things validating your model's performance um working together with your team to decide when you um can go or no go and then finding places to improve um in light of that let's talk about some some desired properties of a good evaluation so what makes good evaluation the first and probably most important quality of a good evaluation is it should be a number or a set of numbers that is highly correlated with the outcomes that you actually care about as a business um if they're if it's not correlated with the outcomes that you care about then it's not going to be a very useful metric to help you make um decisions about whether to shift the model one other thing that helps with evaluations and this this sort of cuts both ways is you know in a perfect world you'd have a single metric like a single number that you could make go up or make go down um and that's that would be your evaluation metric um the reason why that's valuable is because machine learning as a field is kind of like um the whole field has evolved over the course of decades to be really really good at like finding different ways to make number go up or make number go down um as soon as you start getting into things that are qualitative or uh or having many metrics then this can fall apart now in the real world um doing evaluation in a granular way is also really important so it's helpful to have a single number that you're looking at but it's also helpful to have um many different ways of slicing and dicing that number so you can understand performance in a more granular fashion and then finally um the third sort of desired property of a good evaluation is that it should be fast and it should be automatic now um not every evaluation is going to be super it's going to be instantaneous and not every evaluation is going to be completely automatic because as we'll talk about in more detail humans still play a role here but the faster and more automated the evaluation is the more of a role it can play in your development process right so if if you think about like a perfect evaluation Suite um it's something that you as a developer can just run very very quickly as you make iterations and changes that will tell you okay do these changes actually improve performance or do they not improve performance and so it's fast and reliable um evaluation okay I will pause here and um take any questions there's still someone who's talking and unmuted so I've tried to mute everyone myself but if you don't mind muting yourself um as well that would be helpful what's happening here with the mute it always you can continue Josh no worries oh yeah no no worries um was just gonna pause here and see if there's questions cool all right I will keep going um okay next thing that we're gonna talk about is so we talked about like uh why is evaluation important what makes a good evaluation metric or a valuation Suite next thing we're going to talk about is like why is this hard right so um I'm gonna ground us a little bit in Old School machine learning right so machine learning as of you know Circa 12 months ago or older um the the old way that this used to be done so in the old world we had this notion of a training distribution so this is the data that you're going to train your model on and then you would sample two data sets from this training distribution you sample your training set um and you example your evaluation set the training set is the data you actually train the model on and the evaluation set is the data you hold out to measure um generalization performance then you can look at the difference between the performance of your model in these two sets and that tells you how much your model is overfitting so how much your evaluations uh how much your model is like just too specific to these data points then you deploy your model and you look at this in production on a potentially different distribution of data um ideally you'd sample a test set from that prediction production distribution as well and measure the performance on that the difference between the evaluation set and the test set is a measure of your domain shift so how how much has your production distribution shifted away from what you're actually the data you're actually able to train your model on and then finally on an ongoing basis you'd measure performance on your production data and the difference between your test set performance and your production performance is a measure of uh what people call drift right which is how much is your production data changing over time and how is that affecting the performance of your model so why doesn't this make sense in the llm world the reason this doesn't make sense in the llm world is because let's face it um you probably didn't train your model right um You probably didn't train your model and that means that you probably don't have access to the training distribution or at the very least you know you're using an open source model and you know maybe you could get access to that data if you wanted to but it's pretty impractical because there's a lot of data it has nothing to do with the tasks that you're trying to solve then when you get into production um since we're using these matte pre-trained models that are trained on the whole internet massive amounts of data um the idea that the idea of distribution shifts or data drift is like endemic to the problem the production distribution that you're evaluating your model on is always different from training no matter what there's no such thing as I mean unless you're evaluating on um on just random data from the internet there's no such thing as an llm problem if you're using off-the-shelf albums that doesn't suffer from distribution shift um so in traditional second reason why this doesn't work very well for llms is metrics so traditional ml let's say that you're building a classifier to predict whether an image is a picture of a cat or a dog um the the thing that makes this a lot easier is that we're able to compare the predictions that the model makes to the actual labels of whether this is in fact a cat or a dog and compute metrics like accuracy that says you know how many how like what fraction of the time do we get the answer correct in generative applications which is a lot of what folks are doing with llms this is a lot harder right so your the output of your model might not be a label it might be a sentence like this is an image of a tabby cat and if you have a label that says um you know this image is a photograph of a cat how do you actually tell whether this is whether this prediction is correct or not it's not super clear what metric you can use to distinguish between these two sentences which both might be accurate descriptions of the image that they're describing um so it's hard to Define quantitative metrics for albums and then finally even if you have a quantitative metric the uh the like another big challenge is that oftentimes we're using these LMS since they're so general purpose to solve tasks across lots of different domains and so if you have a model that is 90 accurate let's say on questions about different topics but it's 95 accurate accurate when you're asking about startups it's only 90 or 85 accurate when you're asking about dogs or food but if you ask questions about physics the accuracy drops all the way down to 17 then is this is this actually a good model or not well it depends right it's really really hard to summarize whether um whether this performance on this diverse set of tasks is good or not because it depends a lot on the problem that you're trying to solve like if this is a chat bot for um you know uh nine-year-olds who just have lots of questions about dogs and food this is you know maybe a pretty good chatbot but if you needed to answer questions about physics this is not a very good chatbot um so to summarize why does traditional evaluations like uh machine learning evaluation break down for llms first you know the these models are traded on the whole internet so there's always drifts and data drift doesn't really matter so much in this context the the metrics the the sort of outcomes that we're aiming for are often qualitative so it's hard to come up with a number to measure success and finally um we're often aiming for our diversity of behaviors and so the the goal of um that we talked to before talked about before of like pointing at a single metric as the measure of quality for this model is more difficult um so how should we actually think about evaluating language models um the recipe for a language model evaluation really has like two components the first is you need some data to evaluate the model on so what data we're going to feed into the model to see how it responds and then you need some metrics some some ways that you some functions that you can use to measure the output of these models maybe compared to a reference answer or maybe not to tell uh quantitatively how performance is doing so what this looks like is you know you take this data set you run one or more models on it you compute one or more metrics on the outputs of each of those models and you summarize this in some sort of report um so one of the kind of key things to understand about this is that if our recipe for building language model evaluations involves um picking data to evaluate on and then picking metrics um on to run on that data to compute the number for the evaluation then better evaluations correspond to having data that is better in metrics that are better what does it mean for data to be better data is better if it is more representative of the data you're actually going to feed the model in the real world for the tasks that you're trying to solve so if your data is more like your production data that's a better evaluation data set if it has nothing to do with your production data set um your production data then it's not that those the metrics that you compute on that are useless but they're much less relevant to the problem that you're trying to solve what does it mean for an evaluation metric to be better well an evaluation an evaluation metric is better if that evaluation metric is more predictive of the actual outcomes that you're aiming for with the product that you're building so if the metric through your Computing you know has almost nothing to do with the the thing that your users are trying to do then it doesn't matter if you are running on the perfect data set it's still not a very helpful um uh sense of what how well the model is doing um but if the evolution metric is perfectly predictive of how humans would rate the outputs of this model then that's a really good metric and you need both of these things to be true in order to have a great evaluation set so this is another good stopping point I will jump back over and chat and see if folks have questions no questions so far I think all right um cool so we kind of on this previous slide we talked about um I sort of put public benchmarks all the way in the bottom left here um so I want to talk a little bit I want to justify that a little bit talk about some of the challenges with using publicly available benchmarks to make decisions about um language models um so let's talk through some of the different categories of publicly available benchmarks um so the first category and the most useful category is benchmarks for functional correctness so these benchmarks what they do is they take the output of a model and they actually run or they use that output to try to solve a task um and so most of these benchmarks that are publicly available for functional correctness are operating on code generation tasks the great thing about code generation is you know code is like a thing that computers can run so if you generate some code and then you run that code and it solves the task that you wanted to solve that's a very good indication that the the model is doing the right thing um but even just the the code compiling or uh or having some general properties of like a correct code is a pretty decent indicator that the model is doing something that's good so these publicly available benchmarks are actually very helpful if you are um if you're doing a task that corresponds to them so if you can do this you should do it the next category of publicly available benchmarks are live Human evaluation benchmarks so the the one that is um kind of the most popular for this right now is called the chatbot Arena and the chatbot Arena the way that this works is they have um they host like kind of two models side by side I think I have this on the next slide actually um yeah so they host two models side by side and then you get to type in what you um you know a question that you have for the models you see both responses and you pick the one that you prefer and so they're crowdsourcing answers to um to the uh uh to the questions um are there crowdsourcing feedback on the responses that the different models are getting um and so this allows you to kind of Stack rank the different models that are available uh if you know based on the feedback from like actual people oh um I think there's kind of like a common perception that human evaluation is the best way to evaluate whether language models are doing the right thing or not um we'll talk a little bit more in the next section about sort of General challenges with human evaluation um but I personally find the chatbot Arena to be you know probably the most helpful thing for or the thing that's most correlated with my um perception of model performance on most tasks third sort of set of publicly available benchmarks is um uh categorized by models evaluating other models and so this is kind of a sounds like kind of a crazy idea but the idea is you know rather than having a human say like which of these two outputs is better instead you can just ask g54 okay which of these two outputs is better um turns out that that actually works like surprisingly well and gvt4 is like a pretty good evaluator of other language model outputs so these types of evaluations I think are growing rapidly in popularity they're powerful because they're very general if you can anything you can prompt a model to do you can get it to evaluate and so I think these are going to play an increasing role and again in the next question we'll talk a little bit about the limitations of them limitations of this technique in general next category publicly available benchmarks is Task specific performance so the most um the most popular or the most like famous versions of this are Helm and big bench um these are great because they're aiming to be as holistic as possible in terms of the different tasks that you might want to evaluate the language model on and the way that they compute the evaluations is they formulate the tasks so that they're such that there's a clear right answer um so these are like decent ways to get a rough sense of comparison of different models but the they don't include the tasks that you care about as model developer um in most cases and so you know generally these benchmarks are not super useful for picking a model and then finally um kind of the the lowest quality form of publicly available benchmarks um are sort of automated evaluation metrics that compare a gold standard output to the output of the model in a way that does not use language models to do that evaluation so these metrics have been around for a long time in NLP um they're kind of falling out of favor recently because recent papers have shown that they're actually not very correlated with um with with the way that humans would do the same evaluation um and again yeah chatbot arena is you know when I'm looking when I'm trying to get a sense of a new model um how good it is this is usually the first place that I'll start um in addition to just playing off the models on my own um okay so what's wrong with publicly available benchmarks in general they the key issue with publicly available benchmarks is they don't measure performance on your use case just because you know um GPT 3.5 is slightly better or worse than some other model on a general Benchmark does not mean that that'll be true on the tasks that you're trying to solve um also the methodology that a lot of these benchmarks use is not evolving as quickly as the rest of the field so um the uh like a lot of these benchmarks um you know aren't using modern sort of Chain of Thought prompting techniques um they're often not using like in context learning um and they're definitely not using the prompting or in context learning or fine-tuning techniques that you're using in your application um and in addition to this like measurements of model performance is really hard in general which we'll talk more about in the context of building your own evaluations uh and the publicly available benchmarks although they're very carefully thought out still have all of the measurement issues we're going to talk about in a second all right another good pausing point um okay there's a question question go for it yeah so um I think he's trying to uh talk about the the data aspect of things and I think in one of your slides you mentioned uh um better data for for evaluation if I'm not mistaken so he was talking about an experiment to run the designing and training and evaluation data sets in accordance to factorial design principles and that was with 1.2 000 data points and achieved 93X accuracy so generally what do you think about um you know the factorial what is it called um design principles for uh for for this sort of sense yeah um well let me reach these questions because I'm trying to assimilate as well yeah maybe um if you don't mind whoever asked that just kind of um helping us understand um what your what you're trying to answer here um or have us answer here um so um we talked about the problem of just like kind of Googling a benchmark and using that Benchmark to make decisions about models um so what do you do instead well what you should do if like as you continue to invest in a project that you're working on is build your own evaluation set and choose your own evaluation metrics that are specific to the tasks that you're trying to solve so let's talk about how to do that um so the kind of key points for building your evaluation set um are you know you want to start incrementally um you want to use your language model to help and then as you roll out your your project to more and more users you want to add more data from production that corresponds to the data your model is actually faced with in the real world so starting incrementally um usually when I'm starting out a new llm project I'll start by just evaluating the model in kind of like an ad hoc way so if I was writing a prompt to try to you know get a model to write a short story about different subjects um I might try out a few different subjects in this prompt like I might try to have it write a short story about dogs or LinkedIn or hats um then as I sort of try out these examples ad hoc I'll find some ones that are kind of interesting in some way and interesting might mean um maybe the model does a bad job on this data or interesting also might mean this is just another way that I think a user might use this prompt that I didn't think of before and as I find these interesting examples so as I try out inputs to the model that are interesting I will organize those examples into a small data set and then once I have that small data set it might just be you know two three five ten examples then as I make a change to the prompt rather than um going ad hoc sort of input by input to try the model on those instead I'll run my model on every example in the data set and so yeah interesting examples are ones that are hard or ones that are different so um second thing that you can do to make this process less manual is you can use your language model to help generate test cases for your language model and so the way this works is you'll write a prompt that is focused on getting the model to sort of generate inputs uh to your uh to your prompts and then you'll run that model generate some inputs and then add those to your evaluation data set so there's an open source library that helps with this in the context of um question answering in particular um that you can poke around to get some inspiration about how to do these data generation prompts um also show you kind of how this looks in Gantry as well um this is like one of the pieces of functionality that we have and um uh lastly I would say like the one caution here is that we have found that these models that it's difficult to get models to generate like super diverse inputs they'll generate inputs that are valid and interesting but they won't really cover all the possibilities that you as a human might think of and so it's just good to be aware of that limitation as you use models for this and then finally you know probably the key Point here is you shouldn't think of your evaluation data set as static evaluation data is something that you build over the course of your project as you encounter more and more sort of use cases and failure modes um and so as you roll this out and this interacts with more users your team your friends and eventually your end users you want to capture examples from production like what's your users dislike um maybe if you have annotators what do they dislike uh which is another model dislike you might want to look for outliers relative to your current evaluation set you might want to look at um you know underrepresented topics in your current data set so all these different heuristics all sort of building up to taking production data that is interesting so different or hard and feeding that back into your evaluation set so that you're progressively evaluating on more and more challenging and more and more interesting data over time so that's kind of the quick version of building your own evaluation set um next thing I want to talk about is how you should think about you know picking metrics to run on that evaluation set here's the flowchart you can kind of think through as you make as you make this decision so first key question is is there a correct answer to the problem that you're asking the language model to solve if there is then this problem gets a lot easier because you can use evaluation metrics like in you know classical ml so if you're trying to get your models to classify uh sentences about cats or dogs then you can still use accuracy as a metric if there's no correct answer then it's helpful to have a reference answer that you can point to if there is then you can use that reference answer as a way to judge whether your answer is correct even though you're not expecting your answer to be exactly the same as the reference answer if you don't have a reference answer then looking for other kind of examples to guide you is still really helpful so if you have a previous example or an example from a different version of the model that you might expect to be reasonable you can use that and if you have human feedback then you can also use that as a way to evaluate malls and at the end of the day if you don't have any of that there's still metrics you can run on your output that based on your task can help get you a metric um so expand on this a little bit um you know you've got your your normal kind of machine learning evaluation metrics then you've got some metrics for um matching a reference score so um again the context here is you have an example output from a model and then you have a prediction made by uh do you have you have an example correct answer from a human and then you have a um an example that's generated by your model and you want to compare these two things and say like is the model doing the same thing as the reference answer you can do this using sort of deterministic things like semantic similarity like you can embed these two answers and see how close together they are or you can ask another llm like is the answer that um I know is correct and the answer that the model generated are these two things factually consistent so you can write a prompt to have an llm run that evaluation um if you have um two different answers so you have one model that says this is the answer in another model that says this is the answer you can again ask another language model which of these two answers is better according to some criteria so you can write a prompt that says um okay your job is to assess the factual accuracy of these two answers um to the question here's the question here's answer a and here's answer B which one is more factually accurate um you can also write metrics that assess whether the the output of the language model incorporates feedback from an old from the uh from feedback given on an old output so if um you run your model and uh you know one of your friends says Hey like I asked this a question about Taylor Swift and it um gave me back an answer that was you know about Taylor Swift's old albums and I wanted an answer about the new albums well you can write down that feedback and then um you can take the question that the user asked you can run your new language model on that question look at the output and then ask an evaluation model whether the new output incorporates that feedback on which albums from your friend and so the language model is assessing whether the new answer incorporates the feedback on the old answer and that can be a pretty effective way to evaluate uh the model and then finally you know if you don't have access to any of that then you can compute static metrics on the output of a single model so the you know the the easiest and most deterministic way is to verify that the output has the right structure but you know more interesting but harder to get right is asking the model to grade the answer like give it a score on a scale of one to five so um I I want to give the model score a score on a score on a scale of one to five as to like how um how factually correct the answer is for example um okay so you know what I think this a lot of these metrics incorporate this kind of key idea here which is using models to evaluate other models um what you might be thinking is that sounds like kind of a crazy idea right like how how do we verify that the model doing the evaluation itself is actually correct and that's true um but this is empirically this is still very useful and like a lot of papers are sort of moving in this direction for doing evaluation um so the reason why is that automated evaluation using language models can unlock more parallel experimentation if you need to rely on humans to do your evaluation then that's just going to really slow you down in running your experiments you still before you actually um roll out a model into production that was only evaluated using other models you probably still want to do some manual checks um and uh and that'll give you more confidence that it's actually doing the right thing um more concretely the research literature has been identifying some limitations of using language models to evaluate other llms and um there's really two like kind of categories of criticism um the first is that there's a number of papers that are have been discovering biases in the llm evaluation of other LMS so um some of those biases have been you know if you're asking models to Output a score on a scale of one to five the LMS tend to prefer like one number over other numbers um models tend to prefer their own outputs so if you're asking gpt4 to evaluate you know Claude and gpt4 it's going to be a little bit biased towards its own answers um maybe even over human answers the if you're asking a model to compare to two different answers some research has found that the order of the answer is like which one comes first in the evaluation actually matters and language models also tend to prefer like kind of characteristics of the text that might not have anything to do with whether it's correct or not like which one is longer so there's a bunch of biases in language model evaluation um I think all of these have paths to being mitigated and but it's just something to be aware of if you're going to use this today and then the second category of um of objection to this is like okay well if we're if the problem with language models the reason why they need to be evaluated is that they're unreliable then why are we trusting language models to do the evaluation so I think the way forward here is really not either human evaluation or language model evaluation but it's a smart combination of both where um you use the best of both of both of these things to get great evaluations so as a developer you primarily interact with a an auto evaluator um and the reason why you do that is because that lets you move fast right like every change you make you can Auto evaluate and determine whether it's actually worth spending the money to do a human eval but then that auto evaluation should be verified on your task um and the way that you verify that is through high quality human evaluation um so last thing I want to talk about before kind of showing you this in like a more practical setting is the role of human evaluation um so there's different like ways that people commonly collect feedback from humans on language model outputs the probably the most common one that you'll see is just asking people to rate this the answer on a score from one to five this is maybe the most common way of doing this but it's also um uh probably the worst and the issue is that people are inconsistent right so what one person says is a three might be a four for someone else or two for someone else and that makes it really difficult to make metrics that you compute on top of these assessments reliable um to kind of combat that a lot of the fuels moved from asking people to rate individual responses to asking them to compare two responses and so people are more reliable at saying you know which of A or B is better than they are giving a or b a score um and so this is kind of part of why um you know a versus B evaluations are what's typically used in reinforcing learning from you and feedback and this is how a lot of the kind of evaluation in the field is moving um this is also not without its challenges so when people are doing a versus B evaluations or um then like one big issue is that human evaluators tend to look at surface level attributes of the um of of the outputs rather than the actual um sort of factual accuracy of them and so um this is a really interesting paper called The False promise of imitating proprietary llms where they did kind of a an assessment of this and they found that um when you know like there's been a lot of Buzz about um open source fine-tuned llms being close to GPT 3.5 quality um it turns out that they're close to gbt 3.5 in terms of human preference but they're not very close in terms of like the actual factors factual accuracy and so humans are kind of picking up on like how things are worded and formatted um in to a much larger degree than like the underlying accuracy of the statements when they do their evaluations and so kind of one research direction that the field is moving in is moving away from just asking people to like read a whole statement and then give it a score or a comparison score and instead asking people to give more fine-grained evaluations so rather than looking at the entire output and saying like is this good or is this bad these approaches um are asking humans to select a particular passage within the sentence that is irrelevant or untruthful um and some of the early Research indicates that this might be a more reliable way of getting evaluations from humans um but at the end of the day human just like llm evaluation human evaluation is also highly limited um so the main limitations are quality cost and speed um the reality today is that gpd4 writes better evaluations than most people that you hire on mturk so um which is like kind of surprising and crazy fact but I I was reading somewhere I think maybe this is a paper that came out in the past week or so that a very large percentage of people on Turk are just using you know gft 3.5 or gpt4 to write their evaluations anyway so maybe it's not that surprising um and with you know the second big challenge for quality is that if unless you design your experiment really carefully human evaluators might not measure the thing that you really care about right so again like if you're just asking people for their preference a lot of times their preference is going to be based on surface level attributes of the text not underlying factuality but human evaluation is also really costly and really slow right so this is not this is not like the Silver Bullet answer for eval um again coming back to you know I think the way forward here is human verified um automated eval with language models all right um final thing I want to talk about is I want to just kind of like show you what this looks like a little bit more concretely um and so we're gonna um talk through like a process that you can use to evaluate language models progressively on data that looks like your data from production using metrics that correspond to the things that you actually care about and so the way this works is kind of analogous to test driven development in you know traditional software development so start out with getting a simple version of your model out there in production running in the real world then you gather feedback on how that model is doing you use that feedback to build to like add your evaluation data set um and also to iterate on your prompt or your model to try to improve performance when it's when you think the model is good enough then you run a systematic evaluation um using Auto eval using human eval whatever technique makes the most sense for the stage of the project that you're in then if you're in a bigger company you'll approve those those changes you'll deploy the model and start this this Loop over again um so I will flip over here um yeah so let's walk through like a simple example of this so what we're going to do is we're going to write um a we're going to build like a simple application to do some grammar correction so um let's write a like a naive prompt like correct the grammar of the users and then let's add the user's inputs okay so this is like a naive prop uh prompt that might use for a grammar correction and so like let me just get a quick sense check for whether this does something reasonable at all so let's do like um this sentence as bad grammars and then I don't know another one the sentence has okay okay grammar let's just see let's just make sure the model does something reasonable um Okay so high level sense check this seems fine right but the question is now like okay just because it worked on these two examples that I just made up on the spot how do we know that it's actually working more generally than that so that's where evaluation comes in um so we'll take a look at this this prompt that we just wrote and we'll run an evaluation on this and again the two key challenges for evaluation are where do you get the data and then what are the what metrics do you use what criteria do you use to evaluate performance um and so in Gantry our approach to both is using LMS to help you so we don't have any data to evaluate this on yet so let's evalu let's um generate some so I'll describe like the sentence that I want and in this case we want like sentences with bad grammar so this is going to hit open AI API behind the scenes um and it's going to pull back like give us some examples of sentences that have this characteristic right so there's like some interesting ones here she don't like spicy food um eyeballing this it seems like these are pretty reasonable examples that we might want our model to perform well on so let's create a new data center on this called needles um and then the second question is evaluation criteria and so um what we're going to do here is we're going to write like a simple prompt to describe to the model um what characteristics we want the output to have in this case we want the the outputs uh to be grammatically correct um and so now we have this data set and we have this about this evaluation criteria so let's run this and this is like a take a few seconds because we're you know subject to open AI uh latency as we all are in the field these days um but what's going on behind the scenes is we're generating the outputs for that prompt template that I wrote for each of the inputs and then we're evaluating them using the prompt that we wrote as a criteria and so what we see here is like maybe not too surprising the model's doing really well on these examples because these are simple examples simple prompts models doing fine and so it's getting five out of five on all of them um and uh um if we want to if we want to understand why the model thinks that this is a good grammar correction we can look at it as Chain of Thought as well so we can see the reasoning that the model applied to give the to give the answer to the score so in the real world we'd probably do some more like validation of this model but for the sake of this demo I'm just going to deploy this yellow this into prod and let's do um let's make this a little bit more interactive so I'm going to drop a link um here let me just verify that this is working yep um so I'll drop a link here and this is like you can imagine this as being what you did say again there's a couple of questions I thought you might be interested in the next eight minutes to also touch on as well yeah um we'd love to kind of like show the make this more concrete with the rest of the loop um and then hopefully I'll get to the questions in like the last five minutes um so I dropped a link um feel free to drop in and kind of um you know play around with this and you know interact with this as a user and give feedback on like whether this is doing the right thing or not um I guess while folks do that just for the sake of time I'll just you know move into kind of the next phase of this right which is we talked about evaluation data sets are not static these are something that you should continue to build um using production data as it comes in so I um have like pre-loaded some production data in here or some you know fake production data in here and um what we see is like kind of some high level statistics on what's going on and what we're looking for here is we're looking for examples that are interesting to add to our evaluation data set and to use to um potentially make our model better right so we can you know we can look at all the inputs that people are sending in let's see if people are actually sending inputs in um oh yeah here are the ones that I sent in I guess people are still working on it um but you know one thing that we notice here is um there's there's inputs here that are in French so if we zoom in on these inputs and see what the model is doing um in for some of these inputs it's translating the French to English which is really probably not what we want like not a characteristic that we want our model to have so again we're going to progressively build our evaluation set by adding these to this evaluation data set um and so we jump over to this evaluation data set now we have these like 10 examples that we generated as our starting point as well as the four um from production where our model wasn't doing the right thing and so we can um now make an improvement to our prompts to you know try to fix these issues so I'll just you know I'll copy and paste the one that I pre-wrote for this purpose um I guess it's loading let me let me just do a little live prompt dictionary so it's uh let's do um uh maintain the language um and so what you do is you would you know make your changes to your prompt that you think um would fix the problem you found in production then you go back you would evaluate this new version compared to the last version um and then maybe add some additional evaluation criteria like you know outputs should be in the same language as the inputs and then rerun your evaluation and so what will happen behind the scenes here is you'll take your new prompt and your old prompts you'll run both of those against all of the evaluation data that you've collected so both the data that you started with as well as the data that you added from prod um and then you'll ask a model to compare the performance of those two versions on those inputs and that's kind of the um this sort of iterative process of building an evaluation set and evaluation criteria as you develop your model and so that's kind of the the process that I think makes the most sense for um for building these things going forward so um this is a good pausing point for me to um answer questions right we're almost out of time so I'll try to take the the the questions and Order and the first question was was from any mentioned would psychometric data preparation methods be helpful for evaluating llms uh I don't know what psychometric data preparation methods are um would you mind expanding on those and then happy to happy together okay give it a shot yeah so please expand on it in the chat but again so the next question is from Christopher and um he asked what are the best ways to evaluate one's own fine-tuned LMS you know other open source tools and Frameworks to do this yeah I mean it's kind of following this the same process that um that I uh that I advocated here for right which is like Define your task um come up with some data that you think represents your task uh come up with some some criteria or some metrics run the open source model on those criteria and those metrics um get a sense for performance and then build your evaluation set over time and Gantry is a great tool for doing that great great nice thanks for sharing that and on this next question is from Nathan now um if you're if you're using llms to make the test content how different is it uh how different can the model generate in the test cases be compared to the model being tested I think you touched on this earlier with llms evaluating llms I don't know yeah I think the for generating the test cases um it's a little bit different because um you know the test cases it's it's um it's a different type of issue if the test cases are biased than if the evaluation is biased so if the evaluation is biased then you might um then even if you have the right data you might get the wrong idea about whether the model is doing well in that data um but if you just have the wrong data like if if the data generation process is biased then you still are able to kind of continue this iteration process right so the mindset should be you're not going to have a perfect data set to evaluate your model on at any point and it's something that you should always be improving so I'm a lot less worried about um about you know folks using biased models to generate evaluate generate their initial evaluation set than I am about um using uh biased models to create the evaluation itself yeah and still on the llm's evaluation side of things this question from Jose and he asked you know our GPT models like um General proposed models like um GPT 3.5 better it's generated versus evaluating um um other other results they're better at evaluating so the the um the the mental model you should have is like um this isn't like strictly true but at kind of first order approximation um models think like one token at a time so if you um if you ask a model um what is you know uh like what is the answer to a specific question then you're relying on it to get the to have the next token be like the right answer um and so like one in general in prompt engineering like one way to get more reliable answers for models is to let them think before they answer so you know write up think step by step write out your reasoning and then answer um and evaluation takes that a step further which is that you give the model access to both the question and the full answer and then allow it to think about whether that answer is correct or not and so evaluation tends to be more reliable than generation for our lunch I think you already touched on James and James question and whether you think fine-tunes smaller llms to evaluate models I think there's there's research and that's um could you sort of share that so the question is does it make sense to fine-tune LMS for evaluation yes I do think that makes sense did you have you know examples of research that's been done there um I don't have any examples of research that's being done there now um but I think people are working on it all right so I think this we can take two more questions before the workshop events and the first question from ASA can here is uh how about giving a minimal pair where one is grammatical and the other one is on grammatical and acts in the system to pick the ungrammatical one will that be a better evaluation criteria well um maybe but it's I um I Think It's tricky to set that up right because where do you get the grammatical and ungrammatical answer from um I think if you wanted to follow like more of a valuation best practices for this then what you do is kind of the last step that we didn't quite get to is having a model compare two answers and ask which is more grammatical so you know if you have you made a change to your prompts then you could ask a model like okay did this change improve how grammatical this was or not right it makes it makes one more question before we um help off from [Music] um Mohan and he acts based on your experience apart from the human evaluator uh what are the suitable metrics and approach can be considered when doing annotation with uh llms um yeah I think it's kind of you know if for anything that you can have humans annotate you can also have llms annotate um I think generally speaking if unless you know what the answer is like unless you're comparing something where it has to be exactly the same in which case you know normal metrics work well if the if the right answer is ambiguous then having llms through the evaluation is probably going to be better than any other metric they're going to come up with right okay I just got a signal here that we can take one more question and I think that should be the last question from um Yuan and yes does it make sense to um ask the llms to evaluate their own outputes in the same problems where it generates the ampl what's the what was the feedback look look like yeah totally um so there is like there you know you can use a lot of these evaluation techniques in your chain to make your model outputs better and so this is there's a paper called like self-critique which I think was the first um maybe the first really uh at least like the paper that covered that or the first one that I'm aware of um and so you can apply the evaluation in the loop with the llm um as part of you know like having the model improve its own outputs um and that can work reasonably well you can see lifts and performance obviously what you're trading off is cost and latency at inference time um and so it doesn't make sense for all use cases right and you know do you think any special prompt engineering needs to happen there for that to be really effective uh I I think so yeah yeah I think so it's um okay the uh you know hopefully not too much but I think the um there's uh you know getting we've found as we've built out some of these automated evaluations that like creating the evaluation prompts um is uh like the how good an output that you get is somewhat sensitive to that and so I think um you know there's we need like best practices to emerge and I think that's also a place where um companies like Gantry can just help the rest of the field um answer those questions for everyone and then hopefully not everyone needs to figure that out from scratch great um thank you so much Josh um I think people are super excited about the the your talk it's been really really comprehensive one so far thank you so much great thank you and um I'll just drop a link in the chat here um if folks wants to um play around like with the the Gantry ux that I just showed um we're kind of running an alpha right now and so I'll just drop a link here that gives everyone access to play around with it um totally free um just let us know what you think yeah awesome that was super helpful so thanks a lot everyone um I think we can all join the um other talks now thank you all right great thanks [Music]