I have you on board and of course have a Mac on board Mac is uh the co-founder of primo and uh he's gonna be sharing with us the uh how to fine tune it's going to be talking about fine TNN of llms and you know physically all you need to know around about fine-tuning um especially with Primo of course so um before we go on there's a GitHub repository that you can that's in the chat so this is just really help you with the walkthrough so you can open that somewhere some other Tab and then you know during the workshop as well that's mostly going to be the repository for reference uh as we walk through with uh with Mac Mac is going to be sharing a bit more about what Primo does um during his talk and then uh excited to have your board Mac again so if you're a few housekeeping rules if you have any question you know if you have any concerns during the workshop please leave your questions in the chat happy to uh we're happy to respond and then that could maybe occasionally check in or towards the end of the of this Workshop um check in to see how uh maybe we can get your questions answered if you have any other concerns at all just put them in the chat as well would be willing to sort of help or support you as you go through the workshop as well um thank you and have a great time yeah great thanks Steve for the Stephen for the um introduction um today's Workshop primarily be involved we'll figure out how to get the data to actually fine-tune um on two important parts of it it's the quantity and the quality of the data and then actually running a fine-tuning job um to fine-tune a fairly large model so uh I wanna actually talk about a couple things with respect to the workshop in itself everybody should be able to run through the prompt engineering portion where we dig into a data set and want to manipulate it and synthesize more data however actually fine-tuning the model um may not be accessible to everyone just because uh as the assumption that I have in terms of your run time um you're going to have to need some pretty beefy gpus to do that so on that section of things you won't exactly be able to run all of the um the scripts that I'm going to be showing mostly because you'll need at least uh 40 gigabytes of GPU vram in order to do that um that being said uh if everybody would just clone the GitHub repo so then you could have it locally and then um the first thing that you should do is actually run a poetry install inside of the repository to get all of the library dependencies that you need um for the workshop uh before that I I'm actually going to be going back and forth between slides as well as um as well as my uh code so um I'm going to interleave between the two sections present some of the motivation as well as setting up like the theoretical foundations that you need and then actually going and digging in to the code to um deliver like actual um uh value out of the the fine tuning exercises so um that being said maybe I'll start off with um just introducing myself um I'm Mark I'm the co-founder of Primo I have the uh AI practice um I think that uh you know we're in a very interesting time uh particularly with all of the language models that are being um open sourced as well as all of the new commercial uh language models that are being um are coming out from the larger Enterprises that we see today open AI cohere Etc so uh part of the talk what we're really gonna do is think about how do we actually leverage language models in particular open source language models and customize them for our own purposes um the the motivation would be how do we launch the next uh million AI apps so Primo the company that I founded is um trying to democratize access to language models in allow developers to embed the language models into their production applications um we're going to be launching our product in the summer so then um everybody should have access to it and and hopefully um what we want to present to everybody is the easiest solution um to fine-tune and serve your models so um to start some motivation on the different types of strategies and approaches that you actually want to take in terms of Which models to choose so um typically all the modeling approaches we have uh are you have closed Source models everybody sort of knows gpt4 and Google has their competitor Palm 2 you can also take open source models that are fine-tuned such as star coder and replit these were two coding examples and then we have like smaller specialized models that are the traditional models that showed up you know prior to 2018 that everybody's pretty familiar with um we're not going to go into those mostly because what we really want is to be able to do uh generative AI right so um typically when customers come to us they question the fact of wanting to fine-tune and customize your own model so um open AI is best in class and certainly you can go to it and think hey why don't I just have my uh open uh GPT for X model GPT for legal GPT for finance all those type of different um domains um that's actually not possible so it's really only limited to their base models so most of those are the uh uh text DaVinci um uh flavors and you're able to get they're fine-tuning apis but these are actually not the best of breed models necessarily these are um just models that uh they present and are openly available but um they are certainly um oh sorry I didn't know I was in screen sharing sorry um yeah so kind of going back on the slides uh apologize for that um we have a few modeling approaches here I show those um so you know in the backdrop we have open AI or we have um a lot of the open source fine-tuned models we're gonna be focusing on flannel 2 today it's a pretty powerful model 20 billion parameters openly available open license and also has been shown to be quite uh useful for NLP tasks so you know I kind of presented the information of why you can't really use open AI you can only use their older models and fine-tune those but gpt4 is just a black box model that you can only run in in at best uh do prompt engineering around in order to get the behavior that you want but it's not customizable in the way that you actually want for your Enterprise purposes so um with respect to why you want custom language models versus closed Source models um the important aspect about those are model ownership with respect to having your intellectual property encompassed into a modeling pipeline for that as well as meeting certain customer slas so uh you are you have to meet an SLA to your customer but your customer doesn't really care if you're relying on uh open Ai and they're down for two hours or so you want domain expertise too as well in terms of your custom models so everybody wants to be a you know a a master of their domain and be an expert there so you're seeing a lot of language models pop up that are specifically designed for a a set of tasks Downstream tasks and finally you have security and privacy so we work with a lot of clients personally that want to have models governed within their virtual private cloud or on-prem and they have fairly restrictive policies around where data can be moved in and out of so um that basically precludes them from ever being able to use open AI or the closed Source foundational models so um just to preface what we're actually going to be focusing on in terms of supervised fine tuning um for the rest of the workshop we're going to be doing prompt based fine tuning there's other types like multi-tasks fuchsia and domain specific but uh the the recent literature and the recent set of um the recent set of uh technology that's been released has shown that instruction fine-tuned models uh tend to be the best because they can be zero shot learners for tasks and they understand what you want uh to do so in motivating this I would like to uh kind of delve into what we need to think about do you think that we can actually just um come up with a model in um come up with a model and immediately fine-tune it on anything I think that what you actually need to do is you need to start off in select your task properly in order to figure out um what proxies and what types of data sets that you need there so our task framework that we actually use at Primo um is to uh kind of think about things from a knowledge based task or a reasoning task um in terms of the knowledge-based tasks you can think about uh wanting to have a custom model that's really proficient at doing the set of tasks that you would think are pretty basic and necessary such as name entity recognition certain basic q a or have it be well really well positioned for doing complex tasks such as those that you would expect from you know a knowledge worker and this is these were actually one of the more interesting use cases that are appearing today now on the reasoning side we usually break that down between coding and math so then on the coding side you either want to generate new code or you want to explain a set of code so I would encourage everybody to look at Helm which is an open source initiative which uh where the Stanford University is presenting in trying to create a living breathing instead of evals um for figuring out how to compare across multiple different language models um it's becoming the standard in terms of benchmarking these models and figuring out you know how they actually do um we can actually just go to the website right now and take a look so in terms of the results I would say for question answer you can look on top and it's no surprise actually that uh Da Vinci is the best in terms of the few shot or zero shot um results but um yeah you can kind of use that to proxy for the things that you actually want to accomplish um using your language now um what are the actual challenges that come about so this is the important part that we're going to jump into the notebook uh in a second um so consider the scenario where you actually have your base model you have you typically have two different directions that you'll uh go towards and usually you're choosing between quantity or quality in terms of your data so on the quantity side you can reach for synthetic data or on the quality side you can actually have humans annotate and label the data oftentimes on the quantity side the main challenges are having the wrong format that's not instruction tune instruction data or having just poor quality data that's misaligned with your task and then on the quality side you'll usually see that it's expensive so you have too few examples or you've just spent way too much money for a lot of examples um so how do we actually get around that well what you actually need is a type of data synthesis pipeline in order to generate uh High quantity data as well as high quality data so in motivating the synthesis I want to present um a prototype of uh data synthesis pipeline that uh my research team has worked on um so on the diagram that I'm showing uh we're going to walk through in the notebook what we actually want to do is start out with a prompt generator which will generate the prompts and these are just this is just a string that's um sent into a large language model and then um the inference module itself is a language model so what the language model is doing is it's just producing completions uh or responses uh is the other name for it um with respect to that this top section from prompt generator to inference module and completions is what we typically would call Prompt engineering so in doing that it's a you know iterative process with human being tinkering with the language or the format or the template of the prompt to actually make this scale or make this an actually automated process what you can actually do is you send it into a response post processor in this case an example could be um a processor such as self-consistency so you're getting a number of completions out of there and what you do is the post processor will filter out the responses based upon some majority vote and the majority vote is just like which is the most common answer it doesn't actually involve verification yet but what we want to do is actually get towards the reasoning path so given the filtered responses we can actually move that into an evaluator and the evaluator is actually a reward model that figures out whether or not to run the loop again into the language model to improve the data or um determine if the the set of generated data is already useful for uh adding into this synthetic data set so it's kind of um a a recursive Loop that happens here so um if you actually want to do some further reading on data synthesis we have a really good blog post from one of our scientists um that you know covers a wide range of the wide range of the research and also how to delve into it um that said uh let's move over into the notebook itself so um if everybody can go into the prompt engineering notebook um you can start running some I'll run through some of the code with you and also explain what's actually going on in some of the steps um with that being said maybe uh if anybody has had any problems with installing the dependencies um you could probably send that into the chat and then we can figure it out if not um uh uh I'll determine I guess from whatever hand hands up or thumbs up if everybody's already gotten their environment um uh installed correctly foreign [Music] does everybody um I think the first thing that you have to make sure of in your uh poetry environment is that you change um the python version to the version that you have running so one thing you could do is just do python version um I've tested this on python39 and 310 I'm not exactly sure if 3 8 will work in this environment cool and uh if that's the case um I would say maybe in the public repo you can create some issues in there and then um we can kind of improve the the uh the set of dependencies or um actually figure out what's going on uh just for the purpose of the time um I think we can kind of move on and figure out um uh what to do there um and I appreciate everybody who's sort of uh interacting in the chat and helping other folks in the workshop uh figure out how to install dependencies um typically what I like to say is the only thing that is getting in the blocker for artificial generative intelligence is python dependency resolution so um that being said let's go into the prompt engineering notebook um so you already I actually already uploaded the set of data that you can um work with here um you may have to change your constants file mostly because maybe your home directory or path or wherever you get cloned is not specifically uh set up this way but um if you have that all running you should be able to kind of download the the data set cool it kind of seems like you know most people are getting the dependencies installed correctly due to my bad in terms of uh the the python versioning um so remember we had kind of had the motivation over here that what we want to do is we want to have um prompt generator to generate prompts and feed into a language model so we're kind of on this step right now so just run um you know some of the dependency Imports right here what I actually have are a set of math questions and answers and the goal the entire goal of this is for the language model to uh produce answers to questions that we don't already have answers for so I'll repeat right now this file is question answered pairs and we want to be able we want the language model to generate answers to another file in the data sets that is math questions only [Music] um in terms of what to expect I'll help motivate this by looking at one of these problems and I already am choosing the easiest problem because reading out these other problems starts to make me confused like I was during the SAT um what it actually says is uh James decides to run three Sprints uh three times a week he runs 60 meters each Sprint how many meters does he run a week so the answer to that is three Sprints three times a week which equals nine and then nine times sixty for the 16 meters so then you have 540 meters you'll notice that the answer actually provides a reasoning path so back on to what we wanted right we wanted the answers to be provided step by step and a lot of the literature um tends to talk about Chain of Thought you can look at that up and turn uh in terms of all the papers that are written about that but that is basically what we want if we want to get the rationales and have the language or not uh model learn the rationales to produce the correct answer on an unseen question so the next part that we need to do is we need to uh if you run the cell what we're going to do is that we're going to sample uh and create our sampling pool of examples that we're going to feed into a prompt so it's all basically from a software level it's just string concatenation so um this produces a list of exemplars so we chose eight exemplars and that is because that tends to be um the standard in the literature if you choose too many what actually happens is you're kind of interrupt [Music] can you hear me yes just can you zoom in a bit so it's easier to see your screen sure sure so there should be a list of eight now um interesting trying to see if somebody has [Music] questions here um looks like a poetry installed um so for those who actually have the the environment installed sorry if if some folks don't have it but um you'll get a list of eight question answer pairs all here and on this next cell if everybody run it we're gonna take the math questions that we want the answers for and then pair them with a meta prompt so basically we're telling the language model how do what do we want it to do so solve the math problem step by step and uh provide an answer to that so what do I expect coming out of it is that I will have the actual set of instructions at the top for the language model and then a set of eight question answers are problem answers with an answer blank at the very end so the question that we actually want answered is this one Dax went to their Farm to pick some apples and found half as many bugs as ants in the garden there are 50 ants calculate the total number of insects in the garden so we expect the answer to be produced from here um here I would actually ask for everybody to change the base model if you're running it locally so I would say you should run Google uh long E5 small so that's probably one of the smaller models that you can download and actually run locally um but in my case I actually want to show you why you need a really powerful model so it'd be interesting to see maybe some folks can post their responses for what's actually generated soon um but here I'm just loading flon ul2 into memory and uh it's uh to be frank it's going to take a little bit of time but um it's a 20 billion parameter model so at half Precision which is um uh two bytes per parameter it's going to occupy about 40 gigabytes of memory so if you all are running on CPUs it won't be as much of a a problem the only main problem is if you're running on gpus you're gonna get cudaum errors if you don't have enough memory um to support this model so uh as it's just loading the shards I'm gonna um look into the next set of uh cells for the code to generate the completions so what we're doing right now is um I'm gonna be uh generating uh so I'm going to be providing the configurations for the model to um break down the uh Break Down The Prompt string that I have and then generate the set of code uh let's generate the set of uh responses that I want there so as you see we have the shards are loaded and then I'm going to just run the inference module part of our pipeline so the interest module is the language model itself so give it a little bit of time uh llm inference is a known problem today so um probably it's going to take a couple seconds cool so uh if we look at the the answer to the response if there were half as many bugs that's in the garden there are 50 to about two 25 bucks total number of insects equals 25 plus 50 answer 75. let's go back on the question I just want to double check that it's sensible so you know you're trying to get the number of ants in the or the number of insects in the garden that's actually fairly sensible right um what it did is it provides not only the set of the correct answer here the answer is uh 75 but it also um provides like step by step uh the Intuition or the the rationale for figuring out the answer um just to bring us back onto where in the pipeline we are so we've just created a prompt we sent it into the inference module and then we've generated responses so in our case we're actually just generating one response I could run this uh so in actuality when we internally at Primo when we are generating new data we have to run this language model 32 times per set of uh problem question to generate you know a set of 40 000 data points that's going to take a little bit time and uh we don't really have hours to spend on this particular exercise but um this kind of gives you a sense of like what you need to run in this pipeline to get like a singular uh answer with rationales out of it so um we have to create a contrived example here mostly to demonstrate the self-consistency element so assume that suppose we had uh four answers being generated from the above pipeline so in this case here are like a set of four answers um with the uh reasoning paths here um the whole point of this is actually just to get a majority vote so which is the most common answer so that involves pre-processing the string and then um choosing or fill sorry filtering out the non-majority answers so you get the majority answer which is typically the right answer if you have a powerful enough language model but it would be interesting to see for you folks when you're running it um when you do flaw on T5 small if it's just outputting you know nonsensical completions because it's just not powerful enough that would be my presumption so um here are kind of the list of completions that we're working with and you'll notice that 24 clearly is the correct or the majority answer so all we're trying to do here is just get the list of completions and then we're going to um fill the grid dictionary for separating out the completions that have a answer of 34 and the completions with an answer of 24. so it should be a three to one ratio in the dictionary that's being produced here so that is doing the uh part where we're finally going to move into self-consistency so we have the set of answers and we just want to filter it out so looking back on the post processor we're getting the filtered responses so in this case we went from three so we get three responses where we begin with four responses here so that's kind of the whole point of it how do you filter out the um nonsensical uh completions that we either don't want the reasoning path for or um aren't really useful for us now the final part of the pipeline is the evaluator and this is kind of the most tricky part so the important intuition behind that part is that uh we want to get a diversity of responses to increase the complexity of the data set so Rouge L um is a statistical measure for the amount of overlap between um strings so in this case what you'll see when I run this cell is that we're taking um a candidate and reference set of sentences so um they both semantically mean the same thing but we're comparing them to each other to figure out which one has the least amount of um basically the missed amount of content for the most amount of semantic meaning so the the point of that is language models want to have more diversity in their sense of reasoning paths in order to be able to incorrect resuming class in order to be able to learn better so um this is the evaluation step in which we are trying to um uh choose the set of uh completions that we actually want to keep in our synthetic data set so uh on this part what we're actually doing if you run this cell is we're getting the average pairwise Rouge scores so here is the each part of the list here zero and one are actually the Rouge L scores in comparison to this sentence with every other sentence in taking the average stat and then here's just the average of the average so pretty meta there um finally running this cell what we get is what we expect which is uh a very important distinction here so this sentence says hi it's nice to meet everyone right and what are we comparing there why did we keep that particular sentence there's hi it's nice to meet everyone versus hi everyone it's nice to meet it's a little hard to specifically uh kind of catch the difference but the candidate uh sentence object here is actually a little bit more fluff versus the reference one so it's you know shorter so the reference one actually contains more meaning which is it's nice to meet right whereas the one above says it's nice to meet you as well so um you're taking the the substrings here and then figuring out which ones which parts are overlapping between the two and then um creating a metric that shows diversity between those so in this case right here we're filtering out the Rouge scores that um are too high because a really high risk gorgeous means everything is clustered together and we're not getting good diversity in our data sets so um that's kind of what we're doing here uh and what's the Rouge evaluator we now can see you know the rush scores for all of this data sets um I'll kind of move on because you know for the purpose of time I want to get into the fine tuning element of things um this notebook everybody can kind of use to uh play around with um send your own completions to particularly it's well suited for math right now um but I've generated you know before you everybody got here I've also already previously generated a bunch of GSM 8K um uh uh sorry GSM 8K uh synthetic data so I take the original GSM 8K data and then I synthesize more data on top of it and improved it for the flannel2 model um that brings us into the next step of the uh Workshop so I think everybody probably across the entire Community is very interested in um supervised fine tuning um we already have now demonstrated that we can take uh data and actually get more quantities of data and higher qualities of data as well but um I wanna motivate our discussion by presenting an example of why it's important to count uh you know the amount of memory that you have at your disposal so um when you do naive data parallelism let's do some back of the envelope calculations if you have a 15 billion parameter model you're going to have to hold uh two bytes per parameter in the model in half position and then um you actually have to keep a master copy of the other parameters in full position which is four bytes per parameter and then if you're using atom which is the Workhorse Optimizer for basically all of training for um The Learning gradient descent you're gonna have to keep two variables momentum in the running average in memory as well in full precision and then you have four bytes which are um must also be four bytes per 15 billion parameters for all the gradients so this doesn't included activations but you're already running at a total of 270 gigabytes of vram you can look up the cost of a100 right now online they're pretty expensive um so we need a way of really reducing um reducing the amount of memory footprint in these models so what does that actually look like in terms of uh all of the breakdown of memory um in terms of the activations you're going to expect 25 in the activations 25 in the model parameters and then um 50 in the optimizer um I can illustrate that in a second but uh why does that matter so you have a huge language model where all of these things are sitting into memory and what do we what's the whole point of attention attention is trying to provide more context right like the if you remember what attention is defined as it's like the cross product of all the contacts you pass in so the context is just the huge string or the prompt that you're giving a language model and then you're taking like a cross product of every token in that prompt uh to every other token so um that is why sequence length is actually a memory killer it's going so even though um you can increase your batch size the moment you increase your sequence length the amount of memory footprint is goes up quadratically so I actually took the math GSM 8K data set and then uh plotted a distribution of the token lines here luckily we're okay in our instance because most of the tokens as you can see are less than about uh 256 uh token length that's going to come back to us in a second where let's take a look at the data set itself um so let's open up the notebook actually for the GSM 8K pre-processing um if everybody's able to go to this notebook um uh I think that it would be useful to kind of dig into the data itself so um if we can run this cell uh you'll see um in this instance you'll have to change uh the model path um if everybody else was using flan uh you all or sorry if everybody wants to use a different model such as the um Quantified small which maybe you downloaded uh previously these are the this is basically the line that you'll have to change um I already have the data downloaded but if you don't have the data set downloaded you can change the the data set in here in the data files into a different uh data path so in this case it's the math gsma K data set if you look up on hugging face data sets um you basically just have to point to that path to download the data set memory um but let's just take a look at the data itself and then consider why I want to cut off the token length in order to facilitate um better processing and memory so um you can run it here I'm gonna download the data set it's already cached in my instance so there's not much I need to do um take a close look at the pre-process function um in here the most important parts to note are the max lengths so I'm basically uh you can't have uh in you know generally in matrix multiplication you have to have a a consistent length in your tensor right like each row can be a different length than a tensor otherwise it's kind of a weird shaped um Matrix so um in in our case I we I want to cut down to 256 tokens at most and then um for the ones that are too short that's why I have padding um I'm going to pad them and basically you're going to see a bunch of uh I think negatives for the padding token or negative 100 um and then uh for the ones that are too long I'm just going to cut them off uh you can use a different preprocessor where uh you actually extend the tokens to be one long set of tokens where you don't cut it off but um that's not really within the scope of this particular discussion so I'm just going to run this and using logging faces data sets I can just tokenize the data really quickly so um let's take a look at the question here so again this is the map gsmak uh data set that we synthesized and we have a bunch of questions here there are numerous other questions if you want to look at the first five and um that's the raw data set let's look at the tokenized data sets and see kind of what's going on here so in the tokenized data sets as you can see this particular uh the first question answer that we have here or sorry this is the first uh question um is shorter than the 256 token length how do I know that I know that because as you can see at the end in our case the padding token is just one so it's just adding all the padding into the um the tensor so that uh when I pass it into um the large language model it kind of considers this as you know a uh you know a null token um if you've kind of heard of uh you know industry tokens or padding tokens that's the intuition behind all of it you're just mapping uh empty space or your mapping uh out of vocabulary uh tokens into a particular numerical embedding or a number so um I'm gonna go back to the slide for a second to talk about what's actually going on and how we're going to get across the problem of the uh the Kuda um error so I'm actually gonna uh go over to um another part of the uh the workshop where I have a script here so you guys don't need to exactly know uh the exact details of every part of the script but if you look at it what I'm actually doing in the script is I'm just running the pre-processing and then I'm going to run uh exactly what I showed on this slide so naive beta parallelism here all right yeah naive data parallelism with uh without Laura so um I have to run this as a script because I need a launcher in order to do that but um if you take a look I want to show you the number of gpus I have running on this virtual machine so I'm running on four uh 40 gigabyte a100s cards so this is an extremely beefy uh runtime environment that most people just don't have access to um and it is going to even not be enough for me to run fine tuning on our synthesized data set so if you look into the readme that I provided you'll see the script that you need to run in order to accomplish this so um wherever you have the model path um so this the end of here this is going to be the local model path uh of the model that you want to run but um in our case it's going to be flon you all too and I'm going to be using um some 90 parallelism in order to run this and uh as you can see it's gonna kick off a job oh sorry I actually kicked off the uh correct job that will not happen out of memory error but [Music] in this case um correctly so in this case I'm actually going to be running um a job uh there is a really useful command that you can run to profile your data in its NV top if you have that tool downloaded but right here you can already see that it's loading the model shards into memory and um what deep speed so the library that I use is a hugging face accelerator accelerate to launch this job using naive data parallelism and it pre-allocates the set of memory that I need in order to accomplish the the job you'll see that it's almost at 100 already and it's just loading in the the model and it's going to um basically have an out of memory error fairly soon [Music] yeah so someone put in a question is it using all four gpus without having to code for parallelism it is actually using all four gpus um accelerates very easy uh I'm not gonna go into the exact code but um what everybody should notice uh the training Loop over here um already takes care of all of the parallelism that you need so the main thing that you need to change locally if you want to run uh this job is to change the number of machines so if you just do one process it's just gonna run in a single process assuming that you can hold the entire model and memory on your local environment or whatever runtime environment you're using um and then we're using beat speed in this case I'm just going to put it back to four um in order to accomplish this um let me just check that all the memory has already been um freed up from this entire process so I'm just restarting the kernel so that what I want is this to all go down to zero cool we're getting zeros across the board in terms of the memory except for um this amount of memory that's being occupied on each card is actually literally because of Cuda itself takes a little bit of memory um so as you can see if we look at the stack trace of what just happened right you should see a kudaum error and this is what the Deep Learning Community is always extremely upset about and why we have uh you know effectively a GPU shortage um there wasn't enough memory it tried to allocate three gigabytes of memory when only uh one gigabyte of memory was free so this is the entire you know change everything here from 15 billion parameters to 20 billion parameters and that is what's going on you're having out of memory errors because you're using naive data parallelism with mixed precision and you're not using parameter efficient fine tuning so uh what is our solution I'm sure everybody has sort of heard about parameter efficient fine-tuning um in the community these days but uh specific technique that we want to use is Laura which is a low rank adaptation uh method so basically what it's doing is it's taking the set of data and assuming that you can dimensionally reduce the weight Matrix and learn the weight Matrix um uh on like an incremental basis so it freezes all the previous weights in flon ul2 I have a previous um presentation that actually goes into this a little bit more detail we're actually going to go into it in a practitioner's standpoint and run the script that will achieve Laura fine tuning um but for the purposes of this demonstration I don't need to motivate you know kind of the other parts of it uh the main thing that you just need to know is that the loss function that we're actually trying to optimize you can separate out the Frozen set of parameters with the um you know the low ring parameters so you have the pre-train weights here and what we're trying to do is we're trying to freeze the free train weights and we're trying to learn the new set of weights on the training data that we're going to send over to flon ul2 [Music] so a few caveats I just want to get across the board mostly this is only applied to attention layers you may have heard of a new technique called Q Laura that actually does 4-bit quantization and runs Laura on every single layer I'm not going to get into that today it's outside of the scope of this demonstration but for Laura at least it's been shown that you can actually use um this technique to fine-tune and perform even better than full fine tuning so full fine tuning was the instance that we just showed on the command line where um you had out of memory errors and that's when you don't do any sort of freezing of Weights you just allow all the weights to move around while you're optimizing and you have to hold everything in memory there so what are we going to expect so I'm gonna show you uh the memory profile of what's going to happen when we actually do the loraphine parameter efficient fine tuning but um you know let's just kind of back of the envelope on our mind amount of memory that's going to be occupied using um Laura as opposed to the full fine tuning so it's going to be about a 68 reduction in the memory footprint so this is when we load the model um as you notice like even upon trying to load the weights of the model you're getting towards 80 of the uh memory inside of the model right so we go into here what we'll see is we don't have anything in memory yet and we're going to run this script on the correct Laura fine tuning so in the fine tune folder you'll see that you can run the uh deep speed accelerate um script that does uh parameter efficient fine-tuning and what's kind of great about it is we're just leveraging um uh hugging faces path to library here and it's a one line change so if you do a diff on the two files you'll see that all you actually need to do is to add this part so you have peft so in our case R is a parameter that like the larger the r the more weights you're gonna have to learn so the more memory footprint you're going to use and then um drop is just uh tends to be a parameter I don't really change that much and it doesn't really do much of a difference for our fine tuning but then you just need to call uh what get past model actually does it's just unwrapping the object and then making sure that you use torch.nograd on the set of parameters that you don't want to uh fine tune so I'll just launch the script because it'll take a little bit of time to do um it's kind of fun actually for me because uh whenever I start getting angry and having Kuda um errors um I can kind of memory profile it so it's kind of useful here so I'm running deep speed with the launcher configs here um this is specific to uh hugging face accelerate and um I want to note that I'm not using CPU offloading so what CPU offloading is is like when you're about to run out of memory on the GPU you just actually move the weights into your CPU but you take actually a significant uh memory hit doing that cool it actually seems like some stuff is happening right now um weights are getting loaded and it is occupying all that space so let's see if it will run successfully so it's going to take a little bit of time it's already loaded all the shards into memory pre-allocated all that memory and then it's going to run a bunch of data on it and uh for those that actually have the luxury of getting many more gpus than I was able to get here all you have to do again is just change the number of processes here so presume that you have like one virtual machine in this case I'm running on gcp to do that um so while it's doing this entire process I'm going to go back and just uh compare the chart of um the set of you know memory footprint that we'll actually see so when you use Laura you're actually significantly reducing the amount of Optimizer memory footprint so we're you know we have different batch sizes but in the these are proxies so in the activation the activations again um take up a lot of the memory so the activations are like the set of contacts and tokens that you pass through the model and then the forward pass so like the same thing as saying I want the model to generate completions those are a lot of the memory that's been being taken up by the activations and then you have 45 of the memory being taken up by the parameters themselves so that's why you see like this huge set of um uh memory being allocated here in terms of the parameters so um my GPU utilization is actually not that high uh which is not the best I should have optimized this a little bit more but um some tooling and advice for everyone out there after it's actually running right now so it's running 15 out of 748 um iterates in the training cycle so I ran deep speed zero three um which is a type of parallelism that shards the model parameters the optimizer States and also the gradients all in GPU memory um even in doing that what we see is that um the yellow line here is the memory footprint so like how much memory is being occupied uh during this exact point in time and then the blue line is the um is the GPU computation so basically the ideal state is if the yellow line is almost at 100 and blue line is also tracking it almost at 100 this means that all the memory that's being occupied at every given moment is being used to in a calculation so that's kind of the whole point of you know all of the stuff that you hear about um parallelism in in primary efficient fine-tuning it's to fit everything on the set of cars that you have as well as being able to run everything in memory run all the computations in memory um if you want to read further on more memory usage kind of tricks we have a blog post out there that um actually gives a really good uh set of back of the Anvil calculations and allows you to kind of um heuristically Define how much uh memory you need to train certain models um well that's basically it for what um I wanted to chat about um I'm gonna kill this process right now but um I set a notebook for everybody to uh look at as well um this is just the code to enable you to take what we just fine-tuned so suppose we finish this fine-tuning job um and then load the set of parameters uh into memory so um in this code if you run these cells what's actually going to happen is it's going to load flon ul2 first in this step and then it's going to load the lower parameter weights that were being learned in our fine-tuning job in this step to be able to attach them to this model so now you know flon ul2 had its original set of knowledge and now we are teaching it to perform specifically well on the math reasoning data set that we synthesized um before so that's kind of an entire exercise that we're going through um with that being said um kind of rushed through the end of the workshop but I'd like to open it up for any sort of questions that people have I think it's been a really insightful Workshop so far I don't think there's been any question that you haven't answered yet a lot of things has been troubleshooting in the Box seems like it's mostly a runtime errors in environment errors unfortunately right yeah yeah okay I think a few questions are coming in and this question is from pedram and pedram asks are there limitations on using different GPS together for example you're using the Nvidia h100 and the a100 you know is that bad practice in this case we'll find something um using heterogeneous Hardware is not bad practice it's just difficult like for with respect to that what I mean is um a lot of the Frameworks like deep speed they just use kind of um heuristics to be able to move the memory around for your model but it presumes that every single uh card or the GPU chip is the same so um you may have to roll out or monkey patch some of the code inside of the framework to facilitate that for a fair enough and uh yes um so even there there's a slack channel so I'm just gonna post the link to the slack channel in the in the chat so you can join the envelopes Community slack and uh I think there's a channel there for the llm um production conference too so you can join in and ask all your questions I think most of the speakers are we will be there and then of course the next question is can we get more Hardware yeah I mean they're third-party providers out there like Lambda labs and um um weave that you can kind of use but um at this point it's so saturated honestly that it's uh it's kind of hard to get Hardware no matter who you are yeah that makes sense so um all the all the comments are in fact comments on the workshop it's I think it's what's super executed well uh it was executed pretty well and uh I think that's it so if you have any other question please join the MLS Community Slack and um you know well if you have comments as well I think Mac is also in the community so you can tag him and potentially if he's available of course uh he'd be willing to get to that question uh to your question and Mark could you probably tell us a bit more about Primo if that's something you want yeah yeah sure um so specifically our company is um going to be offering um apis for folks to fine-tune in uh sort of models um so basically it's going to be um you know kind of a managed platform um the main point of all that is to show um You probably don't have to go through all this trouble uh per model per instance per whatever in order to um develop your applications on top of it so um I think that uh you know my experiences with all of this is that The Last Mile um kind of effort to be able to productionize Applications tends to be kind of hard so most people just want to leverage the models and the completions and know that your inference is going to happen really quickly so you notice that even when we're trying to do the synthetic generation it took a little bit of time for flon ul2 to generate one set of answers for our prompt right now imagine trying to do that 50 000 times it's kind of hard so um yeah we're gonna try to democratize this uh access to this uh to all these models for everyone and have developers be able to use that yeah that's that's awesome man um also what are the trends with uh I think self-hosted um llms these days I think most of the tools are coming out there what's sort of your take on uh some cell phones that llms buses uh you know picking uh using a platform that really streamlines the deployment process uh I kind of think I mean obviously there's a new stack um emerging out of the entire uh landscape but um me personally I don't it's not my greatest joy to try to uh set up all the machines and specifically have the exact uh batch size or the sequence length correct um there was one more optimization that we could have done on fun ul2 that I I didn't show but you can monkey patch the attention layers and try to use Flash attention there and that will will um help reduce the memory footprint to be able to train uh you know more and larger batch sizes um but uh the open source stack I think is something the community really needs because people need to be able to create their own custom llms but then um it's just kind of being uh blocked by the fact that the infrastructure is too hard to spin up or you just can't even get gpus right or absolutely absolutely [Music] um thank you so much for sharing it well I think that brings uh the workshop today to close again if you have any question please think uh Mac in the mlaps community you know just type in mlops.community you should be able to see more details on uh uh joining the community podcast and every other thing otherwise um wish you a great time for the rest of the conference right right cool thank you thanks Mark yeah thanks for everyone for joining bye [Music]