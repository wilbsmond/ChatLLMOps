Daniel thank you so much for joining us what's up the prompt engineering game I did I was I was on the edge of my seat that was awesome yeah yeah it's pretty cool I'm gonna have to look at it more later but we are very excited for your talk thank you for your patience and I think without further Ado are you do you already have your slides uh yeah I have uh I can share my screen let me go okay here we go you see those yes here's your screen that we're seeing yes it should be the title slide oh it's just it's like us oh okay let me Sure hold on I'll share the actual window okay let me try that better oops yes that is better awesome cool thanks so much take it away awesome well uh thank you for for having me this is super exciting today I'm gonna talk about controlled and compliant AI applications um let's see okay there we go so I am Daniel whitenack um I've uh spent time as a data scientist and Industry for quite some time I've built up some data data science and data teams at a couple different startups and in international NGO called Sil International done Consulting in a bunch of different places and now I'm the founder of a company called prediction guard I also co-host a podcast called practical AI if you're into podcasting things we've done a crossover with the ml Ops Community podcast which is one of my favorites so check that one out um so in addition to all of those things that I do one of the main things that I want to do is to integrate the latest large language models into my applications uh everyone that's why everyone's here I've listened to a few of the talks everyone's trying some amazing things and accomplishing some amazing things so I want to join in I want to integrate the latest large language models but there's a few problems that I've run into personally I I've tried a bunch of different things with these models and one of the things that I've found is it's great if you put something in a chat GPT and you get this sort of text vomit output right and it's amazing and it's a matte and it's a magical experience and you immediately think oh this could solve so many of my problems but the fact is that that output is also unstructured text and it's fairly inconsistent and I can't really build robust systems with inconsistent unstructured text blob output so if I want to do something for example like sentiment analysis and I don't want to use a traditional sentiment analysis model I could construct a prompt like this and I put it into a large language model and I get out some sort of vomit that looks like the bottom right so I do get a sentiment tag in this case a wrong one I think but then I also get all of these other things that continue on and you might say oh Daniel that's not a problem you can put like a stop token in or you can you know strip out some of that extra but I don't want to do that I mean part of the reason why I want to use large language models is because I want this magical output that's just right so this is annoying that I get this text blob output um also generally large language models scare all of these sorts of people like lawyers and finance departments Security Professionals um due to a lot of different things I know already people have talked about in this conference hallucinations costs most of these language model apis charged by the tokens so you're you know racking up costs especially if you're doing use cases like data extraction and other things there's a lack of compliance in these systems there's a fear of leaking IP and personal information into commercial apis where there's maybe suspect terms and conditions around how that will be used and I love this uh this setup to my talk just now this game about injection vulnerability so that's cool that's another thing that's on people's minds so generally you have a situation like this you've you're in a company over time you've created all of this great infrastructure that's all unit tested it's compliant if you need to be compliant with like HIPAA or sock 2 type 2 Etc it's structured it's secure and you're kind of interacting with a customer maybe there's sensitive data there and all is good then you bring in something like open Ai and to be clear I have nothing against openai amazing amazing API and system and models and all of that but you're going to start making these people like legal goal and security and finance really unhappy in a best case scenario your customer is still happy because they're getting kind of magical experiences that are large language model driven but um everybody else might be having some issues and in the worst case scenario you start hallucinating out some incorrect information to your customers and all of a sudden your customers are now not happy with you as well so what I want to discuss today is the current realities of dealing with inconsistent unstructured results how are people dealing with that generally right now and then how are people maintaining compliance what are the various things that people are trying to do and then I want to introduce um my own opinion for a path forward through all of this complication it wouldn't be a talk at llms in production if I didn't provide some some opinions and hot takes so you'll get some of that towards the end all right so let's let's start by talking about this sort of unstructured text development output and how people are dealing with that and I would say that the major ways or the trends and how people are dealing with this unstructured output is via some type of llm wrappers around large language models this could be like your own code that you've implemented around large language models that involves some regex or some keyword spotting or filtering or whatever um it could be a framework and I'll I'll show a couple of those here in a second um could be like new query languages or schemas even and custom decoding so just to give an example of how some of this is done I'd like to highlight um great work by by Shreya on guard rails uh I think she's talked at this conference before amazing work here around using these sort of like XML like uh specs to wrap large language model calls and guard those calls to check for the specifically structured output so you're not getting random text blobs but you're getting nice structured Json out similar projects around this are guidance from Microsoft um and uh and a few others but this idea that you would kind of wrap a large language model call and like re-ask for things that you don't get and validate the output that's one approach that people are taking here one thing I'd like to highlight about these is they're amazing they work great um I think both guidance and guard rails work great for um open AI there's in my experience these different Frameworks don't work in the same way across all different models um especially if you're using Open Access models there's some extra custom integration or wrapping that you need to do to make Open Access models let's say like Falcon work with some of these uh systems but this is one approach that people are taking another one I I'd like to highlight from Matt Rickard is re llm or regex llm um this is I think a really cool approach because as opposed to wrapping a large language model and then kind of validating output and then re-asking that sort of workflow this actually modifies the way that tokens are generated as you stream tokens out of the large language model so this regex or there's another version um by him called parser llm that does a similar thing with context-free grammars where you actually look at your regex pattern you look at your context-free grammar you figure out what are the next valid tokens that could occur and then you mask your output for your next token generation based on that regex or that context-free grammar so this is really cool this is another approach um I I think in the words of even um Matt he when he tweeted this out saying um you know we have a problem and we introduced regex now we have another problem so this comes with its own problem of having to deal with regex or um context-free grammar type stuff okay so let's talk about hallucinations people are trying a lot of different things with with hallucinations but I I think some of the ones I'd like to highlight are formulating multiple prompts doing some type of consistency checks or I'm actually using large language models to evaluate large language models so when I say self-consistency checks or consistency checks um this might be something like this where instead of just providing input like a prompt to a model and getting output or instead of even you know making that prompt Fancy with something like Chain of Thought prompting um you actually input multiple your prompt multiple times to a model with a temperature that's higher than than uh you know it's it's a higher temperature so you get some variability and then you take a majority vote of that output to get some self-consistency on the output so that's that's one methodology to kind of rein in some of this variation in the output that does have maybe some other issues like how do you how do you compare various outputs that aren't just like single values or numbers or classes or something like that but they're just text output maybe you could compare them with embeddings or other things um another technique that uh that I was going to highlight was language models evaluating language model output um so this for example is is an example from inspired critique which is a package and platform from the cool people over at CMU Graham newbig and Company and you can use a model that's been trained for example in this case uh so the BART model that's been fine-tuned to actually estimate the factuality of a reference text and a source text and estimate the factual consistency between the two so you could do this sort of check inspired critique also implements toxicity checks and other things but those are examples of language models evaluating the output there's some good docs in also in the Llama index docs if you look to that from Jerry and Company who have really thought been thinking more and more about evaluating the output of a retrieval chain or something like that and comparing it to the context to determine the quality of output um in terms of compliance and those sorts of things there's a lot of things people are doing one thing is ignoring the risk so just charging ahead anyway and that's a setup for some disaster some are putting a full stop on LM llm usage or not allowing certain data to be passed to llms this of course is restrictive and it might even that might even be another type of liability on your company because you're not able to get a you're not able to get the advantage of using llms and the Market's kind of leaving you behind um and then there's hosting of private models I know that's been discussed here of course with that you have to learn a little bit more about using gpus and how to do hosting um in your own infrastructure and scale that up which has its own challenges so here's my sort of opinion about what we should do going forward based on these these observations so I've been trying to build large language model applications for use cases both for my clients and for my own use cases and this becomes like exceedingly complicated because using all of what I just talked about now I have multiple open source projects multiple large language models that are evaluating large language models all sorts of different queries and specs that I have to know from regex to XML to other things like um special query languages like uh language model query language I also need to figure out like model hosting and private model hosting and I have to do that for each model that I want to integrate because all of those things work differently for different models so this starts to become more than like a single human can can manage I think so my opinion of how to move forward within this climate is to create a standardized API for both open and closed models um open model open models are then hosted in a compliant Manner and integrated into that standardized API and then some of these things like checks for consistency factuality toxicity and then the structuring of output and output types are kind of brought together in a way where in this standardized API it's familiar to to devs they don't have to learn a new specification they don't have to learn a new query language they just have to know how to make an API call or have to know which type they they want on the output so I'm going to share with you what that actually looks like in practice of course my opinionated take on this is what's represented in the system that we've built in prediction guard so I'm going to show you how that works out and hopefully help you understand why I think this is really useful and can produce a lot of acceleration as you try to build actual um actual large language model applications um all right I don't know let's see okay uh okay so I'm not sure why that's not loading um let me Escape here I don't know if you're seeing that um that prompt now but um here's the prompt that I'm gonna be using throughout the uh throughout the rest of these examples it's a Instagram post that I've um that I've pulled from my wife's company um about candles and we're going to be using this and making some queries uh about this post using different large language models um so let me share my screen again and then we'll just continue on so in order to do this um I'm using Lang chain uh to create that prompt template but everything else I'm going to show is with the prediction guard client which you can install just pip install prediction guard okay we'll just we'll just show it this way and hopefully hopefully you'll be able to see some of that um so here some of you might be familiar with the openai API um the the openai API looks similar to this so like you have openai.completion create and you select a model and then give it a prompt so here it works um almost the same way with the prediction card actually in this case the same way so if we can ask what kind of post this is and then we get this text output down here um okay no dice I'll share these um I'll share these slides afterwards as well so people can see them a little bit bigger um so one the first thing that we want to do I mention is create a standardized API for both open and closed models and close the open models can be hosted then in a compliant way even with uh with compliance like HIPAA compliance so here we have our camel 5 billion model which is hosted in this compliant way and we can use the exact same API that's openai like to query this model what kind of post is this and we get back some kind of again text vomit out of this model although it looks okay maybe um now we want to go to the next phase of okay we can use both open and closed models and the open models being hosted in a compliant way with prediction guard um now we want to start enforcing some of that structure on our output so um here I'm now saying well I just don't want any sort of output here I actually want a certain type of output I want categorical output and I want one of these two categories I want to know if this post is a product announcement or a job posting and now my output is going to only be one of those two categories so here you can see my output has been modified and now I just get one of those two categories on the output this is a product announcement um I can do a similar thing with other types like an integer type so I could ask you know how many new products are mentioned and enforce an integer type on this output and now I get my output as two um this is there's two new products mentioned in this post um so uh right now we Implement you know integer float categorical Json and custom Json formatted output all via this open AI like API um now just because we can structure output doesn't mean that it is um factual or or consistent and so a next thing that we can do is just pull a couple levers um flip a couple switches and say Hey I want this output to be integer but I also want consistency and I want factuality and just by setting these two elements of my output I can now ensure that when I send this off to prediction guard it's going to ping my model multiple times and check for consistency it's also going to use a different large language model to check the factuality and the factual consistency between the output and the input and here I still get success so camel was able to tell me how many new project products are mentioned with factuality and consistency checked I could try to fool this and say how many giraffes are mentioned and if I have those same checks then I get an error that says inconsistent results and so I can avoid that sort of hallucinating Nation um now the last one I'll show here is a another one that's a Json format output so rather than just doing um consistency and factuality checks and type setting uh type enforcement we can actually start to get some structured outputs so I can say list out the product names and prices and make that in Json type um and here we get the text output but then this is automatically parsed and available now as a dictionary in python or a Json blob in the rest API that's returned so this is how we're envisioning um this element of control and compliance with large language models so we would have this sort of easy to parse output configuration for developers that has types that they're familiar with integer float Boolean categorical Json custom Json we'd also have kind of flip of a switch checks for factuality we also have the check for toxicity and consistency and uh and that's kind of our vision of this as we move to the future of course we have a lot on the on the roadmap we'd like to make this sort of wrapping in this consistent API with access to all of this functionality available for people's fine tunes of llms as well and we've got more kind of presets of structure and type on the way along with of course more model Integrations um so thanks to I I'd like to give a sort of special thanks to let's see if this loads special thanks to um a lot of people that have inspired uh this work um and uh and even helped out um working on great Frameworks that have really led the way on this front so you can find some of them here um all right so thank you again this is all the places that you can find me on the interwebs and the social medias uh here's a link to the podcast and a link to prediction guard if you want to check that out and of course I'll be hanging out in the event link and on ML Ops uh slack if you want to reach out directly awesome thank you so much Daniel and let's definitely drop some drops we'll send you over there after if you want to kind of share that with folks as well sure um yeah thanks so much for joining us I mean let's give it a minute too to see if people have um some questions I think that like prompt engineering game sort of like absorbed a lot of people but hopefully they have some good questions um and so one question I have though is like as you're sort of like doing this work like are there things that sort of surprised you uh that you sort of weren't expecting yeah uh great great question so I think the thing that surprised me was um I knew that a lot of smart people were working on kind of individual pieces of this solution and so you can kind of assemble that into something fairly reasonable for a single model but then as soon as you figure out oh that model doesn't work for the use case that I need to do in my next project then you have to sort of repeat everything again because um despite them being like really good Frameworks and there's a lot of functionality out there they all sort of work slightly differently for different models and so you end up kind of in this Loop where you have still have to do a lot on the this like language model wrapping um Custom Plumbing around these things and to get this kind of consistent API and consistent output from Project to project got it yeah that's great and from Sterling weight he says great talk awesome thanks Sterling yeah Good Vibes uh cool so we're also this this is being recorded so we'll send that out to folks and then um we'll also share the slides I think maybe Pauline got those from you or we'll get them from you and I'm just pasting them into uh chat now awesome thank you so much well awesome Daniel we'll let you go on with your day we really appreciate you joining us yeah thanks so much see y'all [Music]