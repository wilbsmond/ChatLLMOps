awesome um thanks so much for the opportunity uh here I'm super excited to give this talk uh the goal of this talk is how llama index can connect your language models with your external data so my name is Jerry I'm one of the creators uh and co-founders of llama index and I'm super excited to be here so the basic context here is that llms are a phenomenal piece of technology for knowledge generation and reasoning uh they're pre-trained on large amounts of publicly available data and so you've all seen the amazing capabilities of llms by just playing around with stuff like attribute you know they can answer questions they can generate New pieces of content they can summarize stuff for you you can even use them as planning agents so basically you can have them perform actions get a response and perform more actions over time pretty much every application developer that's using llms thinks to themselves how do we best augment language models with our own private sources of data and so whether you're an individual or an Enterprise you're going to have a bunch of raw files lying around for instance like PDFs PowerPoints Excel sheets uh pictures audio you might use a few workplace apps like notion slack Salesforce you might have a if you're an Enterprise a heterogeneous sources of data collections from data Lakes structured data Vector DBS you know object stores all these different things and so the key question is how do we best augment our alarms with all this data there's a few paradigms for injecting knowledge into the ways of the network these days uh probably the most classic machine learning example is through you know some sort of fine-tuning or training or distillation process uh the idea is that you know you take this data and basically run some sort of optimization algorithm on top of this data that actually changes the weights of the network itself to try to learn the new content that you're feeding it and so you know if you look at a pre-trained model like chapter BT or gpt4 they already internalize a huge amount of knowledge if you ask about anything about Wikipedia it'll be able to uh understand and and give you uh you know any any information about any Wikipedia article on there however I think another Paradigm that has emerged these days is in context learning and so for a lot of users fine-tuning tends to be a bit inaccessible for both performance and cost reasons and so these days a lot of applications developers are using this Paradigm where they combine a pre-trained language model with some sort of retrieval model to retrieve context and they manage the interactions between the language model itself as well as this retrieval model so the way it works is imagine you have some knowledge Corpus of data and so imagine you have a notion database of various text files this is about uh you know like say the biography of an author it's about Paul Graham and so the idea is that you have this knowledge Corpus of data and then you have this input prompt and given this info prompt it would look something like the following it would basically say here's some context and then the retrieval model will be responsible for retrieving the right context from the knowledge Corpus putting it into the prompts and then giving given the context answer this following question and then you put the question in here and then you send this entire prompt over to the language model in order to get back a response so some of the key challenges in context learning is how do you actually retrieve the right context for the prompt and we'll talk about some common paradigms as well as less common paradigms that might solve more advanced use cases how do you deal with long amounts of context how do you deal with Source data that's potentially very large and also very heterogeneous you know you might have unstructured data semi-structured data structure data and how do you actually trade off between all these different factors like performance latency cost this is basically what llama index's entire mission is focused on and so you know imagine you're just building some sort of knowledge intensive uh language model application whether it's a sales tool marketing tool recruiting tool Etc and your input is going to be some quick Rich query description it's going to be either just a simple task uh like a simple question that you want an answer to it could be a complex task that you feed in the idea is that it's basically something that you would normally feed into chat gbt but here you're feeding it into us as an overall system llama index itself is a central data interface for a language model application development and so we set on top on top of your existing data sources or databases and we manage the interaction between your data and our language model and the response is basically a synthesized response with references actions sources Etc and so going into a little bit about some of the components of wama index and and I think the goal of this talk is is to talk about some of the additional use cases that really solve so we'll get into that in just a few slides our goal is to make this interface fast cheap efficient and performant so we have three components we have data connectors which you can find on llama Hub they're basically just a set of data loaders from all these different data sources into a document format that you can use with llama index or even Line train um and then the other next step is data indexes so once you address this data how do you actually structure this data to solve all the different use cases uh uh of kind of uh knowledge augmented generation and then the last component is this query engine which basically completes this black box and once you have these data structures under the hood now you have a query engine that takes in a query and is able to Route it to these data structures to give you back the response that you want so the goal of this is really just to give you a few concrete use cases that llama index solves and so probably the most simple one that pretty much everybody is doing these days whether using qualama index for using another kind of vector DB stack is semantic search and so imagine you load in some sort of text documents right so first line of code just load in some text documents second line of code you build this like simple Vector index from these documents and typically what that looks like is you Chunk Up the text embed it plot it in a vector database and store it for later then during query time you know you ask some question like you know what did this author do growing up imagine the text data is about the author and you would just do embedding similarity-based retrieval from this uh you know Vector database or a document store and then you would take the top K of relevant trunks inject it into the input prompt and get back a response and so for simple questions like fact-based questions like what did the author do growing up or stuff where there's semantics and the query that map well to semantics and your knowledge Corpus this works pretty well and so you can see we using this Paradigm the answer is you know the author grew up writing short stories programming on an IBM 1401 this is retrieving the relevant chunks from your knowledge Corpus in order to generate this final response there's also summarization so summarization is actually a little bit different because instead of doing top K retrieval you actually want to go through all the context in a document in order to synthesize a final response or summary of the document and so for instance you know we load in some documents through the first line and here this is another data structure called the list index so instead of uh storing you know each node with an embedding associated with it a list index actually just creates a a kind of a flat data structure of all the nodes within a document so that when you ask any sort of query or input prompt over it you explicitly want to go through every node within your list in order to use that as context to generate the final response so for instance for a query like could you give me a summary of this article a new line separated bullet points this is the first response that you would get and then this is an example of the answer there's also connecting to structured data where you can actually run Texas SQL queries on top of your structured data and so that's actually a different Paradigm than unstructured data because you inherently want to convert this into the query language they could run over a structured database and some of our Advanced constructs include stuff like being able to synthesize across heterogeneous data sources if you have notion documents as well as slack documents how can you best synthesize information uh not just you know by treating everything as one combined Vector store but actually explicitly going through and combining information across your notion and slack documents here is an example diagram that shows this use case and probably the last thing I'll talk about is also multi-step queries if you have a complex question how do you actually break it down into multiple smaller ones over an existing data source in order to best get back the results that you would want so for instance let's say you have some existing index or knowledge Corpus about the author and you ask a complex question like who's in the first batch of the accelerator program so therefore you know you can actually break this down into smaller questions use those to you know get back retrieval-based answers and then combine everything together to synthesize a final answer right here long story short there's a lot of stuff you know there's more stuff in these slides and a lot of this is found in the docs and the idea is that llama index solves both the uh kind of short uh the the kind of simple use cases of semantic search as well as more interesting interactions between the retrieval model and your language model awesome thanks for your time thank you so much Jerry that was awesome [Music]