next up we've got Mr Andrew where are you at Andrew let's see it let's see it ah there he is [Music] excited to be here how you doing man uh great great so I sent out an email before this uh all started right and I said if you reply to this email and you let me know what your talk your favorite talk that you're looking forward to is I'll send you out some swag and quite a few people said they were excited about your talk because surge is absolutely killing it right now and we need to let everyone know that and you so you've come on here to talk to us about the very hot topic of what I can never pronounce the rlhf uh if I put those letters correctly in order and uh it's it's hot right now man let's be honest yeah yeah uh cool stretch I've been doing now or yeah and I'm gonna share your screen Pro that it is here and I'll come back in 10 minutes we're switching over to lightning talks now so this is gonna be a 10 minute talk you got 10 minutes on the clock man I'll see you soon uh cool yeah so my name is uh Andrew uh I'm from uh surge AI where I'm leading the engineer uh just to introduce ourselves a little bit we're a full stack uh human feedback company uh that started around three years ago so we cover everything from Hiring Our Own contractors uh to building our own uh labeling platform for them to annotate on uh and and delivering data to clients and recently we've been spending a lot of time on LMS and and lhf uh so to give some more context about our work uh we've been partnering with other groups like anthropic openai Google go here and others to kind of help provide data to provide to power like a lot of these top-of-the-line systems and and make them kind of safe and and useful so this this slide kind of has um a bunch of the research papers that that have used our work and uh it is available on our website which is search hq.ai so today I'm going to go through the the process of our lhf but kind of from the perspective of data collection so first I'll talk about the supervised fine-tuning piece of it uh and then the the ranking to it and kind of give some of the lessons we've learned along the way uh doing this type of work uh so before we jump into the details I'll kind of motivate the problem so let's say you're just Fresh Off training your your new llm you fired up and maybe your hope to hoping to see how well it does compare to chat gbt and you ask it to write a story um about Harry Potter using a identified Voldemort so the problem here is the base album as we all know is only trained to predict the next word on internet text so when I asked to generate a story before any rohf it's actually not going to generate the story it's just going to kind of continue on uh from there so it actually just kind of keeps writing instructions about the story instead of actually writing the story once we're able to successfully uh you know run the riotf uh we'll get the actual content we want so here's an example from gpd4 so it has rohf for similar training when we ask it to write the story uh it actually you know starts doing what we want so how do we actually get this useful Behavior how do we go from like this next word predictor to the kind of like the useful assistant uh character that we're all familiar with in in chat gbt so there's there's kind of two stages to the to the process uh as most typically as the way it's most commonly done right now so the first is is supervised fine-tuning so this involves just collecting a um a few thousand to tens of thousands of prompt completion pairs so an example of a problem completion pair uh could be the story I just showed um or here's another example from a surge labeler so we're basically just collecting a bunch of prompts and then writing from scratch the desired output from our from our model and directly um fine-tuning the model with that so this is kind of like laying the uh Foundation uh for the assistant we want so all the kind of behaviors we want from our assistant should be included as part of this fine-tuning data set so you know in this example the model just gives the Tweet directly when we ask it to write a write a tweet about the matcha flavored sparkling water but you could imagine another type of assistant that's more of a verbose that says hey sure I'm happy to do that for you like here's the tweet you could also imagine that if this was about like medical advice or financial advice you could want to give a disclaimer so any kind of behaviors we want in the assistant we're going to want to put into this fine-tuning data set just to kind of lay the foundation for telling the model like what it can do and the possible outputs so here's kind of like one you know one thing that could happen if you don't do that well so this is a llama model that has had instruction tuning but it it never was actually told it never actually had any data in its fine-tuning data set that that let it know that it could say I don't know so I asked it for um the address of my favorite Italian restaurant in San Francisco all the fine-tuning data the model received it always involves the model being asked the question and the model being answering that question not saying I don't know so instead of saying I don't know because this is like a very specific restaurant it actually just makes up basically completely random random address so I think one lesson learned from the supervised fine tuning spot is that this is where we really like lay the foundation all the behaviors we want the model to have we want to include them in here uh the next part um is is reward modeling so this involves uh basically kind of The Next Step where instead of just feeding the model prompts and completions we're we're kind of uh nudging it towards being in more and more helpful assistant over time so we kind of sample uh different outputs from the model and then we have labelers rank them indicating which is better and we kind of use that those rankings to kind of run the reinforcement learning process and and kind of gradually increase the model like through different iterations of training gradually increase its capability and its helpfulness as an assistant uh so I think one of the interesting things here is it can be really hard uh to do the rankings because when you're looking at a model output there's a ton of model output there's tons of different factors that go into it so you can imagine an example where um maybe we're asking for information about someone and one of the completions is a beautiful answer uh it's like perfectly written it's great it's the perfect length however it has like one or two small like facts wrong hallucinations and then we can imagine comparing that to another answer where all the facts are right but the writing's A little sloppy you know it's not it's not the best besides that so how do we actually weigh that all the different things that go into like what makes the completion good like the factuality the writing quality the length so it can be really hard to manage that and this is what we found is really important to kind of be very clear in your instructions to people about what you're prioritizing and what your desired behavior is so another another problem we noticed here is um sometimes you might sample a bunch of completions and maybe kind of all of them are bad and you don't want to reward any of them so this is a an example from this in the same model um so let's say we're chatting with it and we ask it about the most recent um John Wick movie um there's two answers here and there and they're both kind of wrong because they both say John Wick three um even though John Wick 4 has come out in the meantime so the ideal situation here is we we want the model to basically acknowledge the status training data is not most up-to-date um however if we pick one of these we'll basically be rewarding it you know either way for for giving the wrong answer so another thing we've kind of built into our religious pipelines that's originally an idea from the anthropic rohf paper is the idea of editing so we actually have uh a function in our labeling software where if you actually don't like any of the options uh you can press an edit button and in this case you can imagine the person editing the first the first answer and then actually rewriting it to have the proper response and then instead of comparing the original to um completions we can actually compare the the edited response the the one in the bottom left hand here with with the one that is kind of like the source response so instead of uh telling it to prepare to prefer one of these bad completions over the other back completion we're Italian to prefer the good completion over um over one of the bad ones uh so yeah I'll mention uh briefly like a final kind of uh future research Direction I'm excited about in the lhf so I kind of mentioned the problem with like all these different things we're having to complete so there's actually some recent work from the Allen Institute where they have the labelers explicitly label explicitly Mark in the text uh when there's an error in factuality um and you know when when things are irrelevant and by actually uh specifically indicating these problems we kind of have a more granular data that it can combine in different ways to create a reward signal so you could kind of decide after the fact how much you wanted to penalize the an error and factfulness um and now you want to lay that against other other things in the reward model um looks like um I'm out of time now but yeah I'm happy to to answer any questions and I'll be around in the chat later excellent thank you so much for this this is awesome so I am going to keep us cruising for anyone that has questions for Andrew feel free to throw them in the chat and learn a little bit more about this rlhf I have to think really hard every time I say that which is not the best sign but dude Andrew thank you so much man thank you and it's really cool to see everything that you all are doing at surge I know uh I said it before but I'm a huge fan and I think that um you know what you're doing it's no small fee I think Demetrius yeah I appreciate it all right dude well I'll keep it cruising I'm kicking you off now