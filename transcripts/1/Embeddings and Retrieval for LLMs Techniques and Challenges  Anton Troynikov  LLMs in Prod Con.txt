now I've got Anton bring on the stage and Anton I made this little wardrobe change just for you because I felt like you would vibe with the hat and yeah and the eye hallucinate more than Chachi pizza that's that's a really cool shirt I think I might have to get a version of that myself maybe you guys can send me some swag that'll be sick there we go we'll do a flag exchange speaking swag oh my God I get to give out swag I see some incredible uh questions coming through in this chat Sergio Rio reach out to me man I need to give you some swag and for the other one what do we got here new lunch yes you mentioned that YouTube stuff Ed and it did because we are keeping it all on the platform reach out to me on the platform I'll get you some swag there we go so Anton I think you got swag that uh you're gonna be contributing to the stock title which is awesome chroma has been absolutely on a tear lately and you are going to show us all about some of the embeddings and retrievals for llms I am going to just let you take over man I'll get this uh this commercial off of this screen right now and let you shame your screen if you want I think um yeah we got to make sure that works before I jump off sure so feel free to share it and then we are going to hop in to the uh to the next talk and one quick question dimitrios uh should I be able to see the chat from the audience no so you got to go to the I'll drop the link okay so I actually do have to go to a separate link that's that's weird but I'll take it I'll take it yeah I mean it's 2023 you think this stuff video calling technology one of the hardest problems in computer science I'll make sure that I have that open as well um so that we can do the QA towards the end um yeah I'm gonna I'm gonna stick to time I'm gonna try to get this thing back on track all right let me just click this open and then I'm just gonna make sure that I've got everything that I need yeah later uh and then all right I have I believe I have the chat going here uh yes maybe perhaps messaging disabled I don't know we're looking all right I mean if you want to I can also jump in when there's yeah uh questions yeah if in case everything is broken I would love to have a moderator who can see it that's my job man you are in luck all right that is me today all right all right man um definitely vibing with the outfit here uh I'm gonna kick it off thanks thanks very much for hanging around all right excellent all right um hi folks thanks for joining my talk today as part of this conference uh the llms and prod folks have been really putting on great events and I'm glad to be part of part two uh despite missing out on part one having gone down with a pretty severe non-coveted respiratory virus for some reason um so today we're going to talk about you know using embeddings and retrieval um for llms and specifically I'm going to go into some of the techniques and challenges that you might face if you're deploying this kind of uh idea in your application so let's get started the basics so what's the basic idea of using retrieval in an llm context the basic idea is rather than just relying on the model's trained knowledge in other words the data it was trained on we can actually add additional context to the models uh to the model's input by making sure that we by pulling in relevant information relevant to our query so the model using a programmable memory module which in our case is an embedding story so what is an embedding again for most of you this is just a recap but an embedding model is something that takes data of any kind of modality could be text images or video and returns a set of numbers it's a dense Vector in other words every entry in this Vector is populated and it's called an embedding so each when an embedding model processes a piece of data and creates this Vector that's called an embedding and you can think of each embedding like a point on a map and it's a map of meaning that's the beauty of embedding models embedding models encode semantic that is Meaningful information and they're trained in such a way that when they embed data similar things are embedded close together on this map now embedding models are usually working in very high Dimensions so 500 to say 1546 for open ais82 model so we can't really visualize their full dimensionality but this is what one embedding model might look like when projected down to 2D space where we can see it and we see things that you know things to do with cities are close together body parts food Etc in this map of meaning tend to lie close to one another and we can exploit that fact so this is how you know you typically build a retrieval augmented generated system using an embedding store like chroma um and we exploit the fact that things that are close together in meaning um tend to be close together in embedding Space by basically taking our query so first we you know first we embed our data set um we then take a query from our application using an llm embed that query and get the embedding for it then find the nearest Neighbors in embedding space to that query get the associated documents with them and then use those as additional context to the llm to generate our final answer so in practice what this means we're doing is rather than treating the large language model as a general purpose general knowledge machine we're actually relying mostly on its reasoning capability so its ability to actually draw conclusions from and transform text into one form or another so again by providing additional context we're essentially giving this model programmable memory now you know we've made this very easy to try with chroma you can you know go ahead and try it here it's as simple as pip install chroma DB and these are all the commands you need to get going it's we've designed to we've built this deliberately to be as simple as possible and it's very powerful a lot of people are using chroma for all kinds of applications including document based question answering where you can basically interact with a large Corpus of text using natural language in a non-linear way and that's very powerful in all kinds of domains like legal and and medical and research um and even customer support or um really you know sales automation any sort of domain like that it's a very powerful General technology another emergent place where we've seen this sort of retrieval augmented generation working and this is a much more emergent category but I think will be very powerful in the future is in the category of agents and agents are basically um they're basically AI powered automatons for ones of a better word that use embedding based retrieval as a store for the interactions that they've learned the skills that they've learned and as a basically as um as a memory for their environment and interactions and that makes them very powerful very recently a research paper came out uh called Voyager which created a Minecraft agent and uses chroma as the memory layer which quickly achieves state-of-the-art performance these agents are you know fairly toy-like today but I think in the near future um they'll actually be very powerful so all of this sounds wonderful right it sounds like we have this great general purpose technology and we can go ahead and deploy it right away and you know the previous panel spoke about some of the infrastructure considerations and the requirements for building things like this and those are understandable but what I'd like to talk about in a little bit more depth today are actually the challenges around getting this right from the perspective of the models and the embeddings themselves and these are some common questions that we receive from you know developers and teams building with embedding space retrieval with chroma and top of top of Mind Of course is which embedding model should I use for my data there are plenty of text embedding models available and some of those are specialized to particular domains like code some of those are general purpose and it can be hard to understand which model you should be using for your particular task now the next question that we get and this is again a very frequent question is how do we chunk up our data so typically when you're using an emitting space retrieval approach your data is going to be divided into checks chunks so that it fits into the target large language models context window and contains the relevant information but that can be difficult to do so this is this is another challenge this is a question we get asked very often and finally what we get asked quite often is how do we figure out if the retrieved results we get are actually relevant and and relevant relevant actually means two different things first relevant might be relevant to the task that the user is asking the model to perform but relevant might also be specific to the user for example one user might care about um one attribute of say you know they might care about how fast a given vehicle will drive while the other one a different user cares much more about the miles per gallon that it can get so the things that are relevant to search for differ by user and that's you know that's a very simple example but you can imagine um many other cases like that where you have a retrieval augmented generation system you know using embedding space retrieval as we've discussed here and it needs to adapt to each user individually and actually that's one of the most powerful things about AI it's very flexible it can quickly adapt to all kinds of different use cases and users so well here's the bad news the bad news is today nobody really has the answers to these problems um this technology is still very early and obviously it's getting very rapid adoption today it's an era of very broad experimentation um a lot of the best practices and tooling is being worked out right now today the good news is that these are really important problems and so there's a lot of investment uh both in terms of you know capital and time and compute Power by a variety of groups around the world dedicating to answering them and the fact that we have open source tooling things like chroma to support that kind of Investigation will accelerate how quickly we actually can answer these questions sufficiently well so that we can build great applications on top of them so let's go into some of these questions in a bit more depth so first of all the first question is which embedding model should now the diagram on the right is actually four different embedding models trained in slightly different ways using the same data and this is the sort of map of meaning that you get out of them and what you'll notice is actually these maps are more or less very close in terms of in terms of layout to one another up to basically our rotation and there's some Empirical research that's come out not that long ago which essentially shows that embedding models tend to learn very similar representations as long as they have similar training objectives so that's one thing to remember that maybe maybe the real sort of improvement you can get from your application isn't by switching embedding models but in case you do want to go down this route there are several existing benchmarks so you know I've got a few listed here and and hopefully the slides are shared later so you can click in these links but they're called beer and mtab and Kilt these are existing benchmarks and they are for What's called the information retrieval task which is the backbone of embeddings-based retrieval augmented generation has retrievable right in the name and their data sets but they're also Frameworks for benchmarking retrieval systems and this will allow you to Benchmark your own data with a variety of embedding models and so how would you go about doing that well first you need to collect human feedback on the relevance of The Returned results this can be as simple as adding a thumbs up or thumbs down button in your application so that your users are essentially labeling the relevance of your data for you that allows you to construct your own data set specific to your particular task and then you can obviously embed that data set with a variety of different models and then run the retrieval system and evaluate the effectiveness of that retrieval system using any of the Frameworks developed for these existing benchmarks they are designed to be fairly straightforward to use and you shouldn't be scared by the fact that they're developed for machine learning research they're very relevant to production deployments of this kind of application so the next question that we get is of course how do we chunk our data there's a few things to consider one thing that is a pitfall that I've seen a few people fall into is that the embedding model just like the large language model has a finite context length it's a fixed content context length and so what happens typically if you input a document larger than the model's context length is it will get truncated which means the embedding will only capture the meaning in the first part of the document before truncation so you have to be careful with that you have to really pay attention to make sure you're not throwing away parts of your documents now the next thing to consider is of course when you're chunking how do we consider semantic content and what I mean by that is you probably you don't want to be dividing in the middle of sentences or even in the middle of words because you're losing meaning um you're losing the semantic content that allows the embeddings to actually function properly and also you're going to get garble garbled retrieval results there are research results that show basically distracting content or partial content inserted into a language model's context when they're really degrades their performance fairly significantly so you have to be careful around these parts of chunking now the other thing to consider is a lot of the types of documents and data that we use as humans and we'd like the model to you know perform operations on already has a natural structure things are typically divided into chapters and sections and pages and because they're designed to be read by a human they're also nicely structured to be read by the large language model and so you might consider using some of that structure in your chunking strategy there's a few great tools um built for natural language processing or for large language models in particular the natural language toolkit and ltk has one such thing and it provides um quite a few pieces of tooling that allow you to break things up by sentences or by sections or by paragraphs in an automatic way and Lang chain provides a few different chunking strategies which are already pre-programmed they're just packages you can run and chunk up your uh your data automatically now these are fairly basic ideas and I think that there are more powerful ideas which are currently very much in an experimental mode but which we'd like which we are experimenting with and we hope to see more of the community experiment with too the first of these which is really exciting for me is actually using the language model itself to figure out where the semantic boundaries are where the boundaries of meaning are and the way the way to do that is essentially have the model um predict the likelihood that the next token is the one in the document and when that likelihood gets low in other words the perplexity of that token when the perplexity gets high or when the likelihood of those tokens gets lower this means that okay the model is uncertain about what to predict next which means probably or perhaps we haven't evaluated this yet that's why it's experimental that perhaps this is a semantic boundary this is where the meaning basically where the previous section of meeting has wrapped up and a new one is about to begin and you might consider using that as a chunking boundary we can also try using informational hierarchies so again um oftentimes the sort of data that we use as humans um tense tends to have a natural hierarchy just like it has a natural structure so for example you could first summarize s the model or or do it in a human way Summarize each chapter in a book and embed the summary of each chapter and then when you find that your retrieval step you know selects a particular chapter as the nearest neighbor to the query you can then look at the paragraph embeddings of that entire chapter in a different collection and find the paragraphs inside that chapter which are relevant to the query or the task at hand um and finally sort of a different approach which is again highly experimental we've been trying this lately you can try to use embedding continuity so what I mean by embedding continuity is that basically you can look at the distances between generated vectors as you feed chunks into the embedding model because again by definition the semantic embeddings that we generate if their meaning is similar they'll be close together and you can try to find meaning boundaries by looking at the distances between the previous and next chunks as they're generated this is very similar to sort of like a Time series analysis but in but in higher Dimensions so again some techniques worth trying and then sort of a very interesting question right now for application developers and this is one of the things you you really need to figure out to make your application robust is is the given retrieval result for a particular query actually relevant so the traditional approach um in information retrieval here is called re-ranking and there's a great re-ranking model out from cohere um AI which is worth trying and there's other re-ranking approaches because again information retrieval is historically relevant uh task um you can also add human feedback to your re-ranking model to essentially train the re-ranking a little bit more for your particular task and domain and of course you can use heuristics to augment your retrieval as well things like performing a keyword search alongside the query and then you know taking the best results um according to the re-ranking model between the keyword search and the semantic search over embeddings or you can already scope your search if you know something um in the metadata of the query so if you know that the that the user is asking about a particular um part of your documentation you can already scope the search to only that part of the documentation so that irrelevant results from other parts don't get shown up um chroma supports metadata filtering out of the box so I think the real question here is is there some sort of algorithmic approach and what I mean here by algorithmic is something that we can just run without too much tuning which actually uses some inherent property of the data so what the key insights here are is the relevance of a result depends not just on the embedding model used but also the distribution of the data in that particular data set and so at chroma we're working on an algorithmic approach um to this relevancy problem in embedding space semantic retrieval we hope to show you something interesting uh fairly soon we'll be open sourcing that and and letting people just give it a try so um that's a quick overview I think now is a good time to open it up to q a so I'm going to stop sharing my screen here and I'm going to come back to you guys nice so I can see chunking being an issue for languages where the action verb tends to come at the under the sentence if you thought about that look multilingual embeddings are one of those big open questions right now um I think Google has put in the most effort to make their embeddings really truly multilingual and that's not surprising given Google's Global reach um but things like chunking according to the sentence structure of the language like for example you can imagine you know uh non-european languages or and I mean chunking even Beyond um Beyond you know the the placement of actions and verbs but also the even just the meaning of characters right if we're talking about some East Asian languages where our character has often much more meaning than than an individual English word than the checking strategy has to be fundamentally different this is kind of why I like the model based approaches more because good language models should be able to model those semantic boundaries so long as they're trained on text from those languages it's part of why I'm bullish on the more experimental approaches here makes sense okay so for chunking I find that ntlk or Lang chain quite often cut off a paragraph in the Middle gpd4 with proper prompting actually does better with chunking is there a way to use an llm model to do chunking and convert to embedding in one shot yeah so um there isn't a model right now that will produce that will basically act as a language model and produce embeddings at the same time I think there's definitely a future for that um I think that one reason we haven't seen this yet from the API providers is basically if you have access to the embeddings and the output and the input of a model you can reverse engineer that model with enough API calls you'll be able to distill it much more effectively because you know the internal State um but that aside I think you know I've presented at least one approach to using both an embeddings-based chunking strategy and a model based chunking strategy I think if you have access to a local model where you can also you know get the probabilities of the next tokens from it although I think open AI still provides that over the chat interface it's worth a try with that perplexity based approach that I mentioned but I don't think there is a One-Shot model based approach these are things that you'll have to build for yourselves if you do build it please let us know what the results are we're very interested in this direction nice there we go submit a PR there we go PR welcome look please genuinely genuinely the the sort of contributions that we're getting from the community right now is what are the reasons so gratifying to be an open source we're developing this technology together as a community where we there is nobody better placed in the world today to figure out what to do with these Technologies and the people trying to build with them the era of the era of sort of the machine learning researcher in the Ivory Tower with sort of billion dollars of compute completely dominating what's possible and then dominating research is over you you now have access to these apis you are equally at The Cutting Edge as anyone at open AI so good yes I love it all right so we've got some awesome questions coming through here I'm going to keep it going for another like five minutes and then we will you were true to your word man you are like a clock Tick Tock and I appreciate that because I am not I have been all over the place on timing today now let's see this uh not to get into a pissing contest but what are the benefits of chroma over PG Vector yep that's a really great question um and this is something that is really important to how we're building chroma so PG Vector is designed primarily for a semantic search use case it's great you know it's great in the case where you have a fixed data set that is updated infrequently and you already have your other data in postgres if all you're doing is for example semantic search over a fixed data set p D Vector is fine now there's two issues with PG Vector one is The Recoil performance of larger data sets is not great and it requires a lot of tuning to get right because it uses basically a clustering and inverted index approach to do approximate nearest neighbor search what that also means is if you have online changes online mutations coming into your embedding store because you know you want to store it understand user interactions it's kind of AI that you get to interact with it PG vector's performance would degrade very quickly but chroma is built from the ground up and we use an algorithm called hierarchical navigable small world graphs which is a mouthful every time I say it no matter how many times I say it which is a graph based data structure it exploits the structure of embedding space in a graph-based way to provide good computational efficiency but it allows us to scale and deal with mutations online in a much better way and so we're more suited for this sort of application-based use case where you have a lot of user interactions and your data is being updated online um yeah I would say those are those are two key differences the other part of course with chroma is a lot of these problems or sort of challenges that I've identified in my presentation today are things that we intend to build into the product we will our core product hypothesis is essentially you shouldn't need an infrastructure engineer to run this and you shouldn't need a data scientist to get this capability all of this should be built into the product that you use and that should be chroma yeah that goes back to that idea of the researcher in the Ivory Tower it's like let's get this into everyone's hands and let the community run with it and yeah we've seen how powerful these communities can be yeah strong believer a strong believer in the open source Community is power in AI and I will say that is a perfect segue into a big thank you from our side for you guys sponsoring this event I mean chroma has sponsored this and the community which is huge for us because it helps us create more events like this and also it allows us to do all kinds of in-person events so I can't thank you enough I mean it's uh what do they call the opposite of a vicious circle it's a virtual virtuous cycle yes psycho that's it cycle there we go the cycle is strong so there's a few more questions coming in and since we do have the time I'm going to ask him sure any advice on chunking slash embedding of code for example embedding a whole code base to do some llm assisted information retrieval great yeah so there's a couple of things that I touched on here one is which embedding model to use there are a number of code focused embedding models available um openai would really love you to use ada2 also for code it is trained for both um it's unclear whose wins currently do the benchmarking report your results we'd love to know um now in terms of chunking the code base the great thing about code is you automatically have a lot of structure you can chunk by function you can chunk by class you can chunk by file right and so you can leverage all of that structure and it depends on what information you want to feed the language model I think this is actually a great um candidate and you know if if you're doing software engineering best practices none of your files are too long none of your functions are too long so it's more impetus to do things the right way um and so I would I would genuinely try it like um chunking across their semantic boundaries like specifically saying okay we're going to chunk by function we're going to chunk by file and then maybe connecting all those functions this function belongs to that file and then maybe if we call that function we'll we'll also pull in its documentation so this is the other piece if you have documentation for your code base you can tie that in to the actual definitions of the functions and return both of those to the language model together that's that's one of the things that's really powerful about this kind of Base retrieve what you can send in one query and get both kinds of results together um so yeah so that's that's basically my advice is like try a couple of the code embedding models um and then if if they're succeeding for you if they're returning relevant results Chunk Up on the basis of of like semantic information code is code is highly highly structured and there's no reason not to leverage that structure yeah that's such a great point that is I mean take advantage of it because language is not quite like that so you gotta love it when you can uh Michael was asking same question as I had for Sam at redis any hints and tips for embedding tables whether yes encoding them or otherwise yeah so this is this is actually another challenge which I neglected to put in my slides but it's becoming an increasingly interesting topic so obviously a lot of the world's data today lives in these structured databases right we think of them as algebraic data structures so you know SQL tables Etc Json gloves um and as far as we've been able to figure out the tabular the embedding models don't deal too well with tabular data directly so there's a couple of interesting approaches that we're trying here the first is uh grab your tabular data and then give it to a language model and have it summarize it in natural language and then embed the natural language description and then you can add you know a pointer to the actual table um uh in the metadata so you retrieve a natural language and then you're able to query the appropriate table because now you know what you need to query now here's the other part large language models are great at what you should think of as sequence to sequence tasks and one sequence to sequence task is taking a natural language description of information you're looking for and transforming that into a SQL query given the given the sort of the the description of the columns and the structure of the table right and SQL because there's so much SQL available online they're pretty good at generating SQL statements so this kind of two-stage approach um might work for you I think that trying to embed tabular data directly currently doesn't work very well in the future in the future I think so here's what I think is going to happen in the future I think in the future all of these sort of like hierarchical kind of discrete steps that you have to engineer into your application will be handled by some sort of model whose entire purpose is to figure out what information is relevant to the task and then perform computation over that but we're not there yet and the way to get there is to generate as much data around this these kinds of tasks as possible so we can start training those models and start operating with them and understanding what they need it's a fundamentally different class of model that we need here but I think it's and what's more I mean for me personally one of the parts of one of the parts of the definition for AGI is it's a system that understands when it doesn't know something and knows how to seek and then integrate information right and so the the kinds of models that can perform information based retrieval and synthesis they're also in the path to AGI so these experiments we're doing today might actually lead us there tomorrow so anyone that is playing the drinking game with me take a shot he said AGI there we go that is what we're doing but to note that I don't believe we're there yet yeah I I got that that's a strong uh disclaimer right but there is a part of me that wonders like how much how much value does that actually bring or is it just over engineering like a hammer looking for a nail type thing which which part specifically the using a vector database like on the tabular data right yeah I think I think actually what you want is not necessarily to think about this as a vector database or about tabular data it's a task that you're trying to perform is retrieving the relevant information for the target language model right that's what you're doing so wherever that information lives right now is the right place to keep it right you have to you just have to find a representation that allows you to retrieve it now you have to preserve the fact that you can really flexibly query these things so if that means emitting SQL from a natural language query if it means doing basically uh keyword search because the language model can figure out which entities are in your query and then search for those entities as keywords in your database or the semantic based Vector search these are all valid techniques I don't think that there's over engineering here I think that this is because we have these different types of data and they're represented differently we ultimately have to transform our natural language request our natural language task into something that's understandable at the data level so it's just different ways of doing that yeah great answer last one for you uh and we completely ate the uh The Prompt injection game we're gonna do that tomorrow it's been postponed because this conversation with Anton has been incredible if you wanted the prompt injection game that was on stage two today we did a little bit of an improvising like a jazz band but that's what we're good at running these virtual conferences this question from Manoj Manos this question is awesome first of all uh hit me up because you deserve some swag so we're gonna get you some chroma swag and anyone else's question that I asked Anton we're gonna get you some swag too so hit me up uh the there's more questions in the chat Amazon and I'm gonna direct you there once we're done with this so keep the questions coming what's up all right here we go the last question for you manage is asking is there a limitation on how many different heuristics we could try to co-locate embed in this is a great question I love this question for a variety of reason before I founded chroma um I spent you know probably the last seven years of my career in robotics and the number one thing that you learn about robotics is that all heuristics are brittle if the heuristics worked you wouldn't need a model to actually be able to get the results that you want and I think something very similar is true here which is why I'm talking about a future where models actually handle a lot of the a lot of the tests we want them to perform now there's another implicit part is I don't think I don't think today there's a limit I think it's necessary to experiment and find out what works so that later we can generalize these things into actually usable tools so for now I think just use as many heuristics as get your application into into a usable State and then see what information you can extract from those heuristics um now the broader question here is this question about co-locating embeddings um I'm not sure if this is what's meant but there's a very interesting result which has been folklore for a while in in sort of the AI community and there's been recent Empirical research about this where it seems that embedding models tend to learn similar representations and when they learn different representations it seems that an affine transform so in other words just a matrix multiplication between two Vector spaces is enough to transform one type of embedding from one space into another embedding space and learning those transforms because they're just a single Matrix multiply rather than a large deep neural network is much much cheaper and what that means is if you have pairs in two different embedding spaces it's probably possible to link those together and that has a lot of implications for multimodality it's got a lot of implications for per user relevance tuning um things like that so I hope that addresses the question awesome all right very very cool man so I'm gonna direct you to the chat now now you got to go to the you got to go to the the chat and chat I still have messaging disabled I don't know what that means you may have to fill out your profile okay you'll get your messages uh you got to give us your email so we can send you subscribe email all right we gotta you know what we're gonna do we're gonna send you some chroma sdrs they're gonna chase you down and see if you want to buy some manage promo all right I'll complete my profile hang out in the chat with uh with the first for a little while thanks Demetrius thanks for helping moderate here we go I'll be in SF in two weeks hopefully we can yeah yeah send me an email we'll get it done all right man all right man [Music]