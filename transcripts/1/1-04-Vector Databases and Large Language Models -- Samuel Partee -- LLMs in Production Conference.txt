there we go okay I'll try to Blitz through it because I know we don't have much time um but thanks for everybody to stuck around through the technical difficulties um talking about what Vector databases in large language models I understand party I'm a principal engineer at redis um and without further Ado we'll get started yeah uh first talk about what are vector embeddings uh mostly since not everybody knows what a venture database is for even what goes inside them uh vectors are commonly they commonly represent unstructured data audio text or images they represent these in a highly dimensional embedding and essentially I just say this is a list of numbers where each of those numbers represents some piece of information and these come out of machine learning models which you've heard all about today probably from you know open AI hugging phase so here um and it's become incredibly easy to use these and actually extract embeddings from these apis and use them um we'll give an example here of similarity search and how this actually works uh I'm breezing through this but I actually with Demetrios and the whole crew at MLS Community wrote a blog post on this a little bit ago which includes a lot of this so if you have any more questions about how this kind of search actually works just let me know that link so three semantic vectors that's our search space here represented by the three red lines in the plot do you see to your right and then once semantic vectors are query um that is a happy person you can imagine that any three of these were created by the code on the last screen from a hugging face model or an open AI model or what have you and each of these um three semantic vectors makes up our vectors or space when we take our query Vector that is a happy person what we're doing is calculating how similar they are trying to find the most similar Vector to that query but how we do that we just calculate the distance we say how far is that Vector that list of numbers that represents the input that we put into that machine learning model how far is that from any of the other vectors in our Vector space and we do this through a metric called cosine similarity in this case where that actually calculates the cosine distance between any number of those vectors you see represented in the plot there and so you'll see these numbers down in the bottom right and that is a very happy person is obviously the most similar sentence to that is a happy person even though that is and happy and uh are all words shown in the other sentences that is very happy person is the most semantically similar to that as a happy person and so that is a major advantage of these models is the ability to capture semantic representation which will become very important later next slide please so the search space that we were talking about it can actually be represented inside a database called a vector database and that's where these Vector databases come in is that you have all of these embeddings that you've created from any number of these apis or models and you put them into a badger database which also provides the ability for a secondary index and so in this case you can have an index and all of these embeddings stored in the same place which allows you to then provide a query interface to Applications so Vector databases are essentially a methodology by which you can operationalize the ability of vector similarity search um this makes it easier to deploy and do things like crud operations when you go to production and you're actually using these in a rail application next slide please so why am I talking about this why is redis even in this conversation uh because with redis search redis is a vector database um so when you have both redis and redis search you have the ability to do secondary indexing on hash or Json documents stored inside of redis um we have two index types flat and hierarchical navigatable small worlds um and a bunch of Integrations coming out you probably heard from Harrison Chase earlier or if you're going to the MLS camia in the last meet up in San Francisco later today you'll hear even more from people like Sebastian from Fast API and Harrison and Simba from feature form um about all of the cool things that uh you know they're doing in that space we have Integrations there also really cool in the relevance AI that actually allows you to basically have a GUI on top of your vector database all of these are very important um but really one that we're really excited about is our GPU index that's coming out with uh Nvidia so we're working directly with them to be able to put your index on GPU so that's a little bit of why uh red is just talking about this and what we've been doing in the field um but if you want to try it out there's an open AI example cookbook that Docker run right there will tell you how to spin an instance up and try it out yourself but next slide please but we're here to talk about large language models so what a vector databases have to do with this um and because large was not large enough this essentially means these large language models are already incredibly encompassing you know training on all of Reddit and all of Wikipedia and all of these various places but they don't know everything and especially they don't know everything about um what you're doing your confidential information your proprietary documents your rapidly updating pieces of information so we're going to talk about how Vector database actually solve that portion of the large language model problem next slide please so there's three that we'll talk about today context retrieval large language model memory and large language model caching um I'll talk about each of these in the context of various use cases but essentially the the hottest one right now is called context retrieval you see people doing this with like the Retrievers in line chain the way I like to think about this is uh the vector database is a Golden Retriever and the large language model is essentially a someone playing Fetch and every time they want to go and get something the gold retriever goes out and gets it for him um I say this because the operation is performed by a vector database are relatively simple and straightforward just like playing fetch however the operations before by a large language model are not um and so that's why I like that analogy because it's specifically supplies the large language model with that particular piece of context that it needs for a particular information and retrieves it for it um so that is also relevant to a lot of different use cases we'll go over but a really interesting one is actually large language model memory it's Sim similar in the case that it's providing a con contextual database outside of the large language model that the large language model may not have been fine-tuned on but in this case it also provides specific enhancements for things like chat Bots which we'll talk about and the last one is a simple caching use case but in this uh in this uh type of area you can't just say is this the same piece of text is this query the exact same because really it's not always the exact same words but they might be the same question and so you can imagine how Vector databases might be used for that so we'll talk about each of these as we go down so next slide please okay okay so q a systems are really huge right now for all types of use cases Google Docs really didn't like my bullet points when we uh translated these um but um you'll see a bunch of different ones a bunch of different use cases that should have been listed there at the bottom um but you'll see an architecture here that you can find on GitHub uh redis Ventures is our GitHub so there will be a bunch of different ones in there that you can go out and check out um but here the Venture database is used as an external knowledge base like I've been talking about and so when you go and ask a question what's going to happen is that question is going to be turned into an embedding and that embedding is going to be used to search through the vector database for semantically similar context and that context will be retrieved that golden retriever analogy for the next stage you can think of it like a chain thanks Harrison um you can think of like a chain um that the next stage will be the generation where that context is going to be used to inform the large language model of something that it may not know about something that may be surprised or something that may be confidential or something you might not want to have put into the fine tuning process this is also cheaper than fine-tuning and faster and allows for real-time updates to the knowledge base think about you know you wouldn't want to have something on a millisecond time scale that you needed to fine-tune for instead you could have an external database where that context is rapidly updated so if you're making trades in the stock market or something and you really wanted the latest news that you wanted to go put into something that suggested what stocks to trade on as finances my thing but that kind of thing you would need an external knowledge base that would rapidly updated the pace of the stock market next use case please slide thank you okay so long long term memory I'm gonna Blitz through this um we'll just go on to query cat uh one back one back if you could so just like I mentioned for chat Bots it's really it's really useful to actually have context so um on the previous page you saw how in Q a system you might actually have uh the user asked a question and then the previous chat history be used as the context to answer that question well chat GPT memory is a really interesting project that allows for uh addressing topic changes in multiple user sessions and addresses this problem of context length I know we now have 32k tokens but at not all models have those and at the same time even 32k tokens isn't enough in a lot of cases and this particular methodology allows you to have only the last K messages relevant to a particular message in some chat history isolated for that particular session or use case um so this is a really interesting way to address that problem for a chatbot like scenario highly suggest checking out the project next slide please and then lastly this actually this diagram was taken from uh the zealous team they had just released gbt cash um it's a really interesting concept um some people have already started implementing this with redis that we've seen gbd cache is a really cool project though um it's it's essentially where you use a semantic Vector database to say um if I have a query that is semantically similar enough and I already have cached the answer to that semantic query and there's some threshold I have decided upon that I say is okay for that query to be answered in that amount of time then I can simply say return that answer um and so what this does is it saves on computational monetary costs it speeds up your applications because large language models are slow and generally it's applicable to almost every single use case that employs a large language model we've also been working with a version of this called the Trident response cache with Nvidia which is soon to be coming out as well really interesting work in Saving on computational costs with caching so definitely uh go and check that project out and keep up to date with uh where we are in that project as well um one last slide I know it's a lightning talk I think I'm at way too long given the technical difficulties but um if you have any other questions hit me up at sam.party at redis.com or at Sam parteen on Twitter or GitHub and our Solutions page for the marketing folks that are there um and so if you have any other questions definitely let me know there's a ton more to talk about here and we're gonna be giving more talks about the year but thanks to Demetrius and then lost Community folks for having me on appreciate it thank you so much Sam that was awesome and I appreciate you going at lightning speed [Music]