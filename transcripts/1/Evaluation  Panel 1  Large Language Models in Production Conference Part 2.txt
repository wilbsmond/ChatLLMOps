now we've got a panel coming up all about evaluation so I'm gonna bring our host with the most onto the stage Abby how's it going hey what's up I am excited for this panel that you are going to lead right now and before we jump on and do the full panel I just wanted everyone to know that Abby and I Abby's like one of our most frequented co-hosts on the ml Ops Community podcast so if anyone wants to check out the podcast that we do there is a QR code go ahead and take a shot of that snapshot or just scan it and you will see uh all the different podcast episodes that we have Abby if I had to put you on the spot and say your favorite podcast that we have recorded which one would it be I think I'll have a hard time picking from two one is by Alex Alex strathner from snorkel and the other is from two people I'm very good friends with now which is Marianne pashak um that one just came out so yeah if anyone wants to check out they had this really cool ml Ops framework and maturity assessment tests that you can do if you are wondering where you're at with ml in your organization they talked in depth about that in one of the most recent episodes so I'll leave a link to that in the chat and let everyone dig in that was a really good one I must say and so I uh just want to mention too because there was a few technical difficulties in case anyone cannot see or your uh your stream got paused like my stream did just go ahead and refresh and it should normal hopefully and if it's not getting back to normal let us know in the chat so we can troubleshoot but you should be good now let's bring on the rest of the guests first up Josh Tobin is here with us Mr full stack and deep learning and also the surprise guest of our in-person Workshop that is going on in San Francisco later today so it's good to have you here man I'm glad that we are getting to do this and this is happening we've also got sohini Roy coming up hello and this is where I get to say the last person then Abby I'm handing it over to you to take over all right and where's amrutha you there yes there it is okay so now there's too many people on the stage I will remove myself and let you all take it from here okay hi everyone so very quick intro for Josh uh America as well as sochini Josh is the founder and CEO of gentry they're working on tools to analyze explore and visualize and so you can improve your models using Gantry they have an SDK that is available and Ruka is the CEO and co-founder of structured.ie they're building their engineering tools for llm so everything from your injection to your gear e-processing pipelines they've got you covered so he is the senior developer relations manager at nvto and one of them there's a lot to mention about her she's been very active in the envelopes as well as a Lego mobs Community where one of the things we want to talk about today is the open source toolkit that they've recently released Nemo card triers and Nvidia or any more work that parents released on llm-based conversational systems so I want to start with all of you first because you've been in the space for quite a while all of you from ml to now in l m M's how do you think and but also because now you have a company in this space plus you're leading in for for amruta's best for sohini specifically you're talking to a lot of Enterprise companies right now how do you think about evaluations at your company oh who goes first so Josh sorry in that series beautiful what all the experts talk first yeah um I have a lot to say on this I mean should I just do my whole thing or uh oh I can do the short version um yeah I think um for those of you who have been in the ml World um pre-lms um you know there's kind of a way of thinking about evaluation that we're used to which is um based on the fact that in the pre-lm world most of the time the project that we're working on starts by building a data set um and most of the time since we're training a model we have a clear like objective function that we're trying to optimize on that data set and so that makes you know a naive passive evaluation really easy because it's like okay you hold out some data from your training set you use the same metric that you are training the model on and that gives you an evaluation of how your model is doing now that hides a lot of complexity under the rug but um uh I think moving to generative um Ai and loms um some of those assumptions are violated and that's I think what's making this so difficult for a lot of companies to figure out so first of all um when I started an llm project I used really don't start by building a data set I usually start by you know thinking of what I want this thing to do and then just you know coming up with a prompt to try to encourage it to have that behavior um and so that means that um one of the key challenges for evaluating llms is like what data do we even evaluate them on how do we know what the right data set is to um to test these models on and then the second question comes from um or the second challenge comes from violating the other assumption that we have in traditional ml which is that you know oftentimes in in generative ml we don't have a clear objective function right like let's say that you're building a summarization model how do you measure whether two summaries of the same document whether one is better than the other or whether one is adequate it's really non-obvious question um and so those two challenges I think are at the core of like what makes this really difficult for a lot of companies and um I'll uh I'll pause there and I I think there's a um a framework that you can use to Think Through how to how to do this well um but I I will I'll save that maybe for a later question oh man what a what a cliffview evaluation I agree that you know thinking with the end in mind first that's definitely what we got our clients to do like uh similar to what Josh was saying you know uh but also it's really tough when you're you're creating something for such a generalized environment I think a good example of of where this was interesting actually you know what we'll pause in the example because I think that'll be helpful later um but just coming back to more broadly what do I think about evaluation I think ultimately it comes to accuracy and speed right but accuracy is so dependent so on on what the actual goals are um you know thinking about I guess more detailed questions is it hopeful does the model actually do what you instructed it to do and when it comes to accuracy does the model actually answer um something correctly is it coherent those are two very different ways of measuring you know how accurate how how well you know things were set up I think hallucinations is is another really important um thing to be considering so to the model makeup any part of the response how are you putting controls in for that you know are they are they deviating um another part I think is um context so does the model actually remember what you're talking about are you staying on topic um and I think also for for my particular realm when we talk about the framework Nemo guardrails which I'm sure we can get into in a little bit for how it can actually support but is it safe is it secret kind of like a when candel shakes photo that's like kind of what I keep reminding my clients sorry you know is it secret is it gonna when is it safe are we gonna execute malicious code it's going to call an external application stays robust enough to prevent individual identifications you know especially when thinking about Healthcare and privacy codes um toxicity bias I mean there's so many so many different factors but if you really had to sum it up to the top two is it accurate and what's the speed um and latency for it so much to talk about this is a really great session what do you think um yeah um great answer um so one thing I always think about is when you're doing evaluation for um you know let's say like a classifier there's also some notion of like what ground truth looks like what a correct answer looks like if you're building a classifier of like oh cat versus dog or red versus blue like there's a ground truth that you can always operate from and like evaluate against I think with LMS um one thing that I notice is because everything is so qualitative even from person to person like thinking about what does a good answer look like what does a bad answer look like what is the expected length how concise should it be what kind of tone should it have there's a like they're like I think attributes of like what a good answer looks like and that varies from like based on the expectation of the user of the model so I feel like there's an opportunity to build an evaluation mechanism that is highly personalized and I think The Primitives of how to build that are things that are still um up in the air right now and you know really excited to find that what so I I think I really love the collective purse effective here but and but I wanna I wanna ask you more so about you know how do we think about that perspective for different companies so right now there are two different kinds of interest groups uh when it comes to large language models the first are the model Builders and the second are the people who are developers who are building their own application on top of these large language models forefoot an ideal performance main for both of those interest groups and how do you have how do you decide how to think about building an evaluation framework I mean you're probably on each of those sites yeah I mean I think um one thing that you're pointing out is one big change that I've seen in the industry in the last um uh six months or so is um call it the chat GPT effect right so um in the old world the olden days you know back back before many of you um even you know ml was even a twinkle in your eye um machine learning projects or these like just really technically complex um uh High effort projects in most companies right like there's I personally don't know of any examples in large companies where a project a deep learning project took less than a call at six months and most of them took more than a year um now in the post-jat gbt world um all these companies like you see these announcements every week or every couple of weeks from companies that are building announcing their you know um uh chat gbt empowered feature and if you talk to these folks the amazing thing is a lot of these a lot of these products were built in like weeks like three four weeks um so you ask yourself like how does that happen and the answer unfortunately um for those of us in the on the ml side of the house is the reason why they ship those things so quickly is because the ml people on the team didn't build them um there's the software Engineers that built them and you know that's not that's not to say that ml folks don't have a role to play uh going forward I think actually evaluation is one of the critical places where ml folks have a role to play but what um what Chad gbt did is it really um kind of lowered the um the the barrier to entry and also the um I think what's really critical is like the the level of intimidation that most folks in the org have about interacting with these systems so if you're like an exec in a company and you're sponsoring a deep learning project you kind of you know you kind of like hire the the Nerds the phds and you let them do their thing for six months and you hope that something good comes out of it but now if you're building like a chat GPT project you've been playing around with chat GPT on your own you know you know that it's not um that difficult to get it to do what you want to do and so what I'm seeing is what we're seeing is that a lot of folks in um uh in non-technical roles are a lot more involved in the process of building these applications than they were um six months or a year ago and so the implication that that has for evaluation I think is actually really positive which is that um you know one of the key things for evaluating llm based applications is that it's um I think the thing that we need to find as an industry is the right mix between automated and human based evaluation um but the great thing about having more stakeholders involved is that you can evolve those stakeholders in the process in such a way that they are helping you progressively evaluate the quality of your model as they go um and so I think you know I really see like the um the non-technical stakeholders you know sometimes they're actually doing a lot of development in problems missionary sense but they're also like the producers of evaluations and then the technical folks are more the consumers 100 I mean like I think that's really great I think that it um the the line is also I think for domain specific information like grabbing people who have been in the industry or for that particular domain for 20 25 years right and bringing that information and layering and sitting down with you know software Engineers ml engineers and actually layering that in it's a delicate balance I think again between human evaluation and automated evaluation um it's it's an interesting question I have to mull a little bit on that uh I don't know ambertha any any other things to add Josh it's so great that you're going first I feel like you're you're getting the right like frameworking for us um yeah um I don't think I have too much to add on that one yeah I think that was a very very comprehensive answer I'll say which is basically like I I guess it it depends which is most of at least my personal experience has been talking to a couple of companies is none of the large language models at least and big companies have been deployed in production we're not really talking about opinion we're not talking about Google in all of those companies but let's say about Banks which were huge when it came to financial model again as such or were there adapters or machine learning models in as such um and it's it's great that we've reduced that time for everybody to get started and have more stakeholders because that was one of the problems with machine learning which is like bringing everybody together on a single table and say you know let's let's assess what you really want out of this model so my second question here is we're seeing so many evaluation benchmarks right now from home to um you know to a couple of others and there are so many leaderboards that are available how do we create stable experimental setups to be able to evaluate and monitor the accuracy of our applications during and post deployment llm-based applications during it was deployment yeah um maybe I can kick off on that one so one thing I often think about is how do you like anytime you have an experimental setup you want to keep all the variables the same as much as possible and I think like in this case like having like a set of prompts uh that you uh test against in Benchmark against on like a continuous basis is like super important um so one thing we've noticed even from experimentation that we've done at our company is you can put the same input into like gbt 3.5 turbo like and it gives you a different answer like one hour later versus like one hour before and um I think that you know there's like a lot of reasons for that and it's like not like deterministic in the way that it answers and so it's um the outputs are you know they're going to be different even if you have the same inputs but I think that like if you do a qualitative evaluation where you keep the inputs as stable as possible and like maybe you have like some sampling and ongoing measurement of the system health and like perhaps alerts if it like drops too low you can make turn this into like a um observability problem as well in a lot of ways and I think that building those kinds of systems will help like you know like help us respond when um when the quality of the output goes bad yeah I totally hear that I mean uh not to do the not to the founder thing but that's definitely one of the things we're betting on at Gantry is uh sir Goldie playing a key role here um but uh one thing I would want to add is um I think if your role is a researcher then um or if you're in a really early stage of a project where you're kind of deciding what model to choose then um public benchmarks are helpful um I tend to prefer the sort of Elo based benchmarks um I think they um correlate a little bit better with my subjective experience of interacting with models and how they seem to perform but if your job is not a researcher or you know prototyping something but your job is building an application with language models then you should not rely on public benchmarks like they are um basically almost almost equivalent of useless for you in that role um and the reason for that is they are not evaluating the model on the data that your users care about and they're not measuring the outcomes that your users care about right so if you think about like measurements in machine learning in general there's kind of like I think if there's like a pyramid of uh of you know usefulness versus uh ease of measurement um the most useful signal that you can always look at is outcomes right like is this machine learning is the system that this model is part of is that solving the problem for your end users um difficult thing to measure but really really valuable to measure um and then you know easier to measure but less useful than that are things like hey um are there proxies that we can look at like uh you know accuracy or asking another language model if the output of this language model is good that correlate with the metric that we ultimately care about and then all the way at the bottom of this pyramid really easy really accessible super low hanging fruit but not very useful is like looking at publicly available benchmarks um you know you're just not really going to learn that much about your task by doing that my two sons you know like amartha said standard station is n't when it comes to this kind anything automated testing I think Senate monitoring systems and those custom dashboards through that observability stack I think is is getting really exciting I've seen a lot of really interesting tools that are that are doing this really well um I'm actually curious to dive a little bit more into to Andrew's response to this as well but I know Fiddler shout out to arise I think their CEO or CPO is doing a presentation on this later I think there there's stuff is really interesting hugging face also did um some custom evaluation metrics that I think make it much much easier for for the proper setup and making sure you're synergizing and evaluating consistently throughout um uh I think Josh you mentioned earlier doing user human feedback for order of development ongoing I think is is going to be continuing to important for for how we set up the evaluation and monitoring the accuracy long term but uh you're right I think those those public standard benchmarks is probably the the base and then continuing to to evaluate and use zero metrics will be super super important yeah so one of the questions I wanted to ask you because we're going into the domain of talking about you know we need to have very domain specific companies specific benchmarks uh what are the popular use cases for large language models that you've seen uh in in production right now think one example that that might come to mind is I think Google's uh I don't know is it Palm Med Palm uh PA LM however they choose to say it they release their their second version I think back in in April and it's it's a really interesting case study on how to develop something that's very domain specific because um I like it because it's very much designed for purpose I think Josh said in the beginning about how do you design the prompts and the testing data appropriately for the outcome in mind right how do you design for the end goal in mind um they use data sets that were in Q a form that were long and short answer form um the Imports were you know search queries for biomedical scientific literature and in robust medical knowledge so that was actually a really really great example of a good foundation but I think um they had metrics that also aligns and they tested it against I think um the what is it the U.S the medical licensing questions um as as their metrics for actually determining is it successful um but but in addition to that what I liked is they they reached out for human feedback not only from clinicians uh clinicians to answer the accuracy of the specific responses but also from non-clinicians for you know arranged backgrounds you know tons of different countries and languages to make sure the information that was coming out was accessible um and something reasonable and I think that's actually you know everyone's gone out of you know Google spiral asking medical questions so I think it was actually very relatable and and the results were were really phenomenal in their ability and the results to to answer those medical specific questions in a in a you know in a strong way um I hope I've kind of dug into that but I think that's one particular example where where I think that that was done really well I think another one is actually the Bloomberg GPT I don't know if that's being done out in production but I think um as far as domain specific benchmarking and assessments for financial specific questions that one was was a really strong example I think the use cases that I found really compelling are um in like the customer success and customer response uh categories and I think like it's one of those things where um you know being able to respond to your users quickly is like you know a huge win for companies and being able to scale like uh customer uh interfacing uh roles is like a big Challenge and can be expensive but um I think with LMS you have like an opportunity to um really like like Leverage like knowledge bases that you might have and expose that in a way that can like you know enable your customers to like interact with your product in a more meaningful way and I think that's really powerful um there's also like a really good built-in way to evaluate the quality of responses here because you can just ask the user like oh did this solve your problem for you and you can also look at like um you know other metrics like oh how often are they coming back like how how many messages did it take to resolve this issue and they're like a lot a lot of really great ways to kind of proxy the quality of the responses and so I think that's a good one Josh you want to add on that I mean how can you right sorry I I think Josh got Frozen but in in the meantime silence I know am I having a connection issue by the way no he can move he can move okay um Josh we were asking if you want to add on that uh no I mean I think there's kind of like in terms of real use cases there's the there's Trifecta um it's uh like information retrieval search uh question you know um one category information retrieval um so it's things like search uh document question answering things like that um then uh second category is um uh chat so a lot of companies are building sort of chat Bots into their products um use cases like customer support um product features um and then the third is uh um yeah like uh text generation sort of marketing uh Copy Type use cases those are the three that that um that we see the most which one do you think is like most robust right now where is is doing like meeting the needs like the best right now um I get asked this all the time yeah it depends right like it's um for all of these there's such a range of complexity right it's like really easy to just kind of dump you know uh chat GPT API into your product and call it a chat app but then how well do you actually need it to work for it to be useful depends a lot on the context of your product and what you're trying to get it to solve um so I would say all of them are relatively usable um depending on the broader product context and I think this is kind of like um uh General point on um how how to think about language like machine learning applications in general is like I don't think um I just made the mistake that I don't I don't encourage other people to make which is like I don't think it makes sense to think of ml use cases grouped by like technical use cases um it's really you should think about them in terms of product use cases because that has a lot more of a role in determining the difficulty and um challenges that you're going to face when you're building this application then you know what model you're using or um what set of techniques you're using to glue models together um because ultimately at the end of the day like things that make the the thing that makes Building Products that ml hard is that machine learning models are just don't always get the answer right that's always been true it'll always be true it's a it's the nature of the technology it's probabilistic right so it's not always going to be right and so the question is like how do you build a product in a context where you know one of the components of your product is going to get the answer wrong like you know that it's going to get the answer wrong um and so the broader product context of like what does it take to actually um work around that limitation is more important for the difficulty of the project than what set of algorithms you're using yeah one of the things I also wanted us to touch upon maybe probably the last question I will get before we open it open up the panel to uh audience q a is as you're building the model you need lots of data and um all the data might may not be good there are there techniques like early stopping to be able to filter the right kind of data sets um and after that yes you've built your model you've you're now trying to deploy but you're also putting some checkpoints and doing some sort of logging as well throughout the process how do you guys think about evaluating the performance at those specific points what tools have you seen being used and what are the gaps that you've seen in in terms of the tooling industry when it comes to a router for you very specifically when it comes to DSS what are the apps that you've seen and Josh and so really for both of you I want to understand also from like the perspective of the model checkpointing itself as well as the prom as well as like the prompt trips and stuff so I'll let amruta go first uh her let her cover the idea side of things and then we cover the modern side of things yeah absolutely um so in terms of like data set quality it definitely makes a really really really big difference I think a lot of a lot of systems are very garbage and garbage out and like you know if you start off with like a bad like Baseline uh you're going to be kind of disappointed with the results and I think like oftentimes a curating the data set itself is like a really big challenge um I think like sometimes like having like um rules or heuristics about like oh like this is like understanding your data set like okay this is like the missing data the quality of the data like this is what's um incomplete this is what's perhaps inaccurate or not in the same form as like the rest of the data I think like like you know some standard um data set cleaning techniques make a big difference but then I think also curation makes a big difference I think with a lot of um a lot of these problems please like what you are looking for uh you like hypothetically wouldn't really need a model as well like if you could Define like that with words perfectly um it would be difficult to you wouldn't really need a model to do that it could just be like a bunch of if statements so you know you're kind of dealing with like ambiguity around uh what you're looking for and so there's also ambiguity around that like curation of that data set and so I think it's like actually an iterative process and so like the evaluation isn't really like a one-time thing but it's something that you have to iterate on and like um try to find uh overtime as well um not sure if that was like fully what you're looking for but just general thoughts in that space uh Josh do you want to add on top of that and go into the water side of things yeah I think um I'll take a slightly different position I think um uh I I totally agree with what you're saying if we're talking about machine learning but this is the llm's conference and so um my strong intention is if we're talking about llms most of you like most of the people in this room right now you should not be thinking about training a model um the there is very low likelihood that you're going to get better performance on an NLP task by training a model than you will by prompting gpt4 that's just you know that's unfortunately just the truth it's a it's a new world that we're living in like we've um talked to like many companies at this point that had kind of ongoing NLP projects six-month projects nine-month projects year-long projects that um they're able to beat their performance by switching over to um large language models calling an API doing some prompt engineering maybe some like few shot in context learning in a matter of weeks right so um I think if your goal is to build products with machine learning especially with um with NLP then I think the the rule of thumb is like no training models until product Market fit like if you haven't built this thing out and demonstrated this thing can be really good using models that you can get off the shelf um for NLP then it's not going to work like you're not going to be able to do better if I train in your own model um for other domains of ml that's I would have a very different answer to that question um and then on the tooling side I think yeah you should just come talk to Gantry we've got good solutions for all the problems that you just described tell us about it give the give the pitch yeah I mean pitches um the pitches you know I think the um the way that machine learning evolved is like there's so much focus on training models and our thesis come from the beginning and I think this is definitely true in the llm world is that training models is not the hard part anymore um the hard part is you have this model you've trained it and you've gotten over the hump of like you know the statistic that like every mlops company used to say of like 85 models 85 models never make it to production which is like a BS statistic and it's definitely not true if it ever was true but um you've gotten over that home if you've gotten the small introduction that's when the hard part actually starts right which is like okay how do we know if this thing's actually working how do we know if it's solving the problem for our users um how do we maintain it right like I think um really common issue is um you know as you start to scale up the number of models per um ml engineer per data scientists on your team the percentage of time that ml Engineers start spend on maintenance increases and getting you know quickly dwarf the amount of time that they're spending on actually building new features um and then lastly you know um if you even if you have models up and running in production and you're maintaining them you're leaving a lot of performance on the table if that's all you're doing because if you think about like what distinguishes the really great machine learning powered products from um you know average ones it's like if you look at like chat gbt or Tesla or things like that the thing that they all have in common is that they're maniacal about building this loop around the model building process that involves like looking at outcomes looking at user feedback feeding that back into the training process right so if you're not doing that your your machine learning power product is just not going to be great um it could be okay it could solve the problem but it's not gonna be great um and so what Gantry does is we basically um have uh infrastructure like an infrastructure layer that supports um uh opinionated set of workflows that teams of folks working on machine learning Power Products can use to collaborate on this process of taking a model that's been deployed and using production data to maintain it um and to make it better over time and so we make that um a lot cheaper easier and more effective for machine learning teams to do together it's hard to follow that I mean a long time fan of Gantry a long time fan of your uh blog series it's kind of nice to hear your your perspective on it I think for me uh for tool sets I know there are a couple of ones about prompt engineering that are coming out I have to play with them a little bit more to to know you know which ones are most effective um I'm getting an inkling on which one's going to be the best um but uh I think from the Nvidia standpoint obviously we we talk about guardrails a lot um it's our new open source framework that came out we talk about um you know I mentioned a little bit like how do we make sure even the if the the data in data out problem is is already you know something that that we're gonna it's it's challenging right um but I think if you create the right guard rails for it so making sure that we stay on topic making sure that you know when a someone has to chatbot about your product different offerings are you going to go and talk about um competitor offerings instead of your own right um also making sure that you've got you know I talked a little bit about like secrets and malicious code and management of external application access and uh interactions with you know uh to leading to misinformation and toxic responses and inappropriate content that's also something that we're all going to have to work through I think guardrails will get you part of the way there I think we're still experimenting with with kind of some other tooling that can help um with the rest of it but definitely I would say check it out it's open source Nemo is open source um I think it might you know supplement a lot of the other tools that that kind of were mentioned today I can't see any of the questions by the way Abby so we're gonna have to trust you to so I have I I have one question I think that's probably the I don't know Demetrius is here are you going to kick us off or can we ask like the music I make a Blog for the this Workshop I'm doing tomorrow yes do it and then yeah so doing like a um Hands-On workshop for full hour on um uh this exact topic so um if uh folks want to learn more about it or chat more about it I'll I'll be there um tomorrow at some time in the morning PST I don't remember what time but it's on the schedule so as much as I would love to continue this panel I sadly have to cut it off because we are running short on time and as you know that is my one job it's to keep us on time uh and so I loved everything you all were saying and there was a ton of questions in the chat that were coming through so all of the panelists it would be awesome if you jump in the chat respond to some of these questions that came through and I will uh leave it here and get you all out of here but ask you to stick around and say a few things in the chat yeah foreign