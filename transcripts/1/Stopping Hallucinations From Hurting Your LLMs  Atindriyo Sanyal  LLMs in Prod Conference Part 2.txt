our next guest I'm so pumped about um this is the CTO of Galileo he's had he has over 10 years of experience at some massive companies like uber and apple he was on the Michelangelo team at Uber and I think he was part of the team that created maybe the first Feature Feature store ever um we'll have to bring him on here to uh verify that so without further Ado let's hear from attendrio hello how's it going holy and nice to be here thank you for having me I'm doing great how are you it's nice to see you're looking a little blurry maybe we'll give you a moment okay I think it's looking a little bit better uh yeah how you doing thank you so much for joining us no likewise thank you for having me this is very exciting and very excited to share some new work on hallucinations that we've done over at Galileo yeah I've been really looking forward to this talk so let's let you get started and here are your slides super awesome thank you so much uh so yeah first of all uh thank you for having me here and thank you for joining this lightning talk uh there's a lot to talk about we only have 10 minutes so I'll just get right to it uh just a bit about me I think Lily already kind of mentioned uh I'm one of the founders of Galileo which is an ml data intelligence company and prior to Gallery I've spent most of my career building machine learning platforms and ml systems spanning Post 10 years uh worked on Siri at Apple for many years and as a staff engineer and leading a lot of the core components of the Michelangelo platform at Uber and where I primarily focused on data quality and data diagnostics for machine learning and we also created the first feature store a few years ago as as Lily had mentioned uh so I'll just get right to uh what we're gonna talk about today uh it's all about llms and in particular one of the key data quality issues that plagues a lot of these LMS uh and can potentially be a very big deterrent to you know productionizing practical LM systems in the coming months and years and that's today it's termed as hallucinations uh but want to dive a little bit deeper into what they are what they mean and how you can sort of detect them and avoid them in your systems so there's no question that today's llm especially young gbd3 and above they are extremely impressive in their responses but if you really squint uh they're wrong much more often than you'd think and uh these errors kind of include you know factual mistakes that they make uh there's misleading texts that they spit out there that may look linguistically correct but they're more often wrong uh and this is what typically sort of refers to hallucinations and uh what leads to these hallucinations uh is typically more of a data problem as you would see you know there's when you train these foundational models um there's a lot of issues around overfitting the data to the model there's a hidden class imbalances and insufficient data of a certain type that's missing in the in the training and eval data sets and often validation data sets have less coverage than their training baselines and encoding mistakes as well as poor quality prompting all all this sort of is a drop in the ocean of the reasons why uh an llm could hallucinate and the outcomes of hallucinations is basically what you see on the right I don't think that needs uh much of an introduction so now how do you tell whether a model is hallucinating or not or rather can the LM themselves tell us whether they're hallucinating now to answer this question we need to look a little bit deeper into the model itself now these LMS are essentially next token prediction machines uh where you know at each moment they're essentially choosing you know the next best token or the next best word to spit out from a collection of tokens where there's a probability distribution assigned to each of them now this is a very very like 20 000 foot view of what a sequential model typically looks like uh you can double click into the Transformer box and you know there's many variations of of it but for the purposes of this talk I think we can do with this simple view but the key thing to remember here is that there's token level probabilities and their distributions that tell us a lot about what the you know models outcome or what the llm thinks about its outcome and is one of the key indicators of Health nation and we'll get into that a little bit in the coming slides uh but first I want to base this presentation on a lot of these llm experiments that we've done on hallucinations and some very promising results that we've seen and how we are baking it into the Galileo product I touch upon a bit towards the end but first we wanted to define a problem statement for our experiments and and that essentially kind of came down to quantifying hallucinations through a metric which can automatically be for all llm or all kinds of llm responses uh and beyond that it's important to point out is subtext level hallucination which means that often these these models would spit out Blobs of you know text and often there's subtext within within them maybe a sentence or two which are hallucinated so this metric needs to be very granular uh and the method that we sort of we went over many methods internally but the the the summary of the method that kind of came down to was the fact that we wanted to use an open-ended text generation where we would curate a set of inputs and llm outputs as our data set and we did a lot of exploration on standardized data sets being used uh but then we would examine the token level probabilities for each of these completions from these models Channel them to a third-party neutral models to get some extra signal and then eventually determine if the base completion of the the the output of the base model was hallucinating or not so that's the the summary of the methodology uh there's some assumptions here which we've made and one of the key assumptions is that all these models are state of the art and by state of the art I mean gbd 3.5 and above uh and this is a very important assumption because uh a lot of these models uh especially gbt 3 and above they're trained on a wide range of knowledge and they're capable of answering you know many kinds of questions which would otherwise stump the older models which are below three GB3 uh so this new definition of hallucination that we're creating they cannot be based on some of these you know older research that was done two years ago which involved you know analyzing mistakes which those models made because those are essentially and the newer models are pretty much immune to it uh and the goal for the metric is to you know focus on accuracy it needs to be as accurate as possible in its hallucinations as well as diversity because open-ended like text generative models there they can be used for a wide variety of tasks as opposed to some of the more limiting model architectures so the hallucination metric has to sort of diversify across different kinds of tasks now when we start the experiments we essentially employed a two model Paradigm there's a completion model which is the output of Interest like the the actual model which spits out the output and then there's the probability model which spits out the probability tokens now in an Ideal World the the they would be the same model but more often than not especially with openai models and uh some of these these other proprietary models there's often situations where you get completions but you don't get probabilities so uh and not all models basically give you enough information especially when you consume them through apis so we had to do a bunch of experiments on figuring out which combinations of probability and completion models gave us the highest bank for the buck in terms of hallucinations talking a little bit about the data sets that we experimented on there were a whole host of uh data sets that we kind of explored in general this area is very new so we had to kind of create some data sets on our own uh but amongst the existing ones some of the the top candidates for us which gave us you know decent bank for the buck were you know there's a self-check GPT Wiki bio data set which is essentially a data set of Wikipedia biographies there's a self-instruct human evaluation data set which is a more like open-ended text generation data set and the one which gave us most promise and uh we the one we found most challenging for some of these model newer models uh was the open Assistant data set which is uh one for a an open source chat GPT like assistant uh so these were the data sets that we conducted a a good chunk of our experiments on um and then we kind of got into the metrics and the baselines that we want to create for these these uh these data sets uh and the Baseline we essentially used three uh key metrics uh one was log probs which is essentially the the log of the probabilities of each tokens that appears in the completion uh there's ppl5 which is a metric which was published by one of the the papers I've referenced here in this in this presentation uh and that essentially measures the entropy of the of the model's probability distributions but particularly in the top five tokens uh and then final is the pseudo entropy which is the metric that Galileo has created and it's a heuristic on top of Shannon's entropy uh but again we look at sort of the top five uh token responses uh from um from the llm output foreign and then across these metrics we evaluated average versus minimum uh because here we a lot of these these uh metrics that we get there at a token level and but the output of the LM overall is a blob of text so we've done a good number of comprehensive experiments uh around this almost think of it as a cross join of all this uh there's another set of baselines that we explored which uh sort of term as multi-model baselines and this is sort of driving the intuition that uh the third party model can give us extra information or or key signals about hallucinations because it's not biased by the its their own data uh and here we we looked at uh for example there were three API based baselines which were all using GPD 3.5 or chat GPT uh and here in the first one chat GPD QA we used the gbd 3.5 model to essentially write a question and an answer and then use the same model as a grader for for for the answer and we saw mediocre results from from this particular method chat GPT token was the second sort of Baseline we established where uh it was an improvement over just looking at log probs because often what we noticed is that you know a lot of these llm responses as the initial set of log probes are the model is highly uncertain about some of its initial tokens so that kind of drives the Min log prob to a much lower value than you know almost unnecessarily so in order to avoid those kind of you know phrase uncertainty biases we use chat GPT token and then we created this new method called chat GPD agreement or chat gbd friend which gave us really good results uh and here we basically Leverage The 3.5 model to essentially do reruns on the base model and get multiple outputs and then see if there's enough agreement between the completion and the reruns and the intuition there is again you know if if a model is hallucinating then likely for more than a single run it would give vastly different responses as opposed to something it's very sure about so we got some very interesting results using this particular method uh there's some other multimodal baselines that we've also explored which typically includes self-check mechanisms where you do reruns and then you compute the bird and the blue scores to see the distances between the the all pairs of completions and uh uh so just to highlight uh some of the experiments that we've done uh they involve you know all of these baselines now going a little bit deeper into the results I just want to quickly highlight the most promising results that we saw uh the first one was one minute so the the first one was it gave us 63 Precision uh when we used DaVinci uh as both the completion and probability and then using pseudoentropy we got a 69 accuracy in detecting hallucinations I've pasted some charts here for you to go over offline but here's what we've found these are outputs of state-of-the-art open AI models uh quotations from the Shakespeare's the temp test which were completely made up non-existent books as well as URLs that do not exist on the web and this is a drop in the ocean of kind of things that we saw so we've baked in hallucination as well as some of our other famous well-known metrics such as data error potential in our system and we have two new tools one is to help you create prompts and manage prompts it's called Uh The Prompt inspector and it allows you to select the best prompts and finally if you're fine-tuning a foundational model we we use depth as well as our new hallucination metric to really show you noise in your training data as well as hallucinations in in your evaluation as well as the models outputs so yeah just cutting to the chase these are the two products which will be launched just a quick shout out to the team which has made this possible they're incredible people and finally you know we're launching llm Studio very soon just go to rungalileo.io slash llm studio and you know just uh I can't wait for you to use the tool yeah thank you awesome thank you so much um and definitely share any links in the chat as well um to answer your questions thank kids um all of these videos all of these talks will be recorded slides we're collecting we'll make sure to get those into your hands so thank you so much this was wonderful thank you so much [Music]