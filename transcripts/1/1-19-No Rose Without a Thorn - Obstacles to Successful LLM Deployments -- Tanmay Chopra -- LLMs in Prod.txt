[Music] foreign [Music] so I'm going to hand it over to you and let you get cracking man awesome thank you so much uh hi everyone I'm tanmay um I work in machine learning at niba which is a search AI startup and previously did the same work at Tick Tock uh so I'm here to be a bit of a bus kill about llms and and talk to you about the obstacles to deploying llms successfully to production um so if you've spent any time in on Twitter at all you've probably seen uh hundreds if not thousands of demos of llms um if you work in Industry you've probably seen very very few of these ever get deployed to production um if you work in statistics probably hit me right now for not giving you a scale for this diagram um but why do such great MVPs and demos never make it to production um well there's two big chunks of challenges that we see with deploying elements of prod and they come in the form of sort of infrastructural funds and and output linked fonts where infrastructure thoughts kind of refers to technical or integration linked challenges where output link fonts are essentially the output of the model the text that it's generating uh that might cause problems for and block us from going to to prod so when you come to the infrastructural side there's sort of four big buckets that we look at problems in the first is alarms are slower than some of the status quo experiences we're used to a really good example of this is search right where you have very very quick results that users are used to but now when you start generating llm output um it takes significantly longer for that response to complete as ex as compared to their status quo experience there's also a lot of decisioning that goes into taking llms prod one of the biggest decisions is do you buy or do you build whereby kind of refers to purchasing API access to a foundational model and build usually refers to fine-tuning some sort of Open Source llm and so when it you see the buying case tends to pose much smaller upfront costs but as you scale uh starts creating a lot of challenges in terms of costs on the building side there's much higher upfront cost and a lot more uncertainty on whether your llms will be um at the Quality level that you needed to be able to demo uh or actually go to production there's also some emerging challenges around API reliability in case you do choose to buy um and this usually tends to emerge from cases where um infrastructure serving infrastructure is still being built up by foundational model providers and users can very quickly lose trust um in cases deployed to prod um when these when they experience uh even infrequent downtime one of the bigger challenges is also evaluation we're still leaning somewhat relatively heavily on the manual side um and we're looking for more and more clear quality metrics for this output on the output link side um the the major challenge that we're seeing especially when you start integrating these models into pipelines is output format variability given that these are sort of generative models there is a certain degree one predictability to the response uh which can make it quite challenging to sort of plug into some sort of pipeline that expects certain formats uh this is the easiest one to solve but there is a lack of reproducibility um where the same input might give you different outputs even for the same model um and then finally we sort of come to this world of uh adversarial attacks or adversarial users where you see challenges related to prompt hijacking and and as an extension trust and safety uh where the model might generate or be forced to generate um intentionally malicious output that might be considered undesirable but does this mean we're doomed are we always going to see sort of this case a very small case of production I don't think so um what I've just covered is sort of pretty much the whole landscape of challenges um some of these are mutually exclusive if you buy you'll face some if you build your face some but it's highly unlikely that you would face all of these challenges um so let's talk Solutions when it comes to the infrastructural side um in terms of speed you can make models faster or you can make models seem faster um when you make models faster that's sort of the conventional machine learning techniques of distillation pruning um trying to use smaller models where bigger ones are not necessary this obviously sort of leans towards being able to build um but if you are buying uh there are ways to make models seem Faster by leaning into human computer interaction techniques so you can load animations you can start streaming output um you can start parallely using um outputs to the core tasks that are more complementary versus blocking um and in terms of costing decisioning this is a tough space but there is a somewhat optimal approach which sort of entails buying while you build so using that low upfront investment getting to Market fast validating your MVPs and then over time sort of collecting data and fine-tuning models uh in-house to make sure that your costs aren't getting infeasible in the long term with adoption when it comes to sort of this foundational model reliability space um the best parallel or the best analogy is sort of this multi-cloud approach where you think about these fallbacks across different providers we have yet to see a time where two of the major Foundation providers have failed together so this does seem to be as satisfactory approach to dealing with this um there is also this component of failing gracefully um users do understand that this is a technology in-depth development and so if we are reliant on sort of API reliability it does make a lot of sense to think about what happens in The Last Resort case where you do fail um to deliver output and furthermore when it comes to this evaluation infrastructure um the way I like to think about it is this is a great time to fail fast with fail-safes so make sure that you're not causing sort of trust and safety related failures but when it comes to the core product itself um it's totally fine to start going to production faster um and using very strong user feedback and feedback loops to make sure that you're iterating as you go um another really helpful approach is to link your llm Integrations to some sort of Top Line metric so that could be anything from say stay duration to session length um and and evaluating how this impacts that change on the output link side um output format variability is probably the largest chunk of challenge um I would say in terms of reducing this to a huge amount few short prompting really helps so you actually go ahead and give output examples in the prompt itself um there's also a couple of really cool libraries I think we we had one of those speakers talk about these earlier which is sort of guardrails in realm that can kind of help you validate output formats and actually iterate if you need to um call the llm again um again failing gracefully is always helpful just one easy fallback um in terms of lack of reproducibility this is absolutely the easiest one to solve uh you can just set the temperature to zero um I sort of think about prompt hijacking and trust and safety in one bucket largely because uh the main negative outcome of being prompt hijacked uh does sort of lean towards generating outputs that are trust in the safety layer could solve um so I just want to end with some thoughts or tips around how you can start strong how do you get your first llm to prod and how do you make sure that use case succeeds um the first and probably most vital aspect to think about is Project positioning it helps massively to be able to focus on deploying to non-critical workflows um where you're able to add value but not become a dependency as we're sort of building up more reliable serving infrastructure we can start serving more critical workflows but things like output variability API downtimes sort of push us in this Direction Where We should be trying ideally to add value but not become a dependency uh it also helps to sort of have relatively higher latency use cases um these are scenarios where things where users have lower expectations of how quickly outputs will be generated and so gives you more space to sort of create value um in a low barrier to create value the third one is probably the most key one to make sure your llms don't just go to prod but stay in prod and this is to plan to build while you buy so make sure that as you're working towards deploying your LM with an API solution um you're also figuring out how in the long term you're able to scale those costs uh in a manner that's feasible and lastly do not underestimate the HCI component um in this case llm success is largely determined also by how it interacts with the user and it really really helps to sort of respond seemingly faster or fail gracefully or enable large-scale user feedback and that's pretty much it from my end over to you nice dude awesome thank you so much for this for those who want to keep up with the conversation and you want to ask 10 many questions because that was awesome really incredible there's some really cool questions coming through in the chat go ahead jump in slack he's there go to conference [Music] I know I've got some show everybody what is this [Music]