all right so our next speaker I think everyone is probably pretty pumped about it is Sophie dally I'm I'm not gonna pronounce it right so I'm just gonna bring her on the screen is it dally Daley ah it was so close Okay well productionalizing llms for striped support girl fast uh here are your slides take it away great thank you so much so um my name is Sophie Davey I'm a data scientist at stripe and today I'm going to chat about the Lessons Learned building our first application of large language modeling in Stripes support space so for some very brief context stripe is a payments company we offer a lot of different payments for data products and we serve millions of customers all over the world so as you can imagine having a large Global customer base and a wide Suite of products means that our support operations org is very very busy every week our Asians handle tens of thousands of support cases where they answer questions and solve problems for our customers so as the majority of this support volume is text-based data llms are an especially exciting opportunity for support and there are many applications where this type of ml can add value but for our first undertaking we decided to Target the root of the support problem space which is a very complex but high value task of answering our users questions so our ultimate goal here is to help agents solve cases more efficiently by prompting relevant responses to user questions using GPT our customers will always talk directly to our support agents but we want to prompt these agents with relevant responses so that they no longer have to spend time doing research to look up answers which can be really time consuming given how complex and wide-ranging the support problem space is so for Success we need to make sure that our ml prompted responses are information accurate and hit the right straight tone so The Human Experience is extremely important to us and we don't want our agents to ever sound like Bots of course our ultimate measure of success here is that the agents are actually using this tool and it's helping them solve cases more efficiently foreign so the very first lesson that we learned is that llms are not oracles when we ask out of box DaVinci GPT a basic support question like hey how can I pause payouts GPT will give a very plausible sounding answer but unfortunately it is actually incorrect and this is true for the majority of questions that stripe customers ask because the materials that GPT has been pre-trained on are either out of date perhaps incomplete or else confused with generic instructions that might relate to another payments company and while we could definitely improve this specific answer via prompt engineering the scope and complexity of stripe support space is just too large for this to be a viable solution so we found that to solve problems that require deep subject matter expertise at scale we need to break down the problem into more ml tangible steps first we need to identify whether the user is asking a valid question or not this step will remove any chit chat or questions that don't have clear enough context next we identify what the topic this question relates to and then using topic relevant context we generate the answer to the question finally we modify the answers so that it meets that perfect striped tone which means that the response has the right friendly but succinct tone that our agents Target and the benefits here are that now we have a lot more control of our solution framework and we can expect a lot more reliable and interpretable results thanks to fine-tuning which in our case completely mitigated hallucinations also the other great benefit here is that fine-tuning on GPT requires just about 500 labels per class so we could move really quickly here relying exclusively on Expert agent annotations for our pilot our end solution consists of a sequential GPT framework the first two fine-tuned classification steps filter any candidate user support questions to our fine-un fine-tuned agent response model which generates the information accurate response and these responses are finally adjusted to meet striped tone before being prompted to the agents so the second lesson we learned was how important it is to prioritize online feedback and monitoring during our development we relied on blacktest evaluations to measure ml performance for our classification models this was standard practice using label data sets for our generative models we engage with expert agents who manually reviewed and labeled responses so that we could get quantitative results we also engage with agents for user testing and training data collections so that they could dictate exactly what the ml response prompts should be for different types of input questions so after many ml iterations our offline feedback trended really well and we got to the state where we were really confident in our ml accuracy and ready to ship for our production setup we designed a controlled experiment to measure the effect of cases where agents are prompted with ML generated responses versus those that aren't and unfortunately because online case labeling was not feasible at this scale we had a considerable Gap when it came to unserving online accuracy trends so once we shift we realize that the rate of which agents were using our ml generated prompts was much lower than expected very few cases we're actually using our ml generated answers and because we didn't have visibility into online accuracy trends we were pretty in the dark trying to understand what was going on and whether there was a discrepancy between online and offline performance so in the absence of online accuracy metrics we developed a heuristic based match rate to represent how often our ml generator responses match the response that agents actually send to users so matchway provided a very crude like lower bound measure on our expected accuracy so that we could tell how the model was trending and this was really important to us to be able to validate the quality of our ml responses in production so even though our offline user testing feedback was really positive and our online ml accuracy Trends were good in practice agents were just too accustomed to their flow of solving cases so ignoring our prompts and this lack of agent engagement was a huge bottleneck for us realizing efficiency impact and we really needed a much larger ux effort if we wanted to increase adoption so there were many big learnings from this experiment the first one was to always ask yourself whether human behavior can affect solving your business problem and if so engage with your ux team really early and more practically for ML we also learned the importance of deriving proxy online metrics in cases where we don't have the right data to measure accuracy or whatever metric you're tracking exactly so directional feedback using heuristics is a million times better than being completely in the dark we also learned to ship each stage of the framework in Shadow mode as soon as they're ready besides waiting for one large end-to-end ship that way you can debug as you go and validate functionality and Target expectations per stage sequentially this experience also really taught us to prioritize monitoring efforts as highly as other ml development tasks often monitoring is compromised as something like we can catch up on later after we've shipped and this is especially easy to do when you're working on a lightweight resource constrained group like we were but the lesson learned is that a model is not shipped unless it has full monitoring in a dashboard because this online feedback is key to ensuring that we're actually solving the problem we're targeting so the last lesson I'm going to talk about today is about how data is still the most important player when solving business problems using llms I think there's a bit of a misnomer that newer or more advanced llm architectures will be able to solve everything for us and we just need to integrate or write the right prompt but llms are not a silver bullet production requires data collection and testing experimentation infrastructure Etc just like any other ml model and I think the age owes 80 20 rule for working in data science was very true like writing the code for this llm framework took a matter of days or weeks whereas iterating on the data set to train these models to Commons and in our experience iterating on this label data quality yielded a lot higher performance gains compared to using more advanced GPT engines also so the types of ml errors that we were seeing related to gotchas in the Striped support space as opposed to more General gaps in mindwatch understanding therefore adding or increasing the quality of our data samples usually did the trick when we had a gap in performance we've also seen that scaling is more of a data management effort as opposed to advancing our solution to a newer model we actually found that collecting labels for generative fine-tuning models adds a lot of complexity so for our second iteration of this solution which we're building right now we've swapped out this generative ml component and replaced it with a more straightforward and are moving away from generative more basic classification is so that we can leverage weak supervision techniques like snorkel machine learning or embedding classification to label data at scale without requiring explicitly human labelers we also have in heavily investing in subject matter expertise strategy program to collect and maintain this data set so Stripes support space changes over time as we advance and grow our products we need our labels to stay fresh so that our model is continuously up to date our goal is that this data set becomes a living Oracle that will guarantee our ml responses are fresh and accurate into the future so to wrap to recap the goal of this project is to help our support agents solve cases more efficiently by prompting relevant responses to user questions the three big lessons from this talk are number one llms are not oracles we need to break down our business problems into more ml manageable steps two online feedback really is key monitoring is a priority just as much as any other ml development task and that's the data is King a good data strategy will outweigh any fancy llm architecture especially for solving business problems that require that deep domain expertise at scale so please reach out if you have any questions we have a lot of open roles that strive if anyone wants to join and work on some of these problems with me that would be great awesome thank you so much Sophie I loved it and I always loved like the stripe Graphics they're just so like warm and beautiful thank you all right well thank you so much and have a wonderful rest of your day thank you bye-bye thank you