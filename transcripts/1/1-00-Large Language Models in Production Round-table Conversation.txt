hey everybody super glad that you are listening to this large language model in production Round Table I want to mention before we jump into it that because of the success of this round table we ended up organizing a full-on conference it's a virtual conference it will be held on April 13th and you can register now we're going to have incredible speakers like the founder of human Loop the creator of Lang chain just to name a few and also the CTO of adept AI they just raised 250 million dollars wow all right so if you want to register for that check out the link in the description and we will see you there on April 13th I'm not gonna chat for too long I just want to talk real fast about the survey that we just put out in the mlops community this survey is kind of like playing off of what we're about to talk about and I'm fascinated by like using large language models in production and how people are doing it if they are doing it and if you're not why are you not so please feel free to fill out that survey it'll take like literally five minutes it's the most basic thing ever and that is my like one call to action that I have for everyone before we jump into this great conversation I'm going to hand it over to Diego man I appreciate you running this so we will talk soon I'll be back in a bit excited to have everybody here a great subject great amount of experts so excited about that the next hour we're going to be chatting about large language models so I figured we kind of start really quickly with a quick lightning round of introductions uh Rebecca we'll we'll start with you hi everyone yeah super excited to be here and about this discussion um I'm Rebecca I'm a research engineer at Facebook AI research and so I work on primarily NLP are focusing on the robustness of large language models I also have worked on um fairness and responsible AR um aspective um of MLP as well excellent how about you David hey hey uh good to see everybody I'm a vice president at unusual Ventures VC fund I focus on our investments in ml so I've been thinking a lot about language models and have spent before that like uh six-ish years working on ML Ops across a couple of startups techcon and determined Ai and uh producers at Ford where I did ml platform stuff like like a lot of folks I've talked to in the community so honest hi um my name is honest I'm a machine learning engineer at digits um at the great chance to write machine learning related Publications um one about natural language processing and then about envelopes and we use uh language models in production to assist the company so we're digits are storing is like basically a real-time engine to understand books for business owners and accountants and in basically no time and we use those language models to help uh start the conversation between accountants and the business owners so we can talk about the details later excellent and James everyone uh really started to be here I'm James CEO and co-founder of Bountiful and we're building a monitoring and testing tool for foundation model workflows so one tool for FM app developers that allows them to actually observe and then test the cost quality and latency across their workflows excellent great and I'm Diego Oppenheimer I'll be your opinionated moderator today I uh to give you I'm currently serve as a partner at factory HQ which is a venture fund uh that specializes in AI Investments previous to that I was an executive running on the lops for a company called Data robot and previous to that I was the CEO and founder of algorithmia which was one of the first mlops companies and also is lucky enough to be part of this in my labs Community since I believe there was double digits uh of people in the in the community so uh it's been really really exciting time to see all the hard work that Dimitris and pellen Etc and friends have put into this guy because well what up what size are we at now Dimitrius because it's a it's a cross fifteen thousand right yeah it's around 15K it's wild easy amazing so uh really excited so um a lot a lot of hype around Chachi Pichi and kind of what these large language models look like and so I figured that we could start with grounding on like what do we actually mean by a large language model I'm gonna pick on Rebecca on this one uh to kind of uh you know Define uh what's what's the proper definition of what we would look at as a large language model sure yeah so this is a bit of a contrarian opinion but I think this is actually really poorly defined and this is uh just a new term for you know what we can say is transfer learning which is actually existed for decades but I can give people a bit of an overview um for those who aren't as familiar with the background and history of ml so starting in the 1990s this is when people started talking about machine learning and artificial intelligence and this was a period of smaller statistical models and gradually in 2010 we start seeing deep learning so Indiana coins term so we had larger data sets more computation we'll see higher level features emerge through training on these data sets and so really this is a process of homogenization so rather having bespoke feature engineering type plans for each application like you see that you could have the same neural network well deep neural network architecture use it for a lot of different applications and this process of homogenization has continued and that's really what we see leading up to today where we have a really powerful large language models can be used for a variety of applications and so I'd say another inflection point was 2017 when we had the transfer Wars Paper like the attention is I need paper and then in 2018 we saw the Burt model so then he's um you know the years from 2018 until now we saw so much like exponential growth and NLP and mostly happening on the research side but now on the production side as well so it's been a super exciting time for LLP um you know 2018 we had Bert and then 2019 we had the Roberta paper which showed that a few um train with architecture about like 10x data set you actually can beat Bert on pretty much like all the tasks of the blue benchmark and then we just saw you know this is this is like what showed that a scaling matters and scaling is really powerful and then we just saw this influx of you know money and resources being poured towards training large language models so we had Brit large with 340 million parameters gpd2 with 1.5 billion parameters T5 with 11 billion gpt3 with 175 billion which I imagine a lot of people here are probably using for your apps and and now we we have like really powerful guys from opening eye that we can use and I would say that this is um more of a succession of just us moving from very specific models for specific tasks to large pre-trained models that we can use for a variety of tasks so yeah I would say that this idea that's really Central to Foundation models or large language models is this idea of transfer learning which is that you know you can train a model on a lot of information and you can use it for a variety of Downstream tasks so instead of having like one model for sentiment classification and one model for question answering and one model for maybe like ranking your ads you would just have one large pre-trained model that's very powerful and you can just apply it with very minimal changes to a variety of accounts from tasks okay so I guess to clarify a couple of things we look at it as you know large language models is a subset of foundational models right because I think foundational models in general would actually apply to not just language models but also kind of like image multi-model whatever is coming out there's some characteristics to these large language foundational models I think in general we can call them that they're no code in terms of like we access them via natural language which really expands the ability for people to use them right I mean I think that's really what's one of the biggest things that's inspiring the second kind of what you mentioned Rebecca they're kind of you know there are no tasks right so they're actually general you know pretty General they're not task specific and they generalize quite well and I guess we're still figuring out to what other tasks they generalize on on a daily basis and I think one thing that that I know which is I think also really interesting is that they don't really suffer from a traditional ml cold Stark problem right which is like you can actually start working with them without any sort of like pre-trained data so one thing and I know you said you had a contrarian review but like one of the things that had been like thrown out there is like what do we consider large and it feels like some people say like okay like the billion parameter kind of like uh you know line is the you know we'll dry it it felt kind of of arbitrary I mean it sounds like a nice little round number but uh you know do you have any comment on that yeah yeah so um I I think this is uh also super interesting I'm sure people have seen the the Llama models coming from Facebook um but now we're actually seeing them maybe um you can do more with smaller models but just train it on more data um so for anyone who's curious you can look up the chinchilla paper so there's this idea that models have to be a chinchilla optimal which means that if you have like a one billion parameter model you have to train on x amount of data but then if you increase the number of parameters you have to train on maybe like 10x or a thousand extra data for it to be chinchilla optimal so um really it's not just about the parameter size it's also about the amount of data that it was trained on because a lot of I'd say the first generation of these large models were under trained and that's uh what researchers are realizing right now got it awesome great so so we have these you know access to these really large models uh you know and I think there's a uh I think it's kind of funny because we went from you know very very specific like task models tons of training data hey we're gonna go do this this and this and now to the more generalized models to now like really or say well actually what we really want to do is kind of like the generalized models but also combine them with smaller models and I think a lot of the reasons why combining the smaller models is really around okay these things are actually really hard to put into production they cost a fortune and the kind of like economics are slightly flipped right I think like anybody who's um I've spent almost a decade looking at just production problems and as soon as we kind of looked at these like llms I was like I I don't like how where like how we're gonna make businesses out of it but cool thing is here is that hot is actually spent quite a bit of time doing some of that work of like okay how do we actually make decisions do we use apis we use our own models do we continually pre-train our own model so I just I'd love to give you the floor in time to talk to us about like your use case whatever you're willing to share but also kind of like what the thought process was right and how you broke down getting to uh you know a a specific in production use case here yeah um so there's like we use large language models in a variety of cases like we have this case of like regenerating some text for for some users we use like language models to Define very custom embeddings and I think to me that that is some beauty right now like in fact like five years ago we said like theater is a new oral with the large language models because there's so much work already pre like they're pre-trained it's more important to have clean and good smaller data sets so we can imagine that very custom use cases so it's not like that the quantity of the data anymore where we can bring them into production I think Rebecca made a really good point you need a lot of like data to train it to get to your initial stage but once you start reusing them then you can find this with a lot of uh with smaller data sets so that there's a the number one would be like there's a good motivation to use uh those uh language models because we can get to a production ready model with this smaller sample sets I remember like back in the day like 2015 uh and so on we were working on there on on our similarity model where we didn't have large language models and we spend a lot of time like curating the data just to get to initial state the other thing is like when we talk about like large language models in terms of generating texts we need to think about like how do we integrate this into the product so for example we need to protect the users against hallucinations or against like weird generated texts and so what we do on the digit side when we generate a text it goes through a very stringent not safer work filter if it's like the slightest rating there we store this several away and then we have we check constantly against patterns of hallucinations and if it follows into one of those patterns we also throw the sample and then um we make sure in our app whatever we show is like reviewed by an accounting before they actually send it to their client and then that is another layer of like there's a human in the loop who's like who could Satan this makes sense that doesn't make sense but at the same time um those large language models allow to integrate amazing applications so for example we're helping business owners to understand their their books in real time but accounting is highly subjective so we use large language ones to understand it so to understand the subjectivity with large language models and then we use like we train like similarity-based in machine learning models or we use those generated from AI some of uh the decisions you made previous to starting to work with these models so I think like it really really what I'd love to understand a little bit better is like you have the option of using potentially an API you add the option of grabbing some of these open source models that exist today maybe actually is it worth like for the panel actually like talking about what exists in the open source Community because I think a lot of people are unaware that there's a huge movement that is uh you know it's not just open AI building you know these models or the actually like big cloud providers but there's actually quite a big community of researchers who built open source large language models like Luther um and a couple others so um I'll actually let any of the panelists anything else want to kind of like take that one and uh you know because it sounds like you've all looked at it so uh I don't know if uh I got any volunteers here no pipe up right yeah yeah a lot of time kind of in the open source communities I mean number one it's unbelievable uh some of the capabilities that are kind of communities of people are are turning out I think I'm gonna actually message in the chat a few key Discord servers that I think literally just jumping into summarize some of the best work if you want to go and understand the open source pushes and so yeah you've got to lose the REI you've got copper AI who are championing some of the the attempts to kind of build open source alternatives to some of the largest models uh lion as well doing really interesting work I think look I'm I'm not a developer so I'd love kind of alongside me Hannah's or anyone else to kind of give a developer's viewpoint on this for sure when talking to companies who are building at the application layer right I think they start at least in in our experience they start building on top of apis and then they start to consider okay there's this sea of Open Source equivalent models that I could be using that might give me some Advantage based on what I really care about for my users let me start investigating this right and a great place to start is literally jumping into those three Discord servers to understand what's being done and obviously hugging face as well is kind of a lineup of what models exist and what could be used great so one of the things to talk about and then I'll I'll pass this on to harness right so these models are available for download which are great I mean they're not as easy as accessing them as like you know hitting an API in a lot of cases you're gonna have to do some Mainline pre-training or some fine-tuning yourself to actually make them usable but they're portable right and they're not relying on just being behind an API so when you have a lot of use cases that are potentially not good for just using somebody API and sending data these are definitely options so you're not kind of boxed out of using llms because of it so with that Preamble honest love to like you know walk us through the thought process here of and you don't have to give up obviously any secrets but would love to understand like considerations how you thought about the problem like what were those qualifications and then you know kind of like we can walk into like how you did it yeah there's no there's no secrets I would say like we actually in fact we published a blog post about like how we implemented it so somebody wants to Repro this go ahead and Conquer um yeah but um so when we saw this business problem with generated questions for example um obviously people go to the open AI playground to try it out and you see like okay is it actually visible with a few prompts and and then the person that was like can we actually Repro those things with an off-the-shelf a model which we could host in a house because the thinking there is like if that is possible then we have the opportunity to quantize and prune the model to reduce the latency and we have this major benefit of that not handing over the data to the to an external API which we're not in control over anymore and so for us with DNS financial data this is really Paramount to aerating video we promise our customers that we we're trying to like we're keeping everything secure and in-house so even going to open AI is like it's that burden is tremendous and would change like even if we have a ramification of the terms and conditions with our customers and therefore there's a huge motivation to like every used open source models and hosts them ourselves there's another benefit of like you have lower networks if you have the model sitting right next to you and your instance um there's like lots of cool benefits but to what I did what we were saying was like you do have to point changing the model um but in that moment you also have like lots of opportunities to like make those models smaller and make the domain specific um make them useful to your specific use case and the model can then in the moment forget about all the other like these kids are in the Stream So speaking of uh use cases and you know you kind of mentioned one word like very particular needed to be able to kind of bring get in-house like David you've been looking at uh a bunch of things you've been looking at the developer ecosystem David also wrote a pretty cool blog post uh about the developer ecosystem uh but like what use cases are you seeing and kind of how do uh maybe kind of talk a little bit about I don't think about when to use let's focus on production right and production use cases and what you know when to go in One Direction when to go in the other direction what you're seeing that's really interesting right now yeah I I guess like I I have a framework that I used to think about this which is like okay so we just talked about Foundation models and they've been coming for a while but like this capability kind of got dropped in our lap magically by open AI a couple years ago and so like when you when you think about what's being used in production now I think a lot of how to think about it is just like how much time have we had to execute with this amazing new technology we have and so I frame everything to like how long does it actually take to build it and so the obvious stuff that you saw in production first I think is like the most simple application of language models and stuff like Jasper and Poppy AI that are completing text right or in the like sales category like Reggie and autobound books that are kind of like generating sales copy and sales emails and things like that and so why it's the easiest thing like you the production is like basically just a call to open AI you add one API called your application and you're good and and you can do really incredible things and so a lot of the stuff that requires no infrastructure you see really successful in prod right away and then you get like one layer down like what's the next level of complexity as you try to do things I think like the most obvious stuff is okay we want to incorporate external information about the world or about our users into these models uh sort of like what honest was just talking about in one sense like you need to understand what's going on in the world to be able to do anything in accounting or documents or spreadsheets or whatever maybe and so there you have to start like building infrastructure databases Integrations pulling data from whatever sources you need and there's just like the fundamental time it takes to do engineering and so I think we're in the moment right now where a lot of what we're seeing come to production are tools that where people finally finish that engineering workload that it takes to like make all that happen and it even could be some not to fine-tuning and things like that but for the most part I think it's like just like routing stuff from a database connect to slack or whatever you know so like in that category like yourself like chat GPT it requires a little bit of context of like a whole conversation and rambling that or all of the bespoke chat Bots that got announced when they announced their new API last week a couple of folks that I've worked with like nem which is a competitive notion reads your historical documents and emails and stuff like that to help like complete um but do tax completion but more personalized to you working with a company that's in our portfolio called apt Edge who do this awesome like customer service work where they have to connect to like jira tickets and zendesk tickets and slack to like know what's going on so if that's like my tier two of complexity and for the most part people like in that year still aren't doing the amazing work that Haas has done to like actually go grab a model fine tune it that's hard work still and this stuff is like kind of somewhat straightforward even if it's more complicated so I think of like what's the next tier of complexity and uh I think there's like two big categories you can get to one is um like increasing model complexity so like being chains of calls where you like want to do agents and you want to call it to a language model and get a response and parse the response and call it the language model again and that just kind of gets on unwieldy still like we just haven't really done all like it's just actually kind of hard to be confident in the systems you build with that in a way where you're like ready to put in front of users I don't think a lot of people have gotten there yet so there's like these open source Frameworks like Lane chain and stuff out there where a lot of people are doing cool stuff that making chains but trusting that I think this is probably why James exists is because like trusting some of that back and forth with a model is still challenging and I think this is another engineering Gap where I think it's just going to take us time as a community and as a set of developers to figure out like how do you do it and how do you manage it and what you can trust yeah and the other side stuff like automation where you're like you know like a lot of this stuff you see now is co-pilot honest talked about this where you like you have a human in the loop to validate and that's because like the same thing or it's like hard to build all the tools we need to trust models right now but uh but where we want to go is automation where we have llms doing everything but that again requires just a ton of engineering work and going on that route like that's where people really think fine-tuning is necessary you kind of have or rlhf like the the more complicated methods of of owning and tuning models that you can trust it's yeah I talk about like there's a big difference when you give llm the keys to the car versus the autopilot uh there's a lot more things that can happen and so I guess like long answer but I think there's just a what we're seeing is more and more engineering complexity getting layered on to this amazing ml thing and and use cases spawn up nearly daily because of the amazing progress we're going getting uh but you can sort of track it on a on a straight line of how complex people are willing to go I think the next Frontier to what house is talking about is okay can we Allen weights can we distill models can we do lower latency work because there's so yeah that's a ton of engineering complexity some people are bold and on the frontier doing it um but I think a lot of people are still sort of in level one David you bring up a really good point that every little open source language model comes with it uh its own tokenizer as an example right like you have your bird it has like a workpiece tokenization you have your h11 but there's like an even like fitting those pieces together that is a whole Challenge and I totally glanced over this like I'm sorry about this but like once you once you get over this hump and you actually have the models and the weeds in front of you then you can run like for example the chance of a plugin and like see what can I cut out of this language model and make the latency fast and this is something which maybe you don't have to do for the problem but if like if you see a lot of inferences um that investment might pay off and then your this couple would see from your API you get a faster response then you save money long term um but it to your point if it's like this lots of engineering complexity in there so it's like I can see the trade-off product starts it might be easier to code or an API because you get it up and running faster and versus long-term benefits one of the interesting things is yeah Diego mentioned the front that like a lot of what's happened is this like uh democratization where so many people can develop with language models and so it's like oh I just go write a prompt and I've done something cool and man it's really intimidating just like it always has been to go from that to like I'm gonna download weights and learn tensorflow and the tuna model and use the tensorflow profile like I was an ml engineer for a whole bunch of years and I never use the profiler because like getting to that level is kind of scary still you know and I was like I think a decent one baby uh and it's still like just like depths of ml problems like it gets really complicated really fast and so um I think there's a lot of people sort of on the sideline once you get to that level of complexity because it's suddenly like yeah it's like we've told everybody you don't need to learn ml to do ML and then it's like oh never mind there's go fine tune the 13 billion parameter model good luck and then I think there's still a ton of work we have to do to make that uh as accessible as a prompt maybe I was just gonna double tap on that like a bunch of the we we've talked to many dozens of generative AI companies right and most of them are not the people building these companies are not ml developers they're not ml Engineers right they're not ml people they are software Engineers that are trying to build really interesting software by kind of plugging in these incredible pieces of Technology right and they're playing in the tier 2 complexity over that you just explained right whereby okay we're going Beyond tier one which is kind of you might describe as that kind of UI wrapper style like demo of like this is cool I can build on this but to actually building kind of semi-agentic chained systems right that get highly complex very quickly but even jump into that third tier where it's going to do it yourself ml development I think is just a huge barrier so I think we're going to see very large companies built in that tier two of complexity right so so we saw like Jasper enormous company right built on that tier one I think we're going to see a second wave of a huge number of companies that we're observing right now building that tier 2 complexity so we're we're building the tooling that enables people to actually manage that complexity such that they can build really interesting agentic systems and then potentially automated ones and then I think who knows when we get to like tier three right and like anes is done very non-trivial work as an ml developer an ml engineer to get to that stage I think in that two-tier complexity we're seeing a lot of Highly capitalized companies that are just building that tier to a software application players so that's why we're building what we're building is to try to enable them but I think we're going to see a huge population in those companies yeah so I'm gonna try to completely oversimplify kind of like what kind of three categories that we can talk about for llms in production when we talk about trust and interest we'll talk about testing the results security privacy anything that's regarding kind of like you know the you know the ability to use these let's talk about latency and and I'm pretty late and see a category because I think it's completely under appreciated how much latency matters like building any sort of software maybe to your point James about how all these software Engineers are actually working on these problems it might be more present but you know we'll talk about uh you know kind of latency and then we'll talk about cost right and so I think those three things and I think in terms of in all of these three things I think we can break it down into the engineering tools that are going to exist how we're actually going to apply them what people are doing what research is coming in because I think new architectures are coming out unzipped we can start on any one of them right so I'm going to Define I'm going to Define cost as literally what it costs to run and this isn't just about you know the API call like you know we're talking about when it's experimental the hardware what it takes to uh you know lift up that Hardware what it takes to kind of like run that at scale um as well as potentially use the apis obviously I mentioned a little bit about that trust right so trust will talk about privacy security like the use cases there and then uh I'm forgetting the other one was uh latency but latency which is really the definition of the use cases right so um maybe we'll start with like uh latency to begin with and you know James I know this is near and dear to your heart so yeah first of all Define to us like what latency is in these kind of like llm world use cases how do we look how we should look at it and more importantly maybe kind of like touch on some of the problems I think it's an unsolved Problem by the way so like uh I don't expect you to answer it for us but kind of like we can uh you know if you want to kind of like frame it for us yeah absolutely so I think you can observe and think about latency at two levels right one is that the kind of instance level like specific model and then one is it the the workflow level right from a more kind of user perspective because like you're trying to deliver ultimately some kind of user experience that adds value to someone such that you can build an application potentially build a company Etc so when you're cooling let's say you're at that tier two of complexity that David described right you've got a chained kind of workflow of potentially multiple instances of models it's going to give you an idea here I was talking to a Founder yesterday and he got lost walking me through his own kind of uh workflow here and he's calling seven different instances of models right four of them fine-tuned Babbage uh there are a couple of embeddings models in there right and each of those calls based on the apology is bringing in some sort of latency chained together at the workflow kind of level you have the ultimate latency that the user experiences I think that's going to be king right especially in the second level of complexity it's like I need to build especially for those who are building here thinking software right and to your point doe you said like maybe they're thinking more about this than than others they absolutely are so we we've talked about when in our conversations with companies is a cost quality and latency triangle right and depending on your use case you care about one of those more than the other two and like the best the best example of this is code copilot right like about it latency is everything you you have to stay within a flow state for your user experience such that you know I we could they could make those functionalities better but they don't want to because they want to maintain very very low latency that's how I think we need to start thinking about this is okay there's a latency of making one call to one model but then there's a latency of an end-to-end user experience yeah I love how you picked that example because I've like I've many times used that example as the kind of like state-of-the-art in terms of latency and it's pretty darn slow right like in terms of like when I look at right now it's a suggestion but like I want autocomplete and jumping from suggestion to back you know using doing autocomplete will require fairly significant engineering to be able to get that and actually like my gas and I'm not the expert here is that it will take kind of like re-architecting some of these models or uh you know up from the ground up so I don't know I think we have a question here that people are asking about production I'm a quickly Define production and then I'm going to jump to you Rebecca because I saw you get excited about re-architectures so what we mean about production is actually like an actual application that's being used for users uh you know and I would argue kind of like in a kind of like you know in the production manner so a service that is being like offered to other people that is running all the time this is beyond the demo this is beyond the uh you know kind of like toy weekend you know kind of thing this is like you're actually using it as part of a business process educational process whatever you're using production is uh I usually like to Define it like people scream if it goes down and uh and so I think somebody was asking about like does that count the open API service in Azure absolutely if you're using it as part of a production application like if you're just using it for your boy app at you know on your computer then I wouldn't consider that production so rewinding a little bit we'll go back to you know what is it going to take we're still on the subject of latency and you know state of the art right now is a couple milliseconds I think actually like there was a paper that talked about state of the art and kind of like what I call in a lab environment was a 29 millisecond inference pass which is like you know this this continues a bunch of use cases that I've been traditionally worked on in ml which are kind of like sub 15 millisecond use cases but back to you Rebecca what are we seeing that's going to be able to like hit this latency problem yeah no that's a great question a lot of thoughts on this yeah I think Hannah's uh the optimize optimizations you mentioned with quantization and model distillation awesome yeah there's a lot of great engineering work happening now um so I actually think that engineering especially in for engineering and for knowledge is really relevant so yes it's uh people need to learn you know Pi torch tensorflow uh learn how to find two models but um I think actually more and more just hard engineering skills are increasingly relevant to NLP and just to the productionization of AI going forward so so yeah I just really want to drive that point hell as well you need good engineers in this space on the inference side so um some of the inference being slow is indeed um Diego I think you hit the nail on the head here it's the Transformers architecture so Transformers is great for long range dependencies and capturing context and is actually faster in training because you have access to all of them but but it's not good for fast inference and um you know this is something that is very inherent to the architecture and that's why people are talking about latency and like immigrants needs so much because like we are bottleneck just by the uh Transformer structure and so if you compare Transformers to models for like uh recurrent neural networks like rnns Transformers are not naturally sequential so R and empty coding is going to be a lot faster than Transformers which have you know your multi-headed attention Matrix computations so you just have more computations and added to that we also you know we're talking about llm so there's also just more parameters and these models so in every board pass you have a lot more layers tensors together so it's just um yeah I think just everything just compounds to being slow right so yeah I think some of this uh will require you know if we really want to talk um like just like removing this bottleneck entirely as opposed to like working around Transformers which is what we've been doing it will probably require an architecture change that's going to be hard because we've seen Transformers have been so dominant since 2017 and it looks like it's becoming more and more dominant and not going the other way around if people are interested and this is you know still very much in academic research territory but something I've been excited about is um it's been called State space models uh which is sequential and so um that they underperformed compared to like Transformers models so far but they're I think you know that takes off um watch out for that coming out of Stanford um Jen we can solve the I'll put the paper in the chat the Hungry Hungry Hippos chat I just want to jump in there because Diego you said in passing right it basically invalidates right now a whole host of use cases like that late for that fundamental agency barrier like there are so many things that you can do in principle that currently you can't do in practice because of that fundamental latency Barry even if especially when you're at that tier two of complexity right at once so the founder I was talking about that I was they're selling this company initial pass on that workflow is 15 seconds right now that is a fundamental like end-to-end user Journey that just doesn't work it's a non-starter like everyone's like giggling a little bit right now he managed to get that down to like three and a half seconds through kind of fine-tuning Babbage instances rather than using large models Etc taking this tier 2 complexity perspective and it might be good enough it like that might be good enough to build a really interesting company on top of in that tier 2 complexity but that one cutter for other use cases where three and a half seconds just kind of lost you out of the room right yeah there's plenty of like orders of magnitude like for each person with a seven person or seven calls to an API there's someone with 70 in light or thinking of of 700 to do complicated things and so like they're it's hard to describe how many orders of magnitude room there is for improvement it's it's a lot and and I think like we we're talking about L M's in production right we're talking more broadly about foundational models you can set a kind of multimodal world that's coming down the pipeline right which is just going to be fascinating maybe that's an unhelpful grenade to throw into the room with kind of uh 15 minutes to go omega's health organizations we'll find out but I think uh one thing I wanted to get people's opinion on was like taking the Moore's Law approach of these large models right if we assume that some of these very large models are going to go just get cheaper and cheaper and cheaper and get faster and faster and faster like what changes fundamentally over the next 12 months if okay we don't know why but like maybe it's a new fundamental architecture but in 12 months open AI has a model that is just 10x faster and is 10x cheaper again like how does the world change I'd love to hear kind of instincts on that I think we potentially have like a Divergence right where you actually don't get you know you don't get actually the uh you know one like larger model Mega model that it just gets smaller and cheaper but we actually get to back to its highly specialized models or you know I think one of the I've actually seen implemented I mean recently and it's really interesting these kind of like start with one of these very large language models and actually use them as a router like a task router into a much more specialized task and I see highness is like naughty I'm guessing he's that's what he's doing internally uh which is kind of starting from a more generalized task right because you take advantage of the you take advantage of the kind of like uh no need for the pre-training kind of like you know kind of like no cold start problem you can like go into a use case and then route from there into something a lot more specialized um that accumulates latency though right and so I think like going back to kind of like because I want to like close down the thought on the on the latency thing like I think you know the the most important part that I see here is like you really need to go down to the use case and say okay what is this use case requirement right for a latency because you know if I'm looking at near real time use cases like the reality is that at least today right I'm kind of out of luck right like and so you're not really going to be able to achieve those you're going to have to look at different architectures smaller models distill it down to like very specialized to get anything even close to that and if you start thinking about again like you know you look at things like you've mentioned like you know chaining these things you're just accumulating latency right and you know across across those chains so you get into and again if you have a big batch process it doesn't really matter but I think that's a you know that's that's really interesting because I think we're you know I don't want to be like uh too deterministic here we live in a probabilistic world but like I think right now truly real-time use cases are kind of out of the question in like point today and time at least from uh you know kind of like production perspective uh at least from what we've seen now that said I'm sure that the Facebooks and Googles and uh you know kind of like microsofts are very much working on this problem but I think that's a reality at least in the in its current state anybody disagree with that or is that kind of uh you know can we get uh I don't disagree but I just think it's bold to uh be recorded making claims about where we are I set a point in time I said point in time you know yeah see you Mom and we'll be like ah do you remember when latency was a problem yeah but but I think latency is a function of like uh the business case the the way we deploy like with for most of the applications we're talking about we don't need the general model like chat GPT right like we can go through like a debate specific application and what worked really well for us just like sort of like the teacher student approach for like we take a larger model and then break this down into something smaller and cut out layers and and then so if I compare like the latency from our first deployment a couple of days ago to like what we have now it's like magnitude sticker it is like and we we were able to get like reduced models we would get the profilers we get Excel xla uh we get all these optimizations and so we get now we can even run those models on the CPU in a very condensed version and then and the costs player player role as well right like you can throw one at it and as somebody said in a chat like you could just use GPU right so if your business case pays for this that's the option but I think they're they're to make it very domain specific is the biggest uh latency saver you can you could go after right and there's there's a comment on the chat and I think like like let's talk about I think we we I'll rephrase it like we're talking about latency across the entire workflow right which also includes like everything else right like the system You're Building likely has overhead the problem is that you're actually significantly adding a bunch of latency while usually in like any of these kind of like online systems your offline system you know the latency your system's adding everything from i o to database pulls to everything like you're now adding significant amount more uh as you kind of like chain through these uh to these models so I love it you gave me the perfect segue Hunters here as they talk about and you might be able to kind of like put it on CPUs and uh anybody who's worked in the space knows that that is the uh Holy Grail for your wallet given that uh running gpus is about 4X what you can do on CPUs and so um I'll go back to I'm definitely stealing this James I love the triangle of the quality uh cost and latency and so you know determining the the use cases right because um when you look at in France and inference on gpus and you look at that on average and I think it can be on average it's about 4X more expensive that's what really you need to be like talking about so let's talk about cost and the use cases you know around that and how to think about I'm gonna pick on you David because you're probably looking at a bunch of companies and being like yeah this sounds great but if they scale like they can't afford it that's actually like uh you know run this company uh you know at any like cost like that makes sense even though you know GPT 3.5 is now 10x uh cheaper actually I'll just let you answer it's funny like um I think this is an interesting question because it's really important I and I've seen what happens at scale and it's almost like worth handing to Rebecca because like metascale cost becomes pay for whole departments by making one percent improvements on like their ad in for a ranking engine right like it's it's insane what happens at scale with inference for startups again when you're getting started I just like I don't know like money on fire like beat people win you know and I I just haven't seen it be a constraint early on it's certainly have to do the math so like it's really easy to back the envelope how much does one inference cost how many inferences will you do what's your margin per inference and you can even figure all that at math out like you can get there on the apis in particular pricing is all pretty easy to predict and so I always do that math it's good to do that math it's good to know where your margin's at but you know I think part of how I think about it is thanks to folks like honestly have gone down the road before in show notes that you can build domain specific models and there is room to optimize like it's a question of figuring out when to optimize for cost is for most of the people that I work with yeah and for most people it's not day one for meta it's certainly true that you have to do it and it's worth firing 100 Engineers to work on it maybe or a lot uh because it's so important but it's just this thing that you evolved with over time monitor I always recommend people to be aware of but I haven't seen it block a lot of use cases early on that said I know a lot of people that were really happy when the new chat API came out was 10 times cheaper because yeah this is like for startups at least you're talking about Runway like how much money do you have in the bank to survive is changing and that's uh critical but I don't know you can do it like I think you can hire an ml person you can learn ml you can fine-tune models you can like I think there are tools out there to avert cost which is something you have to be thoughtful of more than anything is this an exercise honest that you date up front like did you think about even back of the envelope like okay we're gonna go do XYZ this is what it's going to cost like you talk to people in your team that were beyond the ml team because I'm assuming this is like at least involved product management or leadership of the company in terms of like yeah we're going down this route absolutely uh we've been uh backup of limo calculations it's a little bit tricky with open AI like how many tokens does it actually require to solve this town we did some uh back a level of estimation went to the CEO it was like would you be okay if the stator goes to another API um there's all these privacy questions there and then there's obviously a Cost question and for the for the tasks we had invented um it was more beneficial to do this in-house we just given like a little bit of insights on the scale but we we have to process and close to 100 million transactions a day that is not like um something we could easily ship over to an API and that moment we have to talk about like latencies Etc and in that moment even the project of like investigating the model and figuring out what could be cut to so so we could actually run it on a CPU Festival was super beneficial so then we can we can talk about like what happens to be product but as a batch process we're streaming that or something but yeah it costs is basically the first question to be trans of the Holy Star project so we're gonna we're gonna go into the last uh subject of of the morning here I guess morning for us evening for James which is kind of trust and trust in these systems and so there's one term that I think it's worth defining uh which is like and some people might have heard of it that these models hallucinate and I'm pretty sure that's not because they're just trading on a bunch of people's Burning Man pictures maybe Rebecca you can actually kind of like help Define what a Hallucination is in uh this uh in the in this context and uh why uh I know we'll kind of discuss why this needs to happen I would say uh hallucinations yeah just anything that's not factually accurate and just falls into the accuracy problem when you're thinking about evaluating language model outputs you know sometimes it's open-ended like how engaging or interesting are these outputs sometimes it's like as a scoring are there grammatical mistakes um but what a lot of people are caring about when we're prioritizing these is I don't want the model to say that you know Michelle Obama is the president for example although it's like embarrassing with my brand embarrassed for our company um and and this is a problem that has actually existed in language models for for years but I think it's become a bigger problem because now people are actually using these lens for real world tasks so now you know it's not just like researchers writing papers about hallucinations how to improve it it's like actual uh you know users of these all um saying uh live model just you know wrote this this text for this Downstream use case like maybe like generating legal text for example and it's just totally wrong and really embarrassing for a company so yeah I think that this is um you know we talked a little bit about research directions I I think uh David maybe or has mentioned uh you know having uh basically external knowledge sources plugging in like retrieval right I don't think he's super true but like I think we're describing as like retrieval augmented Generation Um so yeah large language models currently as it stands uh they're very um poor sources of facts and knowledge uh and this is because it's just not very parameter efficient to be learning facts and so I think the way forward really is having these knowledge bases and combining it maybe with like a retrieval and then it actually assume that Chad venkat actually hit it uh really well because this is how I like to describe it I mean like we're used to very deterministic workflows and code and we've been building deterministic systems for a really long time but now we're applying kind of like a probabilistic uh you know kind of like implementation to some of those so maybe some of the tips and tricks and I mean not tricks really but like how how have people been thinking about making these things feel more deterministic and there's been a lot of techniques that have been used uh by different teams you know to do it but I'd love anybody jump in here uh in terms of how to think about some of the introduction of some of the techniques you can use to make these workflows feel more deterministic I think you talked about kind of like the teacher student kind of model that there's uh like kind of the you know uh human reinforcement like you know kind of model there's a bunch of these different models that can be used not to confuse with the actual models uh you know in terms of how to do it so I'll open up the room to anybody who wants to kind of like clutch on that because I think this is so important for us to build trust in these systems right and and truly adopt them in uh and and you know most people don't understand what a probabilistic workflow is uh By Nature humans are bad at it I think you've got to start the use case again right there are some use cases right now that have like high affordability of something not quite being right right such that I can I can generate an outfit and there's affordance for my my users such that okay if I if it gets inside you wrong or very wrong or whatever hey that's fine like I wouldn't shown the user and the user won't take a bad action on the back of it there are obviously low affordance use cases that we just can't surf today right like I don't want to like a medical diagnosis for an llm right because that's very low affordance right if it gets it wrong like uh it's game over right so and I think I wonder how long it will take until we we start to slide down that affordance scale that's kind of a as an aside to your question but I think it's good context a lot of others have those really cool approaches like they're basically databases with a constant lookup and so because we have to compute the weights uh there's just a fixed number of Weights but as a downside is like we lose out the accuracy of the of the results being returned right and so what I would I found really interesting was like okay because there's like this there's some submission uh risk in there and then when I saw the implementation what what Bing is doing is basically they they use like generate me a query that query just did it based that gives you the top five results and then it goes back into summarization summarization application so in that area but in the benefit here is like it leaves out on the constant lookup but the benefit is you basically get all the references whenever it states something and maybe in the meantime we need to think about on a system level in such a way they're like okay how can you trades back references or things like this and for the moment give up the the constant lookup concept and then work towards that maybe it's part of the model as well fantastic we are at the hour here so I want to be respectful of everybody's time first of all I want to thank all of you David my name is James Rebecca this was awesome this was uh really really fun conversation I think I told Demetrius we should make a conference out of this this is uh this sounds uh like there's so much to talk about such an exciting space uh I totally agree with what James said you know uh probably eat crowl and anything I said today in two months from now just based on uh you know like the speed at which is moving everything's moving uh and that's the fun part uh so I I always reserve the right to be wrong constantly so with that I will pass this on to Demetrius thanks for having us yes dude this was so good we're gonna do a conference I'm already planning it the last thing I'll say if you haven't put your information into this survey I'm gonna release it to everyone so I'm dropping it in the chat right now again throw in if you're using these large language models in production or if you're not we want to hear why and then I will reach out to you with the answers and all the data on this I think there's already been some incredible answers that have come through uh I love it and I also will try and make some kind of like uh tldr report on it so that everybody can see what we're all doing and where we're all at with this I thought this conversation was perfect so thank you everyone and I may be giving out some of these bad things happen to good data shirts for some people that fill out this uh this survey so bad things happen to good data we know that and if you drop in your email in the survey you could win one of these shirts all right everybody that's it for today and see y'all great job thanks everyone thank you foreign