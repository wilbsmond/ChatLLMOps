final speaker of the day um which I'm sad to say but I'm excited to have him on um we'll be Dennis and he's going to talk to us about llms versus LMS LM in product I'll let him clarify that title um hello how's it going it's good Lily thanks for having me good good um all right well I will let you take it away I'm very curious to hear all about this talk great let's get started so hi everybody um I'm Dennis I'm the machine learning lead at Voice Low and I'll be talking a little bit about what we've learned working with large language models in production versus just regular old language models in production so uh agenda is pretty straightforward we're going to talk about how we use large language models right now in Productions we'll then talk a little bit about the difference between large language models and large and large language models I get myself tongue-tied talking about our existing use cases and some production deployment differences so a little bit about voice low because I think it's really important to establish what we're building because you have to know what you're building and what problem you're solving before you just go deploy models left right and Center so uh we're a company that's focused on helping you build chat and voice assistance we're a platform which means we're self-serve meaning as a company we need to make sure that our customers have all the tools they need to build these our assistants themselves so for our use cases uh we've had this platform around for almost five years now and around six months ago we added some generative AI features so large language models we added use cases at creation time when you're actually building your chat Bots and assistance but also at runtime when you want to prompt chain or utilize large language models to create some kind of uh additional response for the user so on the left hand here we're creating some data to train our bot and on the right hand here we are adding a generate step that we have onto the canvas so very different use cases because creation time it's not the end user facing one it's for for our platform users and runtime is at the end so two different use cases but but both pretty interesting for large language models so uh we established this by launching our AI playground where you can basically experiment with different types of language models for building your conversational assistance so before we had this cool platform that we've built now we're layering on the ability to prompt chain uh add all these different AI steps choose different models but also mix it up sort of with more traditional chatbot and voice assistant building so images responses you can mix and match we also launched a knowledge base feature which basically lets you upload data return a response to summarized by a large language model so not only do you have to you can manually create your your assistance you can also just do a quick FAQ creation of the data so those are some of the the use cases that we've built out um I'm describing them not to necessarily promote the product but for you to understand what challenges we ran into on how we solve them so now taking a step back um I want to talk about what a large language model is and I have a fairly controversial definition the first question typically ask people is that is Bert a large language model right Bird came out in 2018 uh was it a large language model people are usually like uh hesitant maybe in the chat you can say If you think bird is a large language model then we get into the another model plan T5 right it's playing T5 a large language model and then people are starting to say okay well it's an encoder decoder model it does some really cool stuff and then you get to plan T5 XXL which is an 11 billion parameter model and it's like okay is that a large language model so the way that I've defined a large language model is a way that it's a general purpose language model that you can throw a bunch of tasks on and what I think it is is something that's better than the original GPT the 2020 release of gpt3 where you can do a bunch of cool tasks right summarization generation Etc so that that's how uh I've defined it because within our platform you need to be able to support a bunch of different use cases um and to be able to do this for a bunch of different domains because we support Automotive retail banking all these different verticals so taking a look at all the benchmarks right uh and we get to one Benchmark uh mmlu and we look at the data and there's a couple of uh open source large language models that are better than gpt3 right so franti5 large llama Bland family Falcon came out and there's a bunch of llama derivatives but basically that's what I qualified to be a large language model just for for the purpose of generation and understanding and typically how people are using them so with that definition um this is why we use large language models is for this generational ability across different domains in a number of different tasks so I'll be using that definition and talking a little bit about the model sizes so what's interesting is for large language models typically you need multi-gpus to deploy this if you're using full Precision right um Falcon 7B actually can be deployed on one GPU fairly easily but most of the other models if you're trying to deploy Falcon 40 billion parameters at full Precision you need multiple gpus uh even though the larger llama models pay we've had a lot of interesting progress being made with quantizing these models a couple of different talks probably talked about that where if you're doing three bit or four bit Precision you could do it on a single GPU an a100 for example now integrating large language models with your existing infrastructure is challenging we haven't done this integration ourselves we've been using the Open Eye openai API for this reason because we don't want to be running a fleet of a100s and the space is changing so quickly that you have four bit papers being released before it was 8-bit right uh progress is going so quickly that we haven't invested into that llm infrastructure because it's not core to our business our core to our business is giving these supportive features to our customers rather than doing the research and self-hosting side of things so we've chosen to use an API and that's how we integrated it so we have the service that we use both for large language models and for language models we built it out it's called ml Gateway it basically connects with our services and for each of our models we have an endpoint and a service and for large language models we do a little bit more so same way that you would connect with a voicemail model we connect for our generative functions we just pass it through open AI rather than to our own infrastructure on the back end and then we do some prompt validation rate limiting and usage tracking so we use the same service for integrating it because we already built it out it was convenient just plop it in and replace the API you might have seen already we also have Claude in the platform so it's the same thing connecting to anthropic sort of in the same method now with large language models you typically don't get the same errors that you get with regular ml models because our ml models are encoder based so there's no generation involved it just gives you a class like this is what you're going to do or give this an embedding so very different problem space so for us it was a little bit new figuring out okay what prompt errors are we going to run into so we use large language models to generate Json and that takes some time um to figure out exactly what the format is because these models don't produce the cleanest stuff right so we have to implement some prompt engineering to fix this even then it wasn't great so we need some regex just some handwritten rules uh to format it I know this has changed when was it yesterday where the the functions came out but for us when we were doing this integration work uh late last year right it's it's something that we ran into and we had to test to make sure it was okay so a couple different examples here just Json issues so we actually can pass it back to to our platform and sometimes it's it's a little frustrating right because you'd assume that these large language models are trained on correct data you have all your prompts established but still have some issues there and the way we we went through this is that we'd record all our errors uh that are coming in through through parsing we track metrics for all the different tasks that we had and then we would occasionally run those erroring responses and prompts um through other prompts that we created and created this test Suite that we would back test if it worked well we'd push to our prompt store so this is an example of running one of these tests right you can see here where things would break and then we would go ahead and modify those prompts based on those errors now we also try to fine-tuning for some of these tasks for the formatting in our specific use case but we noticed that it didn't actually improve too too much so we found out that because you can only fine-tune the smaller models on on open AI we didn't get that same performance that we got with GPT 3.5 or Chad GPT or gpt4 so while formatting wise it was much better we lost some of that answering ability so this example here is a q a prompt that we had we saw decreased performance so we didn't actually use the fine tuning it's another example here um we saw that fine tuning gave us an answer but it was just completely hallucinated but the full size DaVinci model worked all right now for fine tuning uh we actually use generated data to fine-tune the model in our experiment we would pass in some documents use some generation of the questions and answers and then myself and other people on my team we we took that and made sure that the answers were correct um and then we've had that training data into the fine-tuning process now when we originally did this chat GPT the API was not available that came out in March if I remember correctly so we went through and redid the tests on chatgpt it was cheaper but the engineering worked to redo some of these prompts and Integrations wasn't necessarily worth it we migrated some models onto chat GPT especially all the new stuff but not the old ones because from an engineering perspective on as a company we decided that it was working well enough and re-refacturing that wasn't necessarily worth it and gpt4 came out and we're like okay this model is really cool should we try it out uh we did it was better it was just way too slow for our use cases so we abandoned it um we still let you use gpd4 on our platform if you saw that in the drop down but we try to provide the users some different guidelines we published articles have those different materials to help users choose which model they want to use and what we figured out internally is that it's actually fairly challenging to test large language models in production from a conversational perspective right we talk about chat GPT giving responses and whatnot so we started building out this large language model framework that we're continuing to iterate on and develop in-house just to make sure that we ourselves can write our own test cases we can have both technical users and some of the the less technical folks who are more Hands-On with customers and understanding their use cases write these test cases and be able to run them and we eventually want to productize this this as well so something that we're looking at too now with large language models it's interesting because there's that discussion of fine-tuning versus using few shot and there's a number of proponents of using feed shots instead of fine tuning and if you shot is great the challenge is that when you give a lot of examples um your costs go up quite a bit so what we learned is that when running in production you have to be careful in those prompt tokens depending if you're using Chachi PT or gpt4 to run a 2K prompt which gets you get there pretty quickly with using few shot learning depending on your task it can be quite expensive for for gpt4 you're looking at six cents per contraction uh chat EPT is still pretty reasonable but you have to be very careful with that cost trade-off especially in production with with higher usage the other thing that we ran into this is a screenshot from helicone but we had similar issues is that there's a lot of fluctuation between latency so with our internal models we saw that there's consistent latency both Chachi BTS and API we saw that there's quite a bit of fluctuation inconsistency p99s where we're pretty crazy at some point so we decided to Benchmark it and think through okay are we offering Azure chat GPT here or just regular chat GPT I found that Azure was almost three times faster um for for doing these kinds of tasks and the standard deviation is also lower so we found a much more consistent experience but the trade-off is obviously in cost there's an uptrend cost for for running that so um we didn't necessarily expect that these things would be uh at play well we hope they weren't but because openai and other LM companies are gone quickly their models are evolving there is some instability there so it's kind of difficult to tell your customer that you have an issue with latency or something else as a platform when it is happening Downstream um if anybody experienced the AWS outage earlier this week it's it's a little bit awkward right because you can't fix your your platform it's just down unless you've invested in some much more mature multi-region multi-model um success criteria so we've also experimented with different models so we have chat GPT Azure chat GPT and we tried out Claude and Cloud instant found some differences there and it actually depends based on how many tokens you use um don't have too much time to go into that but something important for Mara and is just making sure that we can test out these models that as they're coming out and making sure that that they fit the use case um we can't Benchmark all the models because that's its own research task but uh when we find that a use case matches we'll try to Benchmark and add into the platform now we get into the question about should we deploy our own models and we as the company have been deploying our own models into production for around a year and a half now and it's a trade-off right so when we were using an API whether for fine-tuning or inference on open AI uh we're not we don't control our own infrastructure right so here you can see it took a little while to be in the queue and to actually fine-tune the model versus when we run it ourselves we get a lot more logs it's a lot faster because we handle the platform but it takes a lot more effort to build that out and it certainly has trade-offs so now it's shifting into from our large language model use cases to our language and model use cases um we have sort of four main ones utterance recommendation conflict resolution Clarity scoring and our nlu model so basically these are designed around the process of building a conversational assistant where you want to see is your data good how do I get more data and how do I have an actual good model that powers this this chatbot or voice assistant and what was interesting is that the first model we pushed to production was our utterance recommendation that we built in-house but when we integrated with the large language model we actually deprecated that um especially with multilingual customers especially with more and more domains it just made sense to use a third-party API uh something that we weren't necessarily considering before but it just made sense right and as a practitioner it's like made this model it's your babies running in production but just got to deprecate it because it doesn't make business sense you're not getting that that customer value now this certainly has a trade-off right um I'll talk a little bit about how we deploy our language models um you have this option of just using it as an API right I I called this um talked a little bit about this first in January I called this large language models as a service and then llama came out and then now they they blur together when you look at the acronym a little bit but when you're using large language models as a service right it's very easy uh or a lot easier you you send your data or you just call the API and it works but it's certainly a matrix right um are you hosting your own model are you training your own model are using your own data there's all these different considerations to go into and not not as much time to talk about it but you definitely need to make that decision of where you you take this trade off for us you can see on the top right we built our own Mo platform that does our own fine tuning we hosted ourselves we were on inference we actually let our customers train models in real time so that adds an extra level of complexity but we also use open AI which is like in the bottom left so you don't actually have to choose the same hosting solution for all your models you just need to make sure it makes business sense right so we don't do things necessarily in the middle here we've kind of taken a an opinionated approach on either end but as technology changes as value propositions change right you have to always know what makes sense right so for example if we're going to deploy a large language model it might make sense to use a more managed solution rather than trying to Wrangle like a100s for for ourselves so h100s or whatever GPU requirements that now um it's interesting because you get into this conversation of can llms do everything right and one of our primary models is an nlu model uh which in industry it's a little bit of a misnomer typically none OU is something for natural language understanding but in the chatbot space it's focused on the task of intent and entity detection so tent detection a user says something you try to match to the class so I want to order uh Pizza you match to the order pizza intent then entity extraction is that you might say A medium pizza or cheese pizza it's kind of like a keyword and information that you want to get out of that sentence so these models have been around for a while there's several commercialized versions out there and we decided to build one ourselves because it's part of our Core Business problem is that sometimes when you build your own models you have these outages you can see here like a spike and latency that we ran into and that that can be kind of challenging right because product goes down and everybody's like why is prod down why don't we just use a managed service and that goes into the consideration as well so uh we made the decision to host the models ourselves because they're they're quite sort of accustomed to what we've built um but what ended up happening is the original architecture that we built to if we go down to that list of four models we start off with utterance recommendation and and we had certain requirements so we built it for uh Pub sub architecture where some of these requests might be a little longer running we had an SLA of 150 milliseconds in each direction for p50s so that was okay and using the technology that we selected Google Pub sub for that it worked out great we got to do lots of cool things like schema validation uh handle traffic multi-region that was quite good but what ended up happening is when we deployed The Voice Low nlu this model that needed to do inference we realized that with this Pub sub architecture for for our language model right we were getting too high p99s right it took too long for certain requests to come back and part of it was based on the pubs of architecture and it feels bad because you you designed this platform we built it a year before we wanted to deploy this model and it worked great until it didn't so then we had to do a refactor and a re-architecture and what's interesting is that the actual language model had very consistent latencies right it was between 16 and 18 milliseconds uh the language model had great great response times but Pub sub itself you can see here the the p99s got quite high in each direction of actually sending the message back right so the p50s were great you can see here like 20 milliseconds like 30 milliseconds in each Direction but you had those massive spikes because the tech wasn't built there and we didn't actually know we were going to build this model when we were building the platform so we have to re-architect we had to use redis as our queue um and put it deploy it closer to our application layer so that had some some challenges there too but we ended up doing it hit our p50 and P99 targets compared to our earlier deployment and we outperform a lot of the the industry standards um based on what we've been doing our testing on various data sets a couple of competitors we're doing quite well so we invested in that language modeling capability because it was core to our business right it was something that we wanted to bring in-house now the question is well large language models are great right they're amazing they're really cool they can do all these tasks so how do they compare to this custom model that you spent a lot of time building well the first thing is that latency is quite High um that that's one of the challenges the other thing is that the accuracy and the cost just don't make sense so um our nlu model still outperforms gpt4 in this one test that we did both on cost and on accuracy gpt4 does a great job as a model it's very easy to use but it costs a thousand times more than than our model to actually do inference on and this is only 3 000 inferences that you're running so if you're going into production as a large company it's quite expensive right so that's that um large language models versus language models um talked through a whole bunch of topics but happy to answer questions and if you have any more questions you can always reach out to me by email or on LinkedIn awesome thank you so much and yeah everybody who's watching um check out Dennis's email if you have um questions and we'll give it a moment in the chat um but I think maybe folks can reach out a little later because we're wrapping up but that was awesome how did you how'd you feel about the talk yeah I mean it's always always interesting with with these talks um but feels good I mean there's so much to talk about on the subject I could probably do like an eight hour talk but yeah at some point well sometimes like the 10 minute talks I can tell people are just like crunching all the information down and it's so hard yeah well cool all right well thank you so much for being our final Talk of the day um definitely send Dennis your questions and maybe Dennis if you want to jump in the chat um and see if there are questions there and yeah we're wrapping up two days I think 84 talks it's been thanks for having me I'll jump in the chat and answer any questions there okay awesome thank you so much bye Dennis foreign [Music]