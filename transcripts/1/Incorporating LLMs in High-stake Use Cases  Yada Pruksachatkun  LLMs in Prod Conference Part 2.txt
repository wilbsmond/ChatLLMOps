all right so we are going very quickly into our next talk with yada hello how's it going oh good um that's a great trick having the timer um on the other side of the screen should do that and I like it because like as sort of like the MC it's the worst one you have to interrupt like an awesome talk but yesterday I got uh 20 minutes behind schedule so I'm trying not to do that again um cool do you have your do you want to share your slides yes for sure can I just share my screen or how does this work yeah there should be a button I think where you can share your screen all right uh okay oh there you go all right let's see all right all right take it away hello everyone um great talk so far hope everyone's having a great time um I'm yada and today I'll be talking about incorporating large language models in high stakes use cases a little bit about myself um you don't have that much time so basically um I have a background research in ml engineering I've also worked in the instruction of NLP and Healthcare and mental health and currently working at ahr techster.com all right so no we all know that there's a lot of hype and we've also seen a lot of um great headlines on you know medicine and law and applications and so I'm sure lots of folks are thinking how applicable are these models to these use cases and you know they are but there's a huge caveat of all the various um types of failure cases that you can see in these models which are also prevalent in weekly models as well some of these are you know robustness for distribution shifts which you know was talked about yesterday um as well as symmetrical equivalent perturbations degradations and low resource settings and you can go on and on so what does this actually mean um and here let's think about a therapy bot which not saying we should build this but let's just put it as an example um in this case you know you it's really important to structure a therapy bot in um in some framework maybe that's CBT maybe that's family Dynamics um controllability is really important bias and fairness is also important so you can imagine if you're uh you have the spot that you can call in you have uh you know speech to text component um that speech to text could uh have issues with people with accents and that could lead to Downstream difficulties and uh sub-optimal experience for folks with accents essentially so you know it's really important to think about these considerations when thinking about these new Innovations and in terms of how to actually you know approach controllability and these limitations it's usually wise to look at um best practices from previous versions of models and ml systems so for controllability for example you could travel back in time to the land of dialogue flow before language models um you know more structured environments where you think about intents and entities and you know draw out um actual conversation trees and branches and what you could take from those is essentially you know in terms of controllability um having you know natural language understanding for dialogue use case as well as Dallas States so for a therapy but maybe during the intake flow um you should take into account and record the social history the emergency contacts Etc and make sure that that's all recorded accordingly um and of course is speeding through some other best practices um for kind of incorporating uh models into high six environments course human Loop is always best practice we've heard a lot in this conference um so having if a domain expert is the one who's using your product so a doctor for example you could just show and have them check if not um having humans who are experts on the background to see um if there are alerts or if there's um potential fatal cases they can check in the worst case now one thing that is helpful um is to break up tasks into smaller tasks so an example of this is if you have an information retrieval um product you might want to find document that's relevant to a question but instead of trying to do that for the entire document maybe do it for each paragraph in the document and then agree that um and a couple of different thoughts on how to make tasks easier and of course this is a generalization so it depends on the task depends on the model but um usually no classification is easier than Generation Um it's easy to reduce the output space so um you know if there's a thousand different classes that's harder to learn than you know 10 classes um of course also reducing the input space as well so less for our variability in the input space now we've all you know we're all thinking about in context examples so if you are using kind of more off-the-shelf models um the main question is you know there's of course a lot of different best practices here but um you know in terms of choosing in context examples it's really important to also keep a prompt database um and with this pop database you know you're you're usually going to be using some sort of embedding for retrieval and it's also important to fine-tune these embeddings to make sure that um you know their care to your specific use case and aside from that uh there's also you know as we've heard in this conference we need to have a structured approach in building prompts and test sets and fine-grained evaluation Suites so you know exactly where your model is going wrong now um speeding through um some other purchase and sampling has you know been an approach in data science for a while it's this is the same for um this newer class of models and we've seen that in self-consistency and other methods as well so um it's always best to Ensemble not only with black box apis but potentially if you have your own fine-tuned model even you know revix based what not you could Ensemble all these models to really ensure that there's um as much uh information as you have in creating a prediction um and you know of course in high six environments the best practice is don't use LMS if you don't have to um you know you can use regex you can use more traditional models which um you can you know look at their weights or understand what they're doing logistic regression or random Force although of course there's some issues with those models as well um and you know there's still issues with actuality especially as the input and context window and what you're grounding on increases in size and you know it's still even though there's a lot of um gbt tattoo seems to be doing fairly well you know of course in high six environments you really need um humans in the loop for emotional intelligence as well and however if you do want to still use models um it might be interesting to think about finding your own llm not just for you know creating emote or whatnot that folks are talking about but um also you know having more control over the outputs um as well as you know all these other range of things that can do when you own and you have access to your weights um so you know off the top of head uh being able to look at confidence scores um being able to have more control over kind of exactly the performance of your model um there's not updates happening in the background that you don't know about um and also being able to incorporate the other art methods from the community both research and um otherwise into your own models um and a couple kind of last thoughts um in closing first is evaluation it's really important to evaluate over cohorts so in high stakes environments for you know um all the different subpopulations that might be using your product um having those performance metrics over each cohort um looking at robustness as well as calibration so you know what is the correlation of confidence scores to how correct the model is um and for dialogue of course using user simulators and here are a couple um papers and works that could be interesting to focus here as well happy to share in the chat later and two last slides here um when you're looking and you see you know these headlines on Twitter some questions to ask are one how different are those tasks through your own tasks two is how different are your domain and there it is you know think about all your privacy robustness Etc constraints and the first two is really you know if those that Gap is pretty white then um it might be the case that those numbers won't actually translate to good numbers for your own task in um your high stakes use case and some open questions that you know I'd love to talk with folks here later as well is you know how does explainability look like in the world of black box apis and you know what are the best practices for active learning Etc um especially you know you don't get um you might not get confidence scores and so how do you do Active Learning in that setting uh with that said um that's it for now super Speedy happy to take any questions afterwards but thank you so much this is a really great um conference so far thank you so much yeah that was awesome and you know when I was looking over the title before and it was saying you know llms and high stake use cases I was like where's she gonna go what's that height uh high-risk use cases or high stakes and therapy bot is definitely for sure so I'm for sure thank you all right yeah thanks so much cool all right yeah please drop some of those links in the chat that would be awesome um and thanks again yada really appreciate it all right have a good one thanks [Music]