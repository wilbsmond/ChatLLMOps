and I'm super excited to finish off strong man uh I'll let you take over now and then maybe we will uh sing a little song at the end of this just to commemorate what a magical conference this has been with all the technical problems and everything great sounds good all right um thanks to anybody who's listening um so my name is cam feenstra um and today I'm going to talk about how you can use large language models to punch above your wing so whether that means uh you know just doing things faster doing more with less or um you know even as a small company potentially competing with bigger companies where you might not have been to uh able to otherwise so um a little bit about my background uh I when I first came out of college uh looking to go into the software industry uh I was mostly interested in machine learning and I I wanted to go train models and I I had um you know some reasonable skills with sort of python data analysis less so training models but at my first job out of college I I spent a lot of time training machine learning models for doing various uh NLP tasks um and I actually started to gravitate more towards the software side I got more interested in kind of the infrastructure and a big reason for that was just that the feed boot back Loop was so much faster writing software than um training at least that's how I saw um now uh that's very much not necessarily the case anymore which is a big part of what I'm going to talk about uh today um so sorry okay uh so um let me tell you just briefly about Onsen which uh is the company I've been at for about the past year and a half um so the problem Hansen is tackling is um pretty big 23 billion dollars every year is spent on basically employees suing their employers um and that's Rising year over year fairly consistently um and you know today there actually aren't that many good approaches available to businesses um you know they they will buy insurance most of the time and then separately they'll rely on uh legal lawyers legal advice for a lot of things eventually they might hire a head of HR who who knows a lot of domain knowledge about compliance um and so the solution that we're we've come up with that we think uh is pretty interesting is in two parts so number one we actually sell insurance we write uh various different types of lines of coverage to businesses um to protect them if they get sued but we also package a software product uh that aims to help avoid companies being sued in the first place ultimately time is our most valuable asset and so even if you're covered when you get sued uh it's still um not something you want to do and and uh you know sort of a great side effect is oftentimes the things that help companies not be sued are also much better for their employees um and so uh it can help them offer a better employee experience as well uh and for context we're still uh quite a small team at Onsen we're under 20 people uh which was sort of the inspiration for the talk um and um basically I'm gonna talk about uh a couple of specific ways that we've been able to deploy large language models to help uh solve some of our own problems very quickly um and I'll share a few learnings uh of things we've learned along the way um that you might be able to apply to your own work uh so uh you know probably it's not that big of a surprise that it's hard to compete with a big company as a small company um you know they have more money they have um bigger teams they can throw people at a problem uh oftentimes they have lots of valuable data sets that you can't really get otherwise um they have Network effects uh you know in the case of insurance if I know uh that my friend has had a good experience with a particular company I might be more likely to just go with them um but on the other side uh they're also less agile um slower to deploy to new technologies whether that be um sort of a if it's not broke don't fix it type attitude or um some bureaucracy or whatever it is um and so uh specifically with with in uh the insurance side of our business we are competing with really big companies uh and so the first example I'm going to talk about is how we were able to use a large language model to streamline our underwriting process um so underwriting is basically the process of taking a bunch of information about a company um you know financials they're you know if they've ever been sued before a whole bunch of different things um and then a person actually decides is this a company that's too risky or do we want to write them an insurance policy and if we do want to write them an insurance policy how much should we charge for it um and so uh what I have here is kind of a diagram of how uh in the kind of type of insurance we write how this normally looks um so there's someone called a retail insurance broker that sits typically in between uh you know about 98 of the time in between the actual client who is a company um who wants insurance and the insurance company so generally the uh company will give an uh approach the broker they'll have them fill out a long application which I'll talk about in a second um and then they'll go to any number of different insurance carriers and say hey uh can you give me a quote on this those insurance carriers take it back to the broker who then ultimately takes it to the client and then if any of them are good the policy gets issued um and one thing I want to point out here is maybe you can kind of see from this diagram you know each one of these Brokers has many clients and for each one of those clients they're corresponding with many different companies and so it's really important for us to make the process for the broker as simple as it can possibly be or else they're just not going to do business with us um and to give a sort of uh zoom in a little bit on what traditionally happens inside the insurance company would be they get this application um you know emailed or maybe faxed in some cases um they would have actually a team of people who waits for those applications to come in reads them um determines a if they're a duplicate sometimes a company might be working with multiple insurance brokers and you only want to process it once and they'll also manually extract a bunch of different information to make the underwriter's job easier and then they'll hand it off to the underwriting team who will decide if they want to write the policy how much to charge and send a quote um and just a note about the applications these applications um are really complicated there are tons of different formats generally insurance companies will accept any format that you give them um as long as it includes the appropriate information and they're they're long and dense and have all sorts of questions which actually makes extracting that information in an automated way quite a challenge so um what we implemented at Onsen is something like this so um you have the broker sends the application as soon as it hits our inbox we have an automated classifier that classifies if it's an application and then we have a separate component that extracts information from it if the application is a duplicate um it ends there otherwise we we send an alert to our underwriting team um who more or less does the same process uh as in the previous slide they decide uh if they're if it's a company they want to ensure or we want to ensure and how much to charge now um a couple details about how we put this together so uh there's sort of two parts to the solution there's the classification piece and there is the extraction piece um and so for the classification piece we were actually able to put together a classifier that performs pretty well uh with with very little work required on our part so we started with uh we built a classifier based on Google's Burt model and we I mean basically the Baseline performance was essentially you know the same as if we had picked randomly um and we within kind of an afternoon we're able to pull down all the attachments from our team inbox which was about 300 at the time manually label them and then come up with a classifier there that had about 95 accuracy uh 90 precision and 100 recoil now um we did uh test configurations that had better Precision um but there was always a trade-off with recall and for this use case uh we were okay with um getting a little bit of noise as long as we captured as much as we could um and we were also tested out a bunch of different models all open source um on on hugging face um and a bunch of different training configurations and were able to choose the one that had the characteristics we liked the most for the extraction piece we actually ended up using an API AWS textract but particularly the question answering feature of AWS textract which lets you um add a query like what is the company implied applying for insurance and uh it will extract that from a PDF document which works very well um you know they hold their cards a little close to the chest in terms of How It's implemented but safe to assume based on the performance that that there is some kind of large language model powering that under the hood um and uh you know I just really want to stress um especially with the classification model um you know this is something that we couldn't pull off the shelf there weren't any good options um but as I said we were able to put it together and well the the training data we were able to put together in an afternoon and basically end to end we got the whole thing working in under a week um compared to uh training a model from scratch um it's sort of at least an order of magnitude less effort and less data than we would have needed to get similar performance otherwise um so now one more example of something uh that we use large language models for at Onsen um so uh one feature that we wanted to build for our risk management platform or I guess I should say a hypothesis we had that we wanted to build a feature and validate is that um right now uh generally entrepreneurs startups um relies a lot on their lawyers to in to review individual documents and let them know if there's anything potentially wrong with it um things like offer letters uh separation agreements if if someone gets laid off or something like that um and so we we thought it would be a really interesting feature to build um a system where someone could just upload a document and get immediate feedback on potential compliance issues um and you know even though we're not providing legal advice and that's clearly spelled out on our website and everything like that for this type of feature we were really uh wanted the accuracy to be as high as they could possibly be because we certainly don't want to tell someone something is wrong or just have completely kind of bogus outputs like you might occasionally get if you use something like chat GPT so the first step was that we gathered a bunch of domain Knowledge from attorneys we learned you know what are the potential things that that you see in offer letters for example that are issues and then we put together a system um that looks a little bit like this so for for each type of document we have a number of different features that we want to extract for example for an offer letter uh we might want to know what's the salary um is this person an exempt or a non-exempt employee um and then for each one of those features we sort of have a two-step process so first we use uh sentence embedding to extract the most relevant portions of the document so more or less something like semantic search um excuse me and the second piece is that we use a question answering model to extract the exact uh bit that we're looking for so for example after this step for the salary feature we would have forty thousand dollars and then we would have exempt or non-exempt um based on that we run some business logic and come up with a set of positive and negative insights uh based on that domain knowledge that we've gathered from attorneys um so in terms of how we actually implemented this for this uh we were able to just pull models off the shelf um specifically a question answering model and a sentence and betting model uh the question answering model is again based on Google's Bert model the sentence and betting model is actually not really a large language model it's it's based on Microsoft's mini LM but um for this particular task that was uh the the best performing model that we tried um and additionally like it didn't require a ton of data uh we were able we had a few dozen examples of each type of document we wanted and based on that we were able to sort of crap manually write prompts um to help us extract each feature um now um I think for this particular feature uh at this point it's too early for us to say if our you know our hypothesis is true or not we're still developing it and and taking it uh giving it to people for feedback the feedback we've gotten so far has been really good but the main point is that we were able to put together this together in uh about a week at least for a prototype and so rather than you know again training a model from scratch or something like that where it would have been a big investment to validate that hypothesis we were able to do it uh very quickly and the end result it feels actually fairly kind of magical um so uh quickly want to go over a couple of uh learnings obviously this is not a complete list but um so you know probably not a surprise but uh definitely if you want to host these things on your own um it's important to have someone who who knows a lot about infrastructure uh to do it in in a good way I mean so you know for one thing these models are really resource intensive um you know they're you have to understand how they're gonna perform or if it's all going to fall over under high load um and so you know many of the same things that you've always had to do for deploying production systems um you still have to do and they might even be more important when you're deploying llms um and evaluation metrics are also very important um you know ideally you want to be able to quantify uh if your model is performing as well as it did in tests or or as or if the performance is going up or down over time um the third point is um you know if you're running massive production workloads it does make sense to use the best hardware and everything like that but especially if you're just working on a prototype or anything like that um it is definitely possible to deploy pretty large models uh you know maybe when you get to something like um you know many billions of parameters uh it wouldn't work so well but um you know we were able at least for prototypes to deploy models on normal CPU instances that we use for other things as well um and they work well um now the last thing I want to call out is something to factor into your costs if you are relying on an API is that um probably you have some layer of business Logic on top of the outputs that you get from the models and so software is all about iteration and so ultimately you're going to want to change uh that business logic and so um and you know probably you're also going to want to test uh how different you know for example prompts or or different uh inputs uh to the model look um if you're relying on an API doing those changes is is very expensive um and in some cases might even be cost prohibitive um so it's definitely something you have to factor into your expectations of cost um now maybe uh anyone watching is thinking you know why am I not talking about chat gbt or something like that um but uh definitely we do have a lot of cool ideas based on chat GPT um you know I've kind of presented the things that we've been able to get into production and are working well um but we think that there's a lot of potential for uh generative models or or something you know gpt4 one of these models to potentially be a really good underwriting assistant um it might go fetch data about a company it might just sort of make an initial recommendation um but our experiments have have shown that it actually seems to be pretty good at it um and then the the second one um one of the really difficult things about compliance is that the uh information you need to know is scattered all over the place that's why um well one of the reasons people rely on lawyers a lot of the time um and so you know one thing that we think could be interesting would be to be able to ask a question about a specific compliance issue in natural language and get um a really great answer uh based on or you know at least pointed to the right location um but that's something that is is just an idea at this point so uh thanks everybody for listening um you know it's definitely a really amazing time to be alive really cool time to be in software um you know I'm I'm super excited about uh all the advances we've seen in AI um but you know even without all of the brand new models um you know large language models can still be a great asset um like they have been to us at Onsen um and last thing is you know we are we are a startup we're always hiring so uh if any of these uh problems sound interesting to you or or you want to learn more about what we do uh please reach out um we're always looking for for great people um yeah and thanks to the organizers thanks to anyone listening um you know it's really an honor to to speak and um if we have time I guess I can take a question or two for you there's all kinds of time there's just a little bit of a delay between the stream and the actual uh chat that is happening in oh I see um so we can just shoot the here for a minute and then when there is a quality it's like you want to make sure nothing too bad goes yeah man I mean the while we're waiting there is I mean there's so much good stuff that you're talking about there but I'm wondering on more of a again I want to go more to this philosophical level and talk about like if you're working with this with these large language models and you're dealing with like problems that you're not seeing before what's your main way of going about just debugging and being like wow I've never I can't Google this because I'm probably the only one in the world that is seeing this right now right yeah yeah that's a really good question um you know I mean trial and error uh to some extent you know that's always the fallback um you know I think that um you know to some extent you can uh for example if you have a situation where you know you have a bunch of possible parameters like you know it's it's actually feels kind of similar to training a model sometimes because you're like okay let me run all of these different things come up with some Metric yeah I guess that's part of the answer is like really you just need some evaluation metric I mean yeah like I don't think that there's really you know no one on earth can explain what the llm is thinking and so like there's no you know at least not today there aren't tools where you can sort of see the flow of information through it and and use that to um um debug what's going on but but you know traditional kind of software engineering techniques uh you know can also work reasonably well did I share this did I hold on let me share this with you speaking of which I don't know if you've seen this yet yeah but pretty cool that's that's the shirt that I I made because it's like yeah man we gotta call this out like a lot of this when it comes to reliability and just making sure that there's there's this big question that I have in my mind like no amount of prompt and correct prob prompting right or like no matter how good of a prompt engineer in between quotation marks you are um you can't force the model to give you output so yeah like how do you go across those kind of problems right yeah no yeah I mean it is it is it's funny because like you know I probably seen all this like AI agent stuff out there you know putting one of these on a loop and uh you know that was actually like um around the same time it was kind of blowing up it was an idea I was thinking about and I experimented and it's it's really funny because it's like you give chat TPT these instructions of like give me this exact format do not give me anything else I was like yeah sure like let me help you so and and I mean ultimately you know the models are non-deterministic too so you don't even actually know which is a lot different than most software when you you know you give it an input you don't even know that the output's going to be the same especially when you're using something like you know chat like an API they might have completely changed the model under the hood uh without you knowing and so it's definitely not an easy problem um you know I mean I I follow all sorts of people on Twitter and I see uh there's a you know a lot of people are kind of doing kind of ad hoc I would say research on um you know specific techniques and uh I find myself frequently kind of bookmarking things of like oh that's an interesting idea you know maybe I'll try that sometime but it's an emerging field I would say um and uh you know I would guess within the next couple of years it will become you know a formal discipline or something of that nature yeah we'll see we will see uh so there are some awesome questions coming through in the chat uh that are getting a bit more specific with you have you checked with shape what part of the documents get used most in the classification um no I would I guess the answer is like no um mainly we just evaluated uh the outputs um and you know we're a small startup we have a million things to do and so you know we kind of monitor how it works as it goes on but we you know we aren't we haven't really Doven into uh like what you're saying um but that's actually a really interesting idea that maybe we should try is at least just evaluate you know does the first two pages or whatever like usually yield the same classification result um I I just don't know the answer at the moment that's awesome Sebastian great question reach out to me I got some socks for you that is what happens when you ask these awesome questions all right next up what are the core parts of underwriting content from llms that would need to be evaluated to ensure output what is required sure yeah um so I mean that's one of the interesting problems like I think I had one slide where I had one of these sort of nine page applications and you know on top of that whenever we're underwriting we get like financials we get you know their employee handbook and so um you know ideally we would just give that all to the llm and you know it would do our job for us but um you know I mean maybe with gpt4 you know 32k tokens like it'll be doable but context size is definitely one of the um problems we have to think about when we get deeper into it yeah um you know I think that one idea we've been like potentially thinking about is you know maybe asking on the first pass having a large language model or chat gbt or whatever um kind of Summarize each document and then put that put those summaries in the kind of the final uh prompt that makes the evaluation something like that but um I guess the answer to the question is there's a lot of information that's the application financials you know history of if they've been sued um you know and based on all of those things we might even ask for more yeah and like knowing all this do you feel like the method that you're currently using is the only method that could actually do this or are you also looking for like is there another way so doing what specifically like trying to I guess the idea is the end goal that you're you're getting at right and with with feeding all these llms or feeding these llms all this data like I just wonder if there's and I didn't mean to get so philosophical I think it's probably because it's late here but you caught me at a good time I need my my scotch and cigar and this is uh it's what you get for being the last one I get to chat with you on this kind of stuff but yeah I I mean I I love uh pontificating so yeah I won't find any more look at that vocab word man oh my God printing out the real guns all right but at the end of the day it's like the the thing that I wonder about is do you need to use because there's a lot of times that we talk about in the mlops community that machine learning might be over engineering something and then I feel like now with these new large language models potentially using them and in my own experience when I've played around with them sometimes I spend so much time trying to get what I want out of it then I'm like dude if I just did this it would have been faster than actually having to do it so I just wonder do you ever want do you ever think about are you inventing a nail for a hammer or is this the only way in your mind or in your eyes right now that this this can happen uh I mean I think yeah it's a really good question I think I you know the traditional wisdom with machine learning has been like use it as little as possible only when you really need to and I think that that's a lot of the reason that for uh the use cases we put into production that far thus far we weren't like let's just throw a bunch of info at chatgpt and like let them handle it um because yeah I mean I do think but also there's things like you know I mean if we were able if we tried to create a system to like do underwriting uh you know I mean there's actually a lot of examples of unsuccessful insurance companies that have tried to do that and so I think that for unsolved problems it it can work really well um you know and but yeah I mean I agree I think that you know what will be really interesting is seeing you know good software built on top of the large language models like I think a lot of people say you know text is the new UI like yeah I mean you know I love my terminal but I don't think that's gonna happen um you know uh and so you know there's all and and using large language models presents new ux issues right like oftentimes you know I think in the like performance optimization discussion and they were talking they they got a little bit into like ux fixes for for slow models and stuff like that so um yeah I think that uh you know it is a really cool new primitive um you know and and we'll see kind of what what it works for and what it doesn't undoubtedly there's going to be a lot of things made that are way over engineered and you know don't actually make any sense and I mean I think if you start using large language models where they aren't necessary it can actually potentially make your software worse because something you could have just written in code for and it's so slow only once 80 of the time or something yeah yeah it only works eighty percent of the time it's super slow I mean yeah you're just you're degrading the whole system so there is there's another question that came through how are you getting evaluation metrics um um um say that at the moment um for uh our internal for for our internal classification system the evaluation metric is that everything goes to a slack Channel and I take note when I see things that are wrong uh you know and and people give me a heads up if it misses something um and then uh periodically I've been kind of going back and seeing if I can improve the model um and you know I always have this original labeled data set that it was trained on um and yeah I mean I would say at the moment we're we're doing things mostly at a small scale and so um the evaluation metrics are still on the manual side I think that we will certainly over time get to the point where we uh have things like regression testing and stuff like that uh in a more automated way but at the moment it's just something that we we think is important so we're doing uh doing it relatively manually excellent dude cam thank you so much man you finished us you finished strong and this has been incredible um I'm gonna kick you out now all right this is the end of the party you don't gotta go home but you can't stay here and so thank you we will see you later man thanks for having me yeah it was it was like I said talks were amazing uh super honored to to be able to talk and um yeah yeah