rolling for our final talk and the best part about this talk well maybe not the best part but one thing a nice little uh trick about this talk is that our guests our presenter today is actually tomorrow what's up man how you doing hi I'm sorry that's my dog at the back um your dog wants to go out tomorrow for me so it is 16 then it is it is late for you too wait what time so you're in Australia we're just gonna say that right now you're gonna you're gonna take us home starting your day we're finished I'm finishing mine there's a lot of people in California that are uh just finishing up lunch I believe but what uh what time is it where you are it is 8 10 a.m so it's a good decent it's a decent time yeah that's not too bad that's not too bad that's respectable I am glad that I didn't like give you a time that was 3 A.M or anything like that so we did all right anyway you've come to talk to us about some really awesome stuff I just saw your screen but now I don't see it anymore what happened you might have to share it again hmm let me try that's the fun part yeah oh where'd it go where'd it go if it makes you feel any better I'll tell you the stories that I uh I have these big lights right next to me and I didn't realize that the last like hour there I see your screen now for the last like hour I've had my window open and so now I've got these gigantic bugs that are just flying around so in case you've seen me swatting while I'm talking to you that is uh because I've got these gigantic bugs flying in my ears anyway I'm Gonna Leave it to you you're gonna take us home bring it on strong and let's see uh I'll be back in like 10 minutes thanks thanks and nice to see you back again so yeah thanks for having me um all right so um I will be going through a little bit of my our journey of building code completion tools within gitlab I am a engineering manager who looks at a group call model Ops and um we started this journey on building code completions through uh through two cockle suggestions uh pretty much around four or six seven months and and yeah we've learned a lot through it and um and just want to share through all the learnings together so um before we go into exactly what we're completion tools do or not to um or the architecture of it code completion tools what they are they write complete recommend the code you want um it is fundamentally very very close to the heart of any developer and using AI as assistance in decision making for Developers to help them provide that judgment there's a lot of talks on how do you use it what do you consider it usable what part do you optimate but in general when you're deciding on the outcomes of these llms for code completions um there's basically three areas um will this code that a AI assistant completion agent or recommendation be honest um which is consistent with facts this is beyond just evaluating for is a code correct there's this honest to the developer who's developing is this harmless and is it actually helpful so accomplishing exactly the goal of the coder so the coder can code fast and instead of writing 500 lines can actually write thousand lines of code for a usable feature um so um that's sort of what a code completion tool and how we actually decide on llms and based on these outcomes of these three different metrics now I want to actually focus on the last part does it actually help um to get to the uh the goal of the quota um to do that I do want to talk about how you a framework on how you can do evaluated billing and decision trying framework to make these uh llms in uh production useful um and keep in mind a lot of the llms um other than uh chat TV whether it's third party or or um or um or open source um we don't necessarily constantly feed them the training data off uh to judge the quality of the good pattern ugly so then how do you actually then take these third-party open source llamps make them write code that are much better than an average coder can actually do so that's sort of a goal that we want to achieve together um okay so some fun fundamentals obviously of choosing the right role uh raw llms um as we probably call it um things we want to look at is obviously the objective is called completions uh code generation recommendations code uh for developers so this is all probably Everybody by now probably has a better sense we want to look into the parameters uh the training data this is really really important uh to just get a sense of just on a raw side uh how much data how much of the internet has been gone into into this um uh building this model and getting some smarts into this prediction machine um when you go through a completion uh to we want to look into what kind of evaluation benchmarks already there with the alarms that you've chosen um if it is open source we want to understand the model weights are this flexible what kind of tuning uh Frameworks can we do uh or even for third party um cost is obviously another thing and latency I see a lot of posts where we say we want to also assess quality but I do think that this is a really Hardline on how you can actually assist quality Without Really evaluating an N scale and and and how do you actually then use these efforts of work on prompt engineering at scale and to almost like continuous training continuous prompt engineering so to get to that I do want to first look into what an llm architecture from a perspective can look like where you basically in this world of llms you're not just choosing one you're choosing many based on the factors we discussed before and let's say you have an open source pre-trained llam that you can also feed in further data to tune it and then you also have certain third-party um llms as well and what you're then doing a full architecture of starting from the left you're taking the data additional data to enhance your pre-trained llms you're downloading let's say from hugging face wherever full raw data sets publishing pre-processing tokenizing all the way into an environment where you're training it tuning it having those layers of checkpoints then it goes further into what we there's two engines running parallely into it is also then the prompt engine almost where you're instantly with every code a person has written you're going through the same layer of what we call a prompt library or prompt DB of then looking into how do you deconstruct that code into tokens uh understand what was then finally committed uh and then get a better sense of a scoring mechanism of ranking is this a good average good developer understanding and then going into some sort of a Gateway where we have a prompt engine post-processing a validator um calling the models based on the input the user is doing whether it's through the third party or or actually to a through your pre-trained model um now the orange boxes are something we will go to in the next few slides as well as to how do you actually have a continuous evaluation and inference as part of this architecture so focusing on the evaluation um how do you kind of do it in scale um without the human about benchmarks and starting with that and let's say you start that and you've also done a few more of uh few short examples uh and you've done a little bit of user base and then you want to understand how to Version Control it scale it do it continuously have a CI CD continuous training continuous feedback to consistently keep on dialing up the accuracy for your use case um now to do that those orange boxes on the left is a way we would probably have a mechanism to continuously Loop that which would include prompt input to tokenizing to understanding the similarity based on a developer to storing it to evaluating it and then using that as a smart to uh to actually have the output for example let's say we have on a historic database uh We've dissected a large model code to understand a token of completion what a um uh to to understand how you build an extra views model now we've seen that historically in a code base of our company that many developers have develop one double two double three like that um they have different ways of writing the same code they have different uh parameters evaluations and everything and then what we've done is we've rung this algorithm based on the code commit and similarity and agreed on what is an actual agreeable developer output um that then is then evaluated with how the llms open source third party tuned uh is outputting through a technique where based on the objective review school science similarities you can use piercement or however based on the prompt as well as the code through um now this engine so the prompt is here model xgb we then know what an actual agreeable developer output should look like to understand how to map that to a quality and then we know how each of these elements are giving uh outputting it and matching that and then sending the right output uh to to the right uh to the to the user now this makes probably sense in just one line and a lot of times I see people having manual spreadsheets to looking into prompts and how to evaluate it but imagine this having this in scale as engines and that's the beauty of having this sort of a microservice engines added to your layer of full architecture where you can do this in an infinite loop on an instant basis to keep on dialing let's say your code completion uh usefulness to a harmless harmfulness usefulness all the benchmarks and an instantaneously keep on helping with uh with the evaluation part of it so this is something that can help just doing it at scale thousand times for in that continuous background fashion now on the serving then is basically what we call in the reinforcement learning of it so imagine from that evaluation then we go into yes we know what is the actual downfall we know what the L alums are giving now we want to know if there are prompt templates of prompt tunings that we can do on an instantaneous basis have a prompt validator rate output if that can't be done stranded to a fine tuning of the yellow labs again continuous loop having it version based control based each of these can be microservice connected through uh CI and uh having that prompt validator rate limiter becomes a key to really understand the user input as well as the artboard to then put that full continuous loop where you may start on the fact that at a certain level you've started on the raw llms of a combination giving you a 10 acceptance rate for coders then you sort of take this evaluation Benchmark and then continuous prompting through reinforcement learning in a loop and just like an Amazon recommendation engine you're constantly dialing up this accuracy of usability uh to to then get to that output so then let's go back again now putting that inference and the prompt of how this whole becomes that Loop training data completion data continuously and with code completion you will always have code there will always be a coder writing code so you can continuously add this and keep on adding to this data for evaluation for prompt inference for ranking four understanding what is what is how what are the three decision sign framers how does that impact every code to get better in using llms for good completion um that's all I actually have today um I do want to end by saying that it is uh of one of my lines that in this day in age of elements data is still the oil it is very difficult to imagine this that with sufficient data they will remain things that only humans can do so uh in the Journey of llms this is still the key this has not changed uh thank you so much for having me um I haven't put my details here but I can put it on the chat of if any anyone has questions feel free to reach out to me in the mL of slack Community Channel as well as on LinkedIn there we go all right come on thank you so much that is so cool and so valuable I love to hear this journey and I appreciate you sharing it with us and being very transparent and now you get to go off and enjoy your day while we hopefully it's sunny where you are I know it is uh it is winter right that's another it is so maybe you're getting nice winter day and we are going to close the party for now but we'll be back in Action tomorrow same place same time and we've got a whole nother lineup uh but in case anyone would like to know I will be singing more songs I know you will you probably want to know a month I I will have more songs you missed it because you were probably sound asleep but I created a few improvised songs and now uh hopefully I'll sing us all a lullaby and put us to sleep and we will get out of here uh actually no lullaby I'm ready to go to sleep right now to be honest my brain has stopped functioning I'm calling it quits I'll see everybody tomorrow and Mom thanks again thank you