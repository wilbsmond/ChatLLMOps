[Music] he just threw me on screen oh I just threw you up there I a little bit of a surprise man so what do you got for us today I know this is your first of two appearances later on we're going to be doing a prompt injection competition with you and you told me to get ready to get my prompt injections ready so I've been uh you know studying a little Reddit but in the meantime you got a presentation man I'm gonna kick it off to you because you only got 10 minutes dude ready that's good let's do it baby all right I'm gonna be talking a bit about some of the emerging patterns we've seen with owl limbs in production uh but first of it about me um so my name is Willem uh my background is in production ml built a bunch of production ml systems um a company called kojic uh Rudy Wilson double down and focused on ML tools and Frameworks and platforms um so one of which was a feast open source projects we built and open sourced and adopted by a bunch of companies like sharpfi and Twitter and Robinhood and some others but I've already been working with teams building ml tools and platforms and um you know helping them do it in a reliable way and so that's why the generative AI space is interesting to me because of the challenges we're seeing today so what do we what are some of the unique challenges that we see today with uh generative Ai and um you know I I guess I can in general um uh the key the key ones we're seeing are around reliability cost latency and safety and the numbers on the screen there are from um an element production survey that and their basic teams that have responded and said this is an egregious or like a critical problem for us and so reliability is obviously one of them because you're dealing with like unstructured or textual output coercing that into something that your application can use is hard cost is a big one um if you're an AI Builder today you're either asking users to provide an API token or you're absorbing a lot of the cost yourself and so this is a challenge that a lot of you know product Builders are faced with today we've also deployed a bunch of probes globally or I personally did that and I've been monitoring these API endpoints myself the the providers and they're really really slow so they're like 400 500 600 milliseconds um sometimes they they even Spike to like 18 20 milliseconds for completion so how do you build a product around that and especially if you have to do many round trips it's very challenging um and finally safety is hard um you know folks are inputting private data or they're you know your vulnerable to private sorry prompt injection attacks and all kinds of vulnerabilities that are kind of new and unique so it's challenging building on RM today and you know that's kind of like opportunity for us to embed things down a little bit so how do you use elements effectively the same rules apply to structured ml really in the space start simple sort of the basic prompting start with including some examples to do some view shot prompting start introducing external data or exogenous data sources using Lang chain llama index and composing workflows so you incrementally increase your accuracy over time yes you'll increase your costs and latencies a little bit but often that's okay if you're um you know accuracy and reliability improves um but if but soon you'll get into a point where you want to get to it refinement where you do things like prompting where you're doing tool selection calling out the apis like Wolfram Alpha or others to give you more reliable responses and I think most folks end at this stage but you know if you want to take things further you can also start using um you know fine-tuning hosted models or using open source models and training them from scratch but that's something that we wouldn't discuss today but um you know in terms of the techniques that we're seeing out there in the wild I think one of the key ones that you know SRI and some of the others have been introducing and um kind of spearheading is adding structure to your responses so you can ask a model to provide a respondent typescript scheme as a format and it'll do so and you can encourage it to be more reliable by giving examples asking it to take on a Persona um and just bursting into reminding it how you always return Json you can even unfortunately threaten the model sometimes and it will um often be more accurate for that uh response and if it fails you just re-ask and you can keep re-asking until I guess it affects your ux you can increase the temperature or you can start with a more cost effective model and then ramp up to from like a GPT 3.5 to a F4 and this allows you to at least validate the outputs in a structured way instead of dealing with clean text as the output um another technique that we're seeing applied more even in the production setting is self-refinement and so the idea is normally you you make some kind of uh prediction you give a prompt and you get a completion but you can also in the prompt say hey review what you've just given me and score yourself and refine that prompt and you can even ask the model to do this multiple times so it's literally scoring itself and improving the prompt and this has been shown to be surprisingly effective especially for models like gpt4 so you can say oh you've written a tweet for me make the Tweet more engaging rate the tweet and and you know improve it and so that's been surprisingly effective technique and it's I performed bass lines in a lot of use cases um and so another technique that we're seeing out there in the wild is contextual compression and so what you see in what you do in a lot of cases is you're calling external data sources let me see you're using Lang chain or Lama index and you're enriching the data in your context window with fresh data that's relevant to answering a question or doing some kind of task but often it's very unstructured the way people do that today one of the ways you can improve the density of information is by doing compression so you can do you can say so let's say a question is you score the first goal in the FIFA World Cup you can call it a bunch of external sources and then compress based on the questions you're asking that information even drop some of those examples and then only shove in the you know relevant ones into the context window and so this gives you often 2x or 3x the amount of information that you can put into the context window and that really improves the quality of the output completion so now you've got more and more accuracy and a little bit more structure but what about latency so one of the techniques that we're seeing now also being adopted is semantic caching so the ideas with normal caches if a prompt if you're just caching prompts in the completions you often don't have a very high cache hit rate because one character difference and suddenly the cache is going to get missed but you can use a vector DB to also do cache hits so you cache it based on a distance like a similarity distance the problem is that often or in a lot of cases you don't really have a hit right if it's if a prompts are sometimes slightly off maybe the hit isn't a true hit so you need some kind of evaluation function so what folks are doing is they're using an llm again using LM for everything but using LM to evaluate the output response and have a judge whether it was really a cache hit and that that works really well in the case where the completion that you're returning is an expensive completion so in many cases you you're calling an LM you know three four five six times especially if you're using chain of field reasoning or any kind of decomposition just it's expensive to compute those completions and so um even if you have to call and now aim to validate cache hit it's still cheaper than having to recompute everything this technique also works really well for Tool selection we this set of options so if you're using like to like an API that you're going to use or like wool from alpha or something um then if you have a high confidence in a single tool and no confidence in any other tools you don't even need to call now and you can just use a distance metric to evaluate the cachet so this is a very good technique to reduce latencies over time above and beyond just the normal cache and then finally I think one of the interesting things that I've realized from the paper in um you know Sparks of a GI is that LMS like gpd4 are extremely good at pi and prompt injection detection 3.5 is also is also good but these LMS are effective at identifying um leakage of information outperforming even the Baseline purposeful tools at this task and also prompt injections and in fact a little bit later today we're going to be playing one of those games where we can you can actually try this out um but so yeah if you're building on albums today and you're trying to make your systems more reliable faster and improve the ux and preach out dude thank you very much Mr Willem this is not the last that we're going to see of you today I know so we're going to play that prompt injection competition we got some headphones to give away a little later and we're gonna be doing it man thank you so much for this talk