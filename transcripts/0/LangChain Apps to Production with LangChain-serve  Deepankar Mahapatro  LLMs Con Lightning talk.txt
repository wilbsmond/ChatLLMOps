hi everyone this is the tanker I'm here to talk about clientele Stuff how to take your language in apps to production unfortunately I couldn't be online during the call and had to send across the recording if you have any questions feel free to ask them in the mL of slack works please I'll grind lots and then as soon as I can I'll try to rush through the slides as much as possible since I only have 10 minutes a brief introduction about myself my name is the bunker I work as an engineering manager at and I'm based in Bangalore in India at Gina among other things we develop Frameworks for multi-modern AI applications in open source I have shared my socials here feel free to reach out in case you want to collaborate all right so what is a LinkedIn serve the goal behind Langston serve is to take users local vaccine based apps to the cloud without sacrificing the ease of development that Langston already provides so language basically uses Gina to achieve this so to the members in the audience who are not aware of what Gina is it is an mlops framework to build deploy and manage multimodal applications is available on Pi Pi you can install it using pip install links itself all right so and this quick talk let's try to understand how to use LinkedIn serve or using a couple of simple examples uh the first example here is deploying a custom agent uh in just four simple steps let's take an old example from the LinkedIn talks in this example uses the Search tool using the search EPA wrapper then uses this tool to Define an llm chain which is basically used to create the zero shot agent and finally we run agent exit 2. run on a particular question this can be run on your local to get the Chain of Thought and output all right so let's see what changes are required in case you want to deploy this using the language itself and so the first step here is to define or refactor your code into and Define a function so in this case we Define a function called ask then add type hints to this function so here we've added a the input of type string in the return type of string then we add the return value so we just basically do agent executor.run on that input and finally return that at last we basically import the serving decorator from SSR this is decorated that the link chains are python module provides so you add that import that and add that to the ask function that we just defined so this is just to show that we can also run the ask function as is like before on your look all right let's go to the step tube so the step two here is to add requirements.txt that includes all your requirements for that function so in this case open Ai and Google search results are enough so whatever your dependencies are you just add them to our requirements.txt in the same directory and save it okay now that the code refactoring and requirements dot txt are done let's run this uh let's deploy this app locally so the command to do that would be LC serve deploy local and your application name so in this case app.py was a fine name so we just basically passed the module name called app so uh that's for that's what running this will expose uh rest API on your local on the port 8080 uh now you can talk to it using the curl command so here notice that we're basically sending a call request to localhost port 8080 and the function name that we had added called ask has been added as an endpoint on that DPA so you can send a call request here if you if you observe the data the input schema in this case the input of the field input was the one of the arguments under the ask function and this schema also accepts all the environment variables all these environment variables are basically needed so that the function can run so in this case these are the tokens provided by open Ai and subpr all right so once we know that okay the function is all these the function is exposed as a rest API on your local and actually we can interact with it now let's go to the next step and deploy it on the AI cloud so the command here the only change here is uh from local we've shifted to jcloud which is basically ginai Cloud so you basically run LC server deploy jcloud app and that would give us an end point uh this endpoint is a serverless trustee pay endpoint uh with TLS Source on Genie iCloud using this endpoint you can basically send this code to the same call request to validate that all your requests are passing successfully you also get uh swaggered docs using this and also the open EPS cap specs which can be used and as another agents if you want go to a slightly more complex example so in this example we'll try to enable human in the loop on our server which is deployed on Gene III Cloud this is enabled using websocket streaming so let's let's go through this so first thing like before we decorate our functions so we decorate we Define a function called hitl this exception input called question and the output type hint is a gain of type strand we add the return values which is basically again the agent chain not run change here the difference here is basically to define a streaming Handler that streaming Handler is responsible to send the websocket responses back to the user so you get the streaming Handler and I'll add them as a callback manager to whatever llm function you've defined this passing this would be enough to sign all the llm output via this callback manager back to the user via this we also enable human in the loop so whenever there is uh an input required we basically intercept the LC server will basically intercept them and send a response to the user and wait for a wait for users input to come back so that it can proceed with the next steps finally we basically add import and add the serving decorator remember to pass a websocket is true in this case all right so let's keep the local deployment and go to the United Cloud directly so we basically do the same again as we serve deploy jcloud and we pass hitl which are which was a file name and uh doing this will give you a websocket endpoint and you can you notice that this is what WSS which is basically a websocket endpoint in this case other than demoing using curl so we I have written a very simple python client uh so first it connects to the endpoint and sends a Json which has the question and the envs like before uh then it waits for stream responses uh back from the server uh when and prints them to the user's console it intercepts uh the there's a particular format that it intercepts which is expected whenever user input is designed that that format is defined here and whenever that is there we basically ask an input ask for an input to the user and whatever input record will send it back to the server and this is how the human in the loop is interpreted in let's say so yeah so that was a very simple example of of enabling streaming and human in the loop but again this can be expanded to any any complicated cases all right so what's what's coming next so we want to host uh we want to enable hosting streamlined apps on the cloud for these uh blanket Maps so that user will also get an UI for an application then the complete Journey would be available for any user we also want to add authorization for the API endpoints which will uh which will validate the request from a valid users rather than allowing anyone access and we want to add more examples to our talks all right so that's that's the our time I had thank you so much for tuning in uh if you find the if you found this uh useful please let us know if you have any feedback any feature requests uh join us on slack or create any GitHub issues yeah thank you [Music]