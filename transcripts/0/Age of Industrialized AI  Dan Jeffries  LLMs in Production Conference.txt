we have another friend that is coming on where we're gonna call Dan to the stage and in the meantime Dan while I'm bringing you up here on to the stage [Music] um I'm gonna introduce you and so he Dan Jeffries is a good friend of mine he is the uh formerly of stability AI now he heads the AI infrastructure Alliance and or AKA Aya dude it's great to have you here I'm so excited every time I talk to you it's just like my mind is blown the vision that you have for where we're going with this is amazing uh and if that is not too set a very high bar for your presentation I don't know what will so we're running a little behind on time I'm just gonna let you have it and then I'll jump in with some q a afterwards all right sounds perfect man thanks so much all right so I did a variation like on uh uh the concept I've had in the past the age of industrialized uh some of you may have read the article uh so I just kind of used the framework here um but if the if you saw kind of a talk on this in the past this one's pretty different I've adapted it pretty strongly to L Labs we're going to basically talk about where all the stuff's kind of what might look like in 10 years and then we're just going to go backwards and say like how do we get there I think there's just a ton of problems that people aren't solving yet and I wanna I want folks to start thinking about some of those problems so that we get to say kind of this magical future so you know think about this it's 20 33 you're a top-notch concept artist in London and you're building a triple a game with 100 other people and the game looks incredible it's powered by Unreal Engine 9 it's capable of photorealistic Graphics in real time uh and it's got near perfect physics simulation uh you know the chips inside of it were designed with the AI ten years ago I would have taken a team of you know fifteen hundred to five thousand people to make this giant game it's super open-ended and you can talk to any of the characters and they can stay on plot uh and you just you wouldn't be able to do it but now you can do it with 100 people and but that doesn't mean there's less work or less games it means more a lot more we used to get 10 AAA games a year and now we get a thousand the powering a lot of this stuff is really what I'm calling large thinking models so these are the descendants of llms and they've stormed in every aspect of software at every level and call them LTM so these large thinking models they're general purpose next-gen reasoning and logic engines with State massive memory grounding and tether to tethers to millions of outside programs and sources of Truth and these ltms are orchestrating the workflow of tools and other models every stage of game development like a puppet master as lead artist you've already created a number of sketches with Unique Style cross between photorealism oil painting some of the Aesthetics from the Golden Age of anime sci-fi you fed those style through a rapid fine tuner and now the AI model can understand and Rapid crack those assets art is definitely not dead it's just become a collaborative dance with artificial intelligence you can move much faster now you can quickly paint an under sketch of a battle robot and for the game and a bunch of variants pop out that you were dreaming about last night you feed it into the artist's workflow and you talk to it and you tell the LTM you want to see 100 iterations in 20 different poses and behind the scenes it's kicking off a complex 30 stage pipeline that you never see you never need to see it it does all that orchestration invisibly two seconds later you get a high-rise iteration it's ready the 27th one looks promising you snatch it out of the workflow you open it in concept painter you quickly add some flourishes a little bit of a different face mask uh you know the generative engine didn't quite get it right maybe a new wrist rocket you erase the shoulder Cannon or place it with an energy rifle you feed it right back into the engine if fix a few problems like you said it's now good to go pops out another pipeline transformed but you know Guided by the LTM which is automatic 3D transformation rigging all this good stuff in the background it pops out into the 3D artist workflow on the other side of the world that artist's working in Chiang Mai Thailand totally distributed team has to fix you know the artist has to fix a few mangled fingers and some armor bits that didn't quite work right it's ready to go he kicks it off to the story writer who's working out of Poland that seeing that new character gets her inspired she knocks out a story outline in many story language feeds it to a story iterator LTM and it generates 50 completed versions of a story in a few seconds she starts reading one of them is really good but it needs some work you know one of the characters been working right a little bit she weaves in a love story fixes some of the middle feeds it back to the engine new draft coming back new drafts come back it reads so well she fires it off to the animators in New York City so welcome to the age of industrialize AI but how did we get here how do we get to intelligent agents embedded in everything Supply chains economics art uh creation Etc everywhere there's nothing that will not benefit from more intelligence nobody's sitting around saying I wish that my supply chain were Dumber so let's roll back in time and focus on today's llms and how they've morphed into ltms and it really all started with as Engineers across the world are working hard to solve the limitations of weaknesses we're kind of coming out of the labs you have Engineers looking at these problems a lot of folks think you can solve these problems in isolation but you can't you know when you put refrigerator you have to put refrigerators into the real world before you realize that sometimes the the gas leaks and they blow up in the early days and so then you fix that problem you know opening I spent a ton of time in the early days trying to worry about political disinformation in the lab and GPT has been used for that about zero percent of the time but they didn't anticipate you know spam or people writing a ton of marketing you know crap emails with it and uh that's because that we can only figure these things out when it gets into the real world but you know what are the alarms good for how do we deal with them today so first it's it's important to kind of understand that mlms are not really knowledge engines a lot of folks are like well let me let me look something up with that um it's it's kind of like a database it's not a database it is a rudimentary reasoning engine that's its key strength and that's really what it's going to be over the time Horizon as we embed intelligence at every stage of the software life cycle the real strength is acting as that logic engine inside of apps you know that lets us do things we'd never be able to do in the past right you know we could go out there and I can have you know an intelligent research if they can go take a bunch of unstructured data which is 80 to 90 percent of the world go look and Discord and slack and on the web and read blog articles and go extract a bunch of useful information and summarize it for me and then put that into a spreadsheet with all the authors names and go to their LinkedIn profile and dump it in there you know that would take you know I don't know go if I said go look at if I hired a researcher and say go look at you know a podcast and listen you know to 2000 episodes and find me every instance where someone you know talked about artificial intelligence uh you know that would be a huge task and now we're going to have these little reasoning engines that can go do that pull that information out summarize it merge it together with other information these are kinds of things we couldn't we can't do we just can't do with current technology so it really opens up these exciting possibilities but the thing is these things are just really not great reasoning engines yet right they hallucinate they make things up they choose the wrong answer they make mistakes of logic and execution and that's because these systems are massive and open-ended it's literally impossible to test every possible way people will think to use or abuse them in the real world if we're making a login system for a website there are only so many ways that it can go wrong security League sub Library error huge amount of things can go wrong with traditional coding but it pales in comparison to what could go wrong with the production llm prom tax blow past the guard rails hallucinations there's whole websites based on prompt injections now the the llm is used to write malware script complex attacks tricking the L M into revealing internal information which is social engineering with a Twist right an old school acting technique that you used to use on people that are using on the llf unsafe outputs like advising dangerous drug interactions right uh picking the wrong next steps in a chain of commands if I have 30 steps it makes the wrong decision on step five do the rest of them fail how do I even know so the list goes on and on we can start to think of these collectively as bugs in the same way that we think of traditional software bugs and as these systems agent types are seeing things like you know Moto GPT and these kinds of things all over Twitter uh doing some really cool things people are going to become a lot less tolerant of bugs right now if you see if you've played with any of these things you see the agents kind of go off the rails uh maybe 15 to 30 of the time that's an unacceptable error rate they're going to get better over the coming years and they're going to get better even faster but if you prompt an l11 it gives you a messed up answer uh you can just prompt it again and uh but if it's an autonomous agent that has the ability to do a lot of steps without a human in a loop and even if we got the error rate down to say 0.1 percent you might think that's perfect no problem except you know if it auto writes an email to someone that offends a big customer or you know reveals some internal information or just says something tremendously stupid and you it takes that 10 million dollar deal with that customer even though that's in the 0.1 percent error rate that error rate is now way out of proportion right so as these things Asian ties I think people are going to become a lot less tolerant if you're just talking to the thing it gives you a stupid answer you can say well I need to re reprompt it again to get get closer to the answer that I want right but now you're going to have to do a lot more things to kind of get this thing to work effectively and I think that could slow down some of the progress I also think it's an opportunity that a lot of folks are missing so nobody is really fixing these bugs fast enough for it all and to fix them we're going to need a whole new suite of tools and new strategies to get there we've got to break down the problem so that we know where to start in projects in companies that are looking to make a difference can start to look at it like this where can the LM break down in production and there's a few major errors that I'd classify as during the training or fine-tuning and in the middle so in the middle has a bunch of subcategories at the point of the prompt in between the model and you in between the model and other models tools for software at the output AKA after the llm response and or during the workflow AKA a dag breakdown so we're going to look at each of these in turn try to understand the implications and the potential ways to fix these but first in the side there's uh if you're hoping for a magical solution a lot of people are well we're just going to train gbt5 and you know then coherent anthropic and a21 and everyone else is going to come out uh with new versions of the models and they're just going to be better and smarter they're going to fix everything uh just a big spoiler alert they absolutely will not better models will always help we get new emerging properties uh more consistent reasoning engines that means less failures of logic but they're not getting us all the way to ltms because Mo complexity no problems to borrow from Biggie Smalls it will be a game of whack-a-mole it's a bit like asimov's uh you know iRobot where you have a robot psychologists uh you know constantly finding new ways for them to break down right if you think about the other kind of probability machines that we have in the universe human beings you know uh those they often make wrong decisions and break or make more decisions and so we would expect the machines to do equally uh badly at times so prompt Point failures could be the user sign or they could be on the side of the model right the model didn't understand it's got a screwy training data you know it just wasn't fine-tuned well enough or it could be the user asking the question in a weird way um in it's it's that's when you ask it for something that doesn't understand the question there were things that understands the question or makes up an answer or gives you a Bogan answer or if we're talking about chain of reasoning maybe it makes a mistake about you know which tip is next so we're going to need guard rails on the systems and to better understand when an llm fails and some of these things they can take a few forms and that's you could constrain the range of the prompts you can pad the prompts you can add a series of rules or heuristics to interpret the input and output Diego talked a little bit about that in the last uh you know example I'll talk about it a bit more and you can create other Watcher llms or models that watch for specific kinds of characters or attacks before or after the prompt here's a an example of chat TBT they're using these sort of compressed set of uh questions with Emojis or whatever that gets the llm to reveal how to hotwire a car because it thinks it's role-playing in the story so how do you fix that well take something like mid Journey we don't necessarily know that majority has an olm behind it um because it's closed architecture but a similar approach to what I'm talking about was used here in terms of padding the prompts we know that mid Journey 4 did a ton of pad prompting behind the scenes we and we know that because people could type in basically a one word or a few words and get kind of a consistent output they've kind of dialed back on that mid Journey five but it's still there and so they stacked a lot of invisible keywords overall prompt engineering abstraction or prompt padding kind of works uh if you kind of limit the number of ways that a person can ask a prompt drop downs things like that the problem is it can have kind of undesired effects so you know if you put in uh you know prompt stacking to fix maybe a diversity challenge you know you want to get that you type in CEO you want to make sure you're getting uh you know women and uh Asian folks black folks white folks et cetera you want a range of folks non-binary folks whatever it is um if if we prompt to fix that you may end up with uh you know if I ask for Mario as a real person I don't get a male Italian plumber right so at best this is a heuristic kind of rule-based hack and it's only going to get us so far you know we had um you know you could think of these as kind of like a Malwarebytes or filtering based on signatures or looking for prompt injections other things that we want to alter filter at the point of prompting But ultimately they're not good enough and that's because you have to remember the Sutton principle from the bitter lesson and that's that the biggest lesson that can be read from so many years of AI research is that General methods highlight you know italics line General methods that leverage computation are ultimately more effective and by a large margin and he like he highlights two things scale or scale and search right or uh learning excuse me and search learning statistical Methods at scale so machine learning essentially and a lot of people misinterpreted this paper they thought well you know humans are irrelevant you just scale up the computers that humans go bur it's not what he was saying he's saying that you know for instance you know deep blue had a bunch of kind of heuristics built in there for you know Levering Pawn controls right or the controlling the center of the board stockfish which came along after that was a great chess playing program it had Alpha Beta search so it was scaling search but it also had a bunch of like human-based domain knowledge baked in right that said essentially go ahead and control the center of the board Etc so you say don't waste any time with that those kind of small domain knowledge based algorithms go for more generalized ones like RL or um you know statistical learning any of these kinds of things they're always going to beat it in the long run so that's why Alpha goes zero which basically just learned from Morrell and playing itself and didn't have any domain knowledge baked into it other than the basic rules of the game the smashed stockfish over time so we're going to want to get PS the heuristics into more advanced generalized systems they can deal with this I suspect we're gonna have lots of Watcher models you know kind of dealing with these things it's going to be similar to the same way that we had kind of a Towering Inferno of rules in the early days of uh in the early days of spam filters they were maybe 70 accurate it was a natural inclination of the program to say hey there's an email that says dear friend I can write a rule for that or fix it um but over time it starts to break down and then you get a general purpose system like a Beijing filter classifies as the ham and the spam all of a sudden it's 99 accurate so we're gonna have to make that transition but in the short term we might have nothing but heuristics until we kind of figure these things out for a period of time we're going to have these sort of basic rule you know basic towering for no rules for a period of time to kind of help us keep these things on the rail the other place where this the model mostly fails is kind of in the middle and so I think there's a massive opportunity for AI middleware and that's because that's where it's most of the time it's going to fail as we integrate these llms into the in with other tools and make them more autonomous they are going to fail in strange and spectacular ways that traditional software doesn't fail you know you're chaining the other commands and it pick excuse me picks the wrong order throw the wrong step goes into a text generating death spiral there's a million of these we're going to email aware of that checks the input and output at every step that's the key you know and what's that going to look like again it's probably going to be a hybrid mix of traditional code smaller watch Watcher models that can understand and interpret the results and check them to see if they make sense and that kind of ensembling is going to help us make better decisions more often we are not yet used to detecting the kinds of errors that llms deliver it's not a clear coat it's not a 404 error it might seem like a perfectly normal uh formatted correctly formatted answer that's really an error of logic and so how do we know that it's neurologic how do we know the third step shouldn't be to you know push code or ask another question it should have been you know go do an additional search to clarify information there's really nothing that exists to detect these things at an advanced level uh to pinpoint them properly consistently huge opportunity for folks out there to be in the middle to take uh inspiration Solutions of the fast like um API management layers and those kinds of things other places it can fail are basically training failures and you know the model itself might be the problem problem wasn't trained well enough doesn't have the right capabilities hit the limits of its architecture poorly aligned might not have enough emerging capabilities that can be harnessed and most of these get better with better training and better faster fine tunes but it's really just not fast enough now the speed to fixing any of these problems is way too slow at the moment fine tuning is slow it's scattered it's more art than science there are many many many bottlenecks to speeding up these fixes like the need for human labeling or scoring of data sets and uh you know we're starting to get models now at gpd4 and blip 2 they can kind of label things Auto magically but in general uh when you look at something like Foundation models the data is almost universally poorly labeled right it's actually amazing that it works at all and there's no way you could physically label all these things you can find you can label a small data set curated asset but not these massive models so you know what are the fixes going to look like here's an example from The Lion data set uh where the label is diabetes the quiet Scourge you know that's a clever line probably from a blog post but it really has nothing to do with what's in the image stethoscope you know fruit Etc and it's not going to teach the model anything uh if in fact it's going to teach it a wrong idea so llms are about to bootstrap that process and make creating that synthetic data and or labeling largely obsolete we're going to have not only a ton of convincing synthetic data but more well-labeled data at scale and that's the key at scale if the lion data set uh and it's got 20 perfect labels 20 decent labels 30 mediocre labels 30 totally wrong labels that's a huge amount of room for improvement so multimodal LMS diffusion models and the like scene to learn like I said despite themselves if the llms are then able to label that data you know more accurately the 70 80 90 that's a massive Leap Forward just for the foundation models and then it's also going to help speed up the fine tuning so when it comes to llm's labeling RL uh HF examples we're going to need models that can guess 90 of the time what the human preference is uh and then surface only a small subset of those labels if I have to have a data set and a human scoring every time that there's a bug in one of these LMS there's really no chance that this is ever going to be able to scale so we've got to speed up the process some of the other things you can do is there's going to be more grounding so you have Goldberg kind of noticed recently that llm seemed to do really good with coding and he said you know it's kind of because it's a form of grounding and that's because you have the the comments which go with the code so if human you know natural language and the code itself so that's anchoring that knowledge to stuff right and and I expect natural language overlays and labels for just about everything in the near future right so if the LMS can ground better by reading that text storing in a vector DB whatever it is they're good we're going to start having natural labels for everything so that llms can kind of consume this and other forms of grounding obviously are connecting to external data sources you know Wolfram Alpha uh which was for many years you know doing symbolic logic and but I've taken tons to integrated years ago and now it's 10 to 20 lines of natural language rappers around the apicals that's amazing so we're going to start to get these overlaps of symbolic logic uh and external sources to get things like web brain where it's trained on huge uh Corpus of Wikipedia and it can go out and look at Wikipedia and inject that into the prompt as it's generating uh really useful yeah so let's you know expect more clever grounding hacks to come they're uh they're going to be there in the near future and we're also going to start to see Violet teams which are basically a form of red team and self-reflecting llms so these are L M's acting as an engine to fix itself uh spitting out the synthetic data testing it labeling it and then checking it by people quickly so let's say someone catches a model advising people to commit suicide right so the Violet team uh which is a security team variant kicks off a series of engineered prompts to explain why it's never a good idea to kill yourself right so it takes the original prompt and then it says Hey explain why you don't it prompts it by saying hey explain why it's a bad idea to kill yourself and it uses the LM again pairs that original question should I kill myself with the the the modified response pairs that together and then you say to the llm okay give me the original question give me 50 or 100 or a thousand variations on that question and give me a thousand variations on the answer now you can Surface a small subset of those to people to check you could score them with another model and now your data sets complete he runs you know a rapid lore up fine-teen or something like that you're ready to go so we're really going to need a training Revolution too it's just got to be a lot less art more science got to be a lot faster training right now it's horrible it's Emma lopsy it's slow it's ugly it's low level um if again if companies are getting together a data set kick off a long-running job and then run tests and fix every single problem it's just not going to work and you're also going to keep adding you know adding kinds of things we're going to have these things interacting with hundreds or millions of people uh and suit billions it's going to need an order of magnitude faster way to train out the bugs uh one one thing I'm sort of seeing is what I'm calling sort of model patching and so that's adapters things like Laura's they're really just the first step it's easier to train they're smaller they don't change the weights of the original model much or at all or they add parameters to the models there's some challenges with it they could just kind of scale up uh the amount of memory needed the more tasks that you add et cetera uh but the techniques are developing quick you got adapter H AKP tuning you got gmatics you got Laura's I expect to see many more of these I expect to see lots of models with hundreds of thousands of patches hundreds of thousands of adapters kind of working together that make them Stronger Faster more grounded less vulnerable and more secure and stable uh it might need ways to even compress those things together so you might have 20 adapters and then you average them together or whatever to make them smaller to reduce the amount of things that you're going to be using closed Source systems you can't do some of this stuff on unfortunately because it's just an API call and so they're going to have to adapt to allow this kind of extension of the models because they can't be retraining you know uh gbt gbt4 or whatever every time they need to fix something it's just not going to work so we're also really going to need kind of updates and continual learning so adapters can struggle in complex tasks and there's also likely a limit to how many of these we can chain together before a performance starts to suffer or we start just adding somebody fast that the memory you know becomes you know uh just impossible to deal with uh there are it's already challenging enough to deal with in these you know 100 billion trillion parameter models so really going to need advancements in continual learning we have recent papers like memory efficient continual learning from Amazon researchers that adds new tasks and updates the model without scaling memory requirements uh as the parameters increase for each new adapter task so that's really exciting we're going to need more more breakthroughs continue learning is going to be the real answer we need to make these models lifelong Learners and if you look at the anthropic deck for their big you know 300 million dollar raise or whatever they're looking at they basically think that you know within a few years uh most of the gigantic Foundation models are going to be trained and there's not going to be any chance to sort of catch up and that essentially we're gonna we're just continually sort of train those models over time and make them smarter I don't know if I fully agree but it isn't but it does but there are is going to be kind of big moats created by these models especially as they crack continual learning and if I can just keep adding tasks and new data to these systems and and they get smarter and smarter over time without catastrophic forgetting yeah you know then I have a significant mode and that's going to be very interesting that gets us a lot closer to the LTM concept that we talked about in the early part of the presentation so look this is the end uh we're entering this age of industrialized Ai and ai's out of the labs moving into software applications when engineers get their hands on it they think about it in a different way and that's exciting when I saw stuff in the stable diffusion Community where they just started jamming together blending lots of models many people thought that wouldn't work I saw one researcher Jam together 200 models and most people thought the models would collapse and they didn't they made a better model so that's exciting you start to see these techniques the Laura for instance was adapted from llms two diffusion models to the point that the author of the paper was then on the subreddit talking about I never even thought it could be used for this so can how can I adapt the next version of that to fix these kinds of things so that's the kind of thing that Engineers do they take things from a lot of different places they Jam them together they learn from the past they think differently and so we're going to see a lot of the mitigations for these problems a lot of people are worried we're not going to come up with the answer to these things that's ridiculous we are that's what Engineers do that's what engineering is all about it's fixing problems in the real world so we're already seeing the seeds developed in Solutions to the most well-known problems with all of them and they're going to come fast and furious over the next few months and years to make these trusted for Enterprise environments and to make them more explainable to make them more controllable to make them better understood to make them stay on the rails more often we're gonna have smarter more capable more grounded more factual models that are safer and more steerable and it's not just going to be one company doing there's going to be these techniques applied to all the kind of models and the techniques that are out there they're going to be rapidly adapted back Upstream to the foundation model so anything your teams can do to push this forward we don't need another company wrapping something around GPT there's already a million of those that's cool if you want to do that but if you really want to push forward to the kind of age of industrialized AI you got to get in there in the middle you got to get in there in the fixing of these things you got to get in there the engineering side of the house and that's the thing that ends up powering us to the ubiquitous ambient intelligent age much faster that's it dude that is so awesome man ah I love every time I talk to you because I get so in and that is I got so many questions for you but because of my foreign [Music]