all right well thanks everyone for having me um I'm gonna be talking a little bit today about what we've been seeing at human loop with people building uh LM applications I'll give a little bit of introduction about um you know who what human loop is and and sort of why we why we have an insight into this question and then I'll talk a little bit about some of the more successful llm applications that have been built and how they've done that what makes them sort of particularly good and what makes them defensible as well chat through some of the challenges to building llms that we've seen and then talk a little bit at the end about defensibility and how you can think about making your llm applications uh something that you can defend over time so that's that's the plan um if I could have the next slide please tell them thank you um so yeah I'm sort of at a high level what is human Loop we build developer tools to help you build useful applications with llms focused around prototyping and then understanding how well your apps are working in production and being able to use that evaluation data to improve them over time and so because of that we've seen a lot of people go on this journey from idea to deployed application and we started to see emerging best practices and what is and isn't working for people and so that's what I'm going to try and use to inform the talk today and I'm going to try and pick up on a couple of specific instances of applications that have been very successful and kind of talk through how they've managed to manage to achieve that success and the components of an llm application um and then kind of build into that sort of best practices that we're seeing and a little bit of a discussion about how you can improve your own applications and so if I may have the next slide please and the first kind of app I want to talk about today is GitHub co-pilot because I think this is by far and away the most successful and most used maybe other than chat GPT um sort of large language model application that's got Mass adoption right it's uh it's got over a million users I think now and it's growing pretty fast and it's been able to win over a historically quite challenging audience right I think developers are particularly picky about the tools they use especially to have an AI Pro programmer and so what I want to dig into is the anatomy of GitHub co-pilot as an application how is it built and then tell a little bit about what makes it so good because in some senses it's built on the same base models that we all have access to right everyone has access now to the gpt3 model Suite which is what it's kind of based on um and yet somehow they've been able to make an application that you know I think it's significantly better than a lot of the competitors out there and it's not just distribution that's allowed them to do this so I want to I want to chat through that so that's that's the first thing I want to talk about if I may have the next slide um sorry back one okay can you go forward to I think we're missing a slide here we go um and so when it's actually yeah sorry it was the previous slide thank you so I'm not having control visas yeah so the first thing I want to talk about before I dive into the specific instance of GitHub copilot as an app is the kind of components of a large language model application and I think of llm apps as being composed of what I think it was LM blocks so you might have many of these repeated in sequence or put together by an agent but at its core each piece has the same three components which is some kind of base model um so that could be gpt3 it could be an open source model like llama but this is a pre-trained language model that is generic and can be used for many different tasks there's a prompt template which is the structure of the sort of message so this is an instruction to the model with maybe some extra gaps where you're going to have either user-defined input or data that's going to be fed in and then some data selection strategy for how you're going to populate those at test time and so if we go if we look at a specific instance of this in the case of GitHub co-pilot if I could have the next slide then GitHub co-pilot has as its base model right that first component is a 12 billion parameter GPT model um so it's a code trained pre-trained model but it's significantly smaller than cgpt3 which was maybe 10x in size and the reason for that is in this instance they wanted to have something custom fine-tuned for code but also that was small enough to have low latency so that when you're trying to get suggestions from it you're not having to wait for a long time and so that's a critical component right they've chosen carefully in appropriate base model for this use case where latency concerns guide what's possible you would probably get better code generated from a larger model but at the expense of having um you know much higher latency and then they have an interesting and quite complicated strategy for figuring out what to feed into a prompt template so what they're doing is they're looking at where your cursor is looking at neighboring files um that you've recently touched and trying to find based on the code that's just behind your cursor the compare the like edit distance or Jack art similarity of different parts of code and other files and then use that to extract those sections and put them into a prompt that they then feed to this 12 billion parameter model and after that they then also have a system set up for very careful evaluation capture so the way they do this is they look at the acceptance rate of your suggested code but they don't look at just did you accept a suggestion from GitHub co-pilot they also look at whether that code stays in the code base whether it stays there after a few minutes whether it stays there after a few seconds and so they're able to understand whether the suggestion was good in production across millions of users and these components taken together start to give them both an excellent application and also something that becomes defensible over time because they can run this regular Loop of basically putting something into the hands of millions of developers watching them use it and because they have excellent evaluation data they're able to then work out what worked well and retrain and fine-tune on a regular basis and so the model can get better and better at this task over time and so I think copilot has the anatomy of what I think it was like a Proto you know a really excellent example of what you can do here where they've made a very careful decision about an appropriate base model that makes sense for their use case they've iterated on and done a lot of experimentation around what is the right strategy for what I should include in my prompt what data should I be pulling where should be coming from they're deeply integrating with a particular user's code base so it's not just generic but actually sort of is able to be adapted to that user and then they're they're using evaluation them to improve models over time and so if we kind of Step a slide forwards uh if we could step up thank you I think that this sort of like highlights the you know if we think about those three pieces that make up an llm app block or make up GitHub co-pilot we need to find a way to make each of these excellent so we have to have like an appropriate base model that's the right size that's been fine-tuned for the task or that has good sort of performance we need a way to get the prompt engineering to work well and I think sort of on this side I've tried to put together some of the challenges that you face when you're doing that and then finally you need a way to measure performance that you can improve things over time and so what we've seen when people come to do this in production um is that there's kind of a few challenges that come up again and again and the first is that prompt engineering is still a bit of an art so small changes and prompt templates can have surprisingly big outcomes on performance um and that means that it has to be very very iterative you have to have a fast feedback loop and you need to be able to experiment a lot you sort of can get a first version very very quickly but then getting towards something that's truly excellent takes time another problem that we see pretty consistently and I think like others will you know have spoken about this as well is the need to try and make llms more factual and finding ways to overcome their you know the fact that they've hallucinate and make things up um evaluation is another one that's particularly challenging for llm apps and I think this is different for llms than most traditional software because we're beginning to use these things for applications that are much more subjective um than we might have in the past so if you're generating marketing copy or you're sending an email then there isn't a ground truth answer that you can just look at and say okay that's the correct thing you need some way to measure performance based off what your users think is the right answer um latency and cost you know is something that we have to figure out ways to choose from the appropriate app and then I think the one that we'll talk about a little bit when we come to defensibility is if you've just got GPT plus a simple prompt then it's a very thin Mo and you need some way to overcome that um and so I just stepped through these kind of one by one uh so we go to the next slide please just wanted to explain a little bit more about like why prompt engineering is so important and the kinds of small changes that you know make a big difference so no I think last year one of the there was a paper that captured a lot of attention and that has become kind of very commonly known amongst the community now showing that Chain of Thought prompting had a huge impact on the performance of question answering models and other reasoning models so if you simply ask a model not just to answer a question but to provide a reasoning Trace suddenly you get not just a little bit better performance but significantly better like many many accuracy points uh better than you would get just from a base question prompt and you know this is now well known amongst the community but the surprising thing is that there are still many more tricks like this out there to be discovered people are constantly finding new ways whether it's the format you know asking a model to be formatted in certain ways to click on a particular role there's a lot of changes or tweaks that you can make that have a surprisingly large impact in performance and so one of the things that we've seen when it comes to trying to find ways to build defensible apps is having a very fast way to iterate on your prompt templates get feedback from that and tweak them and change them is actually critical to getting good performance um the next thing I wanted to chat about when it comes to prompt engineering as well is if I could have the next slide is that something that we've seen people sort of start to have patterns around but has historically been challenging is finding ways to get factual information into the into large language models and so one thing that we think is going to be super critical here and again we're seeing examples of this in practice is giving llms access to tools and there's been a few other talks touching on this today but a a common emerging pattern for this is to sort of take the documents that you want to give your model access to split them into pieces embed them with a large language model and then make those embeddings available uh two-year model when it's doing generations and what we've been looking at is sort of finding ways to make this accessible in an interactive environment so that you can experiment with that much the same way that you would with your prompt templates themselves and uh if I could have the next slide oh yeah actually sorry can you go back one second for me to the the previous slide um and yeah so this is this is a common pattern and it's sort of again if we think about the pieces of say the GitHub co-pilot app which is one of the more successful ones this is another area where they've clearly spent a lot of time thinking about the right strategy for doing retrieval so there's different methods for doing retrieval into your prompt template to make it factual and they have a big impact on performance question from the chat what technique can be used to feed back the evaluation data into the model as with copilot potentially a form of rohf or something else okay great so that takes us that's that's a great question and if you can take me to the next slide Lily I'll uh I'll expand on this a little bit so the third component of what I think makes a really good llm app once you've figured out good prompt engineering you've maybe found an appropriate base model is having a way both to measure feedback and understand how well it's doing and then use that feedback for continuous Improvement and there's basically three things that we've seen people use successfully to do this so the first thing that we see people do as a very common workflow is they will use the feedback data they're collecting and you know all of the Best in Class apps now capture end user feedback in some way so I mentioned ham GitHub co-pilots looking at suggested code being accepted at regular intervals 15 seconds 30 seconds two minutes 10 minutes um chat GPT has this thumbs up thumbs down followed by various forms of natural language feedback and in general is a best practice that we're seeing is people capture are three different types of feedback and you know so actions what does the user do after they see my generation in the application issues and votes and those are sort of very common that become a common framework for types of feedback we see people collecting and then once you've collected this feedback the things that we see people do to improve their applications one is look at the cases that are failing form a hypothesis about why and then try to edit and try to do prompt engineering to improve that and so that might be realizing that actually your retrieval step is failing it's not giving the model the right um the right section of the code it might be realizing that actually you need to encourage the model to be less repetitive or you need to tweak it a little bit in some way so we see a very common Loop of people putting something in production filtering the data to see the failure cases inspecting them manually and then trying to do some round of prompt engineering to improve that the second kind of Step Beyond that is when people actually come to fine-tune their models and we see two forms of fine-tuning being used and if I could have the next slide um so the first is actually pretty straightforward supervised fine-tuning so the idea here is that you're just filtering the data set by things that have worked well for other customers or that you have some reason to believe it worked well fine-tuning and then repeating that process and so there's this cycle that we see really commonly which is to generate data from a model whether that's in production so you're running gpt3 or in the case of copilot you're running this 12 billion parameter model in production for some time capturing all of this feedback data filtering down to some subset that's worked well and then fine-tuning the model on that and repeating this process and as you do that you can get better and better performance on your specific subset of tasks um and that's something that we're seeing in production but it's also been demonstrated in the academic literature as well so there was a paper called star that was looking at doing this for reasoning so they took a a set of reasoning tasks used a model to generate Chain of Thought prompts filter it and retrain the model and they're able to to show that models get better at reasoning and there's quite a few instances like that and then the third way of doing this as someone asked about is rohf we see fewer people doing rhf in the wild because it's more complicated to do I can think of a few startups that have done it but actually the gap between doing no fine-tuning and even just supervised fine-tuning is really large you can get significant reductions in cost and latency if you're able to fine-tune a smaller model and you also get an application that's more customized for your specific use case and so when it comes to defensibility we've seen that fine-tuning for performance can actually be a very very significant advantage and one thing I wanted to chat about if I could have the next slide is a common question we get which is when to use prompt engineering versus fine tuning because I think there's some skepticism about the benefits of fine-tuning or it requires a lot of extra work and you can get quite far just by adjusting prompts and so when should you think about should I prompt uh should I sort of do prompt engineering or should I fine-tune and so what I would say is the advantages of prompt engineering and that it's very fast to adjust your prompts there's less work needed ultimately you're not having to host models or figure out how to fine tune yourself or even Munch data into the right formats with the fine tuning apis after experimentation you can get good performance and if what you're trying to do is get factual knowledge into the models that's changing fast then prompt engineering and retrieval is the right way to go so fine tuning I don't think is something you should be doing to try and get factual information into your models but what does what fine-tuning does allow you to do is to get smaller models to have similar performance on your specific use case and to get performance in your tasks that might be better than any other model out there so if you have access to some kind of special data set whether that's private data or you have access to feedback data from running a model in production for some time then fine tuning is a way of really building something that's significantly more differentiated than what anyone else out there can have it can allow you to bring down latency significantly so in the case of copilot we saw that they were using a 12 billion ish parameter model and that's primarily a latency concern but also because you're training smaller models you can get lower cost and then it also opens up the door to doing local deployments or private models in the tone of voice of a particular company so there are significant advantages to fine-tuning even though it might be harder and the journey that we've seen most customers go on is almost everyone starts with prompt engineering they get applications to a certain level of performance and then they sort of start to fine tune later because prompt engineering is so much faster to get to a first a first version of a model and so in terms of you know recommended best practice I would say push prompt engineering as far as you possibly can and then think about how do I fine-tune to optimize performance but don't optimize prematurely may I have the next slide cool Okay so we've spoken a little bit about GitHub co-pilot um we've spoken about the anatomy of an llm at the three parts of a base model template for a prompt structure like a strategy for getting it in and strategy for evaluation and the fact that those things get chained together but the title of this talk was how do you build defensible apps with llms um and so I want to chat a little bit about how you can sort of actually get defensibility and differentiation and obviously fine-tuning that we have hinted at is is part of that before I dive into this I do want to say that I think this has become a hot button topic of conversation amongst people in the sort of Builder Community who are building with llms thinking about and and I guess in investors as well like how do I build applications that are differentiated and they're going to be defensible over time and I think that we should be careful not to over index on this and so that's why I put this quote here before I chatted about it from YC who you know great investor into a lot of startups where they say you should ignore your competitors because you're more likely to die of suicide than homicide and I think we shouldn't lose sight of the fact that for building nlm applications the number one thing we should be thinking about is how do we solve a real user need and do that quickly and successfully but that said defensibility is still something that people will worry about on a longer time Horizon and there are things we can do to make our apps more defensible and so if I may have the next slide um what I wanted to chat about sort of as an example here is so the first thing I'll say when it comes to defensibility is I don't think that large language model applications or companies build on large language app models are fundamentally different to other software businesses right all the things that you would be thinking about if you were trying to make a company defensible as a software business still hold you're still thinking about scale you're still thinking about switching costs Network effects brand Etc the things that you would think about always but maybe there are specific things that matter or that you can do with llms that that you might not be able to do in other circumstances and I think it's it's instructive to consider this example um of considering two different companies that have been quite successful as one of the you know amongst the first applications of llms in production which are Jasper and writer so these are both companies that have um built developed writing assistance from marketers um they both had significant success early amongst the first llm company is to really get to scale but they took really different approaches and they've taken different approaches to becoming defensible companies so Jasper focused on scaling really really quickly they had quite a high marketing span they captured a large fraction of the market and their main approach was to build on the open model so sorry build on closed Source models from openai so primarily initially building on gpt3 but scale very very fast and try and get defensibility that way whereas a writer took a very different approach and actually focused on building with custom fine-tuned models and that allowed them to counter position against openai and also their competitors like Jasper because they were able to promise their customers that they wouldn't store any of their data that they would be able to give them customized tone of voice and that everything could run on their machines which allowed them to access an audience that just wasn't accessible to Jasper possible to some people who are building on open Ai and so I think there's an illustrative lesson to take from all three of these applications co-pilot Jasper and Ryder about things that we can be doing to make llm applications more defensible so in the case of co-pilot Beyond just building an excellent product I think they have this incredible data flywheel built in where they're able to capture feedback data use that feedback data to improve a model and get it better at that specific task so that over time that becomes harder for others to catch up to them in the case of Jasper they really went for some form of bit scaling by getting to a a size and brand awareness very quickly over others they were able to I think establish themselves with a very very hot like large share of the market versus writer who counter positioned against everybody else and was able to offer something to customers that others couldn't without changing their products significantly um and if I could have the next slide please I sort of you know I tried to jot down different things that we've seen in terms of strategies for making llm apps defensible and how they map onto maybe some of the more traditional views about what makes apps fermentable in general and so one of these is you know I think a lot of software applications have this and I think this will be particularly true of large language model applications is having high switching costs from integrating deeply with private knowledge sources so if you are able to get you know private customer information a particular company's knowledge base if you're doing customer service or code if you're indexing someone's code base or something like that then that can be a big Advantage because the base large language models don't know anything that wasn't available on the public web so if you're going to be able to answer questions or deliver a service that requires private information you will have to build a lot of Integrations and with those Integrations come High switching costs and that's a form of defensibility a second form of defensibility that we've discussed is the ability to build a flywheel through feedback capture so this is the kind of GitHub co-pilot example but we see this a fair amount with people fine-tuning through human Loop where you gather a lot of data in production you get feedback on how well it's working you filter fine tune repeat and are able therefore to start getting an application that is our model that is better than what others can train um and it because it's getting better and better over time you also have this sort of data defensibility through this network effect and the final one I've spoken about is a and the final one I've spoken about is um counter positioning so can you find ways to do things that maybe people who are building on other models can't and so writer was a good example privacy and having their own models fine-tuned on customer data they were able to do things that weren't accessible to their competitors and then the final one is obviously like in some sense less theoretically sound it doesn't map onto any of these traditional ones but it uh is just focused on building a really great product that solves a real problem and has distinct ux I think GitHub co-pilot is another example of this where they thought about building something that had fault tolerant ux because large language models we know are not completely reliable and so they thought about how can we provide something really useful to people whilst knowing that it can't get everything right all the time and completion and suggestion with sort of having your code in context works really really well for them um so I think that's my last slide and I will end there I don't know if there are any questions can you just hit the next slide just to make sure that's that's it for me so thank you very much